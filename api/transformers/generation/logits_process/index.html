
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../beam_search/">
      
      
        <link rel="next" href="../stopping_criteria/">
      
      
      <link rel="icon" href="../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>logits_process - MindNLP Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindnlp.transformers.generation.logits_process" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="MindNLP Docs" class="md-header__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindNLP Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              logits_process
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="./" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../../../zh/api/transformers/generation/logits_process/" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../tutorials/quick_start/" class="md-tabs__link">
          
  
    
  
  Tutorials

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../supported_models/" class="md-tabs__link">
        
  
    
  
  Supported Models

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../contribute/" class="md-tabs__link">
        
  
    
  
  How-To Contribute

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../accelerate/" class="md-tabs__link">
          
  
    
  
  API Reference

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../notes/changelog/" class="md-tabs__link">
          
  
    
  
  Notes

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="MindNLP Docs" class="md-nav__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    MindNLP Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/data_preprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocess
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_mirror/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Mirror
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../supported_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Supported Models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How-To Contribute
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../accelerate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/load_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    load_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/BaseMapFunction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BaseMapFunction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transforms
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engine
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Engine
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_1" >
        
          
          <label class="md-nav__link" for="__nav_5_4_1" id="__nav_5_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    train_args
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_1">
            <span class="md-nav__icon md-icon"></span>
            train_args
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seq2seq
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_2" >
        
          
          <label class="md-nav__link" for="__nav_5_4_2" id="__nav_5_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_2">
            <span class="md-nav__icon md-icon"></span>
            trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/default_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    default_func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../peft/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_2" >
        
          
          <label class="md-nav__link" for="__nav_5_7_2" id="__nav_5_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tuners
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_2">
            <span class="md-nav__icon md-icon"></span>
            tuners
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adalora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AdaLoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adaption_prompt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adaption_Prompt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/ia3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lokr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoKr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/prompt_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt tuning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_3" >
        
          
          <label class="md-nav__link" for="__nav_5_7_3" id="__nav_5_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/utils/merge_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    merge_utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/peft_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    peft_model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sentence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_2" checked>
        
          
          <label class="md-nav__link" for="__nav_5_9_2" id="__nav_5_9_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9_2">
            <span class="md-nav__icon md-icon"></span>
            generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/generation/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../beam_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process" class="md-nav__link">
    <span class="md-ellipsis">
      logits_process
    </span>
  </a>
  
    <nav class="md-nav" aria-label="logits_process">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.AlternatingCodebooksLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      AlternatingCodebooksLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.BarkEosPrioritizerLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      BarkEosPrioritizerLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.ClassifierFreeGuidanceLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      ClassifierFreeGuidanceLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.EncoderNoRepeatNGramLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      EncoderNoRepeatNGramLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.EncoderRepetitionPenaltyLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      EncoderRepetitionPenaltyLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.EpsilonLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      EpsilonLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.EtaLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      EtaLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.ExponentialDecayLengthPenalty" class="md-nav__link">
    <span class="md-ellipsis">
      ExponentialDecayLengthPenalty
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.ForceTokensLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      ForceTokensLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.ForcedBOSTokenLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      ForcedBOSTokenLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.ForcedEOSTokenLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      ForcedEOSTokenLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.HammingDiversityLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      HammingDiversityLogitsProcessor
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HammingDiversityLogitsProcessor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.HammingDiversityLogitsProcessor.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.InfNanRemoveLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      InfNanRemoveLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.LogitNormalization" class="md-nav__link">
    <span class="md-ellipsis">
      LogitNormalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.LogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      LogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.LogitsProcessorList" class="md-nav__link">
    <span class="md-ellipsis">
      LogitsProcessorList
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LogitsProcessorList">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.LogitsProcessorList.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.LogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      LogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.MinLengthLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      MinLengthLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.MinNewTokensLengthLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      MinNewTokensLengthLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.MinPLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      MinPLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.NoBadWordsLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      NoBadWordsLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.NoRepeatNGramLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      NoRepeatNGramLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.PrefixConstrainedLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      PrefixConstrainedLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.RepetitionPenaltyLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      RepetitionPenaltyLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.SequenceBiasLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      SequenceBiasLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      SuppressTokensAtBeginLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.SuppressTokensLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      SuppressTokensLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.TemperatureLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      TemperatureLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.TopKLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      TopKLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.TopPLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      TopPLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.TypicalLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      TypicalLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.UnbatchedClassifierFreeGuidanceLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      UnbatchedClassifierFreeGuidanceLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.WatermarkLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      WatermarkLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.WhisperNoSpeechDetection" class="md-nav__link">
    <span class="md-ellipsis">
      WhisperNoSpeechDetection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.WhisperTimeStampLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      WhisperTimeStampLogitsProcessor
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stopping_criteria/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stopping_criteria
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../streamers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    streamers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9_3" id="__nav_5_9_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_3">
            <span class="md-nav__icon md-icon"></span>
            models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/albert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    albert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    align
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/altclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    altclip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/audio_spectrogram_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    audio_spectrogram_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/auto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    auto
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    autoformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/baichuan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    baichuan
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/barthez/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    barthez
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bartpho/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bartpho
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/beit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bert_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bert_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bertweet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bertweet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bge_m3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bge_m3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/big_bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    big_bird
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bigbird_pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bigbird_pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/biogpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    biogpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/blenderbot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/blenderbot_small/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot_small
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/blip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/blip_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip_2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bloom
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bridgetower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bros/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bros
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/byt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    byt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/camembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    camembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/canine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    canine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/chatglm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/chatglm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/chatglm3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/codegen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    codegen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cogvlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cogvlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cohere/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/convbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/convnext/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convnext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cpmant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cpmbee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/ctrl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctrl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cvt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cvt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/data2vec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data2vec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/deberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/deberta_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/decision_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decision_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/distilbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    distilbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/efficientformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/efficientnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/electra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    electra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/encodec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encodec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/ernie/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/ernie_m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie_m
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/esm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    esm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/flava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    flava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/funnel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    funnel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gemma
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt_bigcode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_bigcode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt_neo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt_neox/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt_neox_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt_pangu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_pangu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gptj/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gptj
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/graphormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    graphormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/groupvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    groupvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/hubert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hubert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/imagegpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    imagegpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/internlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    internlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/jamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/jetmoe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jetmoe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/layoutlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/layoutlmv2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlmv2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/led/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    led
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/llava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/llava_next/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava_next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/longformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/longt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/luke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    luke
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/marian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    marian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/maskformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maskformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mbart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mbart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/megatron_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/megatron_gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/minicpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minicpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/minigpt4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minigpt4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mixtral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mobilebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mobilevit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilevit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/moss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    moss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mpnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/musicgen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/musicgen_melody/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen_melody
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mvp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mvp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/nezha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nezha
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/nystromformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nystromformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/olmo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    olmo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/openelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    openelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    opt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/owlvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    owlvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/phi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/phi3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/poolformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    poolformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/pop2piano/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pop2piano
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/qwen2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2_moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/regnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/rembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    resnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/roc_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roc_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rwkv
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/sam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/seamless_m4t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/seamless_m4t_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/segformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    segformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/seggpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seggpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/speech_encoder_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_encoder_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/speech_to_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_to_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/squeezebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    squeezebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/stablelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stablelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    starcoder2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/swiftformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    swiftformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/switch_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    switch_transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/t5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    t5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/table_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    table_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/timesformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    timesformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/tinybert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tinybert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/van/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    van
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/vipllava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vipllava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/vision_text_dual_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision_text_dual_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/visual_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visual_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/wav2vec2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/wav2vec2_conformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_conformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/wav2vec2_with_lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_with_lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/wavlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    whisper
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/x_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    x_clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/xlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/xlm_roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/xlm_roberta_xl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta_xl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/xlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlnet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_4" >
        
          
          <label class="md-nav__link" for="__nav_5_9_4" id="__nav_5_9_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_4">
            <span class="md-nav__icon md-icon"></span>
            pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/pipeline/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/automatic_speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    automatic_speech_recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/document_question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    document_question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/fill_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fill_mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text2text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text2text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/zero_shot_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_shot_classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../configuration_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    configuration_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modeling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_fast/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_fast
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TRL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Notes
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Change Log
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code of Conduct
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process" class="md-nav__link">
    <span class="md-ellipsis">
      logits_process
    </span>
  </a>
  
    <nav class="md-nav" aria-label="logits_process">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.AlternatingCodebooksLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      AlternatingCodebooksLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.BarkEosPrioritizerLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      BarkEosPrioritizerLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.ClassifierFreeGuidanceLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      ClassifierFreeGuidanceLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.EncoderNoRepeatNGramLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      EncoderNoRepeatNGramLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.EncoderRepetitionPenaltyLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      EncoderRepetitionPenaltyLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.EpsilonLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      EpsilonLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.EtaLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      EtaLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.ExponentialDecayLengthPenalty" class="md-nav__link">
    <span class="md-ellipsis">
      ExponentialDecayLengthPenalty
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.ForceTokensLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      ForceTokensLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.ForcedBOSTokenLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      ForcedBOSTokenLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.ForcedEOSTokenLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      ForcedEOSTokenLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.HammingDiversityLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      HammingDiversityLogitsProcessor
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HammingDiversityLogitsProcessor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.HammingDiversityLogitsProcessor.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.InfNanRemoveLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      InfNanRemoveLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.LogitNormalization" class="md-nav__link">
    <span class="md-ellipsis">
      LogitNormalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.LogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      LogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.LogitsProcessorList" class="md-nav__link">
    <span class="md-ellipsis">
      LogitsProcessorList
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LogitsProcessorList">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.LogitsProcessorList.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.LogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      LogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.MinLengthLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      MinLengthLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.MinNewTokensLengthLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      MinNewTokensLengthLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.MinPLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      MinPLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.NoBadWordsLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      NoBadWordsLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.NoRepeatNGramLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      NoRepeatNGramLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.PrefixConstrainedLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      PrefixConstrainedLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.RepetitionPenaltyLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      RepetitionPenaltyLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.SequenceBiasLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      SequenceBiasLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      SuppressTokensAtBeginLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.SuppressTokensLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      SuppressTokensLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.TemperatureLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      TemperatureLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.TopKLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      TopKLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.TopPLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      TopPLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.TypicalLogitsWarper" class="md-nav__link">
    <span class="md-ellipsis">
      TypicalLogitsWarper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.UnbatchedClassifierFreeGuidanceLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      UnbatchedClassifierFreeGuidanceLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.WatermarkLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      WatermarkLogitsProcessor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.WhisperNoSpeechDetection" class="md-nav__link">
    <span class="md-ellipsis">
      WhisperNoSpeechDetection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.logits_process.WhisperTimeStampLogitsProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      WhisperTimeStampLogitsProcessor
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindnlp/edit/master/docs/en/api/transformers/generation/logits_process.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindnlp/raw/master/docs/en/api/transformers/generation/logits_process.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>logits_process</h1>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.generation.logits_process" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process</code>


<a href="#mindnlp.transformers.generation.logits_process" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>logits process</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.AlternatingCodebooksLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.AlternatingCodebooksLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.AlternatingCodebooksLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] enforcing alternated generation between the two codebooks of Bark.</p>
<p><Tip warning={true}></p>
<p>This logits processor is exclusively compatible with
<a href="https://huggingface.co/docs/transformers/en/model_doc/bark">Bark</a>'s fine submodel. See the model documentation
for examples.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_start_len</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The length of the initial input sequence.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>semantic_vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vocabulary size of the semantic part, i.e number of tokens associated to the semantic vocabulary.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>codebook_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of tokens associated to the codebook.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AlternatingCodebooksLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] enforcing alternated generation between the two codebooks of Bark.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This logits processor is exclusively compatible with</span>
<span class="sd">    [Bark](https://huggingface.co/docs/transformers/en/model_doc/bark)&#39;s fine submodel. See the model documentation</span>
<span class="sd">    for examples.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        input_start_len (`int`):</span>
<span class="sd">            The length of the initial input sequence.</span>
<span class="sd">        semantic_vocab_size (`int`):</span>
<span class="sd">            Vocabulary size of the semantic part, i.e number of tokens associated to the semantic vocabulary.</span>
<span class="sd">        codebook_size (`int`):</span>
<span class="sd">            Number of tokens associated to the codebook.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_start_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">semantic_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">codebook_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_start_len</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">input_start_len</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`input_starting_length` has to be a non-negative integer, but is </span><span class="si">{</span><span class="n">input_start_len</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_start_len</span> <span class="o">=</span> <span class="n">input_start_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">semantic_vocab_size</span> <span class="o">=</span> <span class="n">semantic_vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">codebook_size</span> <span class="o">=</span> <span class="n">codebook_size</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">curr_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># even -&gt; first codebook, odd -&gt; second codebook</span>
        <span class="n">is_first_codebook</span> <span class="o">=</span> <span class="p">((</span><span class="n">curr_len</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_start_len</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="k">if</span> <span class="n">is_first_codebook</span><span class="p">:</span>
            <span class="n">scores_processed</span><span class="p">[:,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">semantic_vocab_size</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
            <span class="n">scores_processed</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">semantic_vocab_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">codebook_size</span> <span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scores_processed</span><span class="p">[:,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">semantic_vocab_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">codebook_size</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.BarkEosPrioritizerLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.BarkEosPrioritizerLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.BarkEosPrioritizerLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>This processor ensures that the EOS token is selected if its probability is greater than the <code>min_eos_p</code>.</p>
<p><Tip warning={true}></p>
<p>This logits processor is exclusively compatible with
<a href="https://huggingface.co/docs/transformers/en/model_doc/bark">Bark</a>. See the model documentation for examples.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id(s) of the <em>end-of-sequence</em> token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[int, List[int], mindspore.Tensor]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_eos_p</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Minimum end of speech threshold.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">BarkEosPrioritizerLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;This processor ensures that the EOS token is selected if its probability is greater than the `min_eos_p`.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This logits processor is exclusively compatible with</span>
<span class="sd">    [Bark](https://huggingface.co/docs/transformers/en/model_doc/bark). See the model documentation for examples.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        eos_token_id (`Union[int, List[int], mindspore.Tensor]`):</span>
<span class="sd">            The id(s) of the *end-of-sequence* token.</span>
<span class="sd">        min_eos_p (`float`, *optional*):</span>
<span class="sd">            Minimum end of speech threshold.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">min_eos_p</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">eos_token_id</span> <span class="o">=</span> <span class="p">[</span><span class="n">eos_token_id</span><span class="p">]</span>
            <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>

        <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`eos_token_id` has to be a list of positive integers, but is </span><span class="si">{</span><span class="n">eos_token_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">min_eos_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">min_eos_p</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`min_eos_p` has to be a positive float, but is </span><span class="si">{</span><span class="n">min_eos_p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_eos_p</span> <span class="o">=</span> <span class="n">min_eos_p</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_eos_p</span><span class="p">:</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># create scores full of -inf except for the eos_token_id</span>
            <span class="n">early_stop_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
            <span class="n">early_stop_scores</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>

            <span class="n">do_early_stop</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_eos_p</span>
            <span class="n">do_early_stop</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">do_early_stop</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">do_early_stop</span><span class="p">,</span> <span class="n">early_stop_scores</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.ClassifierFreeGuidanceLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.ClassifierFreeGuidanceLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.ClassifierFreeGuidanceLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] for classifier free guidance (CFG). The scores are split over the batch dimension,
where the first half correspond to the conditional logits (predicted from the input prompt) and the second half
correspond to the unconditional logits (predicted from an empty or 'null' prompt). The processor computes a
weighted average across the conditional and unconditional logits, parameterised by the <code>guidance_scale</code>.</p>
<p>See <a href="https://arxiv.org/abs/2306.05284">the paper</a> for more information.</p>
<p><Tip warning={true}></p>
<p>This logits processor is exclusively compatible with
<a href="https://huggingface.co/docs/transformers/main/en/model_doc/musicgen">MusicGen</a></p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>guidance_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The guidance scale for classifier free guidance (CFG). CFG is enabled by setting <code>guidance_scale &gt; 1</code>.
Higher guidance scale encourages the model to generate samples that are more closely linked to the input
prompt, usually at the expense of poorer quality.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">MusicgenForConditionalGeneration</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/musicgen-small&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">MusicgenForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/musicgen-small&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span>
<span class="o">...</span>     <span class="n">text</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;80s pop track with bassy drums and synth&quot;</span><span class="p">,</span> <span class="s2">&quot;90s rock song with loud guitars and heavy drums&quot;</span><span class="p">],</span>
<span class="o">...</span>     <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">,</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">audio_values</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">guidance_scale</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ClassifierFreeGuidanceLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] for classifier free guidance (CFG). The scores are split over the batch dimension,</span>
<span class="sd">    where the first half correspond to the conditional logits (predicted from the input prompt) and the second half</span>
<span class="sd">    correspond to the unconditional logits (predicted from an empty or &#39;null&#39; prompt). The processor computes a</span>
<span class="sd">    weighted average across the conditional and unconditional logits, parameterised by the `guidance_scale`.</span>

<span class="sd">    See [the paper](https://arxiv.org/abs/2306.05284) for more information.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This logits processor is exclusively compatible with</span>
<span class="sd">    [MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen)</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        guidance_scale (float):</span>
<span class="sd">            The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale &gt; 1`.</span>
<span class="sd">            Higher guidance scale encourages the model to generate samples that are more closely linked to the input</span>
<span class="sd">            prompt, usually at the expense of poorer quality.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoProcessor, MusicgenForConditionalGeneration</span>

<span class="sd">    &gt;&gt;&gt; processor = AutoProcessor.from_pretrained(&quot;facebook/musicgen-small&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = MusicgenForConditionalGeneration.from_pretrained(&quot;facebook/musicgen-small&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = processor(</span>
<span class="sd">    ...     text=[&quot;80s pop track with bassy drums and synth&quot;, &quot;90s rock song with loud guitars and heavy drums&quot;],</span>
<span class="sd">    ...     padding=True,</span>
<span class="sd">    ...     return_tensors=&quot;ms&quot;,</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">guidance_scale</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">guidance_scale</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">guidance_scale</span> <span class="o">=</span> <span class="n">guidance_scale</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Require guidance scale &gt;1 to use the classifier free guidance processor, got guidance scale &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">guidance_scale</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># simple check to make sure we have compatible batch sizes between our</span>
        <span class="c1"># logits scores (cond + uncond) and input ids (cond only)</span>
        <span class="k">if</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Logits should have twice the batch size of the input ids, the first half of batches corresponding to &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;the conditional inputs, and the second half of batches corresponding to the unconditional inputs. Got &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;batch size </span><span class="si">{</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> for the logits and </span><span class="si">{</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> for the input ids.&quot;</span>
            <span class="p">)</span>
        <span class="n">unguided_bsz</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">cond_logits</span><span class="p">,</span> <span class="n">uncond_logits</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">unguided_bsz</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">uncond_logits</span> <span class="o">+</span> <span class="p">(</span><span class="n">cond_logits</span> <span class="o">-</span> <span class="n">uncond_logits</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">guidance_scale</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.EncoderNoRepeatNGramLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.EncoderNoRepeatNGramLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.EncoderNoRepeatNGramLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that works similarly to [<code>NoRepeatNGramLogitsProcessor</code>], but applied exclusively to prevent
the repetition of n-grams present in the prompt.</p>
<p>It was designed to promote chattiness in a language model, by preventing the generation of n-grams present in
previous conversation rounds.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>encoder_ngram_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>All ngrams of size <code>ngram_size</code> can only occur within the encoder input ids.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The encoder_input_ids that should not be repeated within the decoder ids.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Alice: I love cats. What do you love?</span><span class="se">\n</span><span class="s2">Bob:&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With greedy decoding, we see Bob repeating Alice&#39;s opinion. If Bob was a chatbot, it would be a poor one.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Alice</span><span class="p">:</span> <span class="n">I</span> <span class="n">love</span> <span class="n">cats</span><span class="o">.</span> <span class="n">What</span> <span class="n">do</span> <span class="n">you</span> <span class="n">love</span><span class="err">?</span>
<span class="n">Bob</span><span class="p">:</span> <span class="n">I</span> <span class="n">love</span> <span class="n">cats</span><span class="o">.</span> <span class="n">What</span> <span class="n">do</span> <span class="n">you</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With this logits processor, we can prevent Bob from repeating Alice&#39;s opinion.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">encoder_no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Alice</span><span class="p">:</span> <span class="n">I</span> <span class="n">love</span> <span class="n">cats</span><span class="o">.</span> <span class="n">What</span> <span class="n">do</span> <span class="n">you</span> <span class="n">love</span><span class="err">?</span>
<span class="n">Bob</span><span class="p">:</span> <span class="n">My</span> <span class="n">cats</span> <span class="n">are</span> <span class="n">very</span> <span class="n">cute</span><span class="o">.</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EncoderNoRepeatNGramLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] that works similarly to [`NoRepeatNGramLogitsProcessor`], but applied exclusively to prevent</span>
<span class="sd">    the repetition of n-grams present in the prompt.</span>

<span class="sd">    It was designed to promote chattiness in a language model, by preventing the generation of n-grams present in</span>
<span class="sd">    previous conversation rounds.</span>

<span class="sd">    Args:</span>
<span class="sd">        encoder_ngram_size (`int`):</span>
<span class="sd">            All ngrams of size `ngram_size` can only occur within the encoder input ids.</span>
<span class="sd">        encoder_input_ids (`int`):</span>
<span class="sd">            The encoder_input_ids that should not be repeated within the decoder ids.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```py</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;Alice: I love cats. What do you love?\nBob:&quot;, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # With greedy decoding, we see Bob repeating Alice&#39;s opinion. If Bob was a chatbot, it would be a poor one.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    Alice: I love cats. What do you love?</span>
<span class="sd">    Bob: I love cats. What do you</span>

<span class="sd">    &gt;&gt;&gt; # With this logits processor, we can prevent Bob from repeating Alice&#39;s opinion.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, encoder_no_repeat_ngram_size=2)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    Alice: I love cats. What do you love?</span>
<span class="sd">    Bob: My cats are very cute.</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_ngram_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">encoder_input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_ngram_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">encoder_ngram_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`encoder_ngram_size` has to be a strictly positive integer, but is </span><span class="si">{</span><span class="n">encoder_ngram_size</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ngram_size</span> <span class="o">=</span> <span class="n">encoder_ngram_size</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">encoder_input_ids</span> <span class="o">=</span> <span class="n">encoder_input_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generated_ngrams</span> <span class="o">=</span> <span class="n">_get_ngrams</span><span class="p">(</span><span class="n">encoder_ngram_size</span><span class="p">,</span> <span class="n">encoder_input_ids</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># B x num_beams</span>
        <span class="n">num_hypos</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">num_hypos</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="n">banned_batch_tokens</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">_get_generated_ngrams</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">generated_ngrams</span><span class="p">[</span><span class="n">hypo_idx</span> <span class="o">//</span> <span class="n">num_beams</span><span class="p">],</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ngram_size</span><span class="p">,</span> <span class="n">cur_len</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">hypo_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">banned_tokens</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">banned_batch_tokens</span><span class="p">):</span>
            <span class="n">scores_processed</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">banned_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.EncoderRepetitionPenaltyLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.EncoderRepetitionPenaltyLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.EncoderRepetitionPenaltyLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that works similarly to [<code>RepetitionPenaltyLogitsProcessor</code>], but with an <em>inverse</em> penalty
that is applied to the tokens present in the prompt. In other words, a penalty above 1.0 increases the odds of
selecting tokens that were present in the prompt.</p>
<p>It was designed to avoid hallucination in input-grounded tasks, like summarization. Although originally intended
for encoder-decoder models, it can also be used with decoder-only models like LLMs.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>penalty</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The parameter for repetition penalty. 1.0 means no penalty. Above 1.0 rewards prompt tokens. Between 0.0
and 1.0 penalizes prompt tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The encoder_input_ids that should be repeated within the decoder ids.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Alice and Bob. The third member&#39;s name was&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">gen_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">gen_out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Alice</span> <span class="ow">and</span> <span class="n">Bob</span><span class="o">.</span> <span class="n">The</span> <span class="n">third</span> <span class="n">member</span><span class="s1">&#39;s name was not mentioned.</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With the `encoder_repetition_penalty` argument we can trigger this logits processor in `generate`, which can</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># promote the use of prompt tokens (&quot;Bob&quot; in this example)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">gen_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">encoder_repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">gen_out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Alice</span> <span class="ow">and</span> <span class="n">Bob</span><span class="o">.</span> <span class="n">The</span> <span class="n">third</span> <span class="n">member</span><span class="s1">&#39;s name was Bob. The third member&#39;</span><span class="n">s</span> <span class="n">name</span> <span class="n">was</span> <span class="n">Bob</span><span class="o">.</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EncoderRepetitionPenaltyLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] that works similarly to [`RepetitionPenaltyLogitsProcessor`], but with an *inverse* penalty</span>
<span class="sd">    that is applied to the tokens present in the prompt. In other words, a penalty above 1.0 increases the odds of</span>
<span class="sd">    selecting tokens that were present in the prompt.</span>

<span class="sd">    It was designed to avoid hallucination in input-grounded tasks, like summarization. Although originally intended</span>
<span class="sd">    for encoder-decoder models, it can also be used with decoder-only models like LLMs.</span>

<span class="sd">    Args:</span>
<span class="sd">        penalty (`float`):</span>
<span class="sd">            The parameter for repetition penalty. 1.0 means no penalty. Above 1.0 rewards prompt tokens. Between 0.0</span>
<span class="sd">            and 1.0 penalizes prompt tokens.</span>
<span class="sd">        encoder_input_ids (`mindspore.Tensor`):</span>
<span class="sd">            The encoder_input_ids that should be repeated within the decoder ids.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer</span>

<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer([&quot;Alice and Bob. The third member&#39;s name was&quot;], return_tensors=&quot;ms&quot;)</span>
<span class="sd">    &gt;&gt;&gt; gen_out = model.generate(**inputs)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])</span>
<span class="sd">    Alice and Bob. The third member&#39;s name was not mentioned.</span>

<span class="sd">    &gt;&gt;&gt; # With the `encoder_repetition_penalty` argument we can trigger this logits processor in `generate`, which can</span>
<span class="sd">    &gt;&gt;&gt; # promote the use of prompt tokens (&quot;Bob&quot; in this example)</span>
<span class="sd">    &gt;&gt;&gt; gen_out = model.generate(**inputs, encoder_repetition_penalty=1.2)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])</span>
<span class="sd">    Alice and Bob. The third member&#39;s name was Bob. The third member&#39;s name was Bob.</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">penalty</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">encoder_input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">penalty</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="p">(</span><span class="n">penalty</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`penalty` has to be a strictly positive float, but is </span><span class="si">{</span><span class="n">penalty</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_input_ids</span> <span class="o">=</span> <span class="n">encoder_input_ids</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_input_ids</span><span class="p">)</span>

        <span class="c1"># if score &lt; 0 then hallucination penalty has to be multiplied to increase the token probabilities</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">score</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">score</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="n">score</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">)</span>

        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_input_ids</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.EpsilonLogitsWarper" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.EpsilonLogitsWarper</code>


<a href="#mindnlp.transformers.generation.logits_process.EpsilonLogitsWarper" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsWarper" href="#mindnlp.transformers.generation.logits_process.LogitsWarper">LogitsWarper</a></code></p>


        <p>[<code>LogitsWarper</code>] that performs epsilon-sampling, i.e. restricting to tokens with <code>prob &gt;= epsilon</code>. Takes the
largest min_tokens_to_keep tokens if no tokens satisfy this constraint. See <a href="https://arxiv.org/abs/2210.15191">Truncation Sampling as Language Model
Desmoothing</a> for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>epsilon</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set to &gt; 0, only the most tokens with probabilities <code>epsilon</code> or higher are kept for generation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>filter_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>All filtered values will be set to this float value.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to -inf</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>-float(&#39;Inf&#39;)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_tokens_to_keep</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Minimum number of tokens that cannot be filtered.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;A sequence: 1, 2&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With sampling, the output is unexpected -- sometimes too unexpected.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="o">|</span> <span class="o">&lt;</span> <span class="mi">4</span> <span class="p">(</span><span class="n">left</span><span class="o">-</span><span class="n">hand</span> <span class="n">pointer</span><span class="p">)</span> <span class="p">;</span>
<span class="o">&lt;</span><span class="n">BLANKLINE</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">BLANKLINE</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With epsilon sampling, the output gets restricted to high-probability tokens. Note that this is similar to</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Top P sampling, which restricts tokens based on their cumulative probability.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Pro tip: The paper recomends using `epsilon_cutoff` values between 3e-4 and 9e-4</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">epsilon_cutoff</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EpsilonLogitsWarper</span><span class="p">(</span><span class="n">LogitsWarper</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsWarper`] that performs epsilon-sampling, i.e. restricting to tokens with `prob &gt;= epsilon`. Takes the</span>
<span class="sd">    largest min_tokens_to_keep tokens if no tokens satisfy this constraint. See [Truncation Sampling as Language Model</span>
<span class="sd">    Desmoothing](https://arxiv.org/abs/2210.15191) for more information.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (`float`):</span>
<span class="sd">            If set to &gt; 0, only the most tokens with probabilities `epsilon` or higher are kept for generation.</span>
<span class="sd">        filter_value (`float`, *optional*, defaults to -inf):</span>
<span class="sd">            All filtered values will be set to this float value.</span>
<span class="sd">        min_tokens_to_keep (`int`, *optional*, defaults to 1):</span>
<span class="sd">            Minimum number of tokens that cannot be filtered.</span>

<span class="sd">    Examples:</span>
<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed</span>

<span class="sd">    &gt;&gt;&gt; set_seed(1)</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;A sequence: 1, 2&quot;, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # With sampling, the output is unexpected -- sometimes too unexpected.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    A sequence: 1, 2, 3 | &lt; 4 (left-hand pointer) ;</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>

<span class="sd">    &gt;&gt;&gt; # With epsilon sampling, the output gets restricted to high-probability tokens. Note that this is similar to</span>
<span class="sd">    &gt;&gt;&gt; # Top P sampling, which restricts tokens based on their cumulative probability.</span>
<span class="sd">    &gt;&gt;&gt; # Pro tip: The paper recomends using `epsilon_cutoff` values between 3e-4 and 9e-4</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True, epsilon_cutoff=0.1)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">),</span> <span class="n">min_tokens_to_keep</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epsilon</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`epsilon_cutoff` has to be a float &gt; 0 and &lt; 1, but is </span><span class="si">{</span><span class="n">epsilon</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">min_tokens_to_keep</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">min_tokens_to_keep</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">min_tokens_to_keep</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`min_tokens_to_keep` has to be a strictly positive integer, but is </span><span class="si">{</span><span class="n">min_tokens_to_keep</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="n">filter_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span> <span class="o">=</span> <span class="n">min_tokens_to_keep</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
        <span class="c1"># Determine which indices to remove</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">probabilities</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>

        <span class="c1"># Keep the words with the &#39;min_tokens_to_keep&#39;-highest probabilities</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Safety check</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">indices_to_remove</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">scores</span> <span class="o">&lt;</span> <span class="n">ops</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>

        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">indices_to_remove</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.EtaLogitsWarper" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.EtaLogitsWarper</code>


<a href="#mindnlp.transformers.generation.logits_process.EtaLogitsWarper" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsWarper" href="#mindnlp.transformers.generation.logits_process.LogitsWarper">LogitsWarper</a></code></p>


        <p>[<code>LogitsWarper</code>] that performs eta-sampling, a technique to filter out tokens with probabilities below a dynamic
cutoff value, <code>eta</code>, which is calculated based on a combination of the hyperparameter <code>epsilon</code> and the entropy of
the token probabilities, i.e. <code>eta := min(epsilon, sqrt(epsilon * e^-entropy(probabilities)))</code>. Takes the largest
min_tokens_to_keep tokens if no tokens satisfy this constraint. It addresses the issue of poor quality in long
samples of text generated by neural language models leading to more coherent and fluent text. See <a href="https://arxiv.org/abs/2210.15191">Truncation
Sampling as Language Model Desmoothing</a> for more information. Note: <code>do_sample</code>
must be set to <code>True</code> for this <code>LogitsWarper</code> to work.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>epsilon</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A float value in the range (0, 1). Hyperparameter used to calculate the dynamic cutoff value, <code>eta</code>. The
suggested values from the paper ranges from 3e-4 to 4e-3 depending on the size of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>filter_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>All values that are found to be below the dynamic cutoff value, <code>eta</code>, are set to this float value. This
parameter is useful when logits need to be modified for very low probability tokens that should be excluded
from generation entirely.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to -inf</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>-float(&#39;Inf&#39;)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_tokens_to_keep</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Specifies the minimum number of tokens that must be kept for generation, regardless of their probabilities.
For example, if <code>min_tokens_to_keep</code> is set to 1, at least one token will always be kept for generation,
even if all tokens have probabilities below the cutoff <code>eta</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;A sequence: 1, 2&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With sampling, the output is unexpected -- sometimes too unexpected.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="o">|</span> <span class="o">&lt;</span> <span class="mi">4</span> <span class="p">(</span><span class="n">left</span><span class="o">-</span><span class="n">hand</span> <span class="n">pointer</span><span class="p">)</span> <span class="p">;</span>
<span class="o">&lt;</span><span class="n">BLANKLINE</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">BLANKLINE</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With eta sampling, the output gets restricted to high-probability tokens. You can see it as a dynamic form of</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># epsilon sampling that adapts its cutoff probability based on the entropy (high entropy = lower cutoff).</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Pro tip: The paper recomends using `eta_cutoff` values between 3e-4 to 4e-3</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eta_cutoff</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EtaLogitsWarper</span><span class="p">(</span><span class="n">LogitsWarper</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsWarper`] that performs eta-sampling, a technique to filter out tokens with probabilities below a dynamic</span>
<span class="sd">    cutoff value, `eta`, which is calculated based on a combination of the hyperparameter `epsilon` and the entropy of</span>
<span class="sd">    the token probabilities, i.e. `eta := min(epsilon, sqrt(epsilon * e^-entropy(probabilities)))`. Takes the largest</span>
<span class="sd">    min_tokens_to_keep tokens if no tokens satisfy this constraint. It addresses the issue of poor quality in long</span>
<span class="sd">    samples of text generated by neural language models leading to more coherent and fluent text. See [Truncation</span>
<span class="sd">    Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191) for more information. Note: `do_sample`</span>
<span class="sd">    must be set to `True` for this `LogitsWarper` to work.</span>


<span class="sd">    Args:</span>
<span class="sd">        epsilon (`float`):</span>
<span class="sd">            A float value in the range (0, 1). Hyperparameter used to calculate the dynamic cutoff value, `eta`. The</span>
<span class="sd">            suggested values from the paper ranges from 3e-4 to 4e-3 depending on the size of the model.</span>
<span class="sd">        filter_value (`float`, *optional*, defaults to -inf):</span>
<span class="sd">            All values that are found to be below the dynamic cutoff value, `eta`, are set to this float value. This</span>
<span class="sd">            parameter is useful when logits need to be modified for very low probability tokens that should be excluded</span>
<span class="sd">            from generation entirely.</span>
<span class="sd">        min_tokens_to_keep (`int`, *optional*, defaults to 1):</span>
<span class="sd">            Specifies the minimum number of tokens that must be kept for generation, regardless of their probabilities.</span>
<span class="sd">            For example, if `min_tokens_to_keep` is set to 1, at least one token will always be kept for generation,</span>
<span class="sd">            even if all tokens have probabilities below the cutoff `eta`.</span>

<span class="sd">    Examples:</span>
<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed</span>

<span class="sd">    &gt;&gt;&gt; set_seed(1)</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;A sequence: 1, 2&quot;, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # With sampling, the output is unexpected -- sometimes too unexpected.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    A sequence: 1, 2, 3 | &lt; 4 (left-hand pointer) ;</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>

<span class="sd">    &gt;&gt;&gt; # With eta sampling, the output gets restricted to high-probability tokens. You can see it as a dynamic form of</span>
<span class="sd">    &gt;&gt;&gt; # epsilon sampling that adapts its cutoff probability based on the entropy (high entropy = lower cutoff).</span>
<span class="sd">    &gt;&gt;&gt; # Pro tip: The paper recomends using `eta_cutoff` values between 3e-4 to 4e-3</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True, eta_cutoff=0.1)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">),</span> <span class="n">min_tokens_to_keep</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">):</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epsilon</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`eta_cutoff` has to be a float &gt; 0 and &lt; 1, but is </span><span class="si">{</span><span class="n">epsilon</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">min_tokens_to_keep</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">min_tokens_to_keep</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">min_tokens_to_keep</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`min_tokens_to_keep` has to be a strictly positive integer, but is </span><span class="si">{</span><span class="n">min_tokens_to_keep</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="n">filter_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span> <span class="o">=</span> <span class="n">min_tokens_to_keep</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">entropy</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">entropy</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">probabilities</span> <span class="o">&lt;</span> <span class="n">eta</span>

        <span class="c1"># Keep the words with the &#39;min_tokens_to_keep&#39;-highest probabilities</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Safety check</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">indices_to_remove</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">scores</span> <span class="o">&lt;</span> <span class="n">ops</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>

        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">indices_to_remove</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.ExponentialDecayLengthPenalty" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.ExponentialDecayLengthPenalty</code>


<a href="#mindnlp.transformers.generation.logits_process.ExponentialDecayLengthPenalty" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that exponentially increases the score of the <code>eos_token_id</code> after <code>start_index</code> has been
reached. This allows generating shorter sequences without having a hard cutoff, allowing the <code>eos_token</code> to be
predicted in a meaningful position.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>exponential_decay_length_penalty</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>This tuple shall consist of: <code>(start_index, decay_factor)</code> where <code>start_index</code> indicates where penalty
starts and <code>decay_factor</code> represents the factor of exponential decay</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(int, float)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id(s) of the <em>end-of-sequence</em> token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[int, List[int], mindspore.Tensor]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_ids_seq_length</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The length of the input sequence.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Just wanted to let you know, I&quot;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Let&#39;s consider that we want short sentences, so we limit `max_length=30`. However, we observe that the answer</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># tends to end abruptly.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">50256</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Just</span> <span class="n">wanted</span> <span class="n">to</span> <span class="n">let</span> <span class="n">you</span> <span class="n">know</span><span class="p">,</span> <span class="n">I</span> <span class="n">received</span> <span class="n">a</span> <span class="n">link</span> <span class="n">to</span> <span class="n">an</span> <span class="n">ebook</span><span class="p">,</span> <span class="n">the</span> <span class="n">book</span> <span class="n">How</span> <span class="n">To</span> <span class="n">Start</span> <span class="n">A</span> <span class="n">Social</span> <span class="n">Network</span> <span class="n">which</span> <span class="n">was</span>
<span class="n">published</span> <span class="ow">in</span> <span class="mf">2010.</span> <span class="n">Although</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># To promote the appearance of the EOS token at the right time, we add the `exponential_decay_length_penalty =</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># (start_index, decay_factor)`. Instead of cutting at max_tokens, the output comes to an end before and usually</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># with more meaning. What happens is that starting from `start_index` the EOS token score will be increased</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># by `decay_factor` exponentially. However, if you set a high decay factor, you may also end up with abruptly</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># ending sequences.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="o">...</span>     <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">50256</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">exponential_decay_length_penalty</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">),</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Just</span> <span class="n">wanted</span> <span class="n">to</span> <span class="n">let</span> <span class="n">you</span> <span class="n">know</span><span class="p">,</span> <span class="n">I</span> <span class="n">received</span> <span class="n">a</span> <span class="n">link</span> <span class="n">to</span> <span class="n">an</span> <span class="n">ebook</span><span class="p">,</span> <span class="n">the</span> <span class="n">book</span> <span class="n">How</span> <span class="n">To</span> <span class="n">Start</span> <span class="n">A</span> <span class="n">Social</span> <span class="n">Network</span>
<span class="n">which</span><span class="o">&lt;|</span><span class="n">endoftext</span><span class="o">|&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With a small decay factor, you will have a higher chance of getting a meaningful sequence.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="o">...</span>     <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">50256</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">exponential_decay_length_penalty</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">),</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Just</span> <span class="n">wanted</span> <span class="n">to</span> <span class="n">let</span> <span class="n">you</span> <span class="n">know</span><span class="p">,</span> <span class="n">I</span> <span class="n">received</span> <span class="n">a</span> <span class="n">link</span> <span class="n">to</span> <span class="n">an</span> <span class="n">ebook</span><span class="p">,</span> <span class="n">the</span> <span class="n">book</span> <span class="n">How</span> <span class="n">To</span> <span class="n">Start</span> <span class="n">A</span> <span class="n">Social</span> <span class="n">Network</span> <span class="n">which</span> <span class="n">was</span>
<span class="n">published</span> <span class="ow">in</span> <span class="mf">2010.</span><span class="o">&lt;|</span><span class="n">endoftext</span><span class="o">|&gt;</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ExponentialDecayLengthPenalty</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] that exponentially increases the score of the `eos_token_id` after `start_index` has been</span>
<span class="sd">    reached. This allows generating shorter sequences without having a hard cutoff, allowing the `eos_token` to be</span>
<span class="sd">    predicted in a meaningful position.</span>

<span class="sd">    Args:</span>
<span class="sd">        exponential_decay_length_penalty (`tuple(int, float)`):</span>
<span class="sd">            This tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates where penalty</span>
<span class="sd">            starts and `decay_factor` represents the factor of exponential decay</span>
<span class="sd">        eos_token_id (`Union[int, List[int], mindspore.Tensor]`):</span>
<span class="sd">            The id(s) of the *end-of-sequence* token.</span>
<span class="sd">        input_ids_seq_length (`int`):</span>
<span class="sd">            The length of the input sequence.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2&quot;)</span>

<span class="sd">    &gt;&gt;&gt; text = &quot;Just wanted to let you know, I&quot;</span>
<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(text, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # Let&#39;s consider that we want short sentences, so we limit `max_length=30`. However, we observe that the answer</span>
<span class="sd">    &gt;&gt;&gt; # tends to end abruptly.</span>
<span class="sd">    &gt;&gt;&gt; set_seed(1)</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True, temperature=0.9, max_length=30, pad_token_id=50256)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs)[0])</span>
<span class="sd">    Just wanted to let you know, I received a link to an ebook, the book How To Start A Social Network which was</span>
<span class="sd">    published in 2010. Although</span>

<span class="sd">    &gt;&gt;&gt; # To promote the appearance of the EOS token at the right time, we add the `exponential_decay_length_penalty =</span>
<span class="sd">    &gt;&gt;&gt; # (start_index, decay_factor)`. Instead of cutting at max_tokens, the output comes to an end before and usually</span>
<span class="sd">    &gt;&gt;&gt; # with more meaning. What happens is that starting from `start_index` the EOS token score will be increased</span>
<span class="sd">    &gt;&gt;&gt; # by `decay_factor` exponentially. However, if you set a high decay factor, you may also end up with abruptly</span>
<span class="sd">    &gt;&gt;&gt; # ending sequences.</span>
<span class="sd">    &gt;&gt;&gt; set_seed(1)</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(</span>
<span class="sd">    ...     **inputs,</span>
<span class="sd">    ...     do_sample=True,</span>
<span class="sd">    ...     temperature=0.9,</span>
<span class="sd">    ...     max_length=30,</span>
<span class="sd">    ...     pad_token_id=50256,</span>
<span class="sd">    ...     exponential_decay_length_penalty=(15, 1.6),</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs)[0])</span>
<span class="sd">    Just wanted to let you know, I received a link to an ebook, the book How To Start A Social Network</span>
<span class="sd">    which&lt;|endoftext|&gt;</span>

<span class="sd">    &gt;&gt;&gt; # With a small decay factor, you will have a higher chance of getting a meaningful sequence.</span>
<span class="sd">    &gt;&gt;&gt; set_seed(1)</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(</span>
<span class="sd">    ...     **inputs,</span>
<span class="sd">    ...     do_sample=True,</span>
<span class="sd">    ...     temperature=0.9,</span>
<span class="sd">    ...     max_length=30,</span>
<span class="sd">    ...     pad_token_id=50256,</span>
<span class="sd">    ...     exponential_decay_length_penalty=(15, 1.01),</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs)[0])</span>
<span class="sd">    Just wanted to let you know, I received a link to an ebook, the book How To Start A Social Network which was</span>
<span class="sd">    published in 2010.&lt;|endoftext|&gt;</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">exponential_decay_length_penalty</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">input_ids_seq_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">regulation_start</span> <span class="o">=</span> <span class="n">exponential_decay_length_penalty</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">input_ids_seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">regulation_factor</span> <span class="o">=</span> <span class="n">exponential_decay_length_penalty</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">eos_token_id</span> <span class="o">=</span> <span class="p">[</span><span class="n">eos_token_id</span><span class="p">]</span>
            <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>

        <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`eos_token_id` has to be a list of positive integers, but is </span><span class="si">{</span><span class="n">eos_token_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">penalties</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="k">if</span> <span class="n">cur_len</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">regulation_start</span><span class="p">:</span>
            <span class="n">penalty_idx</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">regulation_start</span>
            <span class="c1"># To support negative logits we compute the penalty of the absolute value and add to the original logit</span>
            <span class="n">penalty</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">scores</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="nb">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">regulation_factor</span><span class="p">,</span> <span class="n">penalty_idx</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">penalties</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">penalty</span>
            <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">penalties</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.ForceTokensLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.ForceTokensLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.ForceTokensLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>This processor takes a list of pairs of integers which indicates a mapping from generation indices to token
indices that will be forced before generation. The processor will set their log probs to <code>inf</code> so that they are
sampled at their corresponding index. Originally created for
<a href="https://huggingface.co/docs/transformers/model_doc/whisper">Whisper</a>.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ForceTokensLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This processor takes a list of pairs of integers which indicates a mapping from generation indices to token</span>
<span class="sd">    indices that will be forced before generation. The processor will set their log probs to `inf` so that they are</span>
<span class="sd">    sampled at their corresponding index. Originally created for</span>
<span class="sd">    [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">force_token_map</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">_has_warned</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">force_token_map</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">force_token_map</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_warned</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;This `ForceTokensLogitsProcessor` has been deprecated. &quot;</span>
                <span class="s2">&quot;Should you need to provide prompt ids for generation, specify `input_ids` to the generate method for decoder-only models, or `decoder_input_ids` for encoder-decoder models.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">generation_idx</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">current_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">force_token_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">generation_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="k">if</span> <span class="n">current_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>
            <span class="n">scores_processed</span><span class="p">[:,</span> <span class="n">current_token</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.ForcedBOSTokenLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.ForcedBOSTokenLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.ForcedBOSTokenLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that enforces the specified token as the first generated token. Used with encoder-decoder
models.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the token to force as the first generated token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google/flan-t5-small&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google/flan-t5-small&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Translate from English to German: I love cats.&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># By default, it continues generating according to the model&#39;s logits</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="o">&lt;</span><span class="n">pad</span><span class="o">&gt;</span> <span class="n">Ich</span> <span class="n">liebe</span> <span class="n">Kitty</span><span class="o">.&lt;/</span><span class="n">s</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># We can use `forced_bos_token_id` to force the start of generation with an encoder-decoder model</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># (including forcing it to end straight away with an EOS token)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">forced_bos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="o">&lt;</span><span class="n">pad</span><span class="o">&gt;&lt;/</span><span class="n">s</span><span class="o">&gt;</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ForcedBOSTokenLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] that enforces the specified token as the first generated token. Used with encoder-decoder</span>
<span class="sd">    models.</span>

<span class="sd">    Args:</span>
<span class="sd">        bos_token_id (`int`):</span>
<span class="sd">            The id of the token to force as the first generated token.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForSeq2SeqLM</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForSeq2SeqLM.from_pretrained(&quot;google/flan-t5-small&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;google/flan-t5-small&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;Translate from English to German: I love cats.&quot;, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # By default, it continues generating according to the model&#39;s logits</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, max_new_tokens=10)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs)[0])</span>
<span class="sd">    &lt;pad&gt; Ich liebe Kitty.&lt;/s&gt;</span>

<span class="sd">    &gt;&gt;&gt; # We can use `forced_bos_token_id` to force the start of generation with an encoder-decoder model</span>
<span class="sd">    &gt;&gt;&gt; # (including forcing it to end straight away with an EOS token)</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, max_new_tokens=10, forced_bos_token_id=tokenizer.eos_token_id)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs)[0])</span>
<span class="sd">    &lt;pad&gt;&lt;/s&gt;</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="k">if</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
            <span class="n">scores_processed</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.ForcedEOSTokenLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.ForcedEOSTokenLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.ForcedEOSTokenLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that enforces the specified token as the last generated token when <code>max_length</code> is reached.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>max_length</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum length of the sequence to be generated.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id(s) of the <em>end-of-sequence</em> token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[int, List[int], mindspore.Tensor]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;A sequence: 1, 2, 3&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># By default, it continues generating according to the model&#39;s logits</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># `forced_eos_token_id` ensures the generation ends with a EOS token</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">forced_eos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span><span class="o">&lt;|</span><span class="n">endoftext</span><span class="o">|&gt;</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ForcedEOSTokenLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] that enforces the specified token as the last generated token when `max_length` is reached.</span>

<span class="sd">    Args:</span>
<span class="sd">        max_length (`int`):</span>
<span class="sd">            The maximum length of the sequence to be generated.</span>
<span class="sd">        eos_token_id (`Union[int, List[int], mindspore.Tensor]`):</span>
<span class="sd">            The id(s) of the *end-of-sequence* token.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;A sequence: 1, 2, 3&quot;, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # By default, it continues generating according to the model&#39;s logits</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, max_new_tokens=10)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs)[0])</span>
<span class="sd">    A sequence: 1, 2, 3, 4, 5, 6, 7, 8</span>

<span class="sd">    &gt;&gt;&gt; # `forced_eos_token_id` ensures the generation ends with a EOS token</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, max_new_tokens=10, forced_eos_token_id=tokenizer.eos_token_id)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs)[0])</span>
<span class="sd">    A sequence: 1, 2, 3, 4, 5, 6, 7,&lt;|endoftext|&gt;</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">eos_token_id</span> <span class="o">=</span> <span class="p">[</span><span class="n">eos_token_id</span><span class="p">]</span>
            <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>

        <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`eos_token_id` has to be a list of positive integers, but is </span><span class="si">{</span><span class="n">eos_token_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="k">if</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">scores_processed</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.HammingDiversityLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.HammingDiversityLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.HammingDiversityLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that enforces diverse beam search.</p>
<p>Note that this logits processor is only effective for [<code>PreTrainedModel.group_beam_search</code>]. See <a href="https://arxiv.org/pdf/1610.02424.pdf">Diverse Beam
Search: Decoding Diverse Solutions from Neural Sequence Models</a> for more
details.</p>
<p>Traditional beam search often generates very similar sequences across different beams.
<code>HammingDiversityLogitsProcessor</code> addresses this by penalizing beams that generate tokens already chosen by other
beams in the same time step.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>diversity_penalty</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>This value is subtracted from a beam's score if it generates a token same as any beam from other group at a
particular time. A higher <code>diversity_penalty</code> will enforce greater diversity among the beams. Adjusting
this value can help strike a balance between diversity and natural likelihood.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_beams</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of beams for beam search. 1 means no beam search.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_beam_groups</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of groups to divide <code>num_beams</code> into in order to ensure diversity among different groups of beams.
<a href="https://arxiv.org/pdf/1610.02424.pdf">this paper</a> for more details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torch</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initialize the model and tokenizer</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-t5/t5-base&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-t5/t5-base&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># A long text about the solar system</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
<span class="o">...</span>     <span class="s2">&quot;The Solar System is a gravitationally bound system comprising the Sun and the objects that orbit it, &quot;</span>
<span class="o">...</span>     <span class="s2">&quot;either directly or indirectly. Of the objects that orbit the Sun directly, the largest are the eight &quot;</span>
<span class="o">...</span>     <span class="s2">&quot;planets, with the remainder being smaller objects, such as the five dwarf planets and small Solar System &quot;</span>
<span class="o">...</span>     <span class="s2">&quot;bodies. The Solar System formed 4.6 billion years ago from the gravitational collapse of a giant &quot;</span>
<span class="o">...</span>     <span class="s2">&quot;interstellar molecular cloud.&quot;</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;summarize: &quot;</span> <span class="o">+</span> <span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Generate diverse summary</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs_diverse</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="o">...</span>     <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">diversity_penalty</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">summaries_diverse</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs_diverse</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Generate non-diverse summary</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs_non_diverse</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="o">...</span>     <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">summary_non_diverse</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs_non_diverse</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With `diversity_penalty`, the resulting beams are much more diverse</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">summary_non_diverse</span><span class="p">)</span>
<span class="p">[</span><span class="s1">&#39;the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.&#39;</span><span class="p">,</span>
<span class="s1">&#39;the Solar System formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.&#39;</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">summaries_diverse</span><span class="p">)</span>
<span class="p">[</span><span class="s1">&#39;the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.&#39;</span><span class="p">,</span>
<span class="s1">&#39;the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.</span>
<span class="n">the</span> <span class="n">rest</span> <span class="n">of</span> <span class="n">the</span> <span class="n">objects</span> <span class="n">are</span> <span class="n">smaller</span> <span class="n">objects</span><span class="p">,</span> <span class="n">such</span> <span class="k">as</span> <span class="n">the</span> <span class="n">five</span> <span class="n">dwarf</span> <span class="n">planets</span> <span class="ow">and</span> <span class="n">small</span> <span class="n">solar</span> <span class="n">system</span> <span class="n">bodies</span><span class="o">.</span><span class="s1">&#39;]</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">HammingDiversityLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] that enforces diverse beam search.</span>

<span class="sd">    Note that this logits processor is only effective for [`PreTrainedModel.group_beam_search`]. See [Diverse Beam</span>
<span class="sd">    Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf) for more</span>
<span class="sd">    details.</span>

<span class="sd">    Traditional beam search often generates very similar sequences across different beams.</span>
<span class="sd">    `HammingDiversityLogitsProcessor` addresses this by penalizing beams that generate tokens already chosen by other</span>
<span class="sd">    beams in the same time step.</span>

<span class="sd">    Args:</span>
<span class="sd">        diversity_penalty (`float`):</span>
<span class="sd">            This value is subtracted from a beam&#39;s score if it generates a token same as any beam from other group at a</span>
<span class="sd">            particular time. A higher `diversity_penalty` will enforce greater diversity among the beams. Adjusting</span>
<span class="sd">            this value can help strike a balance between diversity and natural likelihood.</span>
<span class="sd">        num_beams (`int`):</span>
<span class="sd">            Number of beams for beam search. 1 means no beam search.</span>
<span class="sd">        num_beam_groups (`int`):</span>
<span class="sd">            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.</span>
<span class="sd">            [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForSeq2SeqLM</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>

<span class="sd">    &gt;&gt;&gt; # Initialize the model and tokenizer</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;google-t5/t5-base&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForSeq2SeqLM.from_pretrained(&quot;google-t5/t5-base&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # A long text about the solar system</span>
<span class="sd">    &gt;&gt;&gt; text = (</span>
<span class="sd">    ...     &quot;The Solar System is a gravitationally bound system comprising the Sun and the objects that orbit it, &quot;</span>
<span class="sd">    ...     &quot;either directly or indirectly. Of the objects that orbit the Sun directly, the largest are the eight &quot;</span>
<span class="sd">    ...     &quot;planets, with the remainder being smaller objects, such as the five dwarf planets and small Solar System &quot;</span>
<span class="sd">    ...     &quot;bodies. The Solar System formed 4.6 billion years ago from the gravitational collapse of a giant &quot;</span>
<span class="sd">    ...     &quot;interstellar molecular cloud.&quot;</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;summarize: &quot; + text, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # Generate diverse summary</span>
<span class="sd">    &gt;&gt;&gt; outputs_diverse = model.generate(</span>
<span class="sd">    ...     **inputs,</span>
<span class="sd">    ...     num_beam_groups=2,</span>
<span class="sd">    ...     diversity_penalty=10.0,</span>
<span class="sd">    ...     max_length=100,</span>
<span class="sd">    ...     num_beams=4,</span>
<span class="sd">    ...     num_return_sequences=2,</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; summaries_diverse = tokenizer.batch_decode(outputs_diverse, skip_special_tokens=True)</span>

<span class="sd">    &gt;&gt;&gt; # Generate non-diverse summary</span>
<span class="sd">    &gt;&gt;&gt; outputs_non_diverse = model.generate(</span>
<span class="sd">    ...     **inputs,</span>
<span class="sd">    ...     max_length=100,</span>
<span class="sd">    ...     num_beams=4,</span>
<span class="sd">    ...     num_return_sequences=2,</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; summary_non_diverse = tokenizer.batch_decode(outputs_non_diverse, skip_special_tokens=True)</span>

<span class="sd">    &gt;&gt;&gt; # With `diversity_penalty`, the resulting beams are much more diverse</span>
<span class="sd">    &gt;&gt;&gt; print(summary_non_diverse)</span>
<span class="sd">    [&#39;the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.&#39;,</span>
<span class="sd">    &#39;the Solar System formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.&#39;]</span>

<span class="sd">    &gt;&gt;&gt; print(summaries_diverse)</span>
<span class="sd">    [&#39;the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.&#39;,</span>
<span class="sd">    &#39;the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.</span>
<span class="sd">    the rest of the objects are smaller objects, such as the five dwarf planets and small solar system bodies.&#39;]</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">diversity_penalty</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">diversity_penalty</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">diversity_penalty</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`diversity_penalty` should be a float strictly larger than 0.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_diversity_penalty</span> <span class="o">=</span> <span class="n">diversity_penalty</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">num_beams</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`num_beams` should be an integer strictly larger than 1.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span> <span class="o">=</span> <span class="n">num_beams</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_beam_groups</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">num_beam_groups</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`num_beam_groups` should be an integer strictly larger than 1.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_beam_groups</span> <span class="o">&gt;</span> <span class="n">num_beams</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`beam_groups` has to be smaller or equal to `num_beams`.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_sub_beams</span> <span class="o">=</span> <span class="n">num_beams</span> <span class="o">//</span> <span class="n">num_beam_groups</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">current_tokens</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">beam_group_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            scores (`mindspore.Tensor` of shape `(batch_size, config.vocab_size)`):</span>
<span class="sd">                Prediction scores of a language modeling head. These can be logits for each vocabulary when not using</span>
<span class="sd">                beam search or log softmax for each vocabulary token when using beam search</span>
<span class="sd">            current_tokens (`mindspore.Tensor` of shape `(batch_size)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary, corresponding to the tokens selected by the other</span>
<span class="sd">                beam groups in the current generation step.</span>
<span class="sd">            beam_group_idx (`int`):</span>
<span class="sd">                The index of the beam group currently being processed.</span>

<span class="sd">        Return:</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size, config.vocab_size)`:</span>
<span class="sd">                The processed prediction scores.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># hamming diversity: penalise using same token in current group which was used in previous groups at</span>
        <span class="c1"># the same time step</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">current_tokens</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span>
        <span class="n">group_start_idx</span> <span class="o">=</span> <span class="n">beam_group_idx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_sub_beams</span>
        <span class="n">group_end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">group_start_idx</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_sub_beams</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span><span class="p">)</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">group_end_idx</span> <span class="o">-</span> <span class="n">group_start_idx</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">group_start_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span>

        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># predicted tokens of last time step of previous groups</span>
            <span class="n">previous_group_tokens</span> <span class="o">=</span> <span class="n">current_tokens</span><span class="p">[</span>
                <span class="n">batch_idx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span> <span class="p">:</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span> <span class="o">+</span> <span class="n">group_start_idx</span>
            <span class="p">]</span>
            <span class="n">token_frequency</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">previous_group_tokens</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
            <span class="n">scores_processed</span><span class="p">[</span><span class="n">batch_idx</span> <span class="o">*</span> <span class="n">group_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">group_size</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_diversity_penalty</span> <span class="o">*</span> <span class="n">token_frequency</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.generation.logits_process.HammingDiversityLogitsProcessor.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">generation</span><span class="o">.</span><span class="n">logits_process</span><span class="o">.</span><span class="n">HammingDiversityLogitsProcessor</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">current_tokens</span><span class="p">,</span> <span class="n">beam_group_idx</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.generation.logits_process.HammingDiversityLogitsProcessor.__call__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary. <a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scores</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Prediction scores of a language modeling head. These can be logits for each vocabulary when not using
beam search or log softmax for each vocabulary token when using beam search</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, config.vocab_size)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>current_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary, corresponding to the tokens selected by the other
beam groups in the current generation step.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>beam_group_idx</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The index of the beam group currently being processed.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p><code>mindspore.Tensor</code> of shape <code>(batch_size, config.vocab_size)</code>:
    The processed prediction scores.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">current_tokens</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">beam_group_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        scores (`mindspore.Tensor` of shape `(batch_size, config.vocab_size)`):</span>
<span class="sd">            Prediction scores of a language modeling head. These can be logits for each vocabulary when not using</span>
<span class="sd">            beam search or log softmax for each vocabulary token when using beam search</span>
<span class="sd">        current_tokens (`mindspore.Tensor` of shape `(batch_size)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary, corresponding to the tokens selected by the other</span>
<span class="sd">            beam groups in the current generation step.</span>
<span class="sd">        beam_group_idx (`int`):</span>
<span class="sd">            The index of the beam group currently being processed.</span>

<span class="sd">    Return:</span>
<span class="sd">        `mindspore.Tensor` of shape `(batch_size, config.vocab_size)`:</span>
<span class="sd">            The processed prediction scores.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># hamming diversity: penalise using same token in current group which was used in previous groups at</span>
    <span class="c1"># the same time step</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">current_tokens</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span>
    <span class="n">group_start_idx</span> <span class="o">=</span> <span class="n">beam_group_idx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_sub_beams</span>
    <span class="n">group_end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">group_start_idx</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_sub_beams</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span><span class="p">)</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">group_end_idx</span> <span class="o">-</span> <span class="n">group_start_idx</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">group_start_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">scores</span>

    <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
    <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="c1"># predicted tokens of last time step of previous groups</span>
        <span class="n">previous_group_tokens</span> <span class="o">=</span> <span class="n">current_tokens</span><span class="p">[</span>
            <span class="n">batch_idx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span> <span class="p">:</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span> <span class="o">+</span> <span class="n">group_start_idx</span>
        <span class="p">]</span>
        <span class="n">token_frequency</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">previous_group_tokens</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
        <span class="n">scores_processed</span><span class="p">[</span><span class="n">batch_idx</span> <span class="o">*</span> <span class="n">group_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">group_size</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_diversity_penalty</span> <span class="o">*</span> <span class="n">token_frequency</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.InfNanRemoveLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.InfNanRemoveLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.InfNanRemoveLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that removes all <code>nan</code> and <code>inf</code> values to avoid the generation method to fail. Note that using
the logits processor should only be used if necessary since it can slow down the generation method.</p>
<p>This logits processor has no <code>generate</code> example, as there shouldn't be a correct combination of flags that warrants
its use.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">InfNanRemoveLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] that removes all `nan` and `inf` values to avoid the generation method to fail. Note that using</span>
<span class="sd">    the logits processor should only be used if necessary since it can slow down the generation method.</span>

<span class="sd">    This logits processor has no `generate` example, as there shouldn&#39;t be a correct combination of flags that warrants</span>
<span class="sd">    its use.</span>
<span class="sd">    &quot;&quot;&quot;</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># set all nan values to 0.0</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">scores</span> <span class="o">!=</span> <span class="n">scores</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span> <span class="c1"># pylint: disable=comparison-with-itself</span>

        <span class="c1"># set all +/-inf values to max/min possible value</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">scores</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">),</span> <span class="n">scores_processed</span><span class="p">)</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">scores</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">),</span> <span class="n">scores_processed</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.LogitNormalization" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.LogitNormalization</code>


<a href="#mindnlp.transformers.generation.logits_process.LogitNormalization" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code>, <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsWarper" href="#mindnlp.transformers.generation.logits_process.LogitsWarper">LogitsWarper</a></code></p>


        <p>[<code>LogitsWarper</code>] and [<code>LogitsProcessor</code>] for normalizing the scores using log-softmax. It's important to normalize
the scores during beam search, after applying the logits processors or warpers, since the search algorithm used in
this library doesn't do it (it only does it before, but they may need re-normalization) but it still supposes that
the scores are normalized when comparing the hypotheses.</p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torch</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;A sequence: 1, 2, 3&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># By default, the scores are not normalized -- the sum of their exponentials is NOT a normalized probability</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># distribution, summing to 1</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mf">1.000</span><span class="p">,)),</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span>
<span class="kc">False</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Normalizing them may have a positive impact on beam methods, or when using the scores on your application</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">renormalize_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mf">1.000</span><span class="p">,)),</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span>
<span class="kc">True</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LogitNormalization</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">,</span> <span class="n">LogitsWarper</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsWarper`] and [`LogitsProcessor`] for normalizing the scores using log-softmax. It&#39;s important to normalize</span>
<span class="sd">    the scores during beam search, after applying the logits processors or warpers, since the search algorithm used in</span>
<span class="sd">    this library doesn&#39;t do it (it only does it before, but they may need re-normalization) but it still supposes that</span>
<span class="sd">    the scores are normalized when comparing the hypotheses.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;A sequence: 1, 2, 3&quot;, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # By default, the scores are not normalized -- the sum of their exponentials is NOT a normalized probability</span>
<span class="sd">    &gt;&gt;&gt; # distribution, summing to 1</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)</span>
<span class="sd">    &gt;&gt;&gt; print(ops.allclose(ops.sum(ops.exp(outputs.scores[-1])), mindspore.Tensor((1.000,)), rtol=1e-4))</span>
<span class="sd">    False</span>

<span class="sd">    &gt;&gt;&gt; # Normalizing them may have a positive impact on beam methods, or when using the scores on your application</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, renormalize_logits=True, return_dict_in_generate=True, output_scores=True)</span>
<span class="sd">    &gt;&gt;&gt; print(ops.allclose(ops.sum(ops.exp(outputs.scores[-1])), mindspore.Tensor((1.000,)), rtol=1e-4))</span>
<span class="sd">    True</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.LogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.LogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.LogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>Abstract base class for all logit processors that can be applied during generation.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LogitsProcessor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract base class for all logit processors that can be applied during generation.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2"> is an abstract class. Only classes inheriting this class can be called.&quot;</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.LogitsProcessorList" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.LogitsProcessorList</code>


<a href="#mindnlp.transformers.generation.logits_process.LogitsProcessorList" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code>list</code></p>


        <p>This class can be used to create a list of [<code>LogitsProcessor</code>] or [<code>LogitsWarper</code>] to subsequently process a
<code>scores</code> input tensor. This class inherits from list and adds a specific <em><strong>call</strong></em> method to apply each
[<code>LogitsProcessor</code>] or [<code>LogitsWarper</code>] to the inputs.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LogitsProcessorList</span><span class="p">(</span><span class="nb">list</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class can be used to create a list of [`LogitsProcessor`] or [`LogitsWarper`] to subsequently process a</span>
<span class="sd">    `scores` input tensor. This class inherits from list and adds a specific *__call__* method to apply each</span>
<span class="sd">    [`LogitsProcessor`] or [`LogitsWarper`] to the inputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            scores (`mindspore.Tensor` of shape `(batch_size, config.vocab_size)`):</span>
<span class="sd">                Prediction scores of a language modeling head. These can be logits for each vocabulary when not using</span>
<span class="sd">                beam search or log softmax for each vocabulary token when using beam search</span>
<span class="sd">            kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">                Additional kwargs that are specific to a logits processor.</span>

<span class="sd">        Return:</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size, config.vocab_size)`:</span>
<span class="sd">                The processed prediction scores.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">processor</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
            <span class="n">function_args</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">processor</span><span class="o">.</span><span class="fm">__call__</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">function_args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">arg</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">function_args</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">2</span><span class="p">:]):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Make sure that all the required parameters: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">function_args</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2"> for &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">processor</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2"> are passed to the logits processor.&quot;</span>
                    <span class="p">)</span>
                <span class="n">scores</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">scores</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.generation.logits_process.LogitsProcessorList.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">generation</span><span class="o">.</span><span class="n">logits_process</span><span class="o">.</span><span class="n">LogitsProcessorList</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.generation.logits_process.LogitsProcessorList.__call__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary. <a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scores</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Prediction scores of a language modeling head. These can be logits for each vocabulary when not using
beam search or log softmax for each vocabulary token when using beam search</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, config.vocab_size)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional kwargs that are specific to a logits processor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Any]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p><code>mindspore.Tensor</code> of shape <code>(batch_size, config.vocab_size)</code>:
    The processed prediction scores.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        scores (`mindspore.Tensor` of shape `(batch_size, config.vocab_size)`):</span>
<span class="sd">            Prediction scores of a language modeling head. These can be logits for each vocabulary when not using</span>
<span class="sd">            beam search or log softmax for each vocabulary token when using beam search</span>
<span class="sd">        kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">            Additional kwargs that are specific to a logits processor.</span>

<span class="sd">    Return:</span>
<span class="sd">        `mindspore.Tensor` of shape `(batch_size, config.vocab_size)`:</span>
<span class="sd">            The processed prediction scores.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">processor</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
        <span class="n">function_args</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">processor</span><span class="o">.</span><span class="fm">__call__</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">function_args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">arg</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">function_args</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">2</span><span class="p">:]):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Make sure that all the required parameters: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">function_args</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2"> for &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">processor</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2"> are passed to the logits processor.&quot;</span>
                <span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.LogitsWarper" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.LogitsWarper</code>


<a href="#mindnlp.transformers.generation.logits_process.LogitsWarper" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>Abstract base class for all logit warpers that can be applied during generation with multinomial sampling.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LogitsWarper</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract base class for all logit warpers that can be applied during generation with multinomial sampling.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2"> is an abstract class. Only classes inheriting this class can be called.&quot;</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.MinLengthLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.MinLengthLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.MinLengthLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] enforcing a min-length by setting EOS probability to 0. Note that, for decoder-only models
like most LLMs, the length includes the prompt.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>min_length</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The minimum length below which the score of <code>eos_token_id</code> is set to <code>-float("Inf")</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id(s) of the <em>end-of-sequence</em> token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[int, List[int], mindspore.Tensor]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;A number:&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">gen_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">gen_out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">number</span><span class="p">:</span> <span class="n">one</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># setting `min_length` to a value smaller than the uncontrolled output length has no impact</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">gen_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">min_length</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">gen_out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">number</span><span class="p">:</span> <span class="n">one</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># setting a larger `min_length` will force the model to generate beyond its natural ending point, which is not</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># necessarily incorrect</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">gen_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">min_length</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">gen_out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">number</span><span class="p">:</span> <span class="n">one</span> <span class="n">thousand</span><span class="p">,</span> <span class="n">nine</span> <span class="n">hundred</span> <span class="ow">and</span> <span class="n">ninety</span><span class="o">-</span><span class="n">four</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MinLengthLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] enforcing a min-length by setting EOS probability to 0. Note that, for decoder-only models</span>
<span class="sd">    like most LLMs, the length includes the prompt.</span>

<span class="sd">    Args:</span>
<span class="sd">        min_length (`int`):</span>
<span class="sd">            The minimum length below which the score of `eos_token_id` is set to `-float(&quot;Inf&quot;)`.</span>
<span class="sd">        eos_token_id (`Union[int, List[int], mindspore.Tensor]`):</span>
<span class="sd">            The id(s) of the *end-of-sequence* token.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer</span>

<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;A number:&quot;, return_tensors=&quot;ms&quot;)</span>
<span class="sd">    &gt;&gt;&gt; gen_out = model.generate(**inputs)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])</span>
<span class="sd">    A number: one</span>

<span class="sd">    &gt;&gt;&gt; # setting `min_length` to a value smaller than the uncontrolled output length has no impact</span>
<span class="sd">    &gt;&gt;&gt; gen_out = model.generate(**inputs, min_length=3)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])</span>
<span class="sd">    A number: one</span>

<span class="sd">    &gt;&gt;&gt; # setting a larger `min_length` will force the model to generate beyond its natural ending point, which is not</span>
<span class="sd">    &gt;&gt;&gt; # necessarily incorrect</span>
<span class="sd">    &gt;&gt;&gt; gen_out = model.generate(**inputs, min_length=10)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])</span>
<span class="sd">    A number: one thousand, nine hundred and ninety-four</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">min_length</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`min_length` has to be a non-negative integer, but is </span><span class="si">{</span><span class="n">min_length</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">eos_token_id</span> <span class="o">=</span> <span class="p">[</span><span class="n">eos_token_id</span><span class="p">]</span>
            <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">min_length</span> <span class="o">=</span> <span class="n">min_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">vocab_tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">eos_token_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">vocab_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="k">if</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_length</span><span class="p">:</span>
            <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">eos_token_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">),</span> <span class="n">scores</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.MinNewTokensLengthLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.MinNewTokensLengthLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.MinNewTokensLengthLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] enforcing a min-length of new tokens by setting EOS (End-Of-Sequence) token probability to 0.
Contrarily to [<code>MinLengthLogitsProcessor</code>], this processor ignores the prompt.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prompt_length_to_skip</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tokens length. Not a valid argument when used with <code>generate</code> as it will automatically assign the
input length.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_new_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The minimum <em>new</em> tokens length below which the score of <code>eos_token_id</code> is set to <code>-float("Inf")</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id(s) of the <em>end-of-sequence</em> token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[int, List[int], mindspore.Tensor]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;A number:&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">gen_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">gen_out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">number</span><span class="p">:</span> <span class="n">one</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># setting `min_new_tokens` will force the model to generate beyond its natural ending point, which is not</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># necessarily incorrect</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">gen_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">min_new_tokens</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">gen_out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">number</span><span class="p">:</span> <span class="n">one</span> <span class="n">thousand</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MinNewTokensLengthLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] enforcing a min-length of new tokens by setting EOS (End-Of-Sequence) token probability to 0.</span>
<span class="sd">    Contrarily to [`MinLengthLogitsProcessor`], this processor ignores the prompt.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt_length_to_skip (`int`):</span>
<span class="sd">            The input tokens length. Not a valid argument when used with `generate` as it will automatically assign the</span>
<span class="sd">            input length.</span>
<span class="sd">        min_new_tokens (`int`):</span>
<span class="sd">            The minimum *new* tokens length below which the score of `eos_token_id` is set to `-float(&quot;Inf&quot;)`.</span>
<span class="sd">        eos_token_id (`Union[int, List[int], mindspore.Tensor]`):</span>
<span class="sd">            The id(s) of the *end-of-sequence* token.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer</span>

<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer([&quot;A number:&quot;], return_tensors=&quot;ms&quot;)</span>
<span class="sd">    &gt;&gt;&gt; gen_out = model.generate(**inputs)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])</span>
<span class="sd">    A number: one</span>

<span class="sd">    &gt;&gt;&gt; # setting `min_new_tokens` will force the model to generate beyond its natural ending point, which is not</span>
<span class="sd">    &gt;&gt;&gt; # necessarily incorrect</span>
<span class="sd">    &gt;&gt;&gt; gen_out = model.generate(**inputs, min_new_tokens=2)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])</span>
<span class="sd">    A number: one thousand</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt_length_to_skip</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">min_new_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="k">for</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="p">(</span><span class="s2">&quot;prompt_length_to_skip&quot;</span><span class="p">,</span> <span class="n">prompt_length_to_skip</span><span class="p">),</span>
            <span class="p">(</span><span class="s2">&quot;min_new_tokens&quot;</span><span class="p">,</span> <span class="n">min_new_tokens</span><span class="p">),</span>
        <span class="p">]:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">arg_value</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">` has to be a positive integer, but is </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">eos_token_id</span> <span class="o">=</span> <span class="p">[</span><span class="n">eos_token_id</span><span class="p">]</span>
            <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prompt_length_to_skip</span> <span class="o">=</span> <span class="n">prompt_length_to_skip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_new_tokens</span> <span class="o">=</span> <span class="n">min_new_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">new_tokens_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompt_length_to_skip</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="n">vocab_tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">eos_token_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">vocab_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_tokens_length</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_new_tokens</span><span class="p">:</span>
            <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">eos_token_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">),</span> <span class="n">scores</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.MinPLogitsWarper" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.MinPLogitsWarper</code>


<a href="#mindnlp.transformers.generation.logits_process.MinPLogitsWarper" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsWarper" href="#mindnlp.transformers.generation.logits_process.LogitsWarper">LogitsWarper</a></code></p>


        <p>[<code>LogitsWarper</code>] that performs min-p, i.e. keeps all tokens that are above a minimum probability, scaled by the
probability of the most likely token. As a result, the filter becomes more agressive in the presence of
high-probability tokens, which is a sign of a confident output that we shouldn't deviate from.</p>
<p>Often used together with [<code>TemperatureLogitsWarper</code>]. Used as an alternative to [<code>TopPLogitsWarper</code>] and
[<code>TopKLogitsWarper</code>].</p>
<p>Created by @menhguin and @kalomaze (github handles). Code adapted from <a href="https://github.com/oobabooga/text-generation-webui/pull/4449/files">this external PR</a></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>min_p</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Minimum token probability, which will be scaled by the probability of the most likely token. It must be a
value between 0 and 1. Typical values are in the 0.01-0.2 range, comparably selective as setting <code>top_p</code> in
the 0.99-0.8 range (use the opposite of normal <code>top_p</code> values).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>filter_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>All filtered values will be set to this float value.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to -inf</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>-float(&#39;Inf&#39;)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_tokens_to_keep</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Minimum number of tokens that cannot be filtered.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;A sequence: 1, 2&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With sampling, the output is unexpected -- sometimes too unexpected.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="o">|</span> <span class="o">&lt;</span> <span class="mi">4</span> <span class="p">(</span><span class="n">left</span><span class="o">-</span><span class="n">hand</span> <span class="n">pointer</span><span class="p">)</span> <span class="p">;</span>
<span class="o">&lt;</span><span class="n">BLANKLINE</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">BLANKLINE</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With `min_p` sampling, the output gets restricted to high-probability tokens.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Pro tip: In practice, LLMs use `min_p` in the 0.01-0.2 range.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">min_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MinPLogitsWarper</span><span class="p">(</span><span class="n">LogitsWarper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsWarper`] that performs min-p, i.e. keeps all tokens that are above a minimum probability, scaled by the</span>
<span class="sd">    probability of the most likely token. As a result, the filter becomes more agressive in the presence of</span>
<span class="sd">    high-probability tokens, which is a sign of a confident output that we shouldn&#39;t deviate from.</span>

<span class="sd">    Often used together with [`TemperatureLogitsWarper`]. Used as an alternative to [`TopPLogitsWarper`] and</span>
<span class="sd">    [`TopKLogitsWarper`].</span>

<span class="sd">    Created by @menhguin and @kalomaze (github handles). Code adapted from [this external PR](https://github.com/oobabooga/text-generation-webui/pull/4449/files)</span>

<span class="sd">    Args:</span>
<span class="sd">        min_p (`float`):</span>
<span class="sd">            Minimum token probability, which will be scaled by the probability of the most likely token. It must be a</span>
<span class="sd">            value between 0 and 1. Typical values are in the 0.01-0.2 range, comparably selective as setting `top_p` in</span>
<span class="sd">            the 0.99-0.8 range (use the opposite of normal `top_p` values).</span>
<span class="sd">        filter_value (`float`, *optional*, defaults to -inf):</span>
<span class="sd">            All filtered values will be set to this float value.</span>
<span class="sd">        min_tokens_to_keep (`int`, *optional*, defaults to 1):</span>
<span class="sd">            Minimum number of tokens that cannot be filtered.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed</span>

<span class="sd">    &gt;&gt;&gt; set_seed(1)</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;A sequence: 1, 2&quot;, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # With sampling, the output is unexpected -- sometimes too unexpected.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    A sequence: 1, 2, 3 | &lt; 4 (left-hand pointer) ;</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>

<span class="sd">    &gt;&gt;&gt; # With `min_p` sampling, the output gets restricted to high-probability tokens.</span>
<span class="sd">    &gt;&gt;&gt; # Pro tip: In practice, LLMs use `min_p` in the 0.01-0.2 range.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True, min_p=0.1)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">),</span> <span class="n">min_tokens_to_keep</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">min_p</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`min_p` has to be a float in the [0, 1] interval, but is </span><span class="si">{</span><span class="n">min_p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_tokens_to_keep</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">min_tokens_to_keep</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`min_tokens_to_keep` has to be a positive integer, but is </span><span class="si">{</span><span class="n">min_tokens_to_keep</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">min_p</span> <span class="o">=</span> <span class="n">min_p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="n">filter_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span> <span class="o">=</span> <span class="n">min_tokens_to_keep</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
        <span class="c1"># Convert logits to probabilities</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Get the probability of the top token for each sequence in the batch</span>
        <span class="n">top_probs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># Calculate the actual min_p threshold by scaling min_p with the top token&#39;s probability</span>
        <span class="n">scaled_min_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_p</span> <span class="o">*</span> <span class="n">top_probs</span>
        <span class="c1"># Create a mask for tokens that have a probability less than the scaled min_p</span>
        <span class="n">tokens_to_remove</span> <span class="o">=</span> <span class="n">probs</span> <span class="o">&lt;</span> <span class="n">scaled_min_p</span>

        <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tokens_to_remove</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sorted_indices</span><span class="p">)</span>
        <span class="c1"># Keep at least min_tokens_to_keep</span>
        <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">sorted_indices_to_remove</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">,</span> <span class="n">sorted_indices_to_remove</span><span class="p">)</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">indices_to_remove</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.NoBadWordsLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.NoBadWordsLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.NoBadWordsLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.SequenceBiasLogitsProcessor" href="#mindnlp.transformers.generation.logits_process.SequenceBiasLogitsProcessor">SequenceBiasLogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that enforces that specified sequences will never be selected.</p>
<p><Tip></p>
<p>In order to get the token ids of the words that should not appear in the generated text, make sure to set
<code>add_prefix_space=True</code> when initializing the tokenizer, and use <code>tokenizer(bad_words,
add_special_tokens=False).input_ids</code>. The <code>add_prefix_space</code> argument is only supported for some slow tokenizers,
as fast tokenizers' prefixing behaviours come from <code>pre tokenizers</code>. Read more
<a href="https://huggingface.co/docs/tokenizers/api/pre-tokenizers">here</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>bad_words_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of list of token ids that are not allowed to be generated.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[List[int]]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id(s) of the <em>end-of-sequence</em> token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[int, List[int], mindspore.Tensor]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;In a word, the cake is a&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">output_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">output_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">In</span> <span class="n">a</span> <span class="n">word</span><span class="p">,</span> <span class="n">the</span> <span class="n">cake</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">bit</span> <span class="n">of</span> <span class="n">a</span> <span class="n">mess</span><span class="o">.</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Now let&#39;s take the bad words out. Please note that the tokenizer is initialized differently</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer_with_prefix_space</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">,</span> <span class="n">add_prefix_space</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">get_tokens_as_list</span><span class="p">(</span><span class="n">word_list</span><span class="p">):</span>
<span class="o">...</span>     <span class="s2">&quot;Converts a sequence of words into a list of tokens&quot;</span>
<span class="o">...</span>     <span class="n">tokens_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="o">...</span>     <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_list</span><span class="p">:</span>
<span class="o">...</span>         <span class="n">tokenized_word</span> <span class="o">=</span> <span class="n">tokenizer_with_prefix_space</span><span class="p">([</span><span class="n">word</span><span class="p">],</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="o">...</span>         <span class="n">tokens_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokenized_word</span><span class="p">)</span>
<span class="o">...</span>     <span class="k">return</span> <span class="n">tokens_list</span>


<span class="o">&gt;&gt;&gt;</span> <span class="n">bad_words_ids</span> <span class="o">=</span> <span class="n">get_tokens_as_list</span><span class="p">(</span><span class="n">word_list</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mess&quot;</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">output_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="o">...</span>     <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">bad_words_ids</span><span class="o">=</span><span class="n">bad_words_ids</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">output_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">In</span> <span class="n">a</span> <span class="n">word</span><span class="p">,</span> <span class="n">the</span> <span class="n">cake</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">bit</span> <span class="n">of</span> <span class="n">a</span> <span class="n">surprise</span><span class="o">.</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">NoBadWordsLogitsProcessor</span><span class="p">(</span><span class="n">SequenceBiasLogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] that enforces that specified sequences will never be selected.</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    In order to get the token ids of the words that should not appear in the generated text, make sure to set</span>
<span class="sd">    `add_prefix_space=True` when initializing the tokenizer, and use `tokenizer(bad_words,</span>
<span class="sd">    add_special_tokens=False).input_ids`. The `add_prefix_space` argument is only supported for some slow tokenizers,</span>
<span class="sd">    as fast tokenizers&#39; prefixing behaviours come from `pre tokenizers`. Read more</span>
<span class="sd">    [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        bad_words_ids (`List[List[int]]`):</span>
<span class="sd">            List of list of token ids that are not allowed to be generated.</span>
<span class="sd">        eos_token_id (`Union[int, List[int], mindspore.Tensor]`, *optional*):</span>
<span class="sd">            The id(s) of the *end-of-sequence* token.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; inputs = tokenizer([&quot;In a word, the cake is a&quot;], return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; output_ids = model.generate(inputs[&quot;input_ids&quot;], max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0])</span>
<span class="sd">    In a word, the cake is a bit of a mess.</span>

<span class="sd">    &gt;&gt;&gt; # Now let&#39;s take the bad words out. Please note that the tokenizer is initialized differently</span>
<span class="sd">    &gt;&gt;&gt; tokenizer_with_prefix_space = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2&quot;, add_prefix_space=True)</span>


<span class="sd">    &gt;&gt;&gt; def get_tokens_as_list(word_list):</span>
<span class="sd">    ...     &quot;Converts a sequence of words into a list of tokens&quot;</span>
<span class="sd">    ...     tokens_list = []</span>
<span class="sd">    ...     for word in word_list:</span>
<span class="sd">    ...         tokenized_word = tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0]</span>
<span class="sd">    ...         tokens_list.append(tokenized_word)</span>
<span class="sd">    ...     return tokens_list</span>


<span class="sd">    &gt;&gt;&gt; bad_words_ids = get_tokens_as_list(word_list=[&quot;mess&quot;])</span>
<span class="sd">    &gt;&gt;&gt; output_ids = model.generate(</span>
<span class="sd">    ...     inputs[&quot;input_ids&quot;], max_new_tokens=5, bad_words_ids=bad_words_ids, pad_token_id=tokenizer.eos_token_id</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0])</span>
<span class="sd">    In a word, the cake is a bit of a surprise.</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">bad_words_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bad_word_ids</span> <span class="o">=</span> <span class="n">bad_words_ids</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_arguments</span><span class="p">()</span>

        <span class="c1"># Filter EOS token from bad_words_ids</span>
        <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                    <span class="n">eos_token_id</span> <span class="o">=</span> <span class="p">[</span><span class="n">eos_token_id</span><span class="p">]</span>
                <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span>

            <span class="n">bad_words_ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
                <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">bad_token_seq</span><span class="p">:</span> <span class="nb">all</span><span class="p">(</span><span class="n">bad_token_seq</span> <span class="o">!=</span> <span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">eos_token_id</span><span class="p">),</span> <span class="n">bad_words_ids</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Forbidding a sequence is equivalent to setting its bias to -inf</span>
        <span class="n">sequence_bias</span> <span class="o">=</span> <span class="p">{</span><span class="nb">tuple</span><span class="p">(</span><span class="n">sequence</span><span class="p">):</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="n">bad_words_ids</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">sequence_bias</span><span class="o">=</span><span class="n">sequence_bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_arguments</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">bad_words_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bad_word_ids</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bad_words_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">bad_words_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`bad_words_ids` has to be a non-empty list, but is </span><span class="si">{</span><span class="n">bad_words_ids</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bad_word_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">bad_word_ids</span> <span class="ow">in</span> <span class="n">bad_words_ids</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`bad_words_ids` has to be a list of lists, but is </span><span class="si">{</span><span class="n">bad_words_ids</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
            <span class="nb">any</span><span class="p">((</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_id</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">))</span> <span class="ow">or</span> <span class="n">token_id</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">bad_word_ids</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">bad_word_ids</span> <span class="ow">in</span> <span class="n">bad_words_ids</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Each list in `bad_words_ids` has to be a list of positive integers, but is </span><span class="si">{</span><span class="n">bad_words_ids</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.NoRepeatNGramLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.NoRepeatNGramLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.NoRepeatNGramLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>N-grams are groups of "n" consecutive words, characters, or tokens taken from a sequence of text. Given the
sentence: "She runs fast", the bi-grams (n=2) would be ("she", "runs") and ("runs", "fast"). In text generation,
avoiding repetitions of word sequences provides a more diverse output. This [<code>LogitsProcessor</code>] enforces no
repetition of n-grams by setting the scores of banned tokens to negative infinity which eliminates those tokens
from consideration when further processing the scores. Note that, for decoder-only models like most LLMs, the
prompt is also considered to obtain the n-grams.
<a href="https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345">Fairseq</a>.</p>
<p><Tip></p>
<p>Use n-gram penalties with care. For instance, penalizing 2-grams (bigrams) in an article about the city of New York
might lead to undesirable outcomes where the city's name appears only once in the entire text.
<a href="https://huggingface.co/blog/how-to-generate">Reference</a></p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>ngram_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>All ngrams of size <code>ngram_size</code> can only occur once.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Today I&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">Today</span> <span class="n">I</span><span class="err">’</span><span class="n">m</span> <span class="ow">not</span> <span class="n">sure</span> <span class="k">if</span> <span class="n">I</span><span class="err">’</span><span class="n">m</span> <span class="n">going</span> <span class="n">to</span> <span class="n">be</span> <span class="n">able</span> <span class="n">to</span> <span class="n">do</span> <span class="n">it</span><span class="o">.</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Now let&#39;s add ngram size using `no_repeat_ngram_size`. This stops the repetitions (&quot;I’m&quot;) in the output.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">Today</span> <span class="n">I</span><span class="err">’</span><span class="n">m</span> <span class="ow">not</span> <span class="n">sure</span> <span class="k">if</span> <span class="n">I</span> <span class="n">can</span> <span class="n">get</span> <span class="n">a</span> <span class="n">better</span> <span class="n">understanding</span> <span class="n">of</span> <span class="n">the</span> <span class="n">nature</span> <span class="n">of</span> <span class="n">this</span> <span class="n">issue</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">NoRepeatNGramLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N-grams are groups of &quot;n&quot; consecutive words, characters, or tokens taken from a sequence of text. Given the</span>
<span class="sd">    sentence: &quot;She runs fast&quot;, the bi-grams (n=2) would be (&quot;she&quot;, &quot;runs&quot;) and (&quot;runs&quot;, &quot;fast&quot;). In text generation,</span>
<span class="sd">    avoiding repetitions of word sequences provides a more diverse output. This [`LogitsProcessor`] enforces no</span>
<span class="sd">    repetition of n-grams by setting the scores of banned tokens to negative infinity which eliminates those tokens</span>
<span class="sd">    from consideration when further processing the scores. Note that, for decoder-only models like most LLMs, the</span>
<span class="sd">    prompt is also considered to obtain the n-grams.</span>
<span class="sd">    [Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345).</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    Use n-gram penalties with care. For instance, penalizing 2-grams (bigrams) in an article about the city of New York</span>
<span class="sd">    might lead to undesirable outcomes where the city&#39;s name appears only once in the entire text.</span>
<span class="sd">    [Reference](https://huggingface.co/blog/how-to-generate)</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        ngram_size (`int`):</span>
<span class="sd">            All ngrams of size `ngram_size` can only occur once.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```py</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; inputs = tokenizer([&quot;Today I&quot;], return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; output = model.generate(**inputs)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.decode(output[0], skip_special_tokens=True))</span>
<span class="sd">    Today I’m not sure if I’m going to be able to do it.</span>

<span class="sd">    &gt;&gt;&gt; # Now let&#39;s add ngram size using `no_repeat_ngram_size`. This stops the repetitions (&quot;I’m&quot;) in the output.</span>
<span class="sd">    &gt;&gt;&gt; output = model.generate(**inputs, no_repeat_ngram_size=2)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.decode(output[0], skip_special_tokens=True))</span>
<span class="sd">    Today I’m not sure if I can get a better understanding of the nature of this issue</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ngram_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ngram_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">ngram_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`ngram_size` has to be a strictly positive integer, but is </span><span class="si">{</span><span class="n">ngram_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ngram_size</span> <span class="o">=</span> <span class="n">ngram_size</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">num_batch_hypotheses</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="n">banned_batch_tokens</span> <span class="o">=</span> <span class="n">_calc_banned_ngram_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ngram_size</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">num_batch_hypotheses</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">banned_tokens</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">banned_batch_tokens</span><span class="p">):</span>
            <span class="n">scores_processed</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">banned_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.PrefixConstrainedLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.PrefixConstrainedLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.PrefixConstrainedLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that enforces constrained generation and is useful for prefix-conditioned constrained
generation. See <a href="https://arxiv.org/abs/2010.00904">Autoregressive Entity Retrieval</a> for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prefix_allowed_tokens_fn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>This function constraints the beam search to allowed tokens only at each step. This function takes 2
arguments <code>inputs_ids</code> and the batch ID <code>batch_id</code>. It has to return a list with the allowed tokens for the
next generation step conditioned on the previously generated tokens <code>inputs_ids</code> and the batch ID
<code>batch_id</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable[[int, mindspore.Tensor], List[int]]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Alice and Bob&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># By default, it continues generating according to the model&#39;s logits</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Alice</span> <span class="ow">and</span> <span class="n">Bob</span> <span class="n">are</span> <span class="n">friends</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># We can contrain it with `prefix_allowed_tokens_fn` to force a certain behavior based on a prefix.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># For instance, we can force an entire entity to be generated when its beginning is detected.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">entity</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot; Bob Marley&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># 3 tokens</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">prefix_allowed_tokens_fn</span><span class="p">(</span><span class="n">batch_id</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
<span class="o">...</span>     <span class="s1">&#39;&#39;&#39;</span>
<span class="s1">...     Attempts to generate &#39;Bob Marley&#39; when &#39;Bob&#39; is detected.</span>
<span class="s1">...     In this case, `batch_id` is not used, but you can set rules for each batch member.</span>
<span class="s1">...     &#39;&#39;&#39;</span>
<span class="o">...</span>     <span class="k">if</span> <span class="n">input_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">entity</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
<span class="o">...</span>         <span class="k">return</span> <span class="p">[</span><span class="n">entity</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="o">...</span>     <span class="k">elif</span> <span class="n">input_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">entity</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">input_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">entity</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
<span class="o">...</span>         <span class="k">return</span> <span class="p">[</span><span class="n">entity</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="o">...</span>     <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">))</span>  <span class="c1"># If no match, allow all tokens</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="n">prefix_allowed_tokens_fn</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Alice</span> <span class="ow">and</span> <span class="n">Bob</span> <span class="n">Marley</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">PrefixConstrainedLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] that enforces constrained generation and is useful for prefix-conditioned constrained</span>
<span class="sd">    generation. See [Autoregressive Entity Retrieval](https://arxiv.org/abs/2010.00904) for more information.</span>

<span class="sd">    Args:</span>
<span class="sd">        prefix_allowed_tokens_fn (`Callable[[int, mindspore.Tensor], List[int]]`):</span>
<span class="sd">            This function constraints the beam search to allowed tokens only at each step. This function takes 2</span>
<span class="sd">            arguments `inputs_ids` and the batch ID `batch_id`. It has to return a list with the allowed tokens for the</span>
<span class="sd">            next generation step conditioned on the previously generated tokens `inputs_ids` and the batch ID</span>
<span class="sd">            `batch_id`.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```py</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;Alice and Bob&quot;, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # By default, it continues generating according to the model&#39;s logits</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, max_new_tokens=5)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    Alice and Bob are friends</span>

<span class="sd">    &gt;&gt;&gt; # We can contrain it with `prefix_allowed_tokens_fn` to force a certain behavior based on a prefix.</span>
<span class="sd">    &gt;&gt;&gt; # For instance, we can force an entire entity to be generated when its beginning is detected.</span>
<span class="sd">    &gt;&gt;&gt; entity = tokenizer(&quot; Bob Marley&quot;, return_tensors=&quot;ms&quot;).input_ids[0]  # 3 tokens</span>
<span class="sd">    &gt;&gt;&gt; def prefix_allowed_tokens_fn(batch_id, input_ids):</span>
<span class="sd">    ...     &#39;&#39;&#39;</span>
<span class="sd">    ...     Attempts to generate &#39;Bob Marley&#39; when &#39;Bob&#39; is detected.</span>
<span class="sd">    ...     In this case, `batch_id` is not used, but you can set rules for each batch member.</span>
<span class="sd">    ...     &#39;&#39;&#39;</span>
<span class="sd">    ...     if input_ids[-1] == entity[0]:</span>
<span class="sd">    ...         return [entity[1].item()]</span>
<span class="sd">    ...     elif input_ids[-2] == entity[0] and input_ids[-1] == entity[1]:</span>
<span class="sd">    ...         return [entity[2].item()]</span>
<span class="sd">    ...     return list(range(tokenizer.vocab_size))  # If no match, allow all tokens</span>

<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, max_new_tokens=5, prefix_allowed_tokens_fn=prefix_allowed_tokens_fn)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    Alice and Bob Marley</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix_allowed_tokens_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">num_beams</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prefix_allowed_tokens_fn</span> <span class="o">=</span> <span class="n">prefix_allowed_tokens_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span> <span class="o">=</span> <span class="n">num_beams</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">batch_id</span><span class="p">,</span> <span class="n">beam_sent</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])):</span>
            <span class="k">for</span> <span class="n">beam_id</span><span class="p">,</span> <span class="n">sent</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">beam_sent</span><span class="p">):</span>
                <span class="n">prefix_allowed_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefix_allowed_tokens_fn</span><span class="p">(</span><span class="n">batch_id</span><span class="p">,</span> <span class="n">sent</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prefix_allowed_tokens</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;`prefix_allowed_tokens_fn` returned an empty list for batch ID </span><span class="si">{</span><span class="n">batch_id</span><span class="si">}</span><span class="s2">.&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;This means that the constraint is unsatisfiable. Please check your implementation&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;of `prefix_allowed_tokens_fn` &quot;</span>
                    <span class="p">)</span>
                <span class="n">mask</span><span class="p">[</span><span class="n">batch_id</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_beams</span> <span class="o">+</span> <span class="n">beam_id</span><span class="p">,</span> <span class="n">prefix_allowed_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">mask</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.RepetitionPenaltyLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.RepetitionPenaltyLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.RepetitionPenaltyLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that prevents the repetition of previous tokens through a penalty. This penalty is applied at
most once per token. Note that, for decoder-only models like most LLMs, the considered tokens include the prompt.</p>
<p>In the original <a href="https://arxiv.org/pdf/1909.05858.pdf">paper</a>, the authors suggest the use of a penalty of around
1.2 to achieve a good balance between truthful generation and lack of repetition. To penalize and reduce
repetition, use <code>penalty</code> values above 1.0, where a higher value penalizes more strongly. To reward and encourage
repetition, use <code>penalty</code> values between 0.0 and 1.0, where a lower value rewards more strongly.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>penalty</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The parameter for repetition penalty. 1.0 means no penalty. Above 1.0 penalizes previously generated
tokens. Between 0.0 and 1.0 rewards previously generated tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing the model and tokenizer for it</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;I&#39;m not going to&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># This shows a normal generate without any specific parameters</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">summary_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">summary_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">I</span><span class="s1">&#39;m not going to be able to do that. I&#39;</span><span class="n">m</span> <span class="n">going</span> <span class="n">to</span> <span class="n">be</span> <span class="n">able</span> <span class="n">to</span> <span class="n">do</span> <span class="n">that</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># This generates a penalty for repeated tokens</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">penalized_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">penalized_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">I</span><span class="s1">&#39;m not going to be able to do that. I&#39;</span><span class="n">ll</span> <span class="n">just</span> <span class="n">have</span> <span class="n">to</span> <span class="n">go</span> <span class="n">out</span> <span class="ow">and</span> <span class="n">play</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">RepetitionPenaltyLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] that prevents the repetition of previous tokens through a penalty. This penalty is applied at</span>
<span class="sd">    most once per token. Note that, for decoder-only models like most LLMs, the considered tokens include the prompt.</span>

<span class="sd">    In the original [paper](https://arxiv.org/pdf/1909.05858.pdf), the authors suggest the use of a penalty of around</span>
<span class="sd">    1.2 to achieve a good balance between truthful generation and lack of repetition. To penalize and reduce</span>
<span class="sd">    repetition, use `penalty` values above 1.0, where a higher value penalizes more strongly. To reward and encourage</span>
<span class="sd">    repetition, use `penalty` values between 0.0 and 1.0, where a lower value rewards more strongly.</span>

<span class="sd">    Args:</span>
<span class="sd">        penalty (`float`):</span>
<span class="sd">            The parameter for repetition penalty. 1.0 means no penalty. Above 1.0 penalizes previously generated</span>
<span class="sd">            tokens. Between 0.0 and 1.0 rewards previously generated tokens.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```py</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM</span>

<span class="sd">    &gt;&gt;&gt; # Initializing the model and tokenizer for it</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; inputs = tokenizer([&quot;I&#39;m not going to&quot;], return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # This shows a normal generate without any specific parameters</span>
<span class="sd">    &gt;&gt;&gt; summary_ids = model.generate(**inputs)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])</span>
<span class="sd">    I&#39;m not going to be able to do that. I&#39;m going to be able to do that</span>

<span class="sd">    &gt;&gt;&gt; # This generates a penalty for repeated tokens</span>
<span class="sd">    &gt;&gt;&gt; penalized_ids = model.generate(**inputs, repetition_penalty=1.1)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(penalized_ids, skip_special_tokens=True)[0])</span>
<span class="sd">    I&#39;m not going to be able to do that. I&#39;ll just have to go out and play</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">penalty</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">penalty</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="p">(</span><span class="n">penalty</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`penalty` has to be a strictly positive float, but is </span><span class="si">{</span><span class="n">penalty</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">=</span> <span class="n">penalty</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_score_penalties</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">)</span>

        <span class="c1"># if score &lt; 0 then repetition penalty has to be multiplied to reduce the token probabilities</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">score</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">score</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="n">score</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">)</span>

        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores_processed</span>

    <span class="k">def</span> <span class="nf">_create_score_penalties</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">logit_penalties</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tf_gather</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">logit_penalties</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">logit_penalties</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="n">logit_penalties</span><span class="p">)</span>
        <span class="n">logit_penalties</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">logit_penalties</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="n">logit_penalties</span><span class="p">)</span>

        <span class="c1"># Scatters the penalties</span>
        <span class="n">token_penalties</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># the sequence length has dynamic size, hence the dynamic shape</span>
        <span class="n">indexable_prev_input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="p">(</span><span class="n">seq_len</span><span class="p">,)),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="p">),</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">token_penalties</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tf_scatter_nd_update</span><span class="p">(</span>
            <span class="n">token_penalties</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indexable_prev_input_ids</span><span class="p">,</span> <span class="n">updates</span><span class="o">=</span><span class="n">ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logit_penalties</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="p">)</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">token_penalties</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.SequenceBiasLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.SequenceBiasLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.SequenceBiasLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that applies an additive bias on sequences. The bias is applied to the last token of a sequence
when the next generated token can complete it. Consequently, to take the most of biasing sequences with more than
one token, consider using beam methods (to gracefully work around partially completed sequences that have a
negative bias) and applying the bias to their prefixes (to ensure the bias is applied earlier).</p>
<p><Tip></p>
<p>In order to get the token ids of the sequences that you want to bias, make sure to set <code>add_prefix_space=True</code> when
initializing the tokenizer, and use <code>tokenizer(bad_words, add_special_tokens=False).input_ids</code>. The
<code>add_prefix_space</code> argument is only supported for some slow tokenizers, as fast tokenizers' prefixing behaviours
come from <code>pre tokenizers</code>. Read more <a href="https://huggingface.co/docs/tokenizers/api/pre-tokenizers">here</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sequence_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of the
sequence being selected, while negative biases do the opposite. If a sequence has a length of 1, its bias
will always be applied. Otherwise, the bias will only be applied if the sequence in question is about to be
completed (in the token selection step after this processor is applied).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[Tuple[int], float]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;The full name of Donald is Donald&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">summary_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">summary_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">The</span> <span class="n">full</span> <span class="n">name</span> <span class="n">of</span> <span class="n">Donald</span> <span class="ow">is</span> <span class="n">Donald</span> <span class="n">J</span><span class="o">.</span> <span class="n">Trump</span> <span class="n">Jr</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Now let&#39;s control generation through a bias. Please note that the tokenizer is initialized differently!</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer_with_prefix_space</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">,</span> <span class="n">add_prefix_space</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">get_tokens_as_tuple</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
<span class="o">...</span>     <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tokenizer_with_prefix_space</span><span class="p">([</span><span class="n">word</span><span class="p">],</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>


<span class="o">&gt;&gt;&gt;</span> <span class="c1"># If we add a negative bias without beam search, it may become &quot;stuck&quot; in a prefix without good continuations</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">sequence_bias</span> <span class="o">=</span> <span class="p">{</span><span class="n">get_tokens_as_tuple</span><span class="p">(</span><span class="s2">&quot;Trump&quot;</span><span class="p">):</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">biased_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sequence_bias</span><span class="o">=</span><span class="n">sequence_bias</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">biased_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">The</span> <span class="n">full</span> <span class="n">name</span> <span class="n">of</span> <span class="n">Donald</span> <span class="ow">is</span> <span class="n">Donald</span> <span class="n">J</span><span class="o">.</span> <span class="n">Donald</span><span class="p">,</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">biased_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sequence_bias</span><span class="o">=</span><span class="n">sequence_bias</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">biased_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">The</span> <span class="n">full</span> <span class="n">name</span> <span class="n">of</span> <span class="n">Donald</span> <span class="ow">is</span> <span class="n">Donald</span> <span class="n">Rumsfeld</span><span class="p">,</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># We can also add a positive bias to nudge the model towards specific tokens or continuations</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">sequence_bias</span> <span class="o">=</span> <span class="p">{</span><span class="n">get_tokens_as_tuple</span><span class="p">(</span><span class="s2">&quot;Donald Duck&quot;</span><span class="p">):</span> <span class="mf">10.0</span><span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">biased_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sequence_bias</span><span class="o">=</span><span class="n">sequence_bias</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">biased_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">The</span> <span class="n">full</span> <span class="n">name</span> <span class="n">of</span> <span class="n">Donald</span> <span class="ow">is</span> <span class="n">Donald</span> <span class="n">Duck</span><span class="o">.</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SequenceBiasLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsProcessor`] that applies an additive bias on sequences. The bias is applied to the last token of a sequence</span>
<span class="sd">    when the next generated token can complete it. Consequently, to take the most of biasing sequences with more than</span>
<span class="sd">    one token, consider using beam methods (to gracefully work around partially completed sequences that have a</span>
<span class="sd">    negative bias) and applying the bias to their prefixes (to ensure the bias is applied earlier).</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    In order to get the token ids of the sequences that you want to bias, make sure to set `add_prefix_space=True` when</span>
<span class="sd">    initializing the tokenizer, and use `tokenizer(bad_words, add_special_tokens=False).input_ids`. The</span>
<span class="sd">    `add_prefix_space` argument is only supported for some slow tokenizers, as fast tokenizers&#39; prefixing behaviours</span>
<span class="sd">    come from `pre tokenizers`. Read more [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        sequence_bias (`Dict[Tuple[int], float]`):</span>
<span class="sd">            Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of the</span>
<span class="sd">            sequence being selected, while negative biases do the opposite. If a sequence has a length of 1, its bias</span>
<span class="sd">            will always be applied. Otherwise, the bias will only be applied if the sequence in question is about to be</span>
<span class="sd">            completed (in the token selection step after this processor is applied).</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; inputs = tokenizer([&quot;The full name of Donald is Donald&quot;], return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; summary_ids = model.generate(inputs[&quot;input_ids&quot;], max_new_tokens=4)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])</span>
<span class="sd">    The full name of Donald is Donald J. Trump Jr</span>

<span class="sd">    &gt;&gt;&gt; # Now let&#39;s control generation through a bias. Please note that the tokenizer is initialized differently!</span>
<span class="sd">    &gt;&gt;&gt; tokenizer_with_prefix_space = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2&quot;, add_prefix_space=True)</span>


<span class="sd">    &gt;&gt;&gt; def get_tokens_as_tuple(word):</span>
<span class="sd">    ...     return tuple(tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0])</span>


<span class="sd">    &gt;&gt;&gt; # If we add a negative bias without beam search, it may become &quot;stuck&quot; in a prefix without good continuations</span>
<span class="sd">    &gt;&gt;&gt; sequence_bias = {get_tokens_as_tuple(&quot;Trump&quot;): -10.0}</span>
<span class="sd">    &gt;&gt;&gt; biased_ids = model.generate(inputs[&quot;input_ids&quot;], max_new_tokens=4, sequence_bias=sequence_bias)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])</span>
<span class="sd">    The full name of Donald is Donald J. Donald,</span>

<span class="sd">    &gt;&gt;&gt; biased_ids = model.generate(inputs[&quot;input_ids&quot;], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])</span>
<span class="sd">    The full name of Donald is Donald Rumsfeld,</span>

<span class="sd">    &gt;&gt;&gt; # We can also add a positive bias to nudge the model towards specific tokens or continuations</span>
<span class="sd">    &gt;&gt;&gt; sequence_bias = {get_tokens_as_tuple(&quot;Donald Duck&quot;): 10.0}</span>
<span class="sd">    &gt;&gt;&gt; biased_ids = model.generate(inputs[&quot;input_ids&quot;], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])</span>
<span class="sd">    The full name of Donald is Donald Duck.</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence_bias</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sequence_bias</span> <span class="o">=</span> <span class="n">sequence_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_arguments</span><span class="p">()</span>

        <span class="c1"># Bias variables that will be populated on the first call (for retrocompatibility purposes, the vocabulary size</span>
        <span class="c1"># is infered in the first usage, which inhibits initializing here)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">length_1_bias</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepared_bias_variables</span> <span class="o">=</span> <span class="kc">False</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># 1 - Prepares the bias tensors. This is only needed the first time the logit processor is called.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepared_bias_variables</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_bias_variables</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

        <span class="c1"># 2 - prepares an empty bias to add</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

        <span class="c1"># 3 - include the bias from length = 1</span>
        <span class="n">bias</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_1_bias</span>

        <span class="c1"># 4 - include the bias from length &gt; 1, after determining which biased sequences may be completed.</span>
        <span class="k">for</span> <span class="n">sequence_ids</span><span class="p">,</span> <span class="n">sequence_bias</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_bias</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># the sequence is of length 1, already applied</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>  <span class="c1"># the sequence is longer than the context, ignore</span>
                <span class="k">continue</span>
            <span class="n">prefix_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_ids</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">last_token</span> <span class="o">=</span> <span class="n">sequence_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">matching_rows</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="n">prefix_length</span><span class="p">:],</span>
                <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sequence_ids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="p">)</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">bias</span><span class="p">[:,</span> <span class="n">last_token</span><span class="p">]</span> <span class="o">+=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">matching_rows</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span>
                <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sequence_bias</span><span class="p">),</span>
                <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="c1"># 5 - apply the bias to the scores</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">bias</span>
        <span class="k">return</span> <span class="n">scores_processed</span>

    <span class="k">def</span> <span class="nf">_prepare_bias_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Check biased tokens out of bounds</span>
        <span class="n">invalid_biases</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sequence_ids</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_bias</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">sequence_ids</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">token_id</span> <span class="o">&gt;=</span> <span class="n">vocabulary_size</span><span class="p">:</span>
                    <span class="n">invalid_biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">invalid_biases</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The model vocabulary size is </span><span class="si">{</span><span class="n">vocabulary_size</span><span class="si">}</span><span class="s2">, but the following tokens were being biased: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">invalid_biases</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Precompute the bias tensors to be applied. Sequences of length 1 are kept separately, as they can be applied</span>
        <span class="c1"># with simpler logic.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">length_1_bias</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocabulary_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">sequence_ids</span><span class="p">,</span> <span class="n">bias</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_bias</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">length_1_bias</span><span class="p">[</span><span class="n">sequence_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">bias</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prepared_bias_variables</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_validate_arguments</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">sequence_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_bias</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sequence_bias</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_bias</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`sequence_bias` has to be a non-empty dictionary, but is </span><span class="si">{</span><span class="n">sequence_bias</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sequence_ids</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">for</span> <span class="n">sequence_ids</span> <span class="ow">in</span> <span class="n">sequence_bias</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`sequence_bias` has to be a dict with tuples as keys, but is </span><span class="si">{</span><span class="n">sequence_bias</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
            <span class="nb">any</span><span class="p">((</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_id</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">))</span> <span class="ow">or</span> <span class="n">token_id</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">sequence_ids</span><span class="p">)</span>
            <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">sequence_ids</span> <span class="ow">in</span> <span class="n">sequence_bias</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Each key in `sequence_bias` has to be a non-empty tuple of positive integers, but is &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">sequence_bias</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="k">for</span> <span class="n">bias</span> <span class="ow">in</span> <span class="n">sequence_bias</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`sequence_bias` has to be a dict with floats as values, but is </span><span class="si">{</span><span class="n">sequence_bias</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>SuppressTokensAtBeginLogitsProcessor</code>] supresses a list of tokens as soon as the <code>generate</code> function starts
generating using <code>begin_index</code> tokens. This should ensure that the tokens defined by <code>begin_suppress_tokens</code> are
not generated at the begining. Originally created for
<a href="https://huggingface.co/docs/transformers/model_doc/whisper">Whisper</a>.</p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">WhisperForConditionalGeneration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai/whisper-tiny.en&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">WhisperForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai/whisper-tiny.en&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span><span class="p">,</span> <span class="s2">&quot;clean&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;audio&quot;</span><span class="p">][</span><span class="s2">&quot;array&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Whisper has `begin_suppress_tokens` set by default (= `[220, 50256]`). 50256 is the EOS token, so this means</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># it can&#39;t generate and EOS token in the first iteration, but it can in the others.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50256</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="n">inf</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50256</span><span class="p">])</span>  <span class="c1"># in other places we can see some probability mass for EOS</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">29.9010</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># If we disable `begin_suppress_tokens`, we can generate EOS in the first iteration.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="o">...</span>     <span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">begin_suppress_tokens</span><span class="o">=</span><span class="kc">None</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50256</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">11.2027</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SuppressTokensAtBeginLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`SuppressTokensAtBeginLogitsProcessor`] supresses a list of tokens as soon as the `generate` function starts</span>
<span class="sd">    generating using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens` are</span>
<span class="sd">    not generated at the begining. Originally created for</span>
<span class="sd">    [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoProcessor, WhisperForConditionalGeneration</span>
<span class="sd">    &gt;&gt;&gt; from datasets import load_dataset</span>

<span class="sd">    &gt;&gt;&gt; processor = AutoProcessor.from_pretrained(&quot;openai/whisper-tiny.en&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = WhisperForConditionalGeneration.from_pretrained(&quot;openai/whisper-tiny.en&quot;)</span>
<span class="sd">    &gt;&gt;&gt; ds = load_dataset(&quot;hf-internal-testing/librispeech_asr_dummy&quot;, &quot;clean&quot;, split=&quot;validation&quot;)</span>
<span class="sd">    &gt;&gt;&gt; inputs = processor(ds[0][&quot;audio&quot;][&quot;array&quot;], return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # Whisper has `begin_suppress_tokens` set by default (= `[220, 50256]`). 50256 is the EOS token, so this means</span>
<span class="sd">    &gt;&gt;&gt; # it can&#39;t generate and EOS token in the first iteration, but it can in the others.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)</span>
<span class="sd">    &gt;&gt;&gt; print(outputs.scores[0][0, 50256])</span>
<span class="sd">    tensor(-inf)</span>
<span class="sd">    &gt;&gt;&gt; print(outputs.scores[-1][0, 50256])  # in other places we can see some probability mass for EOS</span>
<span class="sd">    tensor(29.9010)</span>

<span class="sd">    &gt;&gt;&gt; # If we disable `begin_suppress_tokens`, we can generate EOS in the first iteration.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(</span>
<span class="sd">    ...     **inputs, return_dict_in_generate=True, output_scores=True, begin_suppress_tokens=None</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; print(outputs.scores[0][0, 50256])</span>
<span class="sd">    tensor(11.2027)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_suppress_tokens</span><span class="p">,</span> <span class="n">begin_index</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">begin_suppress_tokens</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">begin_suppress_tokens</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">begin_index</span> <span class="o">=</span> <span class="n">begin_index</span>

    <span class="k">def</span> <span class="nf">set_begin_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_index</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">begin_index</span> <span class="o">=</span> <span class="n">begin_index</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">vocab_tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">suppress_token_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">vocab_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_suppress_tokens</span><span class="p">)</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="k">if</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_index</span><span class="p">:</span>
            <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">suppress_token_mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">scores</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.SuppressTokensLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.SuppressTokensLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.SuppressTokensLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>This processor can be used to suppress a list of tokens. The processor will set their log probs to <code>-inf</code> so
that they are not generated. Originally created for
<a href="https://huggingface.co/docs/transformers/model_doc/whisper">Whisper</a>.</p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">WhisperForConditionalGeneration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai/whisper-tiny.en&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">WhisperForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai/whisper-tiny.en&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span><span class="p">,</span> <span class="s2">&quot;clean&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;audio&quot;</span><span class="p">][</span><span class="s2">&quot;array&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Whisper has a long list of suppressed tokens. For instance, in this case, the token 1 is suppressed by default.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># 1 (and not 0) is the first freely generated token</span>
<span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="n">inf</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># If we disable `suppress_tokens`, we can generate it.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">suppress_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">6.0678</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SuppressTokensLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This processor can be used to suppress a list of tokens. The processor will set their log probs to `-inf` so</span>
<span class="sd">    that they are not generated. Originally created for</span>
<span class="sd">    [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoProcessor, WhisperForConditionalGeneration</span>
<span class="sd">    &gt;&gt;&gt; from datasets import load_dataset</span>

<span class="sd">    &gt;&gt;&gt; processor = AutoProcessor.from_pretrained(&quot;openai/whisper-tiny.en&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = WhisperForConditionalGeneration.from_pretrained(&quot;openai/whisper-tiny.en&quot;)</span>
<span class="sd">    &gt;&gt;&gt; ds = load_dataset(&quot;hf-internal-testing/librispeech_asr_dummy&quot;, &quot;clean&quot;, split=&quot;validation&quot;)</span>
<span class="sd">    &gt;&gt;&gt; inputs = processor(ds[0][&quot;audio&quot;][&quot;array&quot;], return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # Whisper has a long list of suppressed tokens. For instance, in this case, the token 1 is suppressed by default.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)</span>
<span class="sd">    &gt;&gt;&gt; print(outputs.scores[1][0, 1])  # 1 (and not 0) is the first freely generated token</span>
<span class="sd">    tensor(-inf)</span>

<span class="sd">    &gt;&gt;&gt; # If we disable `suppress_tokens`, we can generate it.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True, suppress_tokens=None)</span>
<span class="sd">    &gt;&gt;&gt; print(outputs.scores[1][0, 1])</span>
<span class="sd">    tensor(6.0678)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">suppress_tokens</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">suppress_tokens</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">suppress_tokens</span><span class="p">))</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">vocab_tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">suppress_token_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">vocab_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">suppress_tokens</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">suppress_token_mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">scores</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.TemperatureLogitsWarper" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.TemperatureLogitsWarper</code>


<a href="#mindnlp.transformers.generation.logits_process.TemperatureLogitsWarper" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsWarper" href="#mindnlp.transformers.generation.logits_process.LogitsWarper">LogitsWarper</a></code></p>


        <p>[<code>LogitsWarper</code>] for temperature (exponential scaling output probability distribution), which effectively means
that it can control the randomness of the predicted tokens. Often used together with [<code>TopPLogitsWarper</code>] and
[<code>TopKLogitsWarper</code>].</p>
<p><Tip></p>
<p>Make sure that <code>do_sample=True</code> is included in the <code>generate</code> arguments otherwise the temperature value won't have
any effect.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>temperature</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Strictly positive float value used to modulate the logits distribution. A value smaller than <code>1</code> decreases
randomness (and vice versa), with <code>0</code> being equivalent to shifting all probability mass to the most likely
token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torch</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># for reproducibility</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Hugging Face Company is&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With temperature=1.0, the default, we consistently get random outputs due to random sampling.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">generate_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;num_return_sequences&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">[</span><span class="s1">&#39;Hugging Face Company is one of these companies that is going to take a&#39;</span><span class="p">,</span>
<span class="s2">&quot;Hugging Face Company is a brand created by Brian A. O&#39;Neil&quot;</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># However, with temperature close to 0, it approximates greedy decoding strategies (invariant)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">generate_kwargs</span><span class="p">[</span><span class="s2">&quot;temperature&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">[</span><span class="s1">&#39;Hugging Face Company is a company that has been around for over 20 years&#39;</span><span class="p">,</span>
<span class="s1">&#39;Hugging Face Company is a company that has been around for over 20 years&#39;</span><span class="p">]</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TemperatureLogitsWarper</span><span class="p">(</span><span class="n">LogitsWarper</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsWarper`] for temperature (exponential scaling output probability distribution), which effectively means</span>
<span class="sd">    that it can control the randomness of the predicted tokens. Often used together with [`TopPLogitsWarper`] and</span>
<span class="sd">    [`TopKLogitsWarper`].</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    Make sure that `do_sample=True` is included in the `generate` arguments otherwise the temperature value won&#39;t have</span>
<span class="sd">    any effect.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        temperature (`float`):</span>
<span class="sd">            Strictly positive float value used to modulate the logits distribution. A value smaller than `1` decreases</span>
<span class="sd">            randomness (and vice versa), with `0` being equivalent to shifting all probability mass to the most likely</span>
<span class="sd">            token.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed</span>

<span class="sd">    &gt;&gt;&gt; set_seed(0)  # for reproducibility</span>

<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model.config.pad_token_id = model.config.eos_token_id</span>
<span class="sd">    &gt;&gt;&gt; inputs = tokenizer([&quot;Hugging Face Company is&quot;], return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # With temperature=1.0, the default, we consistently get random outputs due to random sampling.</span>
<span class="sd">    &gt;&gt;&gt; generate_kwargs = {&quot;max_new_tokens&quot;: 10, &quot;do_sample&quot;: True, &quot;temperature&quot;: 1.0, &quot;num_return_sequences&quot;: 2}</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, **generate_kwargs)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True))</span>
<span class="sd">    [&#39;Hugging Face Company is one of these companies that is going to take a&#39;,</span>
<span class="sd">    &quot;Hugging Face Company is a brand created by Brian A. O&#39;Neil&quot;]</span>

<span class="sd">    &gt;&gt;&gt; # However, with temperature close to 0, it approximates greedy decoding strategies (invariant)</span>
<span class="sd">    &gt;&gt;&gt; generate_kwargs[&quot;temperature&quot;] = 0.0001</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, **generate_kwargs)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True))</span>
<span class="sd">    [&#39;Hugging Face Company is a company that has been around for over 20 years&#39;,</span>
<span class="sd">    &#39;Hugging Face Company is a company that has been around for over 20 years&#39;]</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="p">(</span><span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">except_msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`temperature` (=</span><span class="si">{</span><span class="n">temperature</span><span class="si">}</span><span class="s2">) has to be a strictly positive float, otherwise your next token &quot;</span>
                <span class="s2">&quot;scores will be invalid.&quot;</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">and</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
                <span class="n">except_msg</span> <span class="o">+=</span> <span class="s2">&quot; If you&#39;re looking for greedy decoding strategies, set `do_sample=False`.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">except_msg</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.TopKLogitsWarper" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.TopKLogitsWarper</code>


<a href="#mindnlp.transformers.generation.logits_process.TopKLogitsWarper" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsWarper" href="#mindnlp.transformers.generation.logits_process.LogitsWarper">LogitsWarper</a></code></p>


        <p>[<code>LogitsWarper</code>] that performs top-k, i.e. restricting to the k highest probability elements. Often used together
with [<code>TemperatureLogitsWarper</code>] and [<code>TopPLogitsWarper</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>top_k</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of highest probability vocabulary tokens to keep for top-k-filtering.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>filter_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>All filtered values will be set to this float value.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to -inf</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>-float(&#39;Inf&#39;)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_tokens_to_keep</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Minimum number of tokens that cannot be filtered.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;A sequence: A, B, C, D&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With sampling, the output is unexpected -- sometimes too unexpected.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">E</span> <span class="err">—</span> <span class="n">S</span> <span class="err">—</span> <span class="n">O</span><span class="p">,</span> <span class="n">P</span> <span class="err">—</span> <span class="n">R</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With `top_k` sampling, the output gets restricted the k most likely tokens.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Pro tip: In practice, LLMs use `top_k` in the 5-50 range.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">E</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">I</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TopKLogitsWarper</span><span class="p">(</span><span class="n">LogitsWarper</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsWarper`] that performs top-k, i.e. restricting to the k highest probability elements. Often used together</span>
<span class="sd">    with [`TemperatureLogitsWarper`] and [`TopPLogitsWarper`].</span>

<span class="sd">    Args:</span>
<span class="sd">        top_k (`int`):</span>
<span class="sd">            The number of highest probability vocabulary tokens to keep for top-k-filtering.</span>
<span class="sd">        filter_value (`float`, *optional*, defaults to -inf):</span>
<span class="sd">            All filtered values will be set to this float value.</span>
<span class="sd">        min_tokens_to_keep (`int`, *optional*, defaults to 1):</span>
<span class="sd">            Minimum number of tokens that cannot be filtered.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed</span>

<span class="sd">    &gt;&gt;&gt; set_seed(1)</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;A sequence: A, B, C, D&quot;, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # With sampling, the output is unexpected -- sometimes too unexpected.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    A sequence: A, B, C, D, E — S — O, P — R</span>

<span class="sd">    &gt;&gt;&gt; # With `top_k` sampling, the output gets restricted the k most likely tokens.</span>
<span class="sd">    &gt;&gt;&gt; # Pro tip: In practice, LLMs use `top_k` in the 5-50 range.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True, top_k=2)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    A sequence: A, B, C, D, E, F, G, H, I</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">),</span> <span class="n">min_tokens_to_keep</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">top_k</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`top_k` has to be a strictly positive integer, but is </span><span class="si">{</span><span class="n">top_k</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">top_k</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="n">filter_value</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>

        <span class="n">top_k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Safety check</span>
        <span class="c1"># Remove all tokens with a probability less than the last token of the top-k</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">&lt;</span> <span class="n">ops</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">indices_to_remove</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.TopPLogitsWarper" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.TopPLogitsWarper</code>


<a href="#mindnlp.transformers.generation.logits_process.TopPLogitsWarper" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsWarper" href="#mindnlp.transformers.generation.logits_process.LogitsWarper">LogitsWarper</a></code></p>


        <p>[<code>LogitsWarper</code>] that performs top-p, i.e. restricting to top tokens summing to prob_cut_off &lt;= prob_cut_off. Often
used together with [<code>TemperatureLogitsWarper</code>] and [<code>TopKLogitsWarper</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>top_p</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set to &lt; 1, only the smallest set of most probable tokens with probabilities that add up to <code>top_p</code> or
higher are kept for generation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>filter_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>All filtered values will be set to this float value.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to -inf</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>-float(&#39;Inf&#39;)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_tokens_to_keep</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Minimum number of tokens that cannot be filtered.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;A sequence: 1, 2&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With sampling, the output is unexpected -- sometimes too unexpected.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="o">|</span> <span class="o">&lt;</span> <span class="mi">4</span> <span class="p">(</span><span class="n">left</span><span class="o">-</span><span class="n">hand</span> <span class="n">pointer</span><span class="p">)</span> <span class="p">;</span>
<span class="o">&lt;</span><span class="n">BLANKLINE</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">BLANKLINE</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With `top_p` sampling, the output gets restricted to high-probability tokens.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Pro tip: In practice, LLMs use `top_p` in the 0.9-0.95 range.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="n">sequence</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TopPLogitsWarper</span><span class="p">(</span><span class="n">LogitsWarper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsWarper`] that performs top-p, i.e. restricting to top tokens summing to prob_cut_off &lt;= prob_cut_off. Often</span>
<span class="sd">    used together with [`TemperatureLogitsWarper`] and [`TopKLogitsWarper`].</span>

<span class="sd">    Args:</span>
<span class="sd">        top_p (`float`):</span>
<span class="sd">            If set to &lt; 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or</span>
<span class="sd">            higher are kept for generation.</span>
<span class="sd">        filter_value (`float`, *optional*, defaults to -inf):</span>
<span class="sd">            All filtered values will be set to this float value.</span>
<span class="sd">        min_tokens_to_keep (`int`, *optional*, defaults to 1):</span>
<span class="sd">            Minimum number of tokens that cannot be filtered.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed</span>

<span class="sd">    &gt;&gt;&gt; set_seed(1)</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert/distilgpt2&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;A sequence: 1, 2&quot;, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # With sampling, the output is unexpected -- sometimes too unexpected.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    A sequence: 1, 2, 3 | &lt; 4 (left-hand pointer) ;</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>

<span class="sd">    &gt;&gt;&gt; # With `top_p` sampling, the output gets restricted to high-probability tokens.</span>
<span class="sd">    &gt;&gt;&gt; # Pro tip: In practice, LLMs use `top_p` in the 0.9-0.95 range.</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True, top_p=0.1)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">),</span> <span class="n">min_tokens_to_keep</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">top_p</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">top_p</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">top_p</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">top_p</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`top_p` has to be a float &gt; 0 and &lt; 1, but is </span><span class="si">{</span><span class="n">top_p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_tokens_to_keep</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">min_tokens_to_keep</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`min_tokens_to_keep` has to be a positive integer, but is </span><span class="si">{</span><span class="n">min_tokens_to_keep</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">top_p</span> <span class="o">=</span> <span class="n">top_p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="n">filter_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span> <span class="o">=</span> <span class="n">min_tokens_to_keep</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>


        <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_like_call</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>

        <span class="n">sorted_logits</span><span class="p">,</span> <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sorted_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Remove tokens with cumulative top_p above the threshold (token with 0 are kept)</span>
        <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">cumulative_probs</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_p</span><span class="p">)</span>
        <span class="c1"># Keep at least min_tokens_to_keep</span>
        <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># scatter sorted tensors to original indexing</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sorted_indices_to_remove</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">,</span> <span class="n">sorted_indices_to_remove</span><span class="p">)</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">indices_to_remove</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores_processed</span>

    <span class="k">def</span> <span class="nf">tf_like_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">topk_scores</span><span class="p">,</span> <span class="n">topk_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">mask_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">topk_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">score_mask</span> <span class="o">=</span> <span class="n">cumulative_probs</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_p</span>

        <span class="c1"># Also include the token that is higher than top_p (the first false = shift and insert a True on the left)</span>
        <span class="n">score_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">score_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">),</span>
                                 <span class="n">score_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Ensure min tokens to keep</span>
        <span class="n">score_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">score_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">),</span>
                <span class="n">score_mask</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span> <span class="p">:],</span>
            <span class="p">),</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Mask the values that do not fit the criteria</span>
        <span class="n">topk_next_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">score_mask</span><span class="p">,</span> <span class="n">topk_scores</span><span class="p">,</span> <span class="n">mask_scores</span><span class="p">)</span>

        <span class="c1"># Undo the topk sorting: converts the 2D matrix of per-row original indices of shape (batch_size, vocab_size)</span>
        <span class="c1"># to a 3D tensor of shape (batch_size, vocab_size, 2) containing the original score coordinate, from which we</span>
        <span class="c1"># can scatter (i.e. `scatter_indices[row, col, :]` is a tensor containing `[row, topk_indices[row, col]]`)</span>
        <span class="n">scatter_rows</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">topk_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">topk_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">scatter_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">scatter_rows</span><span class="p">,</span> <span class="n">topk_indices</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">next_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tf_scatter_nd</span><span class="p">(</span><span class="n">scatter_indices</span><span class="p">,</span> <span class="n">topk_next_scores</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">topk_next_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">next_scores</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.TypicalLogitsWarper" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.TypicalLogitsWarper</code>


<a href="#mindnlp.transformers.generation.logits_process.TypicalLogitsWarper" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsWarper" href="#mindnlp.transformers.generation.logits_process.LogitsWarper">LogitsWarper</a></code></p>


        <p>[<code>LogitsWarper</code>] that performs typical decoding. Inspired on how humans use language, it prioritizes tokens whose
log probability is close to the entropy of the token probability distribution. This means that the most likely
tokens may be discarded in the process.</p>
<p>See <a href="https://arxiv.org/abs/2202.00666">Typical Decoding for Natural Language Generation</a> for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>mass</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Value of typical_p between 0 and 1 inclusive, defaults to 0.9.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.9</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.9</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>filter_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>All filtered values will be set to this float value.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to -inf</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>-float(&#39;Inf&#39;)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_tokens_to_keep</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Minimum number of tokens that cannot be filtered.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloomz-560m&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;1, 2, 3&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># We can see that greedy decoding produces a sequence of numbers</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># For this particular seed, we can see that sampling produces nearly the same low-information (= low entropy)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># sequence</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">set_seed</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span> <span class="ow">and</span> <span class="mi">10</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># With `typical_p` set, the most obvious sequence is no longer produced, which may be good for your problem</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">set_seed</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="o">...</span>     <span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">typical_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">sequences</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="ow">and</span> <span class="mi">5</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># We can see that the token corresponding to &quot;4&quot; (token 934) in the second position, the most likely token</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># as seen with greedy decoding, was entirely blocked out</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">934</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="n">inf</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TypicalLogitsWarper</span><span class="p">(</span><span class="n">LogitsWarper</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [`LogitsWarper`] that performs typical decoding. Inspired on how humans use language, it prioritizes tokens whose</span>
<span class="sd">    log probability is close to the entropy of the token probability distribution. This means that the most likely</span>
<span class="sd">    tokens may be discarded in the process.</span>

<span class="sd">    See [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666) for more information.</span>

<span class="sd">    Args:</span>
<span class="sd">        mass (`float`, *optional*, defaults to 0.9):</span>
<span class="sd">            Value of typical_p between 0 and 1 inclusive, defaults to 0.9.</span>
<span class="sd">        filter_value (`float`, *optional*, defaults to -inf):</span>
<span class="sd">            All filtered values will be set to this float value.</span>
<span class="sd">        min_tokens_to_keep (`int`, *optional*, defaults to 1):</span>
<span class="sd">            Minimum number of tokens that cannot be filtered.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;bigscience/bloomz-560m&quot;)</span>

<span class="sd">    &gt;&gt;&gt; inputs = tokenizer(&quot;1, 2, 3&quot;, return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # We can see that greedy decoding produces a sequence of numbers</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    1, 2, 3, 4, 5, 6, 7, 8, 9, 10,</span>

<span class="sd">    &gt;&gt;&gt; # For this particular seed, we can see that sampling produces nearly the same low-information (= low entropy)</span>
<span class="sd">    &gt;&gt;&gt; # sequence</span>
<span class="sd">    &gt;&gt;&gt; set_seed(18)</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, do_sample=True)</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])</span>
<span class="sd">    1, 2, 3, 4, 5, 6, 7, 8, 9 and 10</span>

<span class="sd">    &gt;&gt;&gt; # With `typical_p` set, the most obvious sequence is no longer produced, which may be good for your problem</span>
<span class="sd">    &gt;&gt;&gt; set_seed(18)</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(</span>
<span class="sd">    ...     **inputs, do_sample=True, typical_p=0.1, return_dict_in_generate=True, output_scores=True</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; print(tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0])</span>
<span class="sd">    1, 2, 3 and 5</span>

<span class="sd">    &gt;&gt;&gt; # We can see that the token corresponding to &quot;4&quot; (token 934) in the second position, the most likely token</span>
<span class="sd">    &gt;&gt;&gt; # as seen with greedy decoding, was entirely blocked out</span>
<span class="sd">    &gt;&gt;&gt; print(outputs.scores[1][0, 934])</span>
<span class="sd">    tensor(-inf)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mass</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">),</span> <span class="n">min_tokens_to_keep</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">mass</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">mass</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">mass</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">mass</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`typical_p` has to be a float &gt; 0 and &lt; 1, but is </span><span class="si">{</span><span class="n">mass</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_tokens_to_keep</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">min_tokens_to_keep</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`min_tokens_to_keep` has to be a positive integer, but is </span><span class="si">{</span><span class="n">min_tokens_to_keep</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="n">filter_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mass</span> <span class="o">=</span> <span class="n">mass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span> <span class="o">=</span> <span class="n">min_tokens_to_keep</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
        <span class="c1"># calculate entropy</span>
        <span class="n">normalized</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">normalized</span><span class="p">)</span>
        <span class="n">ent</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">normalized</span> <span class="o">*</span> <span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># shift and sort</span>
        <span class="n">shifted_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="o">-</span><span class="n">normalized</span><span class="p">)</span> <span class="o">-</span> <span class="n">ent</span><span class="p">)</span>
        <span class="n">sorted_scores</span><span class="p">,</span> <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">shifted_scores</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">sorted_logits</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">)</span>
        <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sorted_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Remove tokens with cumulative mass above the threshold</span>
        <span class="n">last_ind</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">cumulative_probs</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">mass</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">last_ind</span> <span class="o">=</span> <span class="n">last_ind</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">max</span><span class="o">=</span><span class="n">sorted_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">sorted_scores</span> <span class="o">&gt;</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">sorted_scores</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">last_ind</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_tokens_to_keep</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sorted_indices_to_remove</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">,</span> <span class="n">sorted_indices_to_remove</span><span class="p">)</span>

        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">indices_to_remove</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.UnbatchedClassifierFreeGuidanceLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.UnbatchedClassifierFreeGuidanceLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.UnbatchedClassifierFreeGuidanceLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>Logits processor for Classifier-Free Guidance (CFG). The processors computes a weighted average across scores
from prompt conditional and prompt unconditional (or negative) logits, parameterized by the <code>guidance_scale</code>.
The unconditional scores are computed internally by prompting <code>model</code> with the <code>unconditional_ids</code> branch.</p>
<p>See <a href="https://arxiv.org/abs/2306.17806">the paper</a> for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>guidance_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The guidance scale for classifier free guidance (CFG). CFG is enabled by setting <code>guidance_scale != 1</code>.
Higher guidance scale encourages the model to generate samples that are more closely linked to the input
prompt, usually at the expense of poorer quality. A value smaller than 1 has the opposite effect, while
making the negative prompt provided with negative_prompt_ids (if any) act as a positive prompt.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The model computing the unconditional scores. Supposedly the same as the one computing the conditional
scores. Both models must use the same tokenizer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`PreTrainedModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unconditional_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary for the unconditional branch. If unset, will default to
the last token of the prompt.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unconditional_attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Attention mask for unconditional_ids.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to cache key/values during the negative prompt forward pass.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Today, a dragon flew over Paris, France,&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">guidance_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="s1">&#39;Today, a dragon flew over Paris, France, killing at least 50 people and injuring more than 100&#39;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># with a negative prompt</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">neg_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;A very happy event happened,&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">guidance_scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">negative_prompt_ids</span><span class="o">=</span><span class="n">neg_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="s1">&#39;Today, a dragon flew over Paris, France, killing at least 130 people. French media reported that&#39;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># with a positive prompt</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">neg_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;A very happy event happened,&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">guidance_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">negative_prompt_ids</span><span class="o">=</span><span class="n">neg_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="s2">&quot;Today, a dragon flew over Paris, France, and I&#39;m very happy to be here. I&quot;</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">UnbatchedClassifierFreeGuidanceLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logits processor for Classifier-Free Guidance (CFG). The processors computes a weighted average across scores</span>
<span class="sd">    from prompt conditional and prompt unconditional (or negative) logits, parameterized by the `guidance_scale`.</span>
<span class="sd">    The unconditional scores are computed internally by prompting `model` with the `unconditional_ids` branch.</span>

<span class="sd">    See [the paper](https://arxiv.org/abs/2306.17806) for more information.</span>

<span class="sd">    Args:</span>
<span class="sd">        guidance_scale (`float`):</span>
<span class="sd">            The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale != 1`.</span>
<span class="sd">            Higher guidance scale encourages the model to generate samples that are more closely linked to the input</span>
<span class="sd">            prompt, usually at the expense of poorer quality. A value smaller than 1 has the opposite effect, while</span>
<span class="sd">            making the negative prompt provided with negative_prompt_ids (if any) act as a positive prompt.</span>
<span class="sd">        model (`PreTrainedModel`):</span>
<span class="sd">            The model computing the unconditional scores. Supposedly the same as the one computing the conditional</span>
<span class="sd">            scores. Both models must use the same tokenizer.</span>
<span class="sd">        unconditional_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary for the unconditional branch. If unset, will default to</span>
<span class="sd">            the last token of the prompt.</span>
<span class="sd">        unconditional_attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Attention mask for unconditional_ids.</span>
<span class="sd">        use_cache (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to cache key/values during the negative prompt forward pass.</span>


<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; inputs = tokenizer([&quot;Today, a dragon flew over Paris, France,&quot;], return_tensors=&quot;ms&quot;)</span>
<span class="sd">    &gt;&gt;&gt; out = model.generate(inputs[&quot;input_ids&quot;], guidance_scale=1.5)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer.batch_decode(out, skip_special_tokens=True)[0]</span>
<span class="sd">    &#39;Today, a dragon flew over Paris, France, killing at least 50 people and injuring more than 100&#39;</span>

<span class="sd">    &gt;&gt;&gt; # with a negative prompt</span>
<span class="sd">    &gt;&gt;&gt; neg_inputs = tokenizer([&quot;A very happy event happened,&quot;], return_tensors=&quot;ms&quot;)</span>
<span class="sd">    &gt;&gt;&gt; out = model.generate(inputs[&quot;input_ids&quot;], guidance_scale=2, negative_prompt_ids=neg_inputs[&quot;input_ids&quot;])</span>
<span class="sd">    &gt;&gt;&gt; tokenizer.batch_decode(out, skip_special_tokens=True)[0]</span>
<span class="sd">    &#39;Today, a dragon flew over Paris, France, killing at least 130 people. French media reported that&#39;</span>

<span class="sd">    &gt;&gt;&gt; # with a positive prompt</span>
<span class="sd">    &gt;&gt;&gt; neg_inputs = tokenizer([&quot;A very happy event happened,&quot;], return_tensors=&quot;ms&quot;)</span>
<span class="sd">    &gt;&gt;&gt; out = model.generate(inputs[&quot;input_ids&quot;], guidance_scale=0, negative_prompt_ids=neg_inputs[&quot;input_ids&quot;])</span>
<span class="sd">    &gt;&gt;&gt; tokenizer.batch_decode(out, skip_special_tokens=True)[0]</span>
<span class="sd">    &quot;Today, a dragon flew over Paris, France, and I&#39;m very happy to be here. I&quot;</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">guidance_scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">unconditional_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">unconditional_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">guidance_scale</span> <span class="o">=</span> <span class="n">guidance_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">unconditional_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">unconditional_attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;first_pass&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">get_unconditional_logits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;first_pass&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
                <span class="p">)</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;first_pass&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">],</span>
                    <span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
                <span class="p">],</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_ids</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">attention_mask</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">],</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unconditional_context</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">logits</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">guidance_scale</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_unconditional_logits</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="n">unconditional_logits</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">guidance_scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">scores</span> <span class="o">-</span> <span class="n">unconditional_logits</span><span class="p">)</span> <span class="o">+</span> <span class="n">unconditional_logits</span>
        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.WatermarkLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.WatermarkLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.WatermarkLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>Logits processor for watermarking generated text. The processor modifies model output scores by adding a small bias to
randomized set of "green" tokens before generating the next token. "Green" tokens selection process depends on the
<code>seeding_scheme</code> used. The code was based on the <a href="https://github.com/jwkirchenbauer/lm-watermarking/tree/main">original repo</a>.</p>
<p>The text generated by this <code>LogitsProcessor</code> can be detected using <code>WatermarkDetector</code>. See [<code>~WatermarkDetector.__call__</code>] for details,</p>
<p>See <a href="https://arxiv.org/abs/2306.04634">the paper</a> for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The model tokenizer's vocab_size. Used to calculate "green" tokens ratio.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>greenlist_ratio</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ratio of "green" tokens used to the vocabulary size. Defaults to 0.25.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, optional, *optional*, defaults to 0.25</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.25</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The bias added to the selected "green" tokens' logits. Consider lowering the
<code>bias</code> if the text generation quality degrades. Recommended values are in the
range of [0.5, 2.0]. Defaults to 2.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, optional, *optional*, defaults to 2.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hashing_key</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Key used for hashing. If you deploy this watermark, we advise using another private key.
Defaults to 15485863 (the millionth prime).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, optional, *optional*, defaults to 15485863</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>15485863</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>seeding_scheme</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The seeding scheme used for selecting "green" tokens. Accepts values:
    - "lefthash" (default): "green" tokens selection depend on the last token (Algorithm 2 from paper)
    - "selfhash": "green" tokens selection depends on the current token itself (Algorithm 3 from paper)
        The downside of this scheme is that it considers all possible next tokens and can be slower than "lefthash".
The context length of previous tokens to use in seeding. Higher context length makes watermarking more robust.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, optional, *optional*, defaults to `&#34;lefthash&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;lefthash&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>context_width</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of previous tokens to use when setting the seed.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">WatermarkingConfig</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Alice and Bob are&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># normal generation</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="s1">&#39;Alice and Bob are both in the same room.</span><span class="se">\n\n</span><span class="s1">&quot;I</span><span class="se">\&#39;</span><span class="s1">m not sure if you</span><span class="se">\&#39;</span><span class="s1">re&#39;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># watermarked generation</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">watermarking_config</span> <span class="o">=</span> <span class="n">WatermarkingConfig</span><span class="p">(</span><span class="n">bias</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">context_width</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">seeding_scheme</span><span class="o">=</span><span class="s2">&quot;selfhash&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">watermarking_config</span><span class="o">=</span><span class="n">watermarking_config</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="s1">&#39;Alice and Bob are both still alive and well and the story is pretty much a one-hour adventure&#39;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># to detect watermarked text use the WatermarkDetector class</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">WatermarkDetector</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">detector</span> <span class="o">=</span> <span class="n">WatermarkDetector</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">watermarking_config</span><span class="o">=</span> <span class="n">watermarking_config</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">detection_preds</span> <span class="o">=</span> <span class="n">detector</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">detection_preds</span>
<span class="n">array</span><span class="p">([</span> <span class="kc">True</span><span class="p">])</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span>
<span class="normal">2401</span>
<span class="normal">2402</span>
<span class="normal">2403</span>
<span class="normal">2404</span>
<span class="normal">2405</span>
<span class="normal">2406</span>
<span class="normal">2407</span>
<span class="normal">2408</span>
<span class="normal">2409</span>
<span class="normal">2410</span>
<span class="normal">2411</span>
<span class="normal">2412</span>
<span class="normal">2413</span>
<span class="normal">2414</span>
<span class="normal">2415</span>
<span class="normal">2416</span>
<span class="normal">2417</span>
<span class="normal">2418</span>
<span class="normal">2419</span>
<span class="normal">2420</span>
<span class="normal">2421</span>
<span class="normal">2422</span>
<span class="normal">2423</span>
<span class="normal">2424</span>
<span class="normal">2425</span>
<span class="normal">2426</span>
<span class="normal">2427</span>
<span class="normal">2428</span>
<span class="normal">2429</span>
<span class="normal">2430</span>
<span class="normal">2431</span>
<span class="normal">2432</span>
<span class="normal">2433</span>
<span class="normal">2434</span>
<span class="normal">2435</span>
<span class="normal">2436</span>
<span class="normal">2437</span>
<span class="normal">2438</span>
<span class="normal">2439</span>
<span class="normal">2440</span>
<span class="normal">2441</span>
<span class="normal">2442</span>
<span class="normal">2443</span>
<span class="normal">2444</span>
<span class="normal">2445</span>
<span class="normal">2446</span>
<span class="normal">2447</span>
<span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span>
<span class="normal">2485</span>
<span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span>
<span class="normal">2521</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">WatermarkLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logits processor for watermarking generated text. The processor modifies model output scores by adding a small bias to</span>
<span class="sd">    randomized set of &quot;green&quot; tokens before generating the next token. &quot;Green&quot; tokens selection process depends on the</span>
<span class="sd">    `seeding_scheme` used. The code was based on the [original repo](https://github.com/jwkirchenbauer/lm-watermarking/tree/main).</span>

<span class="sd">    The text generated by this `LogitsProcessor` can be detected using `WatermarkDetector`. See [`~WatermarkDetector.__call__`] for details,</span>

<span class="sd">    See [the paper](https://arxiv.org/abs/2306.04634) for more information.</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_size (`int`):</span>
<span class="sd">            The model tokenizer&#39;s vocab_size. Used to calculate &quot;green&quot; tokens ratio.</span>
<span class="sd">        greenlist_ratio (`float`, optional, *optional*, defaults to 0.25):</span>
<span class="sd">            The ratio of &quot;green&quot; tokens used to the vocabulary size. Defaults to 0.25.</span>
<span class="sd">        bias (`float`, optional, *optional*, defaults to 2.0):</span>
<span class="sd">            The bias added to the selected &quot;green&quot; tokens&#39; logits. Consider lowering the</span>
<span class="sd">            `bias` if the text generation quality degrades. Recommended values are in the</span>
<span class="sd">            range of [0.5, 2.0]. Defaults to 2.0.</span>
<span class="sd">        hashing_key (`int`, optional, *optional*, defaults to 15485863):</span>
<span class="sd">            Key used for hashing. If you deploy this watermark, we advise using another private key.</span>
<span class="sd">            Defaults to 15485863 (the millionth prime).</span>
<span class="sd">        seeding_scheme (`str`, optional, *optional*, defaults to `&quot;lefthash&quot;`):</span>
<span class="sd">            The seeding scheme used for selecting &quot;green&quot; tokens. Accepts values:</span>
<span class="sd">                - &quot;lefthash&quot; (default): &quot;green&quot; tokens selection depend on the last token (Algorithm 2 from paper)</span>
<span class="sd">                - &quot;selfhash&quot;: &quot;green&quot; tokens selection depends on the current token itself (Algorithm 3 from paper)</span>
<span class="sd">                    The downside of this scheme is that it considers all possible next tokens and can be slower than &quot;lefthash&quot;.</span>
<span class="sd">            The context length of previous tokens to use in seeding. Higher context length makes watermarking more robust.</span>
<span class="sd">        context_width (`int`, *optional*, defaults to 1):</span>
<span class="sd">            The number of previous tokens to use when setting the seed.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM, WatermarkingConfig</span>

<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; inputs = tokenizer([&quot;Alice and Bob are&quot;], return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # normal generation</span>
<span class="sd">    &gt;&gt;&gt; out = model.generate(inputs[&quot;input_ids&quot;], max_length=20, do_sample=False)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer.batch_decode(out, skip_special_tokens=True)[0]</span>
<span class="sd">    &#39;Alice and Bob are both in the same room.\n\n&quot;I\&#39;m not sure if you\&#39;re&#39;</span>

<span class="sd">    &gt;&gt;&gt; # watermarked generation</span>
<span class="sd">    &gt;&gt;&gt; watermarking_config = WatermarkingConfig(bias=2.5, context_width=2, seeding_scheme=&quot;selfhash&quot;)</span>
<span class="sd">    &gt;&gt;&gt; out = model.generate(inputs[&quot;input_ids&quot;], watermarking_config=watermarking_config, max_length=20, do_sample=False)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer.batch_decode(out, skip_special_tokens=True)[0]</span>
<span class="sd">    &#39;Alice and Bob are both still alive and well and the story is pretty much a one-hour adventure&#39;</span>

<span class="sd">    &gt;&gt;&gt; # to detect watermarked text use the WatermarkDetector class</span>
<span class="sd">    &gt;&gt;&gt; from transformers import WatermarkDetector</span>
<span class="sd">    &gt;&gt;&gt; detector = WatermarkDetector(model_config=model.config, watermarking_config= watermarking_config)</span>
<span class="sd">    &gt;&gt;&gt; detection_preds = detector(out)</span>
<span class="sd">    &gt;&gt;&gt; detection_preds</span>
<span class="sd">    array([ True])</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">greenlist_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
        <span class="n">hashing_key</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15485863</span><span class="p">,</span>
        <span class="n">seeding_scheme</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lefthash&quot;</span><span class="p">,</span>
        <span class="n">context_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">seeding_scheme</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;selfhash&quot;</span><span class="p">,</span> <span class="s2">&quot;lefthash&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;seeding_scheme has to be one of [`selfhash`, `lefthash`], but found </span><span class="si">{</span><span class="n">seeding_scheme</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">greenlist_ratio</span> <span class="o">&gt;=</span> <span class="mf">1.0</span> <span class="ow">or</span> <span class="n">greenlist_ratio</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;greenlist_ratio has be in range between 0.0 and 1.0, exclusively. but found </span><span class="si">{</span><span class="n">greenlist_ratio</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">greenlist_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">*</span> <span class="n">greenlist_ratio</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seeding_scheme</span> <span class="o">=</span> <span class="n">seeding_scheme</span>
        <span class="c1"># self.rng = torch.Generator(device=device)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hash_key</span> <span class="o">=</span> <span class="n">hashing_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_width</span> <span class="o">=</span> <span class="n">context_width</span>

        <span class="c1"># self.rng.manual_seed(hashing_key)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">table_size</span> <span class="o">=</span> <span class="mi">1_000_003</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fixed_table</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">table_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">input_seq</span> <span class="o">=</span> <span class="n">input_seq</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">context_width</span> <span class="p">:]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">seeding_scheme</span> <span class="o">==</span> <span class="s2">&quot;selfhash&quot;</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fixed_table</span><span class="p">[</span><span class="n">input_seq</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_size</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fixed_table</span><span class="p">[</span><span class="n">input_seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_size</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">seed</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hash_key</span> <span class="o">*</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">seed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hash_key</span> <span class="o">*</span> <span class="n">input_seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">64</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_get_greenlist_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>
        <span class="n">vocab_permutation</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
        <span class="n">greenlist_ids</span> <span class="o">=</span> <span class="n">vocab_permutation</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">greenlist_size</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">greenlist_ids</span>

    <span class="k">def</span> <span class="nf">_score_rejection_sampling</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate greenlist based on current candidate next token. Reject and move on if necessary.</span>
<span class="sd">        Runs for a fixed number of steps only for efficiency, since the methods is not batched.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">final_greenlist</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">greedy_predictions</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># 40 is an arbitrary number chosen to save compute and not run for long (taken from orig repo)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">40</span><span class="p">):</span>
            <span class="n">greenlist_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_greenlist_ids</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">greedy_predictions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">greedy_predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">in</span> <span class="n">greenlist_ids</span><span class="p">:</span>
                <span class="n">final_greenlist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">greedy_predictions</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">final_greenlist</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_width</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`input_ids` should have at least `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">context_width</span><span class="si">}</span><span class="s2">` tokens but has </span><span class="si">{</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="s2">&quot;The seeding will be skipped for this generation step!&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">scores</span>

        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="k">for</span> <span class="n">b_idx</span><span class="p">,</span> <span class="n">input_seq</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">seeding_scheme</span> <span class="o">==</span> <span class="s2">&quot;selfhash&quot;</span><span class="p">:</span>
                <span class="n">greenlist_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_score_rejection_sampling</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">scores</span><span class="p">[</span><span class="n">b_idx</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">greenlist_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_greenlist_ids</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>
            <span class="n">scores_processed</span><span class="p">[</span><span class="n">b_idx</span><span class="p">,</span> <span class="n">greenlist_ids</span><span class="p">]</span> <span class="o">=</span> <span class="n">scores_processed</span><span class="p">[</span><span class="n">b_idx</span><span class="p">,</span> <span class="n">greenlist_ids</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.WhisperNoSpeechDetection" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.WhisperNoSpeechDetection</code>


<a href="#mindnlp.transformers.generation.logits_process.WhisperNoSpeechDetection" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>This processor can be used to detect silence when using Whisper. It should take as input unprocessed logits to follow the original implementation</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">WhisperNoSpeechDetection</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;This processor can be used to detect silence when using Whisper. It should take as input unprocessed logits to follow the original implementation&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">no_speech_token</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">begin_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">scores_is_logprobs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">no_speech_token</span> <span class="o">=</span> <span class="n">no_speech_token</span>
        <span class="c1"># offset between &lt;start-of-transcription&gt; token, &lt;SOT&gt;, in paper and first generated token</span>
        <span class="c1"># is equal to the position of the first generated token index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_of_trans_offset</span> <span class="o">=</span> <span class="n">begin_index</span>

        <span class="c1"># `self.begin_index` is a running value that is changed on the fly</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">begin_index</span> <span class="o">=</span> <span class="n">begin_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_no_speech_prob</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_scores_logprobs</span> <span class="o">=</span> <span class="n">scores_is_logprobs</span>

        <span class="c1"># overwritten dynamically</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">set_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">set_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">),</span> <span class="o">**</span><span class="n">inputs</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">no_speech_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_no_speech_prob</span>

    <span class="k">def</span> <span class="nf">set_begin_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_index</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">begin_index</span> <span class="o">=</span> <span class="n">begin_index</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">is_scores_logprobs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_scores_logprobs</span>

        <span class="k">if</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_index</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_of_trans_offset</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>

                <span class="n">no_speech_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_index</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_of_trans_offset</span>
                <span class="n">no_speech_scores</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="n">no_speech_index</span><span class="p">]</span>
                <span class="n">is_scores_logprobs</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">no_speech_scores</span> <span class="o">=</span> <span class="n">scores</span>

            <span class="k">if</span> <span class="n">is_scores_logprobs</span><span class="p">:</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">no_speech_scores</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">no_speech_scores</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_no_speech_prob</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_speech_token</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.logits_process.WhisperTimeStampLogitsProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.generation.logits_process.WhisperTimeStampLogitsProcessor</code>


<a href="#mindnlp.transformers.generation.logits_process.WhisperTimeStampLogitsProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.logits_process.LogitsProcessor" href="#mindnlp.transformers.generation.logits_process.LogitsProcessor">LogitsProcessor</a></code></p>


        <p>[<code>LogitsProcessor</code>] that modifies the logits for the generation of timestamps in the transcription. When the input
tokens are at a specific threshold, the processor sets the scores to negative infinity. The processor makes sure
that timestamp tokens appear in pairs, by masking out the logits that would break this pairing pattern. This is
done to maintain the consistency and structure of generated timestamps. It also ensures that when the predicted
probability of sampling any of the timestamp token is greater than any individual non-timestamp token, those
non-timestamp logits are set to negative infinity. This is done to ensure the generation of timestamps over other
potential tokens.</p>
<p>See <a href="https://arxiv.org/abs/2212.04356">the paper</a> for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>generate_config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generate config used to generate the output. The following parameters are required:
    eos_token_id (<code>int</code>, <em>optional</em>, defaults to 50257):
        The id of the <em>end-of-sequence</em> token.
    no_timestamps_token_id (<code>int</code>, <em>optional</em>, defaults to 50363):
        The id of the <code>"&lt;|notimestamps|&gt;"</code> token.
    max_initial_timestamp_index (<code>int</code>, <em>optional</em>, defaults to 1):
        Used to set the maximum value of the initial timestamp. This is used to prevent the model from
        predicting timestamps that are too far in the future.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`GenerateConfig`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>begin_index</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Token index of the first token that is generated by the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Optional`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>_detect_timestamp_from_logprob</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether timestamps can be predicted from logprobs over all timestamps.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torch</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">WhisperForConditionalGeneration</span><span class="p">,</span> <span class="n">GenerationConfig</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai/whisper-tiny.en&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">WhisperForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai/whisper-tiny.en&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span><span class="p">,</span> <span class="s2">&quot;clean&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="s2">&quot;audio&quot;</span><span class="p">][</span><span class="s2">&quot;array&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">input_features</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_features</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1">#Displaying timestamps</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="n">return_timestamps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">transcription</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">decode_with_timestamps</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transcription:&quot;</span><span class="p">,</span> <span class="n">transcription</span><span class="p">)</span>
<span class="n">Transcription</span><span class="p">:</span> <span class="o">&lt;|</span><span class="n">startoftranscript</span><span class="o">|&gt;&lt;|</span><span class="mf">0.00</span><span class="o">|&gt;</span> <span class="n">He</span> <span class="n">has</span> <span class="n">grave</span> <span class="n">doubts</span> <span class="n">whether</span> <span class="n">Sir</span> <span class="n">Frederick</span> <span class="n">Layton</span><span class="s1">&#39;s work is really Greek after all, and can&lt;|6.44|&gt;&lt;|6.44|&gt; discover in it but little of rocky Ithaca.&lt;|9.44|&gt;&lt;|endoftext|&gt;</span>


<span class="o">&gt;&gt;&gt;</span> <span class="c1">#No timestamps &amp; change EOS:</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1">#This allows the user to select a specific token to terminate the sequence on, in this case it&#39;s the word &quot;can&quot;(460)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="mi">460</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span><span class="n">return_timestamps</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">transcription</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transcription:&quot;</span><span class="p">,</span> <span class="n">transcription</span><span class="p">)</span>
<span class="n">Transcription</span><span class="p">:</span>  <span class="n">He</span> <span class="n">has</span> <span class="n">grave</span> <span class="n">doubts</span> <span class="n">whether</span> <span class="n">Sir</span> <span class="n">Frederick</span> <span class="n">Layton</span><span class="s1">&#39;s work is really Greek after all and can</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\logits_process.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">WhisperTimeStampLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    [`LogitsProcessor`] that modifies the logits for the generation of timestamps in the transcription. When the input</span>
<span class="sd">    tokens are at a specific threshold, the processor sets the scores to negative infinity. The processor makes sure</span>
<span class="sd">    that timestamp tokens appear in pairs, by masking out the logits that would break this pairing pattern. This is</span>
<span class="sd">    done to maintain the consistency and structure of generated timestamps. It also ensures that when the predicted</span>
<span class="sd">    probability of sampling any of the timestamp token is greater than any individual non-timestamp token, those</span>
<span class="sd">    non-timestamp logits are set to negative infinity. This is done to ensure the generation of timestamps over other</span>
<span class="sd">    potential tokens.</span>


<span class="sd">    See [the paper](https://arxiv.org/abs/2212.04356) for more information.</span>

<span class="sd">    Args:</span>
<span class="sd">        generate_config (`GenerateConfig`):</span>
<span class="sd">            The generate config used to generate the output. The following parameters are required:</span>
<span class="sd">                eos_token_id (`int`, *optional*, defaults to 50257):</span>
<span class="sd">                    The id of the *end-of-sequence* token.</span>
<span class="sd">                no_timestamps_token_id (`int`, *optional*, defaults to 50363):</span>
<span class="sd">                    The id of the `&quot;&lt;|notimestamps|&gt;&quot;` token.</span>
<span class="sd">                max_initial_timestamp_index (`int`, *optional*, defaults to 1):</span>
<span class="sd">                    Used to set the maximum value of the initial timestamp. This is used to prevent the model from</span>
<span class="sd">                    predicting timestamps that are too far in the future.</span>
<span class="sd">        begin_index (`Optional`, *optional*): Token index of the first token that is generated by the model.</span>
<span class="sd">        _detect_timestamp_from_logprob (`bool`, *optional*): Whether timestamps can be predicted from logprobs over all timestamps.</span>

<span class="sd">    Examples:</span>
<span class="sd">    ``` python</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoProcessor, WhisperForConditionalGeneration, GenerationConfig</span>
<span class="sd">    &gt;&gt;&gt; from datasets import load_dataset</span>

<span class="sd">    &gt;&gt;&gt; processor = AutoProcessor.from_pretrained(&quot;openai/whisper-tiny.en&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = WhisperForConditionalGeneration.from_pretrained(&quot;openai/whisper-tiny.en&quot;)</span>
<span class="sd">    &gt;&gt;&gt; ds = load_dataset(&quot;hf-internal-testing/librispeech_asr_dummy&quot;, &quot;clean&quot;, split=&quot;validation&quot;)</span>
<span class="sd">    &gt;&gt;&gt; inputs = processor(ds[3][&quot;audio&quot;][&quot;array&quot;], return_tensors=&quot;ms&quot;)</span>
<span class="sd">    &gt;&gt;&gt; input_features = inputs.input_features</span>

<span class="sd">    &gt;&gt;&gt; #Displaying timestamps</span>
<span class="sd">    &gt;&gt;&gt; generated_ids = model.generate(inputs=input_features, return_timestamps=True)</span>
<span class="sd">    &gt;&gt;&gt; transcription = processor.batch_decode(generated_ids, decode_with_timestamps=True)[0]</span>
<span class="sd">    &gt;&gt;&gt; print(&quot;Transcription:&quot;, transcription)</span>
<span class="sd">    Transcription: &lt;|startoftranscript|&gt;&lt;|0.00|&gt; He has grave doubts whether Sir Frederick Layton&#39;s work is really Greek after all, and can&lt;|6.44|&gt;&lt;|6.44|&gt; discover in it but little of rocky Ithaca.&lt;|9.44|&gt;&lt;|endoftext|&gt;</span>


<span class="sd">    &gt;&gt;&gt; #No timestamps &amp; change EOS:</span>
<span class="sd">    &gt;&gt;&gt; #This allows the user to select a specific token to terminate the sequence on, in this case it&#39;s the word &quot;can&quot;(460)</span>
<span class="sd">    &gt;&gt;&gt; model.generation_config.eos_token_id = 460</span>
<span class="sd">    &gt;&gt;&gt; generated_ids = model.generate(inputs=input_features,return_timestamps=False)</span>
<span class="sd">    &gt;&gt;&gt; transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]</span>
<span class="sd">    &gt;&gt;&gt; print(&quot;Transcription:&quot;, transcription)</span>
<span class="sd">    Transcription:  He has grave doubts whether Sir Frederick Layton&#39;s work is really Greek after all and can</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">generate_config</span><span class="p">,</span>
        <span class="n">begin_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">_detect_timestamp_from_logprob</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># support for the kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">no_timestamps_token_id</span> <span class="o">=</span> <span class="n">generate_config</span><span class="o">.</span><span class="n">no_timestamps_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timestamp_begin</span> <span class="o">=</span> <span class="n">generate_config</span><span class="o">.</span><span class="n">no_timestamps_token_id</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">generate_config</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">or</span> <span class="n">generate_config</span><span class="o">.</span><span class="n">bos_token_id</span>

        <span class="c1"># this variable is mostly just used for testing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_detect_timestamp_from_logprob</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">_detect_timestamp_from_logprob</span>
            <span class="k">if</span> <span class="n">_detect_timestamp_from_logprob</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">generate_config</span><span class="p">,</span> <span class="s2">&quot;_detect_timestamp_from_logprob&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">num_forced_ids</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">generate_config</span><span class="o">.</span><span class="n">forced_decoder_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">generate_config</span><span class="o">.</span><span class="n">forced_decoder_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">begin_index</span> <span class="o">=</span> <span class="n">begin_index</span> <span class="ow">or</span> <span class="p">(</span><span class="n">num_forced_ids</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_initial_timestamp_index</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">generate_config</span><span class="p">,</span> <span class="s2">&quot;max_initial_timestamp_index&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="c1"># TODO(Patrick): Make sure that official models have max_initial_timestamp_index set to 50</span>
        <span class="c1"># self.max_initial_timestamp_index = 50</span>

    <span class="k">def</span> <span class="nf">set_begin_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_index</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">begin_index</span> <span class="o">=</span> <span class="n">begin_index</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># suppress &lt;|notimestamps|&gt; which is handled by without_timestamps</span>
        <span class="n">scores_processed</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="n">scores_processed</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_timestamps_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>

        <span class="c1"># timestamps have to appear in pairs, except directly before eos_token; mask logits accordingly</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">sampled_tokens</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_index</span> <span class="p">:]</span>
            <span class="n">seq</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sampled_tokens</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

            <span class="n">last_was_timestamp</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestamp_begin</span>
            <span class="n">penultimate_was_timestamp</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestamp_begin</span>

            <span class="k">if</span> <span class="n">last_was_timestamp</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">penultimate_was_timestamp</span><span class="p">:</span>  <span class="c1"># has to be non-timestamp</span>
                    <span class="n">scores_processed</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestamp_begin</span> <span class="p">:]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>  <span class="c1"># cannot be normal text tokens</span>
                    <span class="n">scores_processed</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>

            <span class="c1"># timestamps = sampled_tokens[ops.ge(sampled_tokens, self.timestamp_begin)]</span>
            <span class="n">timestamps</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">sampled_tokens</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">sampled_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestamp_begin</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">timestamps</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># `timestamps` shouldn&#39;t decrease; forbid timestamp tokens smaller than the last</span>
                <span class="c1"># The following lines of code are copied from: https://github.com/openai/whisper/pull/914/files#r1137085090</span>
                <span class="k">if</span> <span class="n">last_was_timestamp</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">penultimate_was_timestamp</span><span class="p">:</span>
                    <span class="n">timestamp_last</span> <span class="o">=</span> <span class="n">timestamps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Avoid to emit &lt;|0.00|&gt; again</span>
                    <span class="n">timestamp_last</span> <span class="o">=</span> <span class="n">timestamps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>

                <span class="n">scores_processed</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestamp_begin</span> <span class="p">:</span> <span class="n">timestamp_last</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>

        <span class="c1"># apply the `max_initial_timestamp` option</span>
        <span class="k">if</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_index</span><span class="p">:</span>
            <span class="n">scores_processed</span><span class="p">[:,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestamp_begin</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_initial_timestamp_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">last_allowed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestamp_begin</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_initial_timestamp_index</span>
                <span class="n">scores_processed</span><span class="p">[:,</span> <span class="n">last_allowed</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>

        <span class="c1"># if sum of probability over timestamps is above any other token, sample timestamp</span>
        <span class="n">logprobs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">scores_processed</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">timestamp_logprob</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logprobs</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestamp_begin</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">max_text_token_logprob</span> <span class="o">=</span> <span class="n">logprobs</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestamp_begin</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">timestamp_logprob</span><span class="p">)</span> <span class="ow">and</span> <span class="n">timestamp_logprob</span> <span class="o">&gt;</span> <span class="n">max_text_token_logprob</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_detect_timestamp_from_logprob</span><span class="p">:</span>
                <span class="n">scores_processed</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestamp_begin</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores_processed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../beam_search/" class="md-footer__link md-footer__link--prev" aria-label="Previous: beam_search">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                beam_search
              </div>
            </div>
          </a>
        
        
          
          <a href="../stopping_criteria/" class="md-footer__link md-footer__link--next" aria-label="Next: stopping_criteria">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                stopping_criteria
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 MindSpore Lab and CQU NLP Team.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:lvyufeng@cqu.edu.cn" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindnlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/lu-yu-feng-46-1" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>