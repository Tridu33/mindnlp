
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../ctrl/">
      
      
        <link rel="next" href="../data2vec/">
      
      
      <link rel="icon" href="../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>cvt - MindNLP Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindnlp.transformers.models.cvt.configuration_cvt" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="MindNLP Docs" class="md-header__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindNLP Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              cvt
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="./" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../../../zh/api/transformers/models/cvt/" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../tutorials/quick_start/" class="md-tabs__link">
          
  
    
  
  Tutorials

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../supported_models/" class="md-tabs__link">
        
  
    
  
  Supported Models

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../contribute/" class="md-tabs__link">
        
  
    
  
  How-To Contribute

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../accelerate/" class="md-tabs__link">
          
  
    
  
  API Reference

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../notes/changelog/" class="md-tabs__link">
          
  
    
  
  Notes

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="MindNLP Docs" class="md-nav__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    MindNLP Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/data_preprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocess
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_mirror/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Mirror
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../supported_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Supported Models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How-To Contribute
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../accelerate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/load_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    load_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/BaseMapFunction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BaseMapFunction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transforms
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engine
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Engine
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_1" >
        
          
          <label class="md-nav__link" for="__nav_5_4_1" id="__nav_5_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    train_args
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_1">
            <span class="md-nav__icon md-icon"></span>
            train_args
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seq2seq
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_2" >
        
          
          <label class="md-nav__link" for="__nav_5_4_2" id="__nav_5_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_2">
            <span class="md-nav__icon md-icon"></span>
            trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/default_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    default_func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../peft/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_2" >
        
          
          <label class="md-nav__link" for="__nav_5_7_2" id="__nav_5_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tuners
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_2">
            <span class="md-nav__icon md-icon"></span>
            tuners
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adalora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AdaLoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adaption_prompt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adaption_Prompt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/ia3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lokr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoKr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/prompt_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt tuning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_3" >
        
          
          <label class="md-nav__link" for="__nav_5_7_3" id="__nav_5_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/utils/merge_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    merge_utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/peft_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    peft_model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sentence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_2" >
        
          
          <label class="md-nav__link" for="__nav_5_9_2" id="__nav_5_9_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_2">
            <span class="md-nav__icon md-icon"></span>
            generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/generation/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/logits_process/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/stopping_criteria/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stopping_criteria
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/streamers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    streamers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_3" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9_3" id="__nav_5_9_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9_3">
            <span class="md-nav__icon md-icon"></span>
            models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../albert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    albert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    align
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../altclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    altclip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../audio_spectrogram_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    audio_spectrogram_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../auto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    auto
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    autoformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../baichuan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    baichuan
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../barthez/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    barthez
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bartpho/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bartpho
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../beit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bertweet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bertweet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bge_m3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bge_m3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../big_bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    big_bird
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bigbird_pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bigbird_pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../biogpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    biogpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot_small/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot_small
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip_2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bloom
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bridgetower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bros/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bros
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../byt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    byt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../camembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    camembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../canine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    canine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../codegen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    codegen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cogvlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cogvlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cohere/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convnext/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convnext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpmant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpmbee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ctrl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctrl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    cvt
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    cvt
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.configuration_cvt" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_cvt
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_cvt">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.configuration_cvt.CvtConfig" class="md-nav__link">
    <span class="md-ellipsis">
      CvtConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.configuration_cvt.CvtConfig.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_cvt
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_cvt">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken" class="md-nav__link">
    <span class="md-ellipsis">
      BaseModelOutputWithCLSToken
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention" class="md-nav__link">
    <span class="md-ellipsis">
      CvtAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.prune_heads" class="md-nav__link">
    <span class="md-ellipsis">
      prune_heads
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings" class="md-nav__link">
    <span class="md-ellipsis">
      CvtConvEmbeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtConvEmbeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath" class="md-nav__link">
    <span class="md-ellipsis">
      CvtDropPath
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtDropPath">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.extra_repr" class="md-nav__link">
    <span class="md-ellipsis">
      extra_repr
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings" class="md-nav__link">
    <span class="md-ellipsis">
      CvtEmbeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtEmbeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      CvtEncoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification" class="md-nav__link">
    <span class="md-ellipsis">
      CvtForImageClassification
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtForImageClassification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate" class="md-nav__link">
    <span class="md-ellipsis">
      CvtIntermediate
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtIntermediate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer" class="md-nav__link">
    <span class="md-ellipsis">
      CvtLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel" class="md-nav__link">
    <span class="md-ellipsis">
      CvtModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput" class="md-nav__link">
    <span class="md-ellipsis">
      CvtOutput
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtOutput">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      CvtPreTrainedModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention" class="md-nav__link">
    <span class="md-ellipsis">
      CvtSelfAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtSelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.rearrange_for_multi_head_attention" class="md-nav__link">
    <span class="md-ellipsis">
      rearrange_for_multi_head_attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection" class="md-nav__link">
    <span class="md-ellipsis">
      CvtSelfAttentionConvProjection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtSelfAttentionConvProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection" class="md-nav__link">
    <span class="md-ellipsis">
      CvtSelfAttentionLinearProjection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtSelfAttentionLinearProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection" class="md-nav__link">
    <span class="md-ellipsis">
      CvtSelfAttentionProjection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtSelfAttentionProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput" class="md-nav__link">
    <span class="md-ellipsis">
      CvtSelfOutput
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtSelfOutput">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtStage" class="md-nav__link">
    <span class="md-ellipsis">
      CvtStage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtStage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtStage.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtStage.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.drop_path" class="md-nav__link">
    <span class="md-ellipsis">
      drop_path
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data2vec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data2vec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../decision_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decision_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distilbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    distilbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../electra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    electra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../encodec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encodec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie_m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie_m
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../esm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    esm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    flava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../funnel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    funnel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gemma
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_bigcode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_bigcode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_pangu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_pangu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gptj/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gptj
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../graphormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    graphormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../groupvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    groupvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hubert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hubert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../imagegpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    imagegpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../internlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    internlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jetmoe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jetmoe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlmv2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlmv2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../led/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    led
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava_next/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava_next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../luke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    luke
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../marian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    marian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../maskformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maskformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mbart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mbart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minicpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minicpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minigpt4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minigpt4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mixtral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilevit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilevit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../moss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    moss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen_melody/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen_melody
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mvp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mvp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nezha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nezha
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nystromformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nystromformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../olmo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    olmo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../openelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    openelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    opt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../owlvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    owlvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../poolformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    poolformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pop2piano/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pop2piano
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2_moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    resnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roc_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roc_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rwkv
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seamless_m4t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seamless_m4t_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../segformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    segformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seggpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seggpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_encoder_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_encoder_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_to_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_to_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../squeezebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    squeezebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stablelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stablelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    starcoder2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../swiftformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    swiftformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../switch_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    switch_transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../t5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    t5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../table_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    table_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../timesformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    timesformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tinybert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tinybert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../van/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    van
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vipllava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vipllava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vision_text_dual_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision_text_dual_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../visual_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visual_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_conformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_conformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_with_lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_with_lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wavlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    whisper
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../x_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    x_clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta_xl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta_xl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlnet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_4" >
        
          
          <label class="md-nav__link" for="__nav_5_9_4" id="__nav_5_9_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_4">
            <span class="md-nav__icon md-icon"></span>
            pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/pipeline/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/automatic_speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    automatic_speech_recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/document_question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    document_question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/fill_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fill_mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text2text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text2text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/zero_shot_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_shot_classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../configuration_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    configuration_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modeling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_fast/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_fast
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TRL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Notes
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Change Log
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code of Conduct
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.configuration_cvt" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_cvt
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_cvt">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.configuration_cvt.CvtConfig" class="md-nav__link">
    <span class="md-ellipsis">
      CvtConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.configuration_cvt.CvtConfig.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_cvt
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_cvt">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken" class="md-nav__link">
    <span class="md-ellipsis">
      BaseModelOutputWithCLSToken
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention" class="md-nav__link">
    <span class="md-ellipsis">
      CvtAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.prune_heads" class="md-nav__link">
    <span class="md-ellipsis">
      prune_heads
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings" class="md-nav__link">
    <span class="md-ellipsis">
      CvtConvEmbeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtConvEmbeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath" class="md-nav__link">
    <span class="md-ellipsis">
      CvtDropPath
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtDropPath">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.extra_repr" class="md-nav__link">
    <span class="md-ellipsis">
      extra_repr
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings" class="md-nav__link">
    <span class="md-ellipsis">
      CvtEmbeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtEmbeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      CvtEncoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification" class="md-nav__link">
    <span class="md-ellipsis">
      CvtForImageClassification
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtForImageClassification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate" class="md-nav__link">
    <span class="md-ellipsis">
      CvtIntermediate
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtIntermediate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer" class="md-nav__link">
    <span class="md-ellipsis">
      CvtLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel" class="md-nav__link">
    <span class="md-ellipsis">
      CvtModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput" class="md-nav__link">
    <span class="md-ellipsis">
      CvtOutput
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtOutput">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      CvtPreTrainedModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention" class="md-nav__link">
    <span class="md-ellipsis">
      CvtSelfAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtSelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.rearrange_for_multi_head_attention" class="md-nav__link">
    <span class="md-ellipsis">
      rearrange_for_multi_head_attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection" class="md-nav__link">
    <span class="md-ellipsis">
      CvtSelfAttentionConvProjection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtSelfAttentionConvProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection" class="md-nav__link">
    <span class="md-ellipsis">
      CvtSelfAttentionLinearProjection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtSelfAttentionLinearProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection" class="md-nav__link">
    <span class="md-ellipsis">
      CvtSelfAttentionProjection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtSelfAttentionProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput" class="md-nav__link">
    <span class="md-ellipsis">
      CvtSelfOutput
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtSelfOutput">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtStage" class="md-nav__link">
    <span class="md-ellipsis">
      CvtStage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CvtStage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtStage.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtStage.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cvt.modeling_cvt.drop_path" class="md-nav__link">
    <span class="md-ellipsis">
      drop_path
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindnlp/edit/master/docs/en/api/transformers/models/cvt.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindnlp/raw/master/docs/en/api/transformers/models/cvt.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>cvt</h1>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.cvt.configuration_cvt" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.configuration_cvt</code>


<a href="#mindnlp.transformers.models.cvt.configuration_cvt" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>CvT model configuration</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.configuration_cvt.CvtConfig" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.configuration_cvt.CvtConfig</code>


<a href="#mindnlp.transformers.models.cvt.configuration_cvt.CvtConfig" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.configuration_utils.PretrainedConfig" href="../../configuration_utils/#mindnlp.transformers.configuration_utils.PretrainedConfig">PretrainedConfig</a></code></p>


        <p>This is the configuration class to store the configuration of a [<code>CvtModel</code>]. It is used to instantiate a CvT model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the CvT
<a href="https://huggingface.co/microsoft/cvt-13">microsoft/cvt-13</a> architecture.</p>
<p>Configuration objects inherit from [<code>PretrainedConfig</code>] and can be used to control the model outputs. Read the
documentation from [<code>PretrainedConfig</code>] for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>num_channels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of input channels.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 3</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>patch_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size of each encoder's patch embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*, defaults to `[7, 3, 3]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[7, 3, 3]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>patch_stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride size of each encoder's patch embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*, defaults to `[4, 2, 2]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[4, 2, 2]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>patch_padding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The padding size of each encoder's patch embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*, defaults to `[2, 1, 1]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[2, 1, 1]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of each of the encoder blocks.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*, defaults to `[64, 192, 384]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[64, 192, 384]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in each block of the Transformer encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*, defaults to `[1, 3, 6]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[1, 3, 6]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>depth</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of layers in each encoder block.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*, defaults to `[1, 2, 10]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[1, 2, 10]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_ratios</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Ratio of the size of the hidden layer compared to the size of the input layer of the Mix FFNs in the
encoder blocks.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[float]`, *optional*, defaults to `[4.0, 4.0, 4.0, 4.0]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_drop_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout ratio for the attention probabilities.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[0.0, 0.0, 0.0]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>drop_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout ratio for the patch embeddings probabilities.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[0.0, 0.0, 0.0]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>drop_path_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for stochastic depth, used in the blocks of the Transformer encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.1]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[0.0, 0.0, 0.1]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>qkv_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The bias bool for query, key and value in attentions</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[bool]`, *optional*, defaults to `[True, True, True]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[True, True, True]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cls_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to add a classification token to the output of each of the last 3 stages.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[bool]`, *optional*, defaults to `[False, False, True]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[False, False, True]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>qkv_projection_method</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The projection method for query, key and value Default is depth-wise convolutions with batch norm. For
Linear projection use "avg".</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[string]`, *optional*, defaults to [&#34;dw_bn&#34;, &#34;dw_bn&#34;, &#34;dw_bn&#34;]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;dw_bn&#39;, &#39;dw_bn&#39;, &#39;dw_bn&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_qkv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size for query, key and value in attention layer</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*, defaults to `[3, 3, 3]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[3, 3, 3]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The padding size for key and value in attention layer</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*, defaults to `[1, 1, 1]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[1, 1, 1]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride size for key and value in attention layer</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*, defaults to `[2, 2, 2]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[2, 2, 2]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The padding size for query in attention layer</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*, defaults to `[1, 1, 1]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[1, 1, 1]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride size for query in attention layer</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*, defaults to `[1, 1, 1]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[1, 1, 1]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>initializer_range</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.02</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.02</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_norm_eps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The epsilon used by the layer normalization layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 1e-6</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-12</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">CvtConfig</span><span class="p">,</span> <span class="n">CvtModel</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a Cvt msft/cvt style configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">CvtConfig</span><span class="p">()</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a model (with random weights) from the msft/cvt style configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">CvtModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Accessing the model configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</code></pre></div>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\configuration_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the configuration class to store the configuration of a [`CvtModel`]. It is used to instantiate a CvT model</span>
<span class="sd">    according to the specified arguments, defining the model architecture. Instantiating a configuration with the</span>
<span class="sd">    defaults will yield a similar configuration to that of the CvT</span>
<span class="sd">    [microsoft/cvt-13](https://huggingface.co/microsoft/cvt-13) architecture.</span>

<span class="sd">    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the</span>
<span class="sd">    documentation from [`PretrainedConfig`] for more information.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_channels (`int`, *optional*, defaults to 3):</span>
<span class="sd">            The number of input channels.</span>
<span class="sd">        patch_sizes (`List[int]`, *optional*, defaults to `[7, 3, 3]`):</span>
<span class="sd">            The kernel size of each encoder&#39;s patch embedding.</span>
<span class="sd">        patch_stride (`List[int]`, *optional*, defaults to `[4, 2, 2]`):</span>
<span class="sd">            The stride size of each encoder&#39;s patch embedding.</span>
<span class="sd">        patch_padding (`List[int]`, *optional*, defaults to `[2, 1, 1]`):</span>
<span class="sd">            The padding size of each encoder&#39;s patch embedding.</span>
<span class="sd">        embed_dim (`List[int]`, *optional*, defaults to `[64, 192, 384]`):</span>
<span class="sd">            Dimension of each of the encoder blocks.</span>
<span class="sd">        num_heads (`List[int]`, *optional*, defaults to `[1, 3, 6]`):</span>
<span class="sd">            Number of attention heads for each attention layer in each block of the Transformer encoder.</span>
<span class="sd">        depth (`List[int]`, *optional*, defaults to `[1, 2, 10]`):</span>
<span class="sd">            The number of layers in each encoder block.</span>
<span class="sd">        mlp_ratios (`List[float]`, *optional*, defaults to `[4.0, 4.0, 4.0, 4.0]`):</span>
<span class="sd">            Ratio of the size of the hidden layer compared to the size of the input layer of the Mix FFNs in the</span>
<span class="sd">            encoder blocks.</span>
<span class="sd">        attention_drop_rate (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`):</span>
<span class="sd">            The dropout ratio for the attention probabilities.</span>
<span class="sd">        drop_rate (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`):</span>
<span class="sd">            The dropout ratio for the patch embeddings probabilities.</span>
<span class="sd">        drop_path_rate (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.1]`):</span>
<span class="sd">            The dropout probability for stochastic depth, used in the blocks of the Transformer encoder.</span>
<span class="sd">        qkv_bias (`List[bool]`, *optional*, defaults to `[True, True, True]`):</span>
<span class="sd">            The bias bool for query, key and value in attentions</span>
<span class="sd">        cls_token (`List[bool]`, *optional*, defaults to `[False, False, True]`):</span>
<span class="sd">            Whether or not to add a classification token to the output of each of the last 3 stages.</span>
<span class="sd">        qkv_projection_method (`List[string]`, *optional*, defaults to [&quot;dw_bn&quot;, &quot;dw_bn&quot;, &quot;dw_bn&quot;]`):</span>
<span class="sd">            The projection method for query, key and value Default is depth-wise convolutions with batch norm. For</span>
<span class="sd">            Linear projection use &quot;avg&quot;.</span>
<span class="sd">        kernel_qkv (`List[int]`, *optional*, defaults to `[3, 3, 3]`):</span>
<span class="sd">            The kernel size for query, key and value in attention layer</span>
<span class="sd">        padding_kv (`List[int]`, *optional*, defaults to `[1, 1, 1]`):</span>
<span class="sd">            The padding size for key and value in attention layer</span>
<span class="sd">        stride_kv (`List[int]`, *optional*, defaults to `[2, 2, 2]`):</span>
<span class="sd">            The stride size for key and value in attention layer</span>
<span class="sd">        padding_q (`List[int]`, *optional*, defaults to `[1, 1, 1]`):</span>
<span class="sd">            The padding size for query in attention layer</span>
<span class="sd">        stride_q (`List[int]`, *optional*, defaults to `[1, 1, 1]`):</span>
<span class="sd">            The stride size for query in attention layer</span>
<span class="sd">        initializer_range (`float`, *optional*, defaults to 0.02):</span>
<span class="sd">            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</span>
<span class="sd">        layer_norm_eps (`float`, *optional*, defaults to 1e-6):</span>
<span class="sd">            The epsilon used by the layer normalization layers.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import CvtConfig, CvtModel</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a Cvt msft/cvt style configuration</span>
<span class="sd">        &gt;&gt;&gt; configuration = CvtConfig()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a model (with random weights) from the msft/cvt style configuration</span>
<span class="sd">        &gt;&gt;&gt; model = CvtModel(configuration)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Accessing the model configuration</span>
<span class="sd">        &gt;&gt;&gt; configuration = model.config</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;cvt&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">patch_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
        <span class="n">patch_stride</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
        <span class="n">patch_padding</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">384</span><span class="p">],</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
        <span class="n">depth</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="n">mlp_ratio</span><span class="o">=</span><span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span>
        <span class="n">attention_drop_rate</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
        <span class="n">drop_rate</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
        <span class="n">drop_path_rate</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
        <span class="n">qkv_bias</span><span class="o">=</span><span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span>
        <span class="n">cls_token</span><span class="o">=</span><span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span>
        <span class="n">qkv_projection_method</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;dw_bn&quot;</span><span class="p">,</span> <span class="s2">&quot;dw_bn&quot;</span><span class="p">,</span> <span class="s2">&quot;dw_bn&quot;</span><span class="p">],</span>
        <span class="n">kernel_qkv</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
        <span class="n">padding_kv</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">stride_kv</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
        <span class="n">padding_q</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">stride_q</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
        <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        This method initializes an instance of the CvtConfig class with the provided parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            num_channels (int, optional): Number of input channels. Defaults to 3.</span>
<span class="sd">            patch_sizes (List[int], optional): List of patch sizes for each layer. Defaults to [7, 3, 3].</span>
<span class="sd">            patch_stride (List[int], optional): List of patch strides for each layer. Defaults to [4, 2, 2].</span>
<span class="sd">            patch_padding (List[int], optional): List of patch paddings for each layer. Defaults to [2, 1, 1].</span>
<span class="sd">            embed_dim (List[int], optional): List of embedding dimensions for each layer. Defaults to [64, 192, 384].</span>
<span class="sd">            num_heads (List[int], optional): List of the number of attention heads for each layer. Defaults to [1, 3, 6].</span>
<span class="sd">            depth (List[int], optional): List of the depths for each layer. Defaults to [1, 2, 10].</span>
<span class="sd">            mlp_ratio (List[float], optional): List of the MLP ratio for each layer. Defaults to [4.0, 4.0, 4.0].</span>
<span class="sd">            attention_drop_rate (List[float], optional): List of attention dropout rates for each layer. Defaults to [0.0, 0.0, 0.0].</span>
<span class="sd">            drop_rate (List[float], optional): List of dropout rates for each layer. Defaults to [0.0, 0.0, 0.0].</span>
<span class="sd">            drop_path_rate (List[float], optional): List of drop path rates for each layer. Defaults to [0.0, 0.0, 0.1].</span>
<span class="sd">            qkv_bias (List[bool], optional): List of booleans indicating whether to include bias for query, key, and value projections for each layer. Defaults to [True, True, True].</span>
<span class="sd">            cls_token (List[bool], optional): List of booleans indicating whether to include a class token for each layer. Defaults to [False, False, True].</span>
<span class="sd">            qkv_projection_method (List[str], optional): List of methods for query, key, and value projections for each layer. Defaults to [&#39;dw_bn&#39;, &#39;dw_bn&#39;, &#39;dw_bn&#39;].</span>
<span class="sd">            kernel_qkv (List[int], optional): List of kernel sizes for query, key, and value projections for each layer. Defaults to [3, 3, 3].</span>
<span class="sd">            padding_kv (List[int], optional): List of paddings for key and value projections for each layer. Defaults to [1, 1, 1].</span>
<span class="sd">            stride_kv (List[int], optional): List of strides for key and value projections for each layer. Defaults to [2, 2, 2].</span>
<span class="sd">            padding_q (List[int], optional): List of paddings for query projection for each layer. Defaults to [1, 1, 1].</span>
<span class="sd">            stride_q (List[int], optional): List of strides for query projection for each layer. Defaults to [1, 1, 1].</span>
<span class="sd">            initializer_range (float, optional): The range of the initializer. Defaults to 0.02.</span>
<span class="sd">            layer_norm_eps (float, optional): Epsilon value for layer normalization. Defaults to 1e-12.</span>
<span class="sd">            **kwargs: Additional keyword arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_channels</span> <span class="o">=</span> <span class="n">num_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_sizes</span> <span class="o">=</span> <span class="n">patch_sizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_stride</span> <span class="o">=</span> <span class="n">patch_stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_padding</span> <span class="o">=</span> <span class="n">patch_padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_ratio</span> <span class="o">=</span> <span class="n">mlp_ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_drop_rate</span> <span class="o">=</span> <span class="n">attention_drop_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_rate</span> <span class="o">=</span> <span class="n">drop_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_path_rate</span> <span class="o">=</span> <span class="n">drop_path_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qkv_bias</span> <span class="o">=</span> <span class="n">qkv_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">cls_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qkv_projection_method</span> <span class="o">=</span> <span class="n">qkv_projection_method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_qkv</span> <span class="o">=</span> <span class="n">kernel_qkv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_kv</span> <span class="o">=</span> <span class="n">padding_kv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride_kv</span> <span class="o">=</span> <span class="n">stride_kv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_q</span> <span class="o">=</span> <span class="n">padding_q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride_q</span> <span class="o">=</span> <span class="n">stride_q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_eps</span> <span class="o">=</span> <span class="n">layer_norm_eps</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.configuration_cvt.CvtConfig.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">configuration_cvt</span><span class="o">.</span><span class="n">CvtConfig</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">patch_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">patch_stride</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">patch_padding</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">embed_dim</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">384</span><span class="p">],</span> <span class="n">num_heads</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">depth</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="n">attention_drop_rate</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">drop_rate</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="n">cls_token</span><span class="o">=</span><span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="n">qkv_projection_method</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;dw_bn&#39;</span><span class="p">,</span> <span class="s1">&#39;dw_bn&#39;</span><span class="p">,</span> <span class="s1">&#39;dw_bn&#39;</span><span class="p">],</span> <span class="n">kernel_qkv</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">padding_kv</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">stride_kv</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">padding_q</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">stride_q</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.configuration_cvt.CvtConfig.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method initializes an instance of the CvtConfig class with the provided parameters.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_channels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of input channels. Defaults to 3.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>patch_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of patch sizes for each layer. Defaults to [7, 3, 3].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[7, 3, 3]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>patch_stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of patch strides for each layer. Defaults to [4, 2, 2].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[4, 2, 2]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>patch_padding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of patch paddings for each layer. Defaults to [2, 1, 1].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[2, 1, 1]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of embedding dimensions for each layer. Defaults to [64, 192, 384].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[64, 192, 384]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of the number of attention heads for each layer. Defaults to [1, 3, 6].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[1, 3, 6]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>depth</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of the depths for each layer. Defaults to [1, 2, 10].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[1, 2, 10]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_ratio</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of the MLP ratio for each layer. Defaults to [4.0, 4.0, 4.0].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[4.0, 4.0, 4.0]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_drop_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of attention dropout rates for each layer. Defaults to [0.0, 0.0, 0.0].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[0.0, 0.0, 0.0]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>drop_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of dropout rates for each layer. Defaults to [0.0, 0.0, 0.0].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[0.0, 0.0, 0.0]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>drop_path_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of drop path rates for each layer. Defaults to [0.0, 0.0, 0.1].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[0.0, 0.0, 0.1]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>qkv_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of booleans indicating whether to include bias for query, key, and value projections for each layer. Defaults to [True, True, True].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[True, True, True]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cls_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of booleans indicating whether to include a class token for each layer. Defaults to [False, False, True].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[False, False, True]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>qkv_projection_method</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of methods for query, key, and value projections for each layer. Defaults to ['dw_bn', 'dw_bn', 'dw_bn'].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;dw_bn&#39;, &#39;dw_bn&#39;, &#39;dw_bn&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_qkv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of kernel sizes for query, key, and value projections for each layer. Defaults to [3, 3, 3].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[3, 3, 3]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of paddings for key and value projections for each layer. Defaults to [1, 1, 1].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[1, 1, 1]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of strides for key and value projections for each layer. Defaults to [2, 2, 2].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[2, 2, 2]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of paddings for query projection for each layer. Defaults to [1, 1, 1].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[1, 1, 1]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of strides for query projection for each layer. Defaults to [1, 1, 1].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[1, 1, 1]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>initializer_range</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The range of the initializer. Defaults to 0.02.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.02</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_norm_eps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Epsilon value for layer normalization. Defaults to 1e-12.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-12</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional keyword arguments.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\configuration_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">patch_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="n">patch_stride</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="n">patch_padding</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">384</span><span class="p">],</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="n">depth</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
    <span class="n">mlp_ratio</span><span class="o">=</span><span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span>
    <span class="n">attention_drop_rate</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
    <span class="n">drop_rate</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
    <span class="n">drop_path_rate</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="n">qkv_bias</span><span class="o">=</span><span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span>
    <span class="n">cls_token</span><span class="o">=</span><span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span>
    <span class="n">qkv_projection_method</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;dw_bn&quot;</span><span class="p">,</span> <span class="s2">&quot;dw_bn&quot;</span><span class="p">,</span> <span class="s2">&quot;dw_bn&quot;</span><span class="p">],</span>
    <span class="n">kernel_qkv</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="n">padding_kv</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">stride_kv</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="n">padding_q</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">stride_q</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This method initializes an instance of the CvtConfig class with the provided parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        num_channels (int, optional): Number of input channels. Defaults to 3.</span>
<span class="sd">        patch_sizes (List[int], optional): List of patch sizes for each layer. Defaults to [7, 3, 3].</span>
<span class="sd">        patch_stride (List[int], optional): List of patch strides for each layer. Defaults to [4, 2, 2].</span>
<span class="sd">        patch_padding (List[int], optional): List of patch paddings for each layer. Defaults to [2, 1, 1].</span>
<span class="sd">        embed_dim (List[int], optional): List of embedding dimensions for each layer. Defaults to [64, 192, 384].</span>
<span class="sd">        num_heads (List[int], optional): List of the number of attention heads for each layer. Defaults to [1, 3, 6].</span>
<span class="sd">        depth (List[int], optional): List of the depths for each layer. Defaults to [1, 2, 10].</span>
<span class="sd">        mlp_ratio (List[float], optional): List of the MLP ratio for each layer. Defaults to [4.0, 4.0, 4.0].</span>
<span class="sd">        attention_drop_rate (List[float], optional): List of attention dropout rates for each layer. Defaults to [0.0, 0.0, 0.0].</span>
<span class="sd">        drop_rate (List[float], optional): List of dropout rates for each layer. Defaults to [0.0, 0.0, 0.0].</span>
<span class="sd">        drop_path_rate (List[float], optional): List of drop path rates for each layer. Defaults to [0.0, 0.0, 0.1].</span>
<span class="sd">        qkv_bias (List[bool], optional): List of booleans indicating whether to include bias for query, key, and value projections for each layer. Defaults to [True, True, True].</span>
<span class="sd">        cls_token (List[bool], optional): List of booleans indicating whether to include a class token for each layer. Defaults to [False, False, True].</span>
<span class="sd">        qkv_projection_method (List[str], optional): List of methods for query, key, and value projections for each layer. Defaults to [&#39;dw_bn&#39;, &#39;dw_bn&#39;, &#39;dw_bn&#39;].</span>
<span class="sd">        kernel_qkv (List[int], optional): List of kernel sizes for query, key, and value projections for each layer. Defaults to [3, 3, 3].</span>
<span class="sd">        padding_kv (List[int], optional): List of paddings for key and value projections for each layer. Defaults to [1, 1, 1].</span>
<span class="sd">        stride_kv (List[int], optional): List of strides for key and value projections for each layer. Defaults to [2, 2, 2].</span>
<span class="sd">        padding_q (List[int], optional): List of paddings for query projection for each layer. Defaults to [1, 1, 1].</span>
<span class="sd">        stride_q (List[int], optional): List of strides for query projection for each layer. Defaults to [1, 1, 1].</span>
<span class="sd">        initializer_range (float, optional): The range of the initializer. Defaults to 0.02.</span>
<span class="sd">        layer_norm_eps (float, optional): Epsilon value for layer normalization. Defaults to 1e-12.</span>
<span class="sd">        **kwargs: Additional keyword arguments.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_channels</span> <span class="o">=</span> <span class="n">num_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">patch_sizes</span> <span class="o">=</span> <span class="n">patch_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">patch_stride</span> <span class="o">=</span> <span class="n">patch_stride</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">patch_padding</span> <span class="o">=</span> <span class="n">patch_padding</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_ratio</span> <span class="o">=</span> <span class="n">mlp_ratio</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_drop_rate</span> <span class="o">=</span> <span class="n">attention_drop_rate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop_rate</span> <span class="o">=</span> <span class="n">drop_rate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop_path_rate</span> <span class="o">=</span> <span class="n">drop_path_rate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">qkv_bias</span> <span class="o">=</span> <span class="n">qkv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">cls_token</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">qkv_projection_method</span> <span class="o">=</span> <span class="n">qkv_projection_method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_qkv</span> <span class="o">=</span> <span class="n">kernel_qkv</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_kv</span> <span class="o">=</span> <span class="n">padding_kv</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stride_kv</span> <span class="o">=</span> <span class="n">stride_kv</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_q</span> <span class="o">=</span> <span class="n">padding_q</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stride_q</span> <span class="o">=</span> <span class="n">stride_q</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_eps</span> <span class="o">=</span> <span class="n">layer_norm_eps</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.cvt.modeling_cvt" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>MindSpore CvT model.</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.transformers.modeling_outputs.ModelOutput">ModelOutput</span></code></p>


        <p>Base class for model's outputs, with potential hidden states and attentions.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>last_hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Sequence of hidden-states at the output of the last layer of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cls_token_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Classification token at the output of the last layer of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, 1, hidden_size)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of each layer
plus the initial embedding outputs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseModelOutputWithCLSToken</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for model&#39;s outputs, with potential hidden states and attentions.</span>

<span class="sd">    Args:</span>
<span class="sd">        last_hidden_state (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):</span>
<span class="sd">            Sequence of hidden-states at the output of the last layer of the model.</span>
<span class="sd">        cls_token_value (`mindspore.Tensor` of shape `(batch_size, 1, hidden_size)`):</span>
<span class="sd">            Classification token at the output of the last layer of the model.</span>
<span class="sd">        hidden_states (`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for the output of the embeddings + one for the output of each layer) of</span>
<span class="sd">            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer</span>
<span class="sd">            plus the initial embedding outputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">last_hidden_state</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">cls_token_value</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents an attention mechanism for the Cvt model.
It includes methods for initializing the attention mechanism, pruning specific attention heads,
and forwarding the attention output.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.num_heads">num_heads</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Number of attention heads.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.embed_dim">embed_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Dimension of the input embeddings.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.kernel_size">kernel_size</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Size of the convolutional kernel.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.padding_q">padding_q</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Padding size for query tensor.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.padding_kv">padding_kv</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Padding size for key and value tensors.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.stride_q">stride_q</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Stride size for query tensor.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.stride_kv">stride_kv</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Stride size for key and value tensors.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.qkv_projection_method">qkv_projection_method</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Method for projecting query, key, and value tensors.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.qkv_bias">qkv_bias</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Whether to include bias in query, key, and value projections.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.attention_drop_rate">attention_drop_rate</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Dropout rate for attention scores.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.drop_rate">drop_rate</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Dropout rate for output.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.with_cls_token">with_cls_token</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Whether to include a classification token in the input.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.__init__" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the attention mechanism with the given parameters.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.prune_heads" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.prune_heads">prune_heads</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Prunes specified attention heads based on the provided indices.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.forward" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the attention output using the input hidden state and spatial dimensions.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="inherits-from" open>
  <summary>Inherits from</summary>
  <p>nn.Module</p>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents an attention mechanism for the Cvt model.</span>
<span class="sd">    It includes methods for initializing the attention mechanism, pruning specific attention heads,</span>
<span class="sd">    and forwarding the attention output.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        num_heads (int): Number of attention heads.</span>
<span class="sd">        embed_dim (int): Dimension of the input embeddings.</span>
<span class="sd">        kernel_size (int): Size of the convolutional kernel.</span>
<span class="sd">        padding_q (int): Padding size for query tensor.</span>
<span class="sd">        padding_kv (int): Padding size for key and value tensors.</span>
<span class="sd">        stride_q (int): Stride size for query tensor.</span>
<span class="sd">        stride_kv (int): Stride size for key and value tensors.</span>
<span class="sd">        qkv_projection_method (str): Method for projecting query, key, and value tensors.</span>
<span class="sd">        qkv_bias (bool): Whether to include bias in query, key, and value projections.</span>
<span class="sd">        attention_drop_rate (float): Dropout rate for attention scores.</span>
<span class="sd">        drop_rate (float): Dropout rate for output.</span>
<span class="sd">        with_cls_token (bool): Whether to include a classification token in the input.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__(num_heads, embed_dim, kernel_size, padding_q, padding_kv, stride_q, stride_kv, qkv_projection_method, qkv_bias, attention_drop_rate, drop_rate, with_cls_token=True):</span>
<span class="sd">            Initializes the attention mechanism with the given parameters.</span>

<span class="sd">        prune_heads(heads):</span>
<span class="sd">            Prunes specified attention heads based on the provided indices.</span>

<span class="sd">        forward(hidden_state, height, width):</span>
<span class="sd">            Constructs the attention output using the input hidden state and spatial dimensions.</span>

<span class="sd">    Inherits from:</span>
<span class="sd">        nn.Module</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">padding_q</span><span class="p">,</span>
        <span class="n">padding_kv</span><span class="p">,</span>
        <span class="n">stride_q</span><span class="p">,</span>
        <span class="n">stride_kv</span><span class="p">,</span>
        <span class="n">qkv_projection_method</span><span class="p">,</span>
        <span class="n">qkv_bias</span><span class="p">,</span>
        <span class="n">attention_drop_rate</span><span class="p">,</span>
        <span class="n">drop_rate</span><span class="p">,</span>
        <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a CvtAttention instance with the specified parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtAttention): The current instance of the CvtAttention class.</span>
<span class="sd">            num_heads (int): The number of attention heads to use.</span>
<span class="sd">            embed_dim (int): The dimension of the input embeddings.</span>
<span class="sd">            kernel_size (int): The size of the convolutional kernel.</span>
<span class="sd">            padding_q (int): Padding size for query tensor.</span>
<span class="sd">            padding_kv (int): Padding size for key and value tensors.</span>
<span class="sd">            stride_q (int): Stride size for query tensor.</span>
<span class="sd">            stride_kv (int): Stride size for key and value tensors.</span>
<span class="sd">            qkv_projection_method (str): The method used for query, key, value projection.</span>
<span class="sd">            qkv_bias (bool): Flag indicating whether to include bias in query, key, value projection.</span>
<span class="sd">            attention_drop_rate (float): The dropout rate applied to attention weights.</span>
<span class="sd">            drop_rate (float): The dropout rate applied to the output.</span>
<span class="sd">            with_cls_token (bool): Flag indicating whether to include a classification token.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If num_heads is not a positive integer.</span>
<span class="sd">            ValueError: If embed_dim is not a positive integer.</span>
<span class="sd">            ValueError: If kernel_size is not a positive integer.</span>
<span class="sd">            ValueError: If padding_q is not a non-negative integer.</span>
<span class="sd">            ValueError: If padding_kv is not a non-negative integer.</span>
<span class="sd">            ValueError: If stride_q is not a positive integer.</span>
<span class="sd">            ValueError: If stride_kv is not a positive integer.</span>
<span class="sd">            ValueError: If qkv_projection_method is not a string.</span>
<span class="sd">            ValueError: If attention_drop_rate is not in the range [0, 1].</span>
<span class="sd">            ValueError: If drop_rate is not in the range [0, 1].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">CvtSelfAttention</span><span class="p">(</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">padding_q</span><span class="p">,</span>
            <span class="n">padding_kv</span><span class="p">,</span>
            <span class="n">stride_q</span><span class="p">,</span>
            <span class="n">stride_kv</span><span class="p">,</span>
            <span class="n">qkv_projection_method</span><span class="p">,</span>
            <span class="n">qkv_bias</span><span class="p">,</span>
            <span class="n">attention_drop_rate</span><span class="p">,</span>
            <span class="n">with_cls_token</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">CvtSelfOutput</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method &#39;prune_heads&#39; is defined within the class &#39;CvtAttention&#39; and is used to prune the attention</span>
<span class="sd">        heads based on the provided &#39;heads&#39; parameter.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (object): The instance of the &#39;CvtAttention&#39; class.</span>
<span class="sd">            heads (list): A list containing the indices of attention heads to be pruned.</span>
<span class="sd">                If the list is empty, no pruning is performed.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the length of the &#39;heads&#39; list is invalid or if any of the provided indices are out of range.</span>
<span class="sd">            TypeError: If the &#39;heads&#39; parameter is not a list or if any of the internal operations encounter unexpected data types.</span>
<span class="sd">            RuntimeError: If an unexpected error occurs during the pruning process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">heads</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">find_pruneable_heads_and_indices</span><span class="p">(</span>
            <span class="n">heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span>
        <span class="p">)</span>

        <span class="c1"># Prune linear layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">dense</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Update hyper params and store pruned heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">all_head_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_head_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs an attention output based on the given hidden state, height, and width.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtAttention): An instance of the CvtAttention class.</span>
<span class="sd">            hidden_state: The hidden state used for attention computation.</span>
<span class="sd">            height (int): The height of the attention output.</span>
<span class="sd">            width (int): The width of the attention output.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">self_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">self_output</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtAttention</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding_q</span><span class="p">,</span> <span class="n">padding_kv</span><span class="p">,</span> <span class="n">stride_q</span><span class="p">,</span> <span class="n">stride_kv</span><span class="p">,</span> <span class="n">qkv_projection_method</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="p">,</span> <span class="n">attention_drop_rate</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">,</span> <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a CvtAttention instance with the specified parameters.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The current instance of the CvtAttention class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention">CvtAttention</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads to use.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the input embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the convolutional kernel.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Padding size for query tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Padding size for key and value tensors.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Stride size for query tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Stride size for key and value tensors.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>qkv_projection_method</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The method used for query, key, value projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>qkv_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag indicating whether to include bias in query, key, value projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_drop_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate applied to attention weights.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>drop_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate applied to the output.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>with_cls_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag indicating whether to include a classification token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If num_heads is not a positive integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If embed_dim is not a positive integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If kernel_size is not a positive integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If padding_q is not a non-negative integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If padding_kv is not a non-negative integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If stride_q is not a positive integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If stride_kv is not a positive integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If qkv_projection_method is not a string.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If attention_drop_rate is not in the range [0, 1].</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If drop_rate is not in the range [0, 1].</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">,</span>
    <span class="n">padding_q</span><span class="p">,</span>
    <span class="n">padding_kv</span><span class="p">,</span>
    <span class="n">stride_q</span><span class="p">,</span>
    <span class="n">stride_kv</span><span class="p">,</span>
    <span class="n">qkv_projection_method</span><span class="p">,</span>
    <span class="n">qkv_bias</span><span class="p">,</span>
    <span class="n">attention_drop_rate</span><span class="p">,</span>
    <span class="n">drop_rate</span><span class="p">,</span>
    <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a CvtAttention instance with the specified parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtAttention): The current instance of the CvtAttention class.</span>
<span class="sd">        num_heads (int): The number of attention heads to use.</span>
<span class="sd">        embed_dim (int): The dimension of the input embeddings.</span>
<span class="sd">        kernel_size (int): The size of the convolutional kernel.</span>
<span class="sd">        padding_q (int): Padding size for query tensor.</span>
<span class="sd">        padding_kv (int): Padding size for key and value tensors.</span>
<span class="sd">        stride_q (int): Stride size for query tensor.</span>
<span class="sd">        stride_kv (int): Stride size for key and value tensors.</span>
<span class="sd">        qkv_projection_method (str): The method used for query, key, value projection.</span>
<span class="sd">        qkv_bias (bool): Flag indicating whether to include bias in query, key, value projection.</span>
<span class="sd">        attention_drop_rate (float): The dropout rate applied to attention weights.</span>
<span class="sd">        drop_rate (float): The dropout rate applied to the output.</span>
<span class="sd">        with_cls_token (bool): Flag indicating whether to include a classification token.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If num_heads is not a positive integer.</span>
<span class="sd">        ValueError: If embed_dim is not a positive integer.</span>
<span class="sd">        ValueError: If kernel_size is not a positive integer.</span>
<span class="sd">        ValueError: If padding_q is not a non-negative integer.</span>
<span class="sd">        ValueError: If padding_kv is not a non-negative integer.</span>
<span class="sd">        ValueError: If stride_q is not a positive integer.</span>
<span class="sd">        ValueError: If stride_kv is not a positive integer.</span>
<span class="sd">        ValueError: If qkv_projection_method is not a string.</span>
<span class="sd">        ValueError: If attention_drop_rate is not in the range [0, 1].</span>
<span class="sd">        ValueError: If drop_rate is not in the range [0, 1].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">CvtSelfAttention</span><span class="p">(</span>
        <span class="n">num_heads</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">padding_q</span><span class="p">,</span>
        <span class="n">padding_kv</span><span class="p">,</span>
        <span class="n">stride_q</span><span class="p">,</span>
        <span class="n">stride_kv</span><span class="p">,</span>
        <span class="n">qkv_projection_method</span><span class="p">,</span>
        <span class="n">qkv_bias</span><span class="p">,</span>
        <span class="n">attention_drop_rate</span><span class="p">,</span>
        <span class="n">with_cls_token</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">CvtSelfOutput</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtAttention</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs an attention output based on the given hidden state, height, and width.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CvtAttention class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention">CvtAttention</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden state used for attention computation.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>height</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The height of the attention output.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>width</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The width of the attention output.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs an attention output based on the given hidden state, height, and width.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtAttention): An instance of the CvtAttention class.</span>
<span class="sd">        hidden_state: The hidden state used for attention computation.</span>
<span class="sd">        height (int): The height of the attention output.</span>
<span class="sd">        width (int): The width of the attention output.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">self_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
    <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">self_output</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.prune_heads" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtAttention</span><span class="o">.</span><span class="n">prune_heads</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtAttention.prune_heads" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method 'prune_heads' is defined within the class 'CvtAttention' and is used to prune the attention
heads based on the provided 'heads' parameter.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the 'CvtAttention' class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list containing the indices of attention heads to be pruned.
If the list is empty, no pruning is performed.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>list</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the length of the 'heads' list is invalid or if any of the provided indices are out of range.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the 'heads' parameter is not a list or if any of the internal operations encounter unexpected data types.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If an unexpected error occurs during the pruning process.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method &#39;prune_heads&#39; is defined within the class &#39;CvtAttention&#39; and is used to prune the attention</span>
<span class="sd">    heads based on the provided &#39;heads&#39; parameter.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (object): The instance of the &#39;CvtAttention&#39; class.</span>
<span class="sd">        heads (list): A list containing the indices of attention heads to be pruned.</span>
<span class="sd">            If the list is empty, no pruning is performed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the length of the &#39;heads&#39; list is invalid or if any of the provided indices are out of range.</span>
<span class="sd">        TypeError: If the &#39;heads&#39; parameter is not a list or if any of the internal operations encounter unexpected data types.</span>
<span class="sd">        RuntimeError: If an unexpected error occurs during the pruning process.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">heads</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">find_pruneable_heads_and_indices</span><span class="p">(</span>
        <span class="n">heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span>
    <span class="p">)</span>

    <span class="c1"># Prune linear layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">dense</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Update hyper params and store pruned heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">all_head_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_head_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">num_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Image to Conv Embedding.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtConvEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Image to Conv Embedding.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__</span>

<span class="sd">        Initializes the CvtConvEmbeddings class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            patch_size (int or tuple): The size of the patch or kernel used for convolution.</span>
<span class="sd">                If an int is provided, the patch will be square.</span>
<span class="sd">                If a tuple is provided, it should contain two integers representing the height and width of the patch.</span>
<span class="sd">            num_channels (int): The number of input channels for the convolutional layer.</span>
<span class="sd">            embed_dim (int): The dimensionality of the output embedding.</span>
<span class="sd">            stride (int or tuple): The stride of the convolution operation.</span>
<span class="sd">                If an int is provided, the same stride is used in both dimensions.</span>
<span class="sd">                If a tuple is provided, it should contain two integers</span>
<span class="sd">                representing the stride in the height and width dimensions.</span>
<span class="sd">            padding (int or tuple): The amount of padding to be added to the input data for the convolution operation.</span>
<span class="sd">                If an int is provided, the same padding is added to both dimensions.</span>
<span class="sd">                If a tuple is provided, it should contain two integers representing the padding</span>
<span class="sd">                in the height and width dimensions.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pixel_values</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the pixel embeddings for a given set of pixel values.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtConvEmbeddings): An instance of the CvtConvEmbeddings class.</span>
<span class="sd">            pixel_values (torch.Tensor): A tensor containing the pixel values of the image.</span>
<span class="sd">                It should have the shape (batch_size, num_channels, height, width).</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method modifies the pixel_values tensor in-place.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pixel_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">pixel_values</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span>
        <span class="c1"># rearrange &quot;b c h w -&gt; b (h w) c&quot;</span>
        <span class="n">pixel_values</span> <span class="o">=</span> <span class="n">pixel_values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="p">:</span>
            <span class="n">pixel_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">)</span>
        <span class="c1"># rearrange &quot;b (h w) c&quot; -&gt; b c h w&quot;</span>
        <span class="n">pixel_values</span> <span class="o">=</span> <span class="n">pixel_values</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pixel_values</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtConvEmbeddings</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p><strong>init</strong></p>
<p>Initializes the CvtConvEmbeddings class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>patch_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the patch or kernel used for convolution.
If an int is provided, the patch will be square.
If a tuple is provided, it should contain two integers representing the height and width of the patch.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int or tuple</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_channels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of input channels for the convolutional layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimensionality of the output embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride of the convolution operation.
If an int is provided, the same stride is used in both dimensions.
If a tuple is provided, it should contain two integers
representing the stride in the height and width dimensions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int or tuple</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The amount of padding to be added to the input data for the convolution operation.
If an int is provided, the same padding is added to both dimensions.
If a tuple is provided, it should contain two integers representing the padding
in the height and width dimensions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int or tuple</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    __init__</span>

<span class="sd">    Initializes the CvtConvEmbeddings class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        patch_size (int or tuple): The size of the patch or kernel used for convolution.</span>
<span class="sd">            If an int is provided, the patch will be square.</span>
<span class="sd">            If a tuple is provided, it should contain two integers representing the height and width of the patch.</span>
<span class="sd">        num_channels (int): The number of input channels for the convolutional layer.</span>
<span class="sd">        embed_dim (int): The dimensionality of the output embedding.</span>
<span class="sd">        stride (int or tuple): The stride of the convolution operation.</span>
<span class="sd">            If an int is provided, the same stride is used in both dimensions.</span>
<span class="sd">            If a tuple is provided, it should contain two integers</span>
<span class="sd">            representing the stride in the height and width dimensions.</span>
<span class="sd">        padding (int or tuple): The amount of padding to be added to the input data for the convolution operation.</span>
<span class="sd">            If an int is provided, the same padding is added to both dimensions.</span>
<span class="sd">            If a tuple is provided, it should contain two integers representing the padding</span>
<span class="sd">            in the height and width dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtConvEmbeddings</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the pixel embeddings for a given set of pixel values.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CvtConvEmbeddings class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtConvEmbeddings">CvtConvEmbeddings</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pixel_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tensor containing the pixel values of the image.
It should have the shape (batch_size, num_channels, height, width).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="torch.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method modifies the pixel_values tensor in-place.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pixel_values</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the pixel embeddings for a given set of pixel values.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtConvEmbeddings): An instance of the CvtConvEmbeddings class.</span>
<span class="sd">        pixel_values (torch.Tensor): A tensor containing the pixel values of the image.</span>
<span class="sd">            It should have the shape (batch_size, num_channels, height, width).</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: This method modifies the pixel_values tensor in-place.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pixel_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">pixel_values</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span>
    <span class="c1"># rearrange &quot;b c h w -&gt; b (h w) c&quot;</span>
    <span class="n">pixel_values</span> <span class="o">=</span> <span class="n">pixel_values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="p">:</span>
        <span class="n">pixel_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">)</span>
    <span class="c1"># rearrange &quot;b (h w) c&quot; -&gt; b c h w&quot;</span>
    <span class="n">pixel_values</span> <span class="o">=</span> <span class="n">pixel_values</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pixel_values</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtDropPath</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the CvtDropPath class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            drop_prob (Optional[float]): The probability of dropping a connection during training. Defaults to None.</span>
<span class="sd">                Must be a float value between 0 and 1, inclusive.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span> <span class="o">=</span> <span class="n">drop_prob</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method forwards a modified version of the input hidden_states tensor using the drop_path operation.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtDropPath): The instance of the CvtDropPath class.</span>
<span class="sd">            hidden_states (mindspore.Tensor): The input tensor representing hidden states.</span>
<span class="sd">                It should be a tensor of arbitrary shape and type.</span>

<span class="sd">        Returns:</span>
<span class="sd">            mindspore.Tensor: A tensor of the same shape and type as the input hidden_states tensor,</span>
<span class="sd">                but with the drop_path operation applied.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the input hidden_states tensor is not a valid mindspore.Tensor object.</span>
<span class="sd">            RuntimeError: If an error occurs during the drop_path operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">drop_path</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method provides a string representation for the CvtDropPath class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: CvtDropPath instance. Represents the current instance of the CvtDropPath class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: A string representing the drop probability of the CvtDropPath instance.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;p=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtDropPath</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">drop_prob</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the CvtDropPath class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>drop_prob</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The probability of dropping a connection during training. Defaults to None.
Must be a float value between 0 and 1, inclusive.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code>None</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the CvtDropPath class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        drop_prob (Optional[float]): The probability of dropping a connection during training. Defaults to None.</span>
<span class="sd">            Must be a float value between 0 and 1, inclusive.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span> <span class="o">=</span> <span class="n">drop_prob</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.extra_repr" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtDropPath</span><span class="o">.</span><span class="n">extra_repr</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.extra_repr" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method provides a string representation for the CvtDropPath class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>CvtDropPath instance. Represents the current instance of the CvtDropPath class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>str</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A string representing the drop probability of the CvtDropPath instance.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method provides a string representation for the CvtDropPath class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: CvtDropPath instance. Represents the current instance of the CvtDropPath class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: A string representing the drop probability of the CvtDropPath instance.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;p=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtDropPath</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method forwards a modified version of the input hidden_states tensor using the drop_path operation.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CvtDropPath class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtDropPath">CvtDropPath</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor representing hidden states.
It should be a tensor of arbitrary shape and type.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindspore.Tensor">Tensor</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor: A tensor of the same shape and type as the input hidden_states tensor,
but with the drop_path operation applied.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input hidden_states tensor is not a valid mindspore.Tensor object.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If an error occurs during the drop_path operation.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method forwards a modified version of the input hidden_states tensor using the drop_path operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtDropPath): The instance of the CvtDropPath class.</span>
<span class="sd">        hidden_states (mindspore.Tensor): The input tensor representing hidden states.</span>
<span class="sd">            It should be a tensor of arbitrary shape and type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        mindspore.Tensor: A tensor of the same shape and type as the input hidden_states tensor,</span>
<span class="sd">            but with the drop_path operation applied.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the input hidden_states tensor is not a valid mindspore.Tensor object.</span>
<span class="sd">        RuntimeError: If an error occurs during the drop_path operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">drop_path</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Construct the CvT embeddings.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct the CvT embeddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the CvtEmbeddings class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The object instance.</span>
<span class="sd">            patch_size (int): The size of the patches to be extracted from the input image.</span>
<span class="sd">            num_channels (int): The number of input channels in the image.</span>
<span class="sd">            embed_dim (int): The dimension of the embedded representation.</span>
<span class="sd">            stride (int): The stride of the convolution operation.</span>
<span class="sd">            padding (int): The amount of padding to be added to the input image.</span>
<span class="sd">            dropout_rate (float): The dropout rate to be applied to the convolutional embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convolution_embeddings</span> <span class="o">=</span> <span class="n">CvtConvEmbeddings</span><span class="p">(</span>
            <span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="o">=</span><span class="n">num_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pixel_values</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the hidden state using convolutional embeddings.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtEmbeddings): The instance of the CvtEmbeddings class.</span>
<span class="sd">            pixel_values (array-like): An array-like object containing pixel values for image data.</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.ndarray: The hidden state forwarded using convolutional embeddings.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the pixel_values parameter is empty or not valid.</span>
<span class="sd">            TypeError: If the pixel_values parameter is not array-like.</span>
<span class="sd">            RuntimeError: If an unexpected error occurs during the forwardion process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution_embeddings</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">)</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtEmbeddings</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the CvtEmbeddings class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The object instance.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>patch_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the patches to be extracted from the input image.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_channels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of input channels in the image.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the embedded representation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride of the convolution operation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The amount of padding to be added to the input image.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate to be applied to the convolutional embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the CvtEmbeddings class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The object instance.</span>
<span class="sd">        patch_size (int): The size of the patches to be extracted from the input image.</span>
<span class="sd">        num_channels (int): The number of input channels in the image.</span>
<span class="sd">        embed_dim (int): The dimension of the embedded representation.</span>
<span class="sd">        stride (int): The stride of the convolution operation.</span>
<span class="sd">        padding (int): The amount of padding to be added to the input image.</span>
<span class="sd">        dropout_rate (float): The dropout rate to be applied to the convolutional embeddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">convolution_embeddings</span> <span class="o">=</span> <span class="n">CvtConvEmbeddings</span><span class="p">(</span>
        <span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="o">=</span><span class="n">num_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtEmbeddings</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the hidden state using convolutional embeddings.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CvtEmbeddings class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEmbeddings">CvtEmbeddings</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pixel_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An array-like object containing pixel values for image data.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>array - like</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>numpy.ndarray: The hidden state forwarded using convolutional embeddings.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the pixel_values parameter is empty or not valid.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the pixel_values parameter is not array-like.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If an unexpected error occurs during the forwardion process.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pixel_values</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the hidden state using convolutional embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtEmbeddings): The instance of the CvtEmbeddings class.</span>
<span class="sd">        pixel_values (array-like): An array-like object containing pixel values for image data.</span>

<span class="sd">    Returns:</span>
<span class="sd">        numpy.ndarray: The hidden state forwarded using convolutional embeddings.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the pixel_values parameter is empty or not valid.</span>
<span class="sd">        TypeError: If the pixel_values parameter is not array-like.</span>
<span class="sd">        RuntimeError: If an unexpected error occurs during the forwardion process.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution_embeddings</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">)</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents a converter encoder used for converting pixel values to hidden states. It is a subclass of nn.Module.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder.config">config</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The configuration object for the CvtEncoder.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>Config</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder.stages">stages</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A list of CvtStage instances representing the stages of the converter encoder.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.ModuleList">ModuleList</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a converter encoder used for converting pixel values to hidden states. It is a subclass of nn.Module.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        config (Config): The configuration object for the CvtEncoder.</span>
<span class="sd">        stages (nn.ModuleList): A list of CvtStage instances representing the stages of the converter encoder.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__(self, config)</span>
<span class="sd">            Initializes a new instance of the CvtEncoder class.</span>

<span class="sd">            Args:</span>

<span class="sd">            - config (Config): The configuration object for the CvtEncoder.</span>

<span class="sd">        forward(self, pixel_values, output_hidden_states=False, return_dict=True)</span>
<span class="sd">            Constructs the converter encoder model.</span>

<span class="sd">            Args:</span>

<span class="sd">            - pixel_values (tensor): The input pixel values.</span>
<span class="sd">            - output_hidden_states (bool): Whether to output all hidden states. Defaults to False.</span>
<span class="sd">            - return_dict (bool): Whether to return the model output as a dictionary. Defaults to True.</span>

<span class="sd">            Returns:</span>

<span class="sd">            - BaseModelOutputWithCLSToken: The model output containing the last hidden state, the cls token value, and</span>
<span class="sd">            all hidden states.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the CvtEncoder class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config (object): The configuration object that holds the parameters for the encoder.</span>
<span class="sd">                This object is used to configure the behavior of the encoder.</span>
<span class="sd">                It must be an instance of the Config class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stages</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([])</span>
        <span class="k">for</span> <span class="n">stage_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">depth</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">CvtStage</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">stage_idx</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pixel_values</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the CvTEncoder.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtEncoder): The instance of the CvtEncoder class.</span>
<span class="sd">            pixel_values (Any): The input pixel values.</span>
<span class="sd">            output_hidden_states (bool): Whether to output hidden states or not. Defaults to False.</span>
<span class="sd">            return_dict (bool): Whether to return the result as a dictionary or not. Defaults to True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">pixel_values</span>

        <span class="n">cls_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">stage_module</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stages</span><span class="p">):</span>
            <span class="n">hidden_state</span><span class="p">,</span> <span class="n">cls_token</span> <span class="o">=</span> <span class="n">stage_module</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_state</span><span class="p">,)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">cls_token</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">BaseModelOutputWithCLSToken</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_state</span><span class="p">,</span>
            <span class="n">cls_token_value</span><span class="o">=</span><span class="n">cls_token</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtEncoder</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the CvtEncoder class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object that holds the parameters for the encoder.
This object is used to configure the behavior of the encoder.
It must be an instance of the Config class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the CvtEncoder class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config (object): The configuration object that holds the parameters for the encoder.</span>
<span class="sd">            This object is used to configure the behavior of the encoder.</span>
<span class="sd">            It must be an instance of the Config class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stages</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([])</span>
    <span class="k">for</span> <span class="n">stage_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">depth</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">CvtStage</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">stage_idx</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtEncoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the CvTEncoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CvtEncoder class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder">CvtEncoder</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pixel_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input pixel values.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.Any">Any</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to output hidden states or not. Defaults to False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to return the result as a dictionary or not. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pixel_values</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the CvTEncoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtEncoder): The instance of the CvtEncoder class.</span>
<span class="sd">        pixel_values (Any): The input pixel values.</span>
<span class="sd">        output_hidden_states (bool): Whether to output hidden states or not. Defaults to False.</span>
<span class="sd">        return_dict (bool): Whether to return the result as a dictionary or not. Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">pixel_values</span>

    <span class="n">cls_token</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">stage_module</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stages</span><span class="p">):</span>
        <span class="n">hidden_state</span><span class="p">,</span> <span class="n">cls_token</span> <span class="o">=</span> <span class="n">stage_module</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_state</span><span class="p">,)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">cls_token</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">BaseModelOutputWithCLSToken</span><span class="p">(</span>
        <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_state</span><span class="p">,</span>
        <span class="n">cls_token_value</span><span class="o">=</span><span class="n">cls_token</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtPreTrainedModel" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtPreTrainedModel">CvtPreTrainedModel</a></code></p>


        <p>CvtForImageClassification is a class that represents a model for image classification utilizing the Cvt architecture.
It inherits from the CvtPreTrainedModel class and provides methods for forwarding the model and computing
image classification/regression loss.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.num_labels">num_labels</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Number of labels for classification</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.cvt">cvt</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>CvtModel instance used for image processing</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtModel" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel">CvtModel</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.layernorm">layernorm</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Layer normalization module</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.LayerNorm">LayerNorm</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.classifier">classifier</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Classifier module for final predictions</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Linear">Linear</span> or <span title="mindnlp.core.nn.Identity">Identity</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.__init__" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CvtForImageClassification model with the provided configuration.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.forward" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the model and computes loss for image classification.</p>
<p>Parameters:</p>
<ul>
<li>pixel_values (Optional[mindspore.Tensor]): Tensor containing pixel values of images</li>
<li>labels (Optional[mindspore.Tensor]): Tensor containing labels for computing classification/regression loss</li>
<li>output_hidden_states (Optional[bool]): Flag to indicate whether to output hidden states</li>
<li>return_dict (Optional[bool]): Flag to indicate whether to return output as a dictionary</li>
</ul>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Union[Tuple, ImageClassifierOutputWithNoAttention]: Tuple containing loss and output if return_dict is False.
Otherwise, returns an ImageClassifierOutputWithNoAttention instance.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="notes" open>
  <summary>Notes</summary>
  <ul>
<li>The 'forward' method handles the processing of input pixel values, computation of logits,
and determination of loss based on the configuration settings.</li>
<li>The loss calculation depends on the problem type (regression, single_label_classification,
or multi_label_classification) and the number of labels.</li>
<li>The final output includes logits and optionally hidden states depending on the return_dict flag.</li>
</ul>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtForImageClassification</span><span class="p">(</span><span class="n">CvtPreTrainedModel</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CvtForImageClassification is a class that represents a model for image classification utilizing the Cvt architecture.</span>
<span class="sd">    It inherits from the CvtPreTrainedModel class and provides methods for forwarding the model and computing</span>
<span class="sd">    image classification/regression loss.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        num_labels (int): Number of labels for classification</span>
<span class="sd">        cvt (CvtModel): CvtModel instance used for image processing</span>
<span class="sd">        layernorm (nn.LayerNorm): Layer normalization module</span>
<span class="sd">        classifier (nn.Linear or nn.Identity): Classifier module for final predictions</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__(self, config): Initializes the CvtForImageClassification model with the provided configuration.</span>
<span class="sd">        forward(self, pixel_values, labels, output_hidden_states, return_dict):</span>
<span class="sd">            Constructs the model and computes loss for image classification.</span>

<span class="sd">            Parameters:</span>

<span class="sd">            - pixel_values (Optional[mindspore.Tensor]): Tensor containing pixel values of images</span>
<span class="sd">            - labels (Optional[mindspore.Tensor]): Tensor containing labels for computing classification/regression loss</span>
<span class="sd">            - output_hidden_states (Optional[bool]): Flag to indicate whether to output hidden states</span>
<span class="sd">            - return_dict (Optional[bool]): Flag to indicate whether to return output as a dictionary</span>

<span class="sd">    Returns:</span>
<span class="sd">        Union[Tuple, ImageClassifierOutputWithNoAttention]: Tuple containing loss and output if return_dict is False.</span>
<span class="sd">          Otherwise, returns an ImageClassifierOutputWithNoAttention instance.</span>

<span class="sd">    Notes:</span>
<span class="sd">        - The &#39;forward&#39; method handles the processing of input pixel values, computation of logits,</span>
<span class="sd">        and determination of loss based on the configuration settings.</span>
<span class="sd">        - The loss calculation depends on the problem type (regression, single_label_classification,</span>
<span class="sd">        or multi_label_classification) and the number of labels.</span>
<span class="sd">        - The final output includes logits and optionally hidden states depending on the return_dict flag.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a new instance of the CvtForImageClassification class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The object itself.</span>
<span class="sd">            config:</span>
<span class="sd">                An instance of the class Config containing the configuration settings.</span>

<span class="sd">                - Type: object</span>
<span class="sd">                - Purpose: Stores the configuration settings for the model.</span>
<span class="sd">                - Restrictions: Must be a valid instance of the Config class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cvt</span> <span class="o">=</span> <span class="n">CvtModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># Classifier head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pixel_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">ImageClassifierOutputWithNoAttention</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):</span>
<span class="sd">                Labels for computing the image classification/regression loss. Indices should be in `[0, ...,</span>
<span class="sd">                config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If</span>
<span class="sd">                `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cvt</span><span class="p">(</span>
            <span class="n">pixel_values</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">cls_token</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">sequence_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">cls_token</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">sequence_output</span><span class="o">.</span><span class="n">shape</span>
            <span class="c1"># rearrange &quot;b c h w -&gt; b (h w) c&quot;</span>
            <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">sequence_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">sequence_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">sequence_output_mean</span> <span class="o">=</span> <span class="n">sequence_output</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output_mean</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;regression&quot;</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">labels</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;single_label_classification&quot;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;multi_label_classification&quot;</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;single_label_classification&quot;</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;multi_label_classification&quot;</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">ImageClassifierOutputWithNoAttention</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtForImageClassification</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a new instance of the CvtForImageClassification class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The object itself.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the class Config containing the configuration settings.</p>
<ul>
<li>Type: object</li>
<li>Purpose: Stores the configuration settings for the model.</li>
<li>Restrictions: Must be a valid instance of the Config class.</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a new instance of the CvtForImageClassification class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The object itself.</span>
<span class="sd">        config:</span>
<span class="sd">            An instance of the class Config containing the configuration settings.</span>

<span class="sd">            - Type: object</span>
<span class="sd">            - Purpose: Stores the configuration settings for the model.</span>
<span class="sd">            - Restrictions: Must be a valid instance of the Config class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cvt</span> <span class="o">=</span> <span class="n">CvtModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># Classifier head</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="c1"># Initialize weights and apply final processing</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtForImageClassification</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">pixel_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtForImageClassification.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Labels for computing the image classification/regression loss. Indices should be in <code>[0, ...,
config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`torch.LongTensor` of shape `(batch_size,)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pixel_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">ImageClassifierOutputWithNoAttention</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):</span>
<span class="sd">            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,</span>
<span class="sd">            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If</span>
<span class="sd">            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cvt</span><span class="p">(</span>
        <span class="n">pixel_values</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">cls_token</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">cls_token</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">cls_token</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">sequence_output</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># rearrange &quot;b c h w -&gt; b (h w) c&quot;</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">sequence_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

    <span class="n">sequence_output_mean</span> <span class="o">=</span> <span class="n">sequence_output</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output_mean</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;regression&quot;</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">labels</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;single_label_classification&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;multi_label_classification&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;single_label_classification&quot;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;multi_label_classification&quot;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

    <span class="k">return</span> <span class="n">ImageClassifierOutputWithNoAttention</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Represents an intermediate layer in a Convolutional Vision Transformer (CVT) network.</p>
<p>This class defines an intermediate layer in a CVT network that consists of a dense layer followed by
a GELU activation function. The intermediate layer is used to process the hidden states in the network.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.embed_dim">embed_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dimension of the input embeddings.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.mlp_ratio">mlp_ratio</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The ratio used to determine the size of the hidden layer in the dense layer.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.__init__" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CvtIntermediate object with the specified embedding dimension and MLP ratio.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.forward" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the intermediate layer by applying a dense layer and GELU activation function to the input hidden state.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="inherits-from" open>
  <summary>Inherits from</summary>
  <p>nn.Module</p>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtIntermediate</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents an intermediate layer in a Convolutional Vision Transformer (CVT) network.</span>

<span class="sd">    This class defines an intermediate layer in a CVT network that consists of a dense layer followed by</span>
<span class="sd">    a GELU activation function. The intermediate layer is used to process the hidden states in the network.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        embed_dim (int): The dimension of the input embeddings.</span>
<span class="sd">        mlp_ratio (float): The ratio used to determine the size of the hidden layer in the dense layer.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__(self, embed_dim, mlp_ratio):</span>
<span class="sd">            Initializes the CvtIntermediate object with the specified embedding dimension and MLP ratio.</span>
<span class="sd">        forward(self, hidden_state):</span>
<span class="sd">            Constructs the intermediate layer by applying a dense layer and GELU activation function to the input hidden state.</span>

<span class="sd">    Inherits from:</span>
<span class="sd">        nn.Module</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the CvtIntermediate class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtIntermediate): The instance of the class.</span>
<span class="sd">            embed_dim (int): The dimension of the embedding.</span>
<span class="sd">            mlp_ratio (float): The ratio used to calculate the hidden dimension of the MLP.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the hidden state of the CvtIntermediate class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtIntermediate): An instance of the CvtIntermediate class.</span>
<span class="sd">            hidden_state: The hidden state to be processed. It should be a tensor or array-like object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method modifies the hidden state in-place.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        This method takes in the &#39;hidden_state&#39; and applies transformations to it in order to forward the</span>
<span class="sd">        hidden state of the CvtIntermediate class.</span>
<span class="sd">        The &#39;hidden_state&#39; is first passed through a dense layer using the &#39;self.dense&#39; function.</span>
<span class="sd">        Then, the resulting tensor is passed through the activation function specified by the &#39;self.activation&#39; attribute.</span>
<span class="sd">        The modified hidden state is returned as the output of this method.</span>

<span class="sd">        Note that this method modifies the hidden state in-place and does not create a new object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtIntermediate</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the CvtIntermediate class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate">CvtIntermediate</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_ratio</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ratio used to calculate the hidden dimension of the MLP.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the CvtIntermediate class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtIntermediate): The instance of the class.</span>
<span class="sd">        embed_dim (int): The dimension of the embedding.</span>
<span class="sd">        mlp_ratio (float): The ratio used to calculate the hidden dimension of the MLP.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtIntermediate</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the hidden state of the CvtIntermediate class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CvtIntermediate class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtIntermediate">CvtIntermediate</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden state to be processed. It should be a tensor or array-like object.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method modifies the hidden state in-place.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <p>This method takes in the 'hidden_state' and applies transformations to it in order to forward the
hidden state of the CvtIntermediate class.
The 'hidden_state' is first passed through a dense layer using the 'self.dense' function.
Then, the resulting tensor is passed through the activation function specified by the 'self.activation' attribute.
The modified hidden state is returned as the output of this method.</p>
<p>Note that this method modifies the hidden state in-place and does not create a new object.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the hidden state of the CvtIntermediate class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtIntermediate): An instance of the CvtIntermediate class.</span>
<span class="sd">        hidden_state: The hidden state to be processed. It should be a tensor or array-like object.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: This method modifies the hidden state in-place.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>

<span class="sd">    This method takes in the &#39;hidden_state&#39; and applies transformations to it in order to forward the</span>
<span class="sd">    hidden state of the CvtIntermediate class.</span>
<span class="sd">    The &#39;hidden_state&#39; is first passed through a dense layer using the &#39;self.dense&#39; function.</span>
<span class="sd">    Then, the resulting tensor is passed through the activation function specified by the &#39;self.activation&#39; attribute.</span>
<span class="sd">    The modified hidden state is returned as the output of this method.</span>

<span class="sd">    Note that this method modifies the hidden state in-place and does not create a new object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>CvtLayer composed by attention layers, normalization and multi-layer perceptrons (mlps).</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CvtLayer composed by attention layers, normalization and multi-layer perceptrons (mlps).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">padding_q</span><span class="p">,</span>
        <span class="n">padding_kv</span><span class="p">,</span>
        <span class="n">stride_q</span><span class="p">,</span>
        <span class="n">stride_kv</span><span class="p">,</span>
        <span class="n">qkv_projection_method</span><span class="p">,</span>
        <span class="n">qkv_bias</span><span class="p">,</span>
        <span class="n">attention_drop_rate</span><span class="p">,</span>
        <span class="n">drop_rate</span><span class="p">,</span>
        <span class="n">mlp_ratio</span><span class="p">,</span>
        <span class="n">drop_path_rate</span><span class="p">,</span>
        <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the CvtLayer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The object instance.</span>
<span class="sd">            num_heads (int): The number of attention heads.</span>
<span class="sd">            embed_dim (int): The dimensionality of the embedding.</span>
<span class="sd">            kernel_size (int): The kernel size for the attention computation.</span>
<span class="sd">            padding_q (int): The padding size for queries.</span>
<span class="sd">            padding_kv (int): The padding size for key and value.</span>
<span class="sd">            stride_q (int): The stride size for queries.</span>
<span class="sd">            stride_kv (int): The stride size for key and value.</span>
<span class="sd">            qkv_projection_method (str): The method used for query, key, and value projection.</span>
<span class="sd">            qkv_bias (bool): Whether to include bias in query, key, and value projection.</span>
<span class="sd">            attention_drop_rate (float): The dropout rate for attention weights.</span>
<span class="sd">            drop_rate (float): The dropout rate for the output tensor.</span>
<span class="sd">            mlp_ratio (float): The ratio of the hidden size to the input size in the intermediate layer.</span>
<span class="sd">            drop_path_rate (float): The dropout rate for the residual connection.</span>
<span class="sd">            with_cls_token (bool): Whether to include a classification token.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">CvtAttention</span><span class="p">(</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">padding_q</span><span class="p">,</span>
            <span class="n">padding_kv</span><span class="p">,</span>
            <span class="n">stride_q</span><span class="p">,</span>
            <span class="n">stride_kv</span><span class="p">,</span>
            <span class="n">qkv_projection_method</span><span class="p">,</span>
            <span class="n">qkv_bias</span><span class="p">,</span>
            <span class="n">attention_drop_rate</span><span class="p">,</span>
            <span class="n">drop_rate</span><span class="p">,</span>
            <span class="n">with_cls_token</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate</span> <span class="o">=</span> <span class="n">CvtIntermediate</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">CvtOutput</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="n">CvtDropPath</span><span class="p">(</span><span class="n">drop_prob</span><span class="o">=</span><span class="n">drop_path_rate</span><span class="p">)</span> <span class="k">if</span> <span class="n">drop_path_rate</span> <span class="o">&gt;</span> <span class="mf">0.0</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_after</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method forwards a layer in the CvtLayer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (object): The instance of the CvtLayer class.</span>
<span class="sd">            hidden_state (tensor): The hidden state of the layer.</span>
<span class="sd">            height (int): The height of the input tensor.</span>
<span class="sd">            width (int): The width of the input tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the hidden_state is not a valid tensor.</span>
<span class="sd">            TypeError: If height and width are not integer values.</span>
<span class="sd">            RuntimeError: If an unexpected error occurs during the execution of the method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">self_attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">),</span>  <span class="c1"># in Cvt, layernorm is applied before self-attention</span>
            <span class="n">height</span><span class="p">,</span>
            <span class="n">width</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self_attention_output</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>

        <span class="c1"># first residual connection</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">attention_output</span> <span class="o">+</span> <span class="n">hidden_state</span>

        <span class="c1"># in Cvt, layernorm is also applied after self-attention</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_after</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>

        <span class="c1"># second residual connection is done here</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">layer_output</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtLayer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding_q</span><span class="p">,</span> <span class="n">padding_kv</span><span class="p">,</span> <span class="n">stride_q</span><span class="p">,</span> <span class="n">stride_kv</span><span class="p">,</span> <span class="n">qkv_projection_method</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="p">,</span> <span class="n">attention_drop_rate</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="p">,</span> <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the CvtLayer class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The object instance.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimensionality of the embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size for the attention computation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The padding size for queries.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The padding size for key and value.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride size for queries.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride size for key and value.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>qkv_projection_method</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The method used for query, key, and value projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>qkv_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to include bias in query, key, and value projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_drop_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate for attention weights.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>drop_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate for the output tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_ratio</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ratio of the hidden size to the input size in the intermediate layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>drop_path_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate for the residual connection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>with_cls_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to include a classification token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">,</span>
    <span class="n">padding_q</span><span class="p">,</span>
    <span class="n">padding_kv</span><span class="p">,</span>
    <span class="n">stride_q</span><span class="p">,</span>
    <span class="n">stride_kv</span><span class="p">,</span>
    <span class="n">qkv_projection_method</span><span class="p">,</span>
    <span class="n">qkv_bias</span><span class="p">,</span>
    <span class="n">attention_drop_rate</span><span class="p">,</span>
    <span class="n">drop_rate</span><span class="p">,</span>
    <span class="n">mlp_ratio</span><span class="p">,</span>
    <span class="n">drop_path_rate</span><span class="p">,</span>
    <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the CvtLayer class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The object instance.</span>
<span class="sd">        num_heads (int): The number of attention heads.</span>
<span class="sd">        embed_dim (int): The dimensionality of the embedding.</span>
<span class="sd">        kernel_size (int): The kernel size for the attention computation.</span>
<span class="sd">        padding_q (int): The padding size for queries.</span>
<span class="sd">        padding_kv (int): The padding size for key and value.</span>
<span class="sd">        stride_q (int): The stride size for queries.</span>
<span class="sd">        stride_kv (int): The stride size for key and value.</span>
<span class="sd">        qkv_projection_method (str): The method used for query, key, and value projection.</span>
<span class="sd">        qkv_bias (bool): Whether to include bias in query, key, and value projection.</span>
<span class="sd">        attention_drop_rate (float): The dropout rate for attention weights.</span>
<span class="sd">        drop_rate (float): The dropout rate for the output tensor.</span>
<span class="sd">        mlp_ratio (float): The ratio of the hidden size to the input size in the intermediate layer.</span>
<span class="sd">        drop_path_rate (float): The dropout rate for the residual connection.</span>
<span class="sd">        with_cls_token (bool): Whether to include a classification token.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">CvtAttention</span><span class="p">(</span>
        <span class="n">num_heads</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">padding_q</span><span class="p">,</span>
        <span class="n">padding_kv</span><span class="p">,</span>
        <span class="n">stride_q</span><span class="p">,</span>
        <span class="n">stride_kv</span><span class="p">,</span>
        <span class="n">qkv_projection_method</span><span class="p">,</span>
        <span class="n">qkv_bias</span><span class="p">,</span>
        <span class="n">attention_drop_rate</span><span class="p">,</span>
        <span class="n">drop_rate</span><span class="p">,</span>
        <span class="n">with_cls_token</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">intermediate</span> <span class="o">=</span> <span class="n">CvtIntermediate</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">CvtOutput</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="n">CvtDropPath</span><span class="p">(</span><span class="n">drop_prob</span><span class="o">=</span><span class="n">drop_path_rate</span><span class="p">)</span> <span class="k">if</span> <span class="n">drop_path_rate</span> <span class="o">&gt;</span> <span class="mf">0.0</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_after</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtLayer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtLayer.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method forwards a layer in the CvtLayer class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CvtLayer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden state of the layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>tensor</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>height</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The height of the input tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>width</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The width of the input tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the hidden_state is not a valid tensor.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If height and width are not integer values.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If an unexpected error occurs during the execution of the method.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method forwards a layer in the CvtLayer class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (object): The instance of the CvtLayer class.</span>
<span class="sd">        hidden_state (tensor): The hidden state of the layer.</span>
<span class="sd">        height (int): The height of the input tensor.</span>
<span class="sd">        width (int): The width of the input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the hidden_state is not a valid tensor.</span>
<span class="sd">        TypeError: If height and width are not integer values.</span>
<span class="sd">        RuntimeError: If an unexpected error occurs during the execution of the method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">self_attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">),</span>  <span class="c1"># in Cvt, layernorm is applied before self-attention</span>
        <span class="n">height</span><span class="p">,</span>
        <span class="n">width</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self_attention_output</span>
    <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>

    <span class="c1"># first residual connection</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">attention_output</span> <span class="o">+</span> <span class="n">hidden_state</span>

    <span class="c1"># in Cvt, layernorm is also applied after self-attention</span>
    <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_after</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>

    <span class="c1"># second residual connection is done here</span>
    <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">)</span>
    <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layer_output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtModel</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtPreTrainedModel" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtPreTrainedModel">CvtPreTrainedModel</a></code></p>


        <p>CvtModel is a model class that represents a Convolutional Vision Transformer (Cvt) model for processing visual data.
This class inherits from CvtPreTrainedModel and provides functionalities for initializing the model, pruning heads,
and forwarding the model output.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.config">config</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The configuration object for the model.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.configuration_cvt.CvtConfig" href="#mindnlp.transformers.models.cvt.configuration_cvt.CvtConfig">CvtConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.encoder">encoder</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The encoder component of the CvtModel responsible for processing input data.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtEncoder">CvtEncoder</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.__init__" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CvtModel instance with the provided configuration.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtModel._prune_heads">_prune_heads</span></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Prunes specified heads of the model based on the provided dictionary of layer numbers and heads to prune.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.forward" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the model output by processing the input pixel values and returning the output hidden states.
If pixel_values is not provided, a ValueError is raised.
The output format is determined based on the return_dict flag and the model configuration.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtModel</span><span class="p">(</span><span class="n">CvtPreTrainedModel</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CvtModel is a model class that represents a Convolutional Vision Transformer (Cvt) model for processing visual data.</span>
<span class="sd">    This class inherits from CvtPreTrainedModel and provides functionalities for initializing the model, pruning heads,</span>
<span class="sd">    and forwarding the model output.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        config (CvtConfig): The configuration object for the model.</span>
<span class="sd">        encoder (CvtEncoder): The encoder component of the CvtModel responsible for processing input data.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__:</span>
<span class="sd">            Initializes the CvtModel instance with the provided configuration.</span>

<span class="sd">        _prune_heads:</span>
<span class="sd">            Prunes specified heads of the model based on the provided dictionary of layer numbers and heads to prune.</span>

<span class="sd">        forward:</span>
<span class="sd">            Constructs the model output by processing the input pixel values and returning the output hidden states.</span>
<span class="sd">            If pixel_values is not provided, a ValueError is raised.</span>
<span class="sd">            The output format is determined based on the return_dict flag and the model configuration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a new instance of the CvtModel class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (object): The instance of the CvtModel class.</span>
<span class="sd">            config (object): The configuration object containing model settings and parameters.</span>
<span class="sd">            add_pooling_layer (bool, optional): A flag indicating whether to add a pooling layer. Default is True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the provided config is invalid or missing required parameters.</span>
<span class="sd">            TypeError: If the provided config is not of the expected type.</span>
<span class="sd">            RuntimeError: If an error occurs during initialization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">CvtEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads_to_prune</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base</span>
<span class="sd">        class PreTrainedModel</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">heads</span> <span class="ow">in</span> <span class="n">heads_to_prune</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">prune_heads</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pixel_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutputWithCLSToken</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the CvtModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtModel): The instance of the CvtModel class.</span>
<span class="sd">            pixel_values (Optional[mindspore.Tensor]): The pixel values of the input image. Default is None.</span>
<span class="sd">            output_hidden_states (Optional[bool]): Whether to output hidden states. Default is None.</span>
<span class="sd">            return_dict (Optional[bool]): Whether to return a dictionary output. Default is None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[Tuple, BaseModelOutputWithCLSToken]:</span>
<span class="sd">                The forwarded model output.</span>

<span class="sd">                - If `return_dict` is False, a tuple is returned containing the sequence output and any additional</span>
<span class="sd">                encoder outputs.</span>
<span class="sd">                - If `return_dict` is True, a BaseModelOutputWithCLSToken object is returned, which includes</span>
<span class="sd">                the last hidden state, cls token value, and hidden states.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If `pixel_values` is not specified.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">pixel_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify pixel_values&quot;</span><span class="p">)</span>

        <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
            <span class="n">pixel_values</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">sequence_output</span><span class="p">,)</span> <span class="o">+</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="n">BaseModelOutputWithCLSToken</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">sequence_output</span><span class="p">,</span>
            <span class="n">cls_token_value</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">cls_token_value</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtModel</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a new instance of the CvtModel class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CvtModel class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing model settings and parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_pooling_layer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A flag indicating whether to add a pooling layer. Default is True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the provided config is invalid or missing required parameters.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the provided config is not of the expected type.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If an error occurs during initialization.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a new instance of the CvtModel class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (object): The instance of the CvtModel class.</span>
<span class="sd">        config (object): The configuration object containing model settings and parameters.</span>
<span class="sd">        add_pooling_layer (bool, optional): A flag indicating whether to add a pooling layer. Default is True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the provided config is invalid or missing required parameters.</span>
<span class="sd">        TypeError: If the provided config is not of the expected type.</span>
<span class="sd">        RuntimeError: If an error occurs during initialization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">CvtEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtModel</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">pixel_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the CvtModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CvtModel class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtModel" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtModel">CvtModel</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pixel_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The pixel values of the input image. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to output hidden states. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to return a dictionary output. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="typing.Tuple">Tuple</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken" href="#mindnlp.transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken">BaseModelOutputWithCLSToken</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Union[Tuple, BaseModelOutputWithCLSToken]:
The forwarded model output.</p>
<ul>
<li>If <code>return_dict</code> is False, a tuple is returned containing the sequence output and any additional
encoder outputs.</li>
<li>If <code>return_dict</code> is True, a BaseModelOutputWithCLSToken object is returned, which includes
the last hidden state, cls token value, and hidden states.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If <code>pixel_values</code> is not specified.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pixel_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutputWithCLSToken</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the CvtModel.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtModel): The instance of the CvtModel class.</span>
<span class="sd">        pixel_values (Optional[mindspore.Tensor]): The pixel values of the input image. Default is None.</span>
<span class="sd">        output_hidden_states (Optional[bool]): Whether to output hidden states. Default is None.</span>
<span class="sd">        return_dict (Optional[bool]): Whether to return a dictionary output. Default is None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Union[Tuple, BaseModelOutputWithCLSToken]:</span>
<span class="sd">            The forwarded model output.</span>

<span class="sd">            - If `return_dict` is False, a tuple is returned containing the sequence output and any additional</span>
<span class="sd">            encoder outputs.</span>
<span class="sd">            - If `return_dict` is True, a BaseModelOutputWithCLSToken object is returned, which includes</span>
<span class="sd">            the last hidden state, cls token value, and hidden states.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `pixel_values` is not specified.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="k">if</span> <span class="n">pixel_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify pixel_values&quot;</span><span class="p">)</span>

    <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
        <span class="n">pixel_values</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">sequence_output</span><span class="p">,)</span> <span class="o">+</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="k">return</span> <span class="n">BaseModelOutputWithCLSToken</span><span class="p">(</span>
        <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">sequence_output</span><span class="p">,</span>
        <span class="n">cls_token_value</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">cls_token_value</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>The 'CvtOutput' class represents a conversion output module that is used in neural network models.</p>
<p>This class inherits from the 'nn.Module' class, which is a base class for all neural network cells in the MindSpore framework.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.__init__" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes a new instance of the 'CvtOutput' class.</p>
<p>Args:</p>
<ul>
<li>embed_dim (int): The dimension of the embedded vectors.</li>
<li>mlp_ratio (float): The ratio used to calculate the dimension of the MLP intermediate layer.</li>
<li>drop_rate (float): The probability of an element to be zeroed in the dropout layer.</li>
</ul>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.forward" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the conversion output module by applying operations to the input tensors.</p>
<p>Args:</p>
<ul>
<li>hidden_state (Tensor): The hidden state tensor.</li>
<li>input_tensor (Tensor): The input tensor.</li>
</ul>
<p>Returns:</p>
<ul>
<li>Tensor: The final hidden state tensor obtained after applying the conversion operations.</li>
</ul>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtOutput</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The &#39;CvtOutput&#39; class represents a conversion output module that is used in neural network models.</span>

<span class="sd">    This class inherits from the &#39;nn.Module&#39; class, which is a base class for all neural network cells in the MindSpore framework.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__(self, embed_dim, mlp_ratio, drop_rate):</span>
<span class="sd">            Initializes a new instance of the &#39;CvtOutput&#39; class.</span>

<span class="sd">            Args:</span>

<span class="sd">            - embed_dim (int): The dimension of the embedded vectors.</span>
<span class="sd">            - mlp_ratio (float): The ratio used to calculate the dimension of the MLP intermediate layer.</span>
<span class="sd">            - drop_rate (float): The probability of an element to be zeroed in the dropout layer.</span>

<span class="sd">        forward(self, hidden_state, input_tensor):</span>
<span class="sd">            Constructs the conversion output module by applying operations to the input tensors.</span>

<span class="sd">            Args:</span>

<span class="sd">            - hidden_state (Tensor): The hidden state tensor.</span>
<span class="sd">            - input_tensor (Tensor): The input tensor.</span>

<span class="sd">            Returns:</span>

<span class="sd">            - Tensor: The final hidden state tensor obtained after applying the conversion operations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the CvtOutput class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            embed_dim (int): The dimension of the embedding.</span>
<span class="sd">            mlp_ratio (float): The ratio used to determine the hidden layer size in the MLP.</span>
<span class="sd">            drop_rate (float): The dropout rate applied to the output.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">),</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the output of the CvtOutput class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtOutput): An instance of the CvtOutput class.</span>
<span class="sd">            hidden_state (tensor): The hidden state tensor.</span>
<span class="sd">                This tensor represents the current state of the model and is used as input for further processing.</span>
<span class="sd">                It should have a shape compatible with the dense layer.</span>
<span class="sd">            input_tensor (tensor): The input tensor.</span>
<span class="sd">                This tensor represents the input data and is added to the hidden state tensor.</span>
<span class="sd">                It should have the same shape as the hidden state tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden_state</span> <span class="o">+</span> <span class="n">input_tensor</span>
        <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtOutput</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initialize the CvtOutput class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_ratio</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ratio used to determine the hidden layer size in the MLP.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>drop_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate applied to the output.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the CvtOutput class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        embed_dim (int): The dimension of the embedding.</span>
<span class="sd">        mlp_ratio (float): The ratio used to determine the hidden layer size in the MLP.</span>
<span class="sd">        drop_rate (float): The dropout rate applied to the output.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">),</span> <span class="n">embed_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtOutput</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the output of the CvtOutput class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CvtOutput class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtOutput">CvtOutput</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden state tensor.
This tensor represents the current state of the model and is used as input for further processing.
It should have a shape compatible with the dense layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>tensor</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_tensor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor.
This tensor represents the input data and is added to the hidden state tensor.
It should have the same shape as the hidden state tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>tensor</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the output of the CvtOutput class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtOutput): An instance of the CvtOutput class.</span>
<span class="sd">        hidden_state (tensor): The hidden state tensor.</span>
<span class="sd">            This tensor represents the current state of the model and is used as input for further processing.</span>
<span class="sd">            It should have a shape compatible with the dense layer.</span>
<span class="sd">        input_tensor (tensor): The input tensor.</span>
<span class="sd">            This tensor represents the input data and is added to the hidden state tensor.</span>
<span class="sd">            It should have the same shape as the hidden state tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden_state</span> <span class="o">+</span> <span class="n">input_tensor</span>
    <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtPreTrainedModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtPreTrainedModel</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtPreTrainedModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.modeling_utils.PreTrainedModel" href="../../modeling_utils/#mindnlp.transformers.modeling_utils.PreTrainedModel">PreTrainedModel</a></code></p>


        <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
models.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtPreTrainedModel</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained</span>
<span class="sd">    models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">CvtConfig</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;cvt&quot;</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;pixel_values&quot;</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;CvtLayer&quot;</span><span class="p">]</span>
    <span class="n">_keys_to_ignore_on_load_unexpected</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s1">&#39;num_batches_tracked&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the weights&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)):</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">TruncatedNormal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">):</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="s1">&#39;ones&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">CvtStage</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">cls_token</span><span class="p">[</span><span class="n">module</span><span class="o">.</span><span class="n">stage</span><span class="p">]:</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">TruncatedNormal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents a Convolutional Self-Attention layer for a neural network model. It inherits from the nn.Module class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.num_heads">num_heads</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The number of attention heads.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.embed_dim">embed_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dimension of the input embeddings.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.kernel_size">kernel_size</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The size of the convolutional kernel.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.padding_q">padding_q</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The amount of padding for the query projection convolution.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.padding_kv">padding_kv</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The amount of padding for the key and value projection convolutions.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.stride_q">stride_q</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The stride for the query projection convolution.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.stride_kv">stride_kv</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The stride for the key and value projection convolutions.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.qkv_projection_method">qkv_projection_method</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The projection method used for the query, key, and value projections.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.qkv_bias">qkv_bias</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Indicates whether bias is added to the query, key, and value projections.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.attention_drop_rate">attention_drop_rate</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dropout rate for the attention scores.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.with_cls_token">with_cls_token</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Indicates whether a classification token is included in the input.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.__init__" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CvtSelfAttention instance.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.rearrange_for_multi_head_attention" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.rearrange_for_multi_head_attention">rearrange_for_multi_head_attention</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Rearranges the input hidden state for multi-head attention computations.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.forward" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the CvtSelfAttention layer by performing convolutional projections, multi-head attention calculations, and output rearrangement.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>The CvtSelfAttention layer assumes that the input hidden state is a 4D tensor with shape (batch_size, hidden_size, height, width).</li>
<li>The attention_score and attention_probs computations make use of the Einstein summation convention (einsum).</li>
<li>The context output is a 3D tensor with shape (batch_size, hidden_size, num_heads * head_dim).</li>
</ul>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a Convolutional Self-Attention layer for a neural network model. It inherits from the nn.Module class.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        num_heads (int): The number of attention heads.</span>
<span class="sd">        embed_dim (int): The dimension of the input embeddings.</span>
<span class="sd">        kernel_size (int): The size of the convolutional kernel.</span>
<span class="sd">        padding_q (int): The amount of padding for the query projection convolution.</span>
<span class="sd">        padding_kv (int): The amount of padding for the key and value projection convolutions.</span>
<span class="sd">        stride_q (int): The stride for the query projection convolution.</span>
<span class="sd">        stride_kv (int): The stride for the key and value projection convolutions.</span>
<span class="sd">        qkv_projection_method (str): The projection method used for the query, key, and value projections.</span>
<span class="sd">        qkv_bias (bool): Indicates whether bias is added to the query, key, and value projections.</span>
<span class="sd">        attention_drop_rate (float): The dropout rate for the attention scores.</span>
<span class="sd">        with_cls_token (bool): Indicates whether a classification token is included in the input.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__(self, num_heads, embed_dim, kernel_size, padding_q, padding_kv, stride_q, stride_kv, qkv_projection_method, qkv_bias, attention_drop_rate, with_cls_token=True, **kwargs):</span>
<span class="sd">            Initializes the CvtSelfAttention instance.</span>

<span class="sd">        rearrange_for_multi_head_attention(self, hidden_state):</span>
<span class="sd">            Rearranges the input hidden state for multi-head attention computations.</span>

<span class="sd">        forward(self, hidden_state, height, width):</span>
<span class="sd">            Constructs the CvtSelfAttention layer by performing convolutional projections, multi-head attention calculations, and output rearrangement.</span>

<span class="sd">    Note:</span>
<span class="sd">        - The CvtSelfAttention layer assumes that the input hidden state is a 4D tensor with shape (batch_size, hidden_size, height, width).</span>
<span class="sd">        - The attention_score and attention_probs computations make use of the Einstein summation convention (einsum).</span>
<span class="sd">        - The context output is a 3D tensor with shape (batch_size, hidden_size, num_heads * head_dim).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">padding_q</span><span class="p">,</span>
        <span class="n">padding_kv</span><span class="p">,</span>
        <span class="n">stride_q</span><span class="p">,</span>
        <span class="n">stride_kv</span><span class="p">,</span>
        <span class="n">qkv_projection_method</span><span class="p">,</span>
        <span class="n">qkv_bias</span><span class="p">,</span>
        <span class="n">attention_drop_rate</span><span class="p">,</span>
        <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__</span>

<span class="sd">        Initializes the CvtSelfAttention class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            num_heads (int): The number of attention heads.</span>
<span class="sd">            embed_dim (int): The dimension of the input embeddings.</span>
<span class="sd">            kernel_size (int): The size of the convolutional kernel.</span>
<span class="sd">            padding_q (int): The padding size for the query projection.</span>
<span class="sd">            padding_kv (int): The padding size for the key and value projections.</span>
<span class="sd">            stride_q (int): The stride for the query projection.</span>
<span class="sd">            stride_kv (int): The stride for the key and value projections.</span>
<span class="sd">            qkv_projection_method (str): The method used for query, key, and value projections.</span>
<span class="sd">                Can be &#39;avg&#39; or any other specific projection method.</span>
<span class="sd">            qkv_bias (bool): Indicates whether bias is applied to the query, key, and value projections.</span>
<span class="sd">            attention_drop_rate (float): The dropout rate for attention weights.</span>
<span class="sd">            with_cls_token (bool, optional): Indicates whether the class token is included. Defaults to True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If embed_dim is not a positive integer.</span>
<span class="sd">            ValueError: If num_heads is not a positive integer.</span>
<span class="sd">            ValueError: If kernel_size, padding_q, padding_kv, stride_q, or stride_kv is not a positive integer.</span>
<span class="sd">            ValueError: If qkv_projection_method is not &#39;avg&#39; or a valid specific projection method.</span>
<span class="sd">            ValueError: If attention_drop_rate is not in the range [0, 1].</span>
<span class="sd">            TypeError: If with_cls_token is not a boolean value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">embed_dim</span><span class="o">**-</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="n">with_cls_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_query</span> <span class="o">=</span> <span class="n">CvtSelfAttentionProjection</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">padding_q</span><span class="p">,</span>
            <span class="n">stride_q</span><span class="p">,</span>
            <span class="n">projection_method</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span> <span class="k">if</span> <span class="n">qkv_projection_method</span> <span class="o">==</span> <span class="s2">&quot;avg&quot;</span> <span class="k">else</span> <span class="n">qkv_projection_method</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_key</span> <span class="o">=</span> <span class="n">CvtSelfAttentionProjection</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding_kv</span><span class="p">,</span> <span class="n">stride_kv</span><span class="p">,</span> <span class="n">projection_method</span><span class="o">=</span><span class="n">qkv_projection_method</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_value</span> <span class="o">=</span> <span class="n">CvtSelfAttentionProjection</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding_kv</span><span class="p">,</span> <span class="n">stride_kv</span><span class="p">,</span> <span class="n">projection_method</span><span class="o">=</span><span class="n">qkv_projection_method</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">projection_query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection_key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection_value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">attention_drop_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">rearrange_for_multi_head_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method: rearrange_for_multi_head_attention</span>

<span class="sd">        In the class CvtSelfAttention, this method rearranges the hidden state tensor for multi-head attention computation.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtSelfAttention): The instance of the CvtSelfAttention class.</span>
<span class="sd">                This parameter is required for accessing the attributes and methods of the class.</span>
<span class="sd">            hidden_state (torch.Tensor):</span>
<span class="sd">                The input hidden state tensor of shape (batch_size, hidden_size, _).</span>

<span class="sd">                - batch_size (int): The number of sequences in the batch.</span>
<span class="sd">                - hidden_size (int): The dimensionality of the hidden state.</span>
<span class="sd">                - _ (int): Placeholder dimension for compatibility with the transformer architecture.</span>

<span class="sd">                This tensor represents the input hidden state that needs to be rearranged for multi-head attention computation.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None:</span>
<span class="sd">                This method does not return any value.</span>
<span class="sd">                It rearranges the hidden state tensor in place and does not create a new tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None:</span>
<span class="sd">                This method does not explicitly raise any exceptions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="c1"># rearrange &#39;b t (h d) -&gt; b h t d&#39;</span>
        <span class="k">return</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the self-attention context for the CvtSelfAttention class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the CvtSelfAttention class.</span>
<span class="sd">            hidden_state (Tensor): The hidden state tensor of shape (batch_size, hidden_size, num_channels).</span>
<span class="sd">                It represents the input features.</span>
<span class="sd">            height (int): The height of the hidden state tensor.</span>
<span class="sd">            width (int): The width of the hidden state tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: The context tensor of shape (batch_size, hidden_size, num_heads * head_dim).</span>
<span class="sd">                It represents the output context after applying self-attention mechanism.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span><span class="p">:</span>
            <span class="n">cls_token</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_channels</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># rearrange &quot;b (h w) c -&gt; b c h w&quot;</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_key</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_query</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_value</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">query</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">key</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rearrange_for_multi_head_attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projection_query</span><span class="p">(</span><span class="n">query</span><span class="p">))</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rearrange_for_multi_head_attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projection_key</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rearrange_for_multi_head_attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projection_value</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>

        <span class="n">attention_score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhlk,bhtk-&gt;bhlt&quot;</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_score</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">)</span>

        <span class="n">context</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhlt,bhtv-&gt;bhlv&quot;</span><span class="p">,</span> <span class="n">attention_probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="c1"># rearrange&quot;b h t d -&gt; b t (h d)&quot;</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">context</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtSelfAttention</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding_q</span><span class="p">,</span> <span class="n">padding_kv</span><span class="p">,</span> <span class="n">stride_q</span><span class="p">,</span> <span class="n">stride_kv</span><span class="p">,</span> <span class="n">qkv_projection_method</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="p">,</span> <span class="n">attention_drop_rate</span><span class="p">,</span> <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p><strong>init</strong></p>
<p>Initializes the CvtSelfAttention class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the input embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the convolutional kernel.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The padding size for the query projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The padding size for the key and value projections.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride for the query projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride for the key and value projections.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>qkv_projection_method</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The method used for query, key, and value projections.
Can be 'avg' or any other specific projection method.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>qkv_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indicates whether bias is applied to the query, key, and value projections.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_drop_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate for attention weights.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>with_cls_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indicates whether the class token is included. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If embed_dim is not a positive integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If num_heads is not a positive integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If kernel_size, padding_q, padding_kv, stride_q, or stride_kv is not a positive integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If qkv_projection_method is not 'avg' or a valid specific projection method.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If attention_drop_rate is not in the range [0, 1].</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If with_cls_token is not a boolean value.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">,</span>
    <span class="n">padding_q</span><span class="p">,</span>
    <span class="n">padding_kv</span><span class="p">,</span>
    <span class="n">stride_q</span><span class="p">,</span>
    <span class="n">stride_kv</span><span class="p">,</span>
    <span class="n">qkv_projection_method</span><span class="p">,</span>
    <span class="n">qkv_bias</span><span class="p">,</span>
    <span class="n">attention_drop_rate</span><span class="p">,</span>
    <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    __init__</span>

<span class="sd">    Initializes the CvtSelfAttention class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        num_heads (int): The number of attention heads.</span>
<span class="sd">        embed_dim (int): The dimension of the input embeddings.</span>
<span class="sd">        kernel_size (int): The size of the convolutional kernel.</span>
<span class="sd">        padding_q (int): The padding size for the query projection.</span>
<span class="sd">        padding_kv (int): The padding size for the key and value projections.</span>
<span class="sd">        stride_q (int): The stride for the query projection.</span>
<span class="sd">        stride_kv (int): The stride for the key and value projections.</span>
<span class="sd">        qkv_projection_method (str): The method used for query, key, and value projections.</span>
<span class="sd">            Can be &#39;avg&#39; or any other specific projection method.</span>
<span class="sd">        qkv_bias (bool): Indicates whether bias is applied to the query, key, and value projections.</span>
<span class="sd">        attention_drop_rate (float): The dropout rate for attention weights.</span>
<span class="sd">        with_cls_token (bool, optional): Indicates whether the class token is included. Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If embed_dim is not a positive integer.</span>
<span class="sd">        ValueError: If num_heads is not a positive integer.</span>
<span class="sd">        ValueError: If kernel_size, padding_q, padding_kv, stride_q, or stride_kv is not a positive integer.</span>
<span class="sd">        ValueError: If qkv_projection_method is not &#39;avg&#39; or a valid specific projection method.</span>
<span class="sd">        ValueError: If attention_drop_rate is not in the range [0, 1].</span>
<span class="sd">        TypeError: If with_cls_token is not a boolean value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">embed_dim</span><span class="o">**-</span><span class="mf">0.5</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="n">with_cls_token</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_query</span> <span class="o">=</span> <span class="n">CvtSelfAttentionProjection</span><span class="p">(</span>
        <span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">padding_q</span><span class="p">,</span>
        <span class="n">stride_q</span><span class="p">,</span>
        <span class="n">projection_method</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span> <span class="k">if</span> <span class="n">qkv_projection_method</span> <span class="o">==</span> <span class="s2">&quot;avg&quot;</span> <span class="k">else</span> <span class="n">qkv_projection_method</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_key</span> <span class="o">=</span> <span class="n">CvtSelfAttentionProjection</span><span class="p">(</span>
        <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding_kv</span><span class="p">,</span> <span class="n">stride_kv</span><span class="p">,</span> <span class="n">projection_method</span><span class="o">=</span><span class="n">qkv_projection_method</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_value</span> <span class="o">=</span> <span class="n">CvtSelfAttentionProjection</span><span class="p">(</span>
        <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding_kv</span><span class="p">,</span> <span class="n">stride_kv</span><span class="p">,</span> <span class="n">projection_method</span><span class="o">=</span><span class="n">qkv_projection_method</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">projection_query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projection_key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projection_value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">attention_drop_rate</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtSelfAttention</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the self-attention context for the CvtSelfAttention class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CvtSelfAttention class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden state tensor of shape (batch_size, hidden_size, num_channels).
It represents the input features.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>height</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The height of the hidden state tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>width</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The width of the hidden state tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>Tensor</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The context tensor of shape (batch_size, hidden_size, num_heads * head_dim).
It represents the output context after applying self-attention mechanism.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the self-attention context for the CvtSelfAttention class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: An instance of the CvtSelfAttention class.</span>
<span class="sd">        hidden_state (Tensor): The hidden state tensor of shape (batch_size, hidden_size, num_channels).</span>
<span class="sd">            It represents the input features.</span>
<span class="sd">        height (int): The height of the hidden state tensor.</span>
<span class="sd">        width (int): The width of the hidden state tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: The context tensor of shape (batch_size, hidden_size, num_heads * head_dim).</span>
<span class="sd">            It represents the output context after applying self-attention mechanism.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span><span class="p">:</span>
        <span class="n">cls_token</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_channels</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># rearrange &quot;b (h w) c -&gt; b c h w&quot;</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

    <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_key</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_query</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection_value</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">query</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">key</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>

    <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rearrange_for_multi_head_attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projection_query</span><span class="p">(</span><span class="n">query</span><span class="p">))</span>
    <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rearrange_for_multi_head_attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projection_key</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
    <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rearrange_for_multi_head_attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projection_value</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>

    <span class="n">attention_score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhlk,bhtk-&gt;bhlt&quot;</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_score</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">)</span>

    <span class="n">context</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhlt,bhtv-&gt;bhlv&quot;</span><span class="p">,</span> <span class="n">attention_probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="c1"># rearrange&quot;b h t d -&gt; b t (h d)&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">context</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.rearrange_for_multi_head_attention" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtSelfAttention</span><span class="o">.</span><span class="n">rearrange_for_multi_head_attention</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention.rearrange_for_multi_head_attention" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>In the class CvtSelfAttention, this method rearranges the hidden state tensor for multi-head attention computation.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CvtSelfAttention class.
This parameter is required for accessing the attributes and methods of the class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttention">CvtSelfAttention</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input hidden state tensor of shape (batch_size, hidden_size, _).</p>
<ul>
<li>batch_size (int): The number of sequences in the batch.</li>
<li>hidden_size (int): The dimensionality of the hidden state.</li>
<li>_ (int): Placeholder dimension for compatibility with the transformer architecture.</li>
</ul>
<p>This tensor represents the input hidden state that needs to be rearranged for multi-head attention computation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="torch.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method does not return any value.
It rearranges the hidden state tensor in place and does not create a new tensor.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>None</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>This method does not explicitly raise any exceptions.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">rearrange_for_multi_head_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method: rearrange_for_multi_head_attention</span>

<span class="sd">    In the class CvtSelfAttention, this method rearranges the hidden state tensor for multi-head attention computation.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtSelfAttention): The instance of the CvtSelfAttention class.</span>
<span class="sd">            This parameter is required for accessing the attributes and methods of the class.</span>
<span class="sd">        hidden_state (torch.Tensor):</span>
<span class="sd">            The input hidden state tensor of shape (batch_size, hidden_size, _).</span>

<span class="sd">            - batch_size (int): The number of sequences in the batch.</span>
<span class="sd">            - hidden_size (int): The dimensionality of the hidden state.</span>
<span class="sd">            - _ (int): Placeholder dimension for compatibility with the transformer architecture.</span>

<span class="sd">            This tensor represents the input hidden state that needs to be rearranged for multi-head attention computation.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None:</span>
<span class="sd">            This method does not return any value.</span>
<span class="sd">            It rearranges the hidden state tensor in place and does not create a new tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None:</span>
<span class="sd">            This method does not explicitly raise any exceptions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="c1"># rearrange &#39;b t (h d) -&gt; b h t d&#39;</span>
    <span class="k">return</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>CvtSelfAttentionConvProjection represents a class for performing convolution and normalization operations
on input data. This class inherits from nn.Module and provides methods for initializing the
convolution and normalization layers, as well as for forwarding the output from the input hidden state.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.embed_dim">embed_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dimension of the input embedding.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.kernel_size">kernel_size</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The size of the convolutional kernel.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.padding">padding</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The amount of padding to apply to the input data.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.stride">stride</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The stride of the convolution operation.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.__init__" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CvtSelfAttentionConvProjection class with the specified parameters.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.forward" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the output from the input hidden state by applying convolution and normalization operations.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtSelfAttentionConvProjection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CvtSelfAttentionConvProjection represents a class for performing convolution and normalization operations</span>
<span class="sd">    on input data. This class inherits from nn.Module and provides methods for initializing the</span>
<span class="sd">    convolution and normalization layers, as well as for forwarding the output from the input hidden state.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        embed_dim (int): The dimension of the input embedding.</span>
<span class="sd">        kernel_size (int): The size of the convolutional kernel.</span>
<span class="sd">        padding (int): The amount of padding to apply to the input data.</span>
<span class="sd">        stride (int): The stride of the convolution operation.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the CvtSelfAttentionConvProjection class with the specified parameters.</span>
<span class="sd">        forward: Constructs the output from the input hidden state by applying convolution and normalization operations.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a new instance of the CvtSelfAttentionConvProjection class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtSelfAttentionConvProjection): The object itself.</span>
<span class="sd">            embed_dim (int): The number of channels in the input and output tensors.</span>
<span class="sd">            kernel_size (int or Tuple[int, int]): The size of the convolving kernel.</span>
<span class="sd">            padding (int or Tuple[int, int]): The amount of padding added to the input.</span>
<span class="sd">            stride (int or Tuple[int, int]): The stride of the convolution.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convolution</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs a hidden state using convolution, normalization, and projection in the CvtSelfAttentionConvProjection class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtSelfAttentionConvProjection): An instance of the CvtSelfAttentionConvProjection class.</span>
<span class="sd">            hidden_state (any): The input hidden state.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtSelfAttentionConvProjection</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a new instance of the CvtSelfAttentionConvProjection class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The object itself.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection">CvtSelfAttentionConvProjection</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of channels in the input and output tensors.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the convolving kernel.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int or <span title="typing.Tuple">Tuple</span>[int, int]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The amount of padding added to the input.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int or <span title="typing.Tuple">Tuple</span>[int, int]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride of the convolution.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int or <span title="typing.Tuple">Tuple</span>[int, int]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a new instance of the CvtSelfAttentionConvProjection class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtSelfAttentionConvProjection): The object itself.</span>
<span class="sd">        embed_dim (int): The number of channels in the input and output tensors.</span>
<span class="sd">        kernel_size (int or Tuple[int, int]): The size of the convolving kernel.</span>
<span class="sd">        padding (int or Tuple[int, int]): The amount of padding added to the input.</span>
<span class="sd">        stride (int or Tuple[int, int]): The stride of the convolution.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">convolution</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
        <span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtSelfAttentionConvProjection</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs a hidden state using convolution, normalization, and projection in the CvtSelfAttentionConvProjection class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CvtSelfAttentionConvProjection class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionConvProjection">CvtSelfAttentionConvProjection</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input hidden state.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>any</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a hidden state using convolution, normalization, and projection in the CvtSelfAttentionConvProjection class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtSelfAttentionConvProjection): An instance of the CvtSelfAttentionConvProjection class.</span>
<span class="sd">        hidden_state (any): The input hidden state.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>The 'CvtSelfAttentionLinearProjection' class is a Python class that inherits from the 'nn.Module' class.
It represents a linear projection operation applied to hidden states in a self-attention mechanism.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection.forward" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Applies a linear projection to the input hidden state.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtSelfAttentionLinearProjection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The &#39;CvtSelfAttentionLinearProjection&#39; class is a Python class that inherits from the &#39;nn.Module&#39; class.</span>
<span class="sd">    It represents a linear projection operation applied to hidden states in a self-attention mechanism.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        None.</span>

<span class="sd">    Methods:</span>
<span class="sd">        forward(hidden_state): Applies a linear projection to the input hidden state.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs a linear projection of hidden state for self-attention in the CvtSelfAttentionLinearProjection class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtSelfAttentionLinearProjection): The instance of the CvtSelfAttentionLinearProjection class.</span>
<span class="sd">            hidden_state (torch.Tensor): The hidden state tensor with shape (batch_size, num_channels, height, width),</span>
<span class="sd">                where batch_size is the number of samples in the batch, num_channels is the number of channels,</span>
<span class="sd">                height is the height of the hidden state tensor, and width is the width of the hidden state tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: The linearly projected hidden state tensor with shape (batch_size, hidden_size, num_channels),</span>
<span class="sd">                where batch_size is the number of samples in the batch, hidden_size is the product of height and width</span>
<span class="sd">                of the hidden state tensor, and num_channels is the number of channels.</span>
<span class="sd">                The tensor is permuted to have the dimensions (batch_size, hidden_size, num_channels).</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span>
        <span class="c1"># rearrange &quot; b c h w -&gt; b (h w) c&quot;</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtSelfAttentionLinearProjection</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs a linear projection of hidden state for self-attention in the CvtSelfAttentionLinearProjection class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CvtSelfAttentionLinearProjection class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionLinearProjection">CvtSelfAttentionLinearProjection</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden state tensor with shape (batch_size, num_channels, height, width),
where batch_size is the number of samples in the batch, num_channels is the number of channels,
height is the height of the hidden state tensor, and width is the width of the hidden state tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="torch.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>torch.Tensor: The linearly projected hidden state tensor with shape (batch_size, hidden_size, num_channels),
where batch_size is the number of samples in the batch, hidden_size is the product of height and width
of the hidden state tensor, and num_channels is the number of channels.
The tensor is permuted to have the dimensions (batch_size, hidden_size, num_channels).</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a linear projection of hidden state for self-attention in the CvtSelfAttentionLinearProjection class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtSelfAttentionLinearProjection): The instance of the CvtSelfAttentionLinearProjection class.</span>
<span class="sd">        hidden_state (torch.Tensor): The hidden state tensor with shape (batch_size, num_channels, height, width),</span>
<span class="sd">            where batch_size is the number of samples in the batch, num_channels is the number of channels,</span>
<span class="sd">            height is the height of the hidden state tensor, and width is the width of the hidden state tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The linearly projected hidden state tensor with shape (batch_size, hidden_size, num_channels),</span>
<span class="sd">            where batch_size is the number of samples in the batch, hidden_size is the product of height and width</span>
<span class="sd">            of the hidden state tensor, and num_channels is the number of channels.</span>
<span class="sd">            The tensor is permuted to have the dimensions (batch_size, hidden_size, num_channels).</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span>
    <span class="c1"># rearrange &quot; b c h w -&gt; b (h w) c&quot;</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>A class representing the projection layer for self-attention in a Convolutional Transformer network.</p>
<p>This class is responsible for projecting the input hidden state using convolutional and linear projections.
It provides methods to initialize the projections and apply them sequentially to the input hidden state.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.embed_dim">embed_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dimensionality of the input embeddings.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.kernel_size">kernel_size</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The size of the convolutional kernel.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.padding">padding</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The amount of padding to apply during convolution.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.stride">stride</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The stride of the convolution operation.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.projection_method">projection_method</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The method used for projection, default is 'dw_bn' (depthwise batch normalization).</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.__init__" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the projection layer with the specified parameters.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.forward" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Applies the convolutional projection followed by the linear projection to the input hidden state.
Returns the projected hidden state.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <p>This class inherits from nn.Module and is designed to be used within a Convolutional Transformer network.</p>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtSelfAttentionProjection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class representing the projection layer for self-attention in a Convolutional Transformer network.</span>

<span class="sd">    This class is responsible for projecting the input hidden state using convolutional and linear projections.</span>
<span class="sd">    It provides methods to initialize the projections and apply them sequentially to the input hidden state.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        embed_dim (int): The dimensionality of the input embeddings.</span>
<span class="sd">        kernel_size (int): The size of the convolutional kernel.</span>
<span class="sd">        padding (int): The amount of padding to apply during convolution.</span>
<span class="sd">        stride (int): The stride of the convolution operation.</span>
<span class="sd">        projection_method (str): The method used for projection, default is &#39;dw_bn&#39; (depthwise batch normalization).</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__:</span>
<span class="sd">            Initializes the projection layer with the specified parameters.</span>

<span class="sd">        forward:</span>
<span class="sd">            Applies the convolutional projection followed by the linear projection to the input hidden state.</span>
<span class="sd">            Returns the projected hidden state.</span>

<span class="sd">    Note:</span>
<span class="sd">        This class inherits from nn.Module and is designed to be used within a Convolutional Transformer network.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">projection_method</span><span class="o">=</span><span class="s2">&quot;dw_bn&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the CvtSelfAttentionProjection class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtSelfAttentionProjection): The instance of the class.</span>
<span class="sd">            embed_dim (int): The dimensionality of the input embeddings.</span>
<span class="sd">            kernel_size (int): The size of the convolutional kernel.</span>
<span class="sd">            padding (int): The amount of padding to be added to the input.</span>
<span class="sd">            stride (int): The stride value for the convolution operation.</span>
<span class="sd">            projection_method (string, optional): The method used for projection. Defaults to &#39;dw_bn&#39;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">projection_method</span> <span class="o">==</span> <span class="s2">&quot;dw_bn&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection</span> <span class="o">=</span> <span class="n">CvtSelfAttentionConvProjection</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_projection</span> <span class="o">=</span> <span class="n">CvtSelfAttentionLinearProjection</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the self-attention projection for the CvtSelfAttentionProjection class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtSelfAttentionProjection): The instance of the CvtSelfAttentionProjection class.</span>
<span class="sd">            hidden_state (Tensor): The hidden state tensor to be projected.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: The method modifies the hidden_state in-place after applying convolution and linear projections.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_projection</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtSelfAttentionProjection</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">projection_method</span><span class="o">=</span><span class="s1">&#39;dw_bn&#39;</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the CvtSelfAttentionProjection class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection">CvtSelfAttentionProjection</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimensionality of the input embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the convolutional kernel.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The amount of padding to be added to the input.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride value for the convolution operation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>projection_method</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The method used for projection. Defaults to 'dw_bn'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>string</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;dw_bn&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">projection_method</span><span class="o">=</span><span class="s2">&quot;dw_bn&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the CvtSelfAttentionProjection class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtSelfAttentionProjection): The instance of the class.</span>
<span class="sd">        embed_dim (int): The dimensionality of the input embeddings.</span>
<span class="sd">        kernel_size (int): The size of the convolutional kernel.</span>
<span class="sd">        padding (int): The amount of padding to be added to the input.</span>
<span class="sd">        stride (int): The stride value for the convolution operation.</span>
<span class="sd">        projection_method (string, optional): The method used for projection. Defaults to &#39;dw_bn&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">projection_method</span> <span class="o">==</span> <span class="s2">&quot;dw_bn&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection</span> <span class="o">=</span> <span class="n">CvtSelfAttentionConvProjection</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_projection</span> <span class="o">=</span> <span class="n">CvtSelfAttentionLinearProjection</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtSelfAttentionProjection</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the self-attention projection for the CvtSelfAttentionProjection class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CvtSelfAttentionProjection class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfAttentionProjection">CvtSelfAttentionProjection</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden state tensor to be projected.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The method modifies the hidden_state in-place after applying convolution and linear projections.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the self-attention projection for the CvtSelfAttentionProjection class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtSelfAttentionProjection): The instance of the CvtSelfAttentionProjection class.</span>
<span class="sd">        hidden_state (Tensor): The hidden state tensor to be projected.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: The method modifies the hidden_state in-place after applying convolution and linear projections.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution_projection</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_projection</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>The residual connection is defined in CvtLayer instead of here (as is the case with other models), due to the
layernorm applied before each block.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtSelfOutput</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The residual connection is defined in CvtLayer instead of here (as is the case with other models), due to the</span>
<span class="sd">    layernorm applied before each block.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the CvtSelfOutput class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtSelfOutput): The instance of the class.</span>
<span class="sd">            embed_dim (int): The dimension of the embedding.</span>
<span class="sd">            drop_rate (float): The dropout rate to be applied.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the output of the CvtSelfOutput class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtSelfOutput): An instance of the CvtSelfOutput class.</span>
<span class="sd">            hidden_state (Tensor): The hidden state to be processed.</span>
<span class="sd">                This tensor represents the current state of the model and is expected to have shape (batch_size, hidden_size).</span>
<span class="sd">                It serves as input to the dense layer and will be transformed.</span>
<span class="sd">            input_tensor (Tensor): The input tensor to the method.</span>
<span class="sd">                This tensor represents additional input to the forward method and can be of any shape.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtSelfOutput</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the CvtSelfOutput class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput">CvtSelfOutput</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>drop_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate to be applied.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the CvtSelfOutput class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtSelfOutput): The instance of the class.</span>
<span class="sd">        embed_dim (int): The dimension of the embedding.</span>
<span class="sd">        drop_rate (float): The dropout rate to be applied.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtSelfOutput</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the output of the CvtSelfOutput class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CvtSelfOutput class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtSelfOutput">CvtSelfOutput</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden state to be processed.
This tensor represents the current state of the model and is expected to have shape (batch_size, hidden_size).
It serves as input to the dense layer and will be transformed.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_tensor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor to the method.
This tensor represents additional input to the forward method and can be of any shape.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the output of the CvtSelfOutput class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtSelfOutput): An instance of the CvtSelfOutput class.</span>
<span class="sd">        hidden_state (Tensor): The hidden state to be processed.</span>
<span class="sd">            This tensor represents the current state of the model and is expected to have shape (batch_size, hidden_size).</span>
<span class="sd">            It serves as input to the dense layer and will be transformed.</span>
<span class="sd">        input_tensor (Tensor): The input tensor to the method.</span>
<span class="sd">            This tensor represents additional input to the forward method and can be of any shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtStage" class="doc doc-heading">
            <code>mindnlp.transformers.models.cvt.modeling_cvt.CvtStage</code>


<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtStage" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>The CvtStage class represents a stage in the Cross Vision Transformer (Cvt) model. It inherits from nn.Module and
is designed to handle the processing and transformation of input data within a specific stage of the Cvt model.</p>
<p>This class includes methods for initializing the stage with configuration and stage information, as well as
forwarding the hidden state through a series of operations involving embeddings, layer processing,
and token manipulation.</p>
<p>The class supports the configuration of parameters such as patch size, stride, number of channels,
embedding dimensions, padding, dropout rates, depth, number of heads, kernel size, attention and
multi-layer perceptron (MLP) settings, and the inclusion of a classification (cls) token.</p>
<p>The forward method is responsible for processing the hidden state by applying the configured embeddings,
manipulating the hidden state based on the existence of a cls token, and iterating through the
layers to transform the hidden state. Additionally, it handles the splitting and reshaping of the hidden state
before returning the updated hidden state and cls token.</p>
<p>Overall, the CvtStage class provides a structured and configurable framework for managing the transformation of
data within a specific stage of the Cvt model.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CvtStage</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The CvtStage class represents a stage in the Cross Vision Transformer (Cvt) model. It inherits from nn.Module and</span>
<span class="sd">    is designed to handle the processing and transformation of input data within a specific stage of the Cvt model.</span>

<span class="sd">    This class includes methods for initializing the stage with configuration and stage information, as well as</span>
<span class="sd">    forwarding the hidden state through a series of operations involving embeddings, layer processing,</span>
<span class="sd">    and token manipulation.</span>

<span class="sd">    The class supports the configuration of parameters such as patch size, stride, number of channels,</span>
<span class="sd">    embedding dimensions, padding, dropout rates, depth, number of heads, kernel size, attention and</span>
<span class="sd">    multi-layer perceptron (MLP) settings, and the inclusion of a classification (cls) token.</span>

<span class="sd">    The forward method is responsible for processing the hidden state by applying the configured embeddings,</span>
<span class="sd">    manipulating the hidden state based on the existence of a cls token, and iterating through the</span>
<span class="sd">    layers to transform the hidden state. Additionally, it handles the splitting and reshaping of the hidden state</span>
<span class="sd">    before returning the updated hidden state and cls token.</span>

<span class="sd">    Overall, the CvtStage class provides a structured and configurable framework for managing the transformation of</span>
<span class="sd">    data within a specific stage of the Cvt model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method initializes an instance of the CvtStage class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the CvtStage class.</span>
<span class="sd">            config (object): The configuration object containing various parameters such as patch size, stride,</span>
<span class="sd">                number of channels, embedding dimensions, padding, dropout rate, depth, number of heads, kernel</span>
<span class="sd">                size, padding for query, key, and value, stride for key and value, stride for query, method</span>
<span class="sd">                for QKV projection, QKV bias, attention dropout rate, drop rate, drop path rate, MLP ratio,</span>
<span class="sd">                and presence of a classification token.</span>
<span class="sd">            stage (int): The stage of the CvtStage.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the config.cls_token[self.stage] does not exist or is not a valid value.</span>
<span class="sd">            TypeError: If the config.drop_path_rate[self.stage] is not a valid type.</span>
<span class="sd">            IndexError: If the drop_path_rates[self.stage] does not exist or is not a valid index.</span>
<span class="sd">            TypeError: If any of the parameters in the CvtLayer instantiation are of an invalid type.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stage</span> <span class="o">=</span> <span class="n">stage</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">cls_token</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">CvtEmbeddings</span><span class="p">(</span>
            <span class="n">patch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">patch_sizes</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">patch_stride</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
            <span class="n">num_channels</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_channels</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">patch_padding</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
            <span class="n">dropout_rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">drop_rate</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="n">drop_path_rates</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">ops</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">drop_path_rate</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span> <span class="n">config</span><span class="o">.</span><span class="n">depth</span><span class="p">[</span><span class="n">stage</span><span class="p">])]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span>
                <span class="n">CvtLayer</span><span class="p">(</span>
                    <span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_heads</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">embed_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">kernel_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">kernel_qkv</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">padding_q</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">padding_q</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">padding_kv</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">padding_kv</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">stride_kv</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">stride_kv</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">stride_q</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">stride_q</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">qkv_projection_method</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">qkv_projection_method</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">qkv_bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">qkv_bias</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">attention_drop_rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_drop_rate</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">drop_rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">drop_rate</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">drop_path_rate</span><span class="o">=</span><span class="n">drop_path_rates</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">mlp_ratio</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                    <span class="n">with_cls_token</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">cls_token</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">depth</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">])</span>
            <span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the hidden state for the CvtStage class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CvtStage): The instance of the CvtStage class.</span>
<span class="sd">            hidden_state: The hidden state input for forwarding the hidden state. It should be a tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple:</span>
<span class="sd">                A tuple containing the forwarded hidden state and cls_token.</span>

<span class="sd">                The hidden state is a tensor with dimensions (batch_size, num_channels, height, width), representing</span>
<span class="sd">                the forwarded hidden state. The cls_token is a tensor with dimensions (batch_size, 1, num_channels),</span>
<span class="sd">                representing the cls_token if it exists, otherwise it is None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># rearrange b c h w -&gt; b (h w) c&quot;</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">cls_token</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">]:</span>
            <span class="n">cls_token</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
            <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">layer_outputs</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">cls_token</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">]:</span>
            <span class="n">cls_token</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">cls_token</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtStage.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtStage</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">stage</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtStage.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method initializes an instance of the CvtStage class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CvtStage class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing various parameters such as patch size, stride,
number of channels, embedding dimensions, padding, dropout rate, depth, number of heads, kernel
size, padding for query, key, and value, stride for key and value, stride for query, method
for QKV projection, QKV bias, attention dropout rate, drop rate, drop path rate, MLP ratio,
and presence of a classification token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stage</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stage of the CvtStage.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the config.cls_token[self.stage] does not exist or is not a valid value.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the config.drop_path_rate[self.stage] is not a valid type.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>IndexError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the drop_path_rates[self.stage] does not exist or is not a valid index.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If any of the parameters in the CvtLayer instantiation are of an invalid type.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method initializes an instance of the CvtStage class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the CvtStage class.</span>
<span class="sd">        config (object): The configuration object containing various parameters such as patch size, stride,</span>
<span class="sd">            number of channels, embedding dimensions, padding, dropout rate, depth, number of heads, kernel</span>
<span class="sd">            size, padding for query, key, and value, stride for key and value, stride for query, method</span>
<span class="sd">            for QKV projection, QKV bias, attention dropout rate, drop rate, drop path rate, MLP ratio,</span>
<span class="sd">            and presence of a classification token.</span>
<span class="sd">        stage (int): The stage of the CvtStage.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the config.cls_token[self.stage] does not exist or is not a valid value.</span>
<span class="sd">        TypeError: If the config.drop_path_rate[self.stage] is not a valid type.</span>
<span class="sd">        IndexError: If the drop_path_rates[self.stage] does not exist or is not a valid index.</span>
<span class="sd">        TypeError: If any of the parameters in the CvtLayer instantiation are of an invalid type.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stage</span> <span class="o">=</span> <span class="n">stage</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">cls_token</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">CvtEmbeddings</span><span class="p">(</span>
        <span class="n">patch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">patch_sizes</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">patch_stride</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
        <span class="n">num_channels</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_channels</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">patch_padding</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">drop_rate</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="n">drop_path_rates</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">ops</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">drop_path_rate</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span> <span class="n">config</span><span class="o">.</span><span class="n">depth</span><span class="p">[</span><span class="n">stage</span><span class="p">])]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="o">*</span><span class="p">[</span>
            <span class="n">CvtLayer</span><span class="p">(</span>
                <span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_heads</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">embed_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">kernel_qkv</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">padding_q</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">padding_q</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">padding_kv</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">padding_kv</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">stride_kv</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">stride_kv</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">stride_q</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">stride_q</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">qkv_projection_method</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">qkv_projection_method</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">qkv_bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">qkv_bias</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">attention_drop_rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_drop_rate</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">drop_rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">drop_rate</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">drop_path_rate</span><span class="o">=</span><span class="n">drop_path_rates</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">mlp_ratio</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
                <span class="n">with_cls_token</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">cls_token</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">depth</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">])</span>
        <span class="p">]</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cvt.modeling_cvt.CvtStage.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">CvtStage</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtStage.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the hidden state for the CvtStage class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CvtStage class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cvt.modeling_cvt.CvtStage" href="#mindnlp.transformers.models.cvt.modeling_cvt.CvtStage">CvtStage</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden state input for forwarding the hidden state. It should be a tensor.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tuple</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A tuple containing the forwarded hidden state and cls_token.</p>
<p>The hidden state is a tensor with dimensions (batch_size, num_channels, height, width), representing
the forwarded hidden state. The cls_token is a tensor with dimensions (batch_size, 1, num_channels),
representing the cls_token if it exists, otherwise it is None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the hidden state for the CvtStage class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CvtStage): The instance of the CvtStage class.</span>
<span class="sd">        hidden_state: The hidden state input for forwarding the hidden state. It should be a tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple:</span>
<span class="sd">            A tuple containing the forwarded hidden state and cls_token.</span>

<span class="sd">            The hidden state is a tensor with dimensions (batch_size, num_channels, height, width), representing</span>
<span class="sd">            the forwarded hidden state. The cls_token is a tensor with dimensions (batch_size, 1, num_channels),</span>
<span class="sd">            representing the cls_token if it exists, otherwise it is None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cls_token</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># rearrange b c h w -&gt; b (h w) c&quot;</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">cls_token</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">]:</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">layer_outputs</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">cls_token</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">]:</span>
        <span class="n">cls_token</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">cls_token</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.cvt.modeling_cvt.drop_path" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cvt</span><span class="o">.</span><span class="n">modeling_cvt</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">drop_prob</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cvt.modeling_cvt.drop_path" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).</p>
<p>Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,
however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the
layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the
argument.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cvt\modeling_cvt.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">drop_path</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).</span>

<span class="sd">    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,</span>
<span class="sd">    however, the original name is misleading as &#39;Drop Connect&#39; is a different form of dropout in a separate paper...</span>
<span class="sd">    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I&#39;ve opted for changing the</span>
<span class="sd">    layer and argument names to &#39;drop path&#39; rather than mix DropConnect as a layer name and use &#39;survival rate&#39; as the</span>
<span class="sd">    argument.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">drop_prob</span> <span class="o">==</span> <span class="mf">0.0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">training</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">input</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">drop_prob</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># work with diff dim tensors, not just 2D ConvNets</span>
    <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">keep_prob</span> <span class="o">+</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">random_tensor</span><span class="o">.</span><span class="n">floor</span><span class="p">()</span>  <span class="c1"># binarize</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">)</span> <span class="o">*</span> <span class="n">random_tensor</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../ctrl/" class="md-footer__link md-footer__link--prev" aria-label="Previous: ctrl">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                ctrl
              </div>
            </div>
          </a>
        
        
          
          <a href="../data2vec/" class="md-footer__link md-footer__link--next" aria-label="Next: data2vec">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                data2vec
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 MindSpore Lab and CQU NLP Team.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:lvyufeng@cqu.edu.cn" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindnlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/lu-yu-feng-46-1" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>