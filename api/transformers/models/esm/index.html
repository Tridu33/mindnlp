
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../ernie_m/">
      
      
        <link rel="next" href="../falcon/">
      
      
      <link rel="icon" href="../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>esm - MindNLP Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindnlp.transformers.models.esm.configuration_esm" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="MindNLP Docs" class="md-header__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindNLP Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              esm
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="./" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../../../zh/api/transformers/models/esm/" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../tutorials/quick_start/" class="md-tabs__link">
          
  
    
  
  Tutorials

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../supported_models/" class="md-tabs__link">
        
  
    
  
  Supported Models

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../contribute/" class="md-tabs__link">
        
  
    
  
  How-To Contribute

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../accelerate/" class="md-tabs__link">
          
  
    
  
  API Reference

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../notes/changelog/" class="md-tabs__link">
          
  
    
  
  Notes

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="MindNLP Docs" class="md-nav__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    MindNLP Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/data_preprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocess
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_mirror/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Mirror
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../supported_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Supported Models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How-To Contribute
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../accelerate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/load_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    load_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/BaseMapFunction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BaseMapFunction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transforms
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engine
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Engine
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_1" >
        
          
          <label class="md-nav__link" for="__nav_5_4_1" id="__nav_5_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    train_args
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_1">
            <span class="md-nav__icon md-icon"></span>
            train_args
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seq2seq
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_2" >
        
          
          <label class="md-nav__link" for="__nav_5_4_2" id="__nav_5_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_2">
            <span class="md-nav__icon md-icon"></span>
            trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/default_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    default_func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../peft/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_2" >
        
          
          <label class="md-nav__link" for="__nav_5_7_2" id="__nav_5_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tuners
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_2">
            <span class="md-nav__icon md-icon"></span>
            tuners
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adalora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AdaLoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adaption_prompt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adaption_Prompt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/ia3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lokr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoKr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/prompt_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt tuning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_3" >
        
          
          <label class="md-nav__link" for="__nav_5_7_3" id="__nav_5_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/utils/merge_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    merge_utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/peft_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    peft_model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sentence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_2" >
        
          
          <label class="md-nav__link" for="__nav_5_9_2" id="__nav_5_9_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_2">
            <span class="md-nav__icon md-icon"></span>
            generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/generation/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/logits_process/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/stopping_criteria/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stopping_criteria
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/streamers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    streamers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_3" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9_3" id="__nav_5_9_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9_3">
            <span class="md-nav__icon md-icon"></span>
            models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../albert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    albert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    align
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../altclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    altclip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../audio_spectrogram_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    audio_spectrogram_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../auto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    auto
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    autoformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../baichuan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    baichuan
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../barthez/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    barthez
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bartpho/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bartpho
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../beit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bertweet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bertweet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bge_m3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bge_m3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../big_bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    big_bird
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bigbird_pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bigbird_pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../biogpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    biogpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot_small/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot_small
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip_2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bloom
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bridgetower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bros/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bros
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../byt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    byt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../camembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    camembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../canine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    canine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../codegen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    codegen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cogvlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cogvlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cohere/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convnext/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convnext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpmant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpmbee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ctrl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctrl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cvt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cvt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data2vec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data2vec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../decision_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decision_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distilbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    distilbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../electra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    electra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../encodec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encodec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie_m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie_m
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    esm
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    esm
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_esm
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_esm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmConfig" class="md-nav__link">
    <span class="md-ellipsis">
      EsmConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmConfig.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmConfig.to_dict" class="md-nav__link">
    <span class="md-ellipsis">
      to_dict
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.__post_init__" class="md-nav__link">
    <span class="md-ellipsis">
      __post_init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.to_dict" class="md-nav__link">
    <span class="md-ellipsis">
      to_dict
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig" class="md-nav__link">
    <span class="md-ellipsis">
      StructureModuleConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StructureModuleConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig.to_dict" class="md-nav__link">
    <span class="md-ellipsis">
      to_dict
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig" class="md-nav__link">
    <span class="md-ellipsis">
      TrunkConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TrunkConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.__post_init__" class="md-nav__link">
    <span class="md-ellipsis">
      __post_init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.to_dict" class="md-nav__link">
    <span class="md-ellipsis">
      to_dict
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.get_default_vocab_list" class="md-nav__link">
    <span class="md-ellipsis">
      get_default_vocab_list
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM" class="md-nav__link">
    <span class="md-ellipsis">
      EsmForMaskedLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmForMaskedLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.get_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_output_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.predict_contacts" class="md-nav__link">
    <span class="md-ellipsis">
      predict_contacts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.set_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_output_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification" class="md-nav__link">
    <span class="md-ellipsis">
      EsmForSequenceClassification
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmForSequenceClassification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification" class="md-nav__link">
    <span class="md-ellipsis">
      EsmForTokenClassification
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmForTokenClassification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel" class="md-nav__link">
    <span class="md-ellipsis">
      EsmModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.predict_contacts" class="md-nav__link">
    <span class="md-ellipsis">
      predict_contacts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      EsmPreTrainedModel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_esmfold
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_esmfold">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture" class="md-nav__link">
    <span class="md-ellipsis">
      EsmCategoricalMixture
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmCategoricalMixture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.log_prob" class="md-nav__link">
    <span class="md-ellipsis">
      log_prob
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.mean" class="md-nav__link">
    <span class="md-ellipsis">
      mean
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldAngleResnet
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldAngleResnet">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldAngleResnetBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldAngleResnetBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldBackboneUpdate
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldBackboneUpdate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldDropout
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldDropout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldInvariantPointAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldInvariantPointAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldLinear
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldLinear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldPairToSequence
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldPairToSequence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldPreTrainedModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldRelativePosition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldRelativePosition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldResidueMLP
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldResidueMLP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldSelfAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldSelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldSequenceToPair
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldSequenceToPair">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldStructureModule
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldStructureModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.frames_and_literature_positions_to_atom14_pos" class="md-nav__link">
    <span class="md-ellipsis">
      frames_and_literature_positions_to_atom14_pos
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.torsion_angles_to_frames" class="md-nav__link">
    <span class="md-ellipsis">
      torsion_angles_to_frames
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldStructureModuleTransition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldStructureModuleTransition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldStructureModuleTransitionLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldStructureModuleTransitionLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldTriangleAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldTriangleAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldTriangleMultiplicativeUpdate
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldTriangleMultiplicativeUpdate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldTriangularSelfAttentionBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldTriangularSelfAttentionBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldingTrunk
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldingTrunk">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.distogram" class="md-nav__link">
    <span class="md-ellipsis">
      distogram
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.set_chunk_size" class="md-nav__link">
    <span class="md-ellipsis">
      set_chunk_size
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding" class="md-nav__link">
    <span class="md-ellipsis">
      EsmForProteinFolding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmForProteinFolding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.af2_idx_to_esm_idx" class="md-nav__link">
    <span class="md-ellipsis">
      af2_idx_to_esm_idx
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.bert_mask" class="md-nav__link">
    <span class="md-ellipsis">
      bert_mask
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.compute_language_model_representations" class="md-nav__link">
    <span class="md-ellipsis">
      compute_language_model_representations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer" class="md-nav__link">
    <span class="md-ellipsis">
      infer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer_pdb" class="md-nav__link">
    <span class="md-ellipsis">
      infer_pdb
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer_pdbs" class="md-nav__link">
    <span class="md-ellipsis">
      infer_pdbs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.output_to_pdb" class="md-nav__link">
    <span class="md-ellipsis">
      output_to_pdb
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFoldingOutput" class="md-nav__link">
    <span class="md-ellipsis">
      EsmForProteinFoldingOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.categorical_lddt" class="md-nav__link">
    <span class="md-ellipsis">
      categorical_lddt
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.collate_dense_tensors" class="md-nav__link">
    <span class="md-ellipsis">
      collate_dense_tensors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.dict_multimap" class="md-nav__link">
    <span class="md-ellipsis">
      dict_multimap
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.flatten_final_dims" class="md-nav__link">
    <span class="md-ellipsis">
      flatten_final_dims
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.get_axial_mask" class="md-nav__link">
    <span class="md-ellipsis">
      get_axial_mask
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.ipa_point_weights_init_" class="md-nav__link">
    <span class="md-ellipsis">
      ipa_point_weights_init_
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.permute_final_dims" class="md-nav__link">
    <span class="md-ellipsis">
      permute_final_dims
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.softmax_no_cast" class="md-nav__link">
    <span class="md-ellipsis">
      softmax_no_cast
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.trunc_normal_init_" class="md-nav__link">
    <span class="md-ellipsis">
      trunc_normal_init_
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization_esm
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tokenization_esm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      EsmTokenizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.vocab_size" class="md-nav__link">
    <span class="md-ellipsis">
      vocab_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.build_inputs_with_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      build_inputs_with_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.get_special_tokens_mask" class="md-nav__link">
    <span class="md-ellipsis">
      get_special_tokens_mask
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.get_vocab" class="md-nav__link">
    <span class="md-ellipsis">
      get_vocab
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.id_to_token" class="md-nav__link">
    <span class="md-ellipsis">
      id_to_token
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.save_vocabulary" class="md-nav__link">
    <span class="md-ellipsis">
      save_vocabulary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.token_to_id" class="md-nav__link">
    <span class="md-ellipsis">
      token_to_id
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.load_vocab_file" class="md-nav__link">
    <span class="md-ellipsis">
      load_vocab_file
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    flava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../funnel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    funnel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gemma
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_bigcode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_bigcode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_pangu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_pangu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gptj/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gptj
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../graphormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    graphormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../groupvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    groupvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hubert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hubert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../imagegpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    imagegpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../internlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    internlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jetmoe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jetmoe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlmv2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlmv2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../led/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    led
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava_next/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava_next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../luke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    luke
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../marian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    marian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../maskformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maskformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mbart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mbart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minicpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minicpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minigpt4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minigpt4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mixtral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilevit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilevit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../moss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    moss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen_melody/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen_melody
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mvp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mvp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nezha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nezha
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nystromformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nystromformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../olmo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    olmo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../openelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    openelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    opt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../owlvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    owlvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../poolformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    poolformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pop2piano/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pop2piano
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2_moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    resnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roc_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roc_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rwkv
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seamless_m4t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seamless_m4t_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../segformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    segformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seggpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seggpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_encoder_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_encoder_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_to_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_to_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../squeezebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    squeezebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stablelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stablelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    starcoder2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../swiftformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    swiftformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../switch_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    switch_transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../t5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    t5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../table_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    table_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../timesformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    timesformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tinybert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tinybert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../van/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    van
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vipllava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vipllava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vision_text_dual_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision_text_dual_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../visual_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visual_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_conformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_conformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_with_lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_with_lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wavlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    whisper
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../x_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    x_clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta_xl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta_xl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlnet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_4" >
        
          
          <label class="md-nav__link" for="__nav_5_9_4" id="__nav_5_9_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_4">
            <span class="md-nav__icon md-icon"></span>
            pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/pipeline/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/automatic_speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    automatic_speech_recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/document_question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    document_question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/fill_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fill_mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text2text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text2text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/zero_shot_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_shot_classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../configuration_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    configuration_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modeling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_fast/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_fast
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TRL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Notes
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Change Log
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code of Conduct
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_esm
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_esm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmConfig" class="md-nav__link">
    <span class="md-ellipsis">
      EsmConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmConfig.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmConfig.to_dict" class="md-nav__link">
    <span class="md-ellipsis">
      to_dict
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.__post_init__" class="md-nav__link">
    <span class="md-ellipsis">
      __post_init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.to_dict" class="md-nav__link">
    <span class="md-ellipsis">
      to_dict
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig" class="md-nav__link">
    <span class="md-ellipsis">
      StructureModuleConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StructureModuleConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig.to_dict" class="md-nav__link">
    <span class="md-ellipsis">
      to_dict
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig" class="md-nav__link">
    <span class="md-ellipsis">
      TrunkConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TrunkConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.__post_init__" class="md-nav__link">
    <span class="md-ellipsis">
      __post_init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.to_dict" class="md-nav__link">
    <span class="md-ellipsis">
      to_dict
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.configuration_esm.get_default_vocab_list" class="md-nav__link">
    <span class="md-ellipsis">
      get_default_vocab_list
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM" class="md-nav__link">
    <span class="md-ellipsis">
      EsmForMaskedLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmForMaskedLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.get_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_output_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.predict_contacts" class="md-nav__link">
    <span class="md-ellipsis">
      predict_contacts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.set_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_output_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification" class="md-nav__link">
    <span class="md-ellipsis">
      EsmForSequenceClassification
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmForSequenceClassification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification" class="md-nav__link">
    <span class="md-ellipsis">
      EsmForTokenClassification
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmForTokenClassification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel" class="md-nav__link">
    <span class="md-ellipsis">
      EsmModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.predict_contacts" class="md-nav__link">
    <span class="md-ellipsis">
      predict_contacts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      EsmPreTrainedModel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_esmfold
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_esmfold">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture" class="md-nav__link">
    <span class="md-ellipsis">
      EsmCategoricalMixture
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmCategoricalMixture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.log_prob" class="md-nav__link">
    <span class="md-ellipsis">
      log_prob
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.mean" class="md-nav__link">
    <span class="md-ellipsis">
      mean
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldAngleResnet
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldAngleResnet">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldAngleResnetBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldAngleResnetBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldBackboneUpdate
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldBackboneUpdate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldDropout
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldDropout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldInvariantPointAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldInvariantPointAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldLinear
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldLinear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldPairToSequence
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldPairToSequence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldPreTrainedModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldRelativePosition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldRelativePosition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldResidueMLP
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldResidueMLP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldSelfAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldSelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldSequenceToPair
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldSequenceToPair">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldStructureModule
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldStructureModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.frames_and_literature_positions_to_atom14_pos" class="md-nav__link">
    <span class="md-ellipsis">
      frames_and_literature_positions_to_atom14_pos
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.torsion_angles_to_frames" class="md-nav__link">
    <span class="md-ellipsis">
      torsion_angles_to_frames
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldStructureModuleTransition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldStructureModuleTransition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldStructureModuleTransitionLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldStructureModuleTransitionLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldTriangleAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldTriangleAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldTriangleMultiplicativeUpdate
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldTriangleMultiplicativeUpdate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldTriangularSelfAttentionBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldTriangularSelfAttentionBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk" class="md-nav__link">
    <span class="md-ellipsis">
      EsmFoldingTrunk
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmFoldingTrunk">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.distogram" class="md-nav__link">
    <span class="md-ellipsis">
      distogram
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.set_chunk_size" class="md-nav__link">
    <span class="md-ellipsis">
      set_chunk_size
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding" class="md-nav__link">
    <span class="md-ellipsis">
      EsmForProteinFolding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmForProteinFolding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.af2_idx_to_esm_idx" class="md-nav__link">
    <span class="md-ellipsis">
      af2_idx_to_esm_idx
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.bert_mask" class="md-nav__link">
    <span class="md-ellipsis">
      bert_mask
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.compute_language_model_representations" class="md-nav__link">
    <span class="md-ellipsis">
      compute_language_model_representations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer" class="md-nav__link">
    <span class="md-ellipsis">
      infer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer_pdb" class="md-nav__link">
    <span class="md-ellipsis">
      infer_pdb
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer_pdbs" class="md-nav__link">
    <span class="md-ellipsis">
      infer_pdbs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.output_to_pdb" class="md-nav__link">
    <span class="md-ellipsis">
      output_to_pdb
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFoldingOutput" class="md-nav__link">
    <span class="md-ellipsis">
      EsmForProteinFoldingOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.categorical_lddt" class="md-nav__link">
    <span class="md-ellipsis">
      categorical_lddt
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.collate_dense_tensors" class="md-nav__link">
    <span class="md-ellipsis">
      collate_dense_tensors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.dict_multimap" class="md-nav__link">
    <span class="md-ellipsis">
      dict_multimap
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.flatten_final_dims" class="md-nav__link">
    <span class="md-ellipsis">
      flatten_final_dims
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.get_axial_mask" class="md-nav__link">
    <span class="md-ellipsis">
      get_axial_mask
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.ipa_point_weights_init_" class="md-nav__link">
    <span class="md-ellipsis">
      ipa_point_weights_init_
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.permute_final_dims" class="md-nav__link">
    <span class="md-ellipsis">
      permute_final_dims
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.softmax_no_cast" class="md-nav__link">
    <span class="md-ellipsis">
      softmax_no_cast
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.modeling_esmfold.trunc_normal_init_" class="md-nav__link">
    <span class="md-ellipsis">
      trunc_normal_init_
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization_esm
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tokenization_esm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      EsmTokenizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EsmTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.vocab_size" class="md-nav__link">
    <span class="md-ellipsis">
      vocab_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.build_inputs_with_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      build_inputs_with_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.get_special_tokens_mask" class="md-nav__link">
    <span class="md-ellipsis">
      get_special_tokens_mask
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.get_vocab" class="md-nav__link">
    <span class="md-ellipsis">
      get_vocab
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.id_to_token" class="md-nav__link">
    <span class="md-ellipsis">
      id_to_token
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.save_vocabulary" class="md-nav__link">
    <span class="md-ellipsis">
      save_vocabulary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.token_to_id" class="md-nav__link">
    <span class="md-ellipsis">
      token_to_id
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.esm.tokenization_esm.load_vocab_file" class="md-nav__link">
    <span class="md-ellipsis">
      load_vocab_file
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindnlp/edit/master/docs/en/api/transformers/models/esm.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindnlp/raw/master/docs/en/api/transformers/models/esm.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>esm</h1>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.esm.configuration_esm" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.configuration_esm</code>


<a href="#mindnlp.transformers.models.esm.configuration_esm" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>ESM model configuration</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.configuration_esm.EsmConfig" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.configuration_esm.EsmConfig</code>


<a href="#mindnlp.transformers.models.esm.configuration_esm.EsmConfig" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.configuration_utils.PretrainedConfig" href="../../configuration_utils/#mindnlp.transformers.configuration_utils.PretrainedConfig">PretrainedConfig</a></code></p>


        <p>This is the configuration class to store the configuration of a [<code>ESMModel</code>]. It is used to instantiate a ESM model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the ESM
<a href="https://hf-mirror.com/facebook/esm-1b">facebook/esm-1b</a> architecture.</p>
<p>Configuration objects inherit from [<code>PretrainedConfig</code>] and can be used to control the model outputs. Read the
documentation from [<code>PretrainedConfig</code>] for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vocabulary size of the ESM model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling [<code>ESMModel</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The index of the mask token in the vocabulary. This must be included in the config because of the
"mask-dropout" scaling trick, which will scale the inputs depending on the number of masked tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The index of the padding token in the vocabulary. This must be included in the config because certain parts
of the ESM code use this instead of the attention mask.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimensionality of the encoder layers and the pooler layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 768</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>768</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_hidden_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the Transformer encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 12</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>12</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in the Transformer encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 12</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>12</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>intermediate_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimensionality of the "intermediate" (often named feed-forward) layer in the Transformer encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 3072</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3072</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_dropout_prob</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_probs_dropout_prob</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout ratio for the attention probabilities.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1026</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1026</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>initializer_range</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.02</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.02</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_norm_eps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The epsilon used by the layer normalization layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 1e-12</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-12</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_embedding_type</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Type of position embedding. Choose one of <code>"absolute"</code>, <code>"relative_key"</code>, <code>"relative_key_query", "rotary"</code>.
For positional embeddings use <code>"absolute"</code>. For more information on <code>"relative_key"</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>"relative_key_query"</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;absolute&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;absolute&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_decoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the model is used as a decoder or not. If <code>False</code>, the model is used as an encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>emb_layer_norm_before</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to apply layer normalization after embeddings but before the main stem of the network.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When this is enabled, masked tokens are treated as if they had been dropped out by input dropout.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">EsmModel</span><span class="p">,</span> <span class="n">EsmConfig</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a ESM facebook/esm-1b style configuration &gt;&gt;&gt; configuration = EsmConfig()</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a model from the configuration &gt;&gt;&gt; model = ESMModel(configuration)</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Accessing the model configuration &gt;&gt;&gt; configuration = model.config</span>
</code></pre></div>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the configuration class to store the configuration of a [`ESMModel`]. It is used to instantiate a ESM model</span>
<span class="sd">    according to the specified arguments, defining the model architecture. Instantiating a configuration with the</span>
<span class="sd">    defaults will yield a similar configuration to that of the ESM</span>
<span class="sd">    [facebook/esm-1b](https://hf-mirror.com/facebook/esm-1b) architecture.</span>

<span class="sd">    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the</span>
<span class="sd">    documentation from [`PretrainedConfig`] for more information.</span>


<span class="sd">    Args:</span>
<span class="sd">        vocab_size (`int`, *optional*):</span>
<span class="sd">            Vocabulary size of the ESM model. Defines the number of different tokens that can be represented by the</span>
<span class="sd">            `inputs_ids` passed when calling [`ESMModel`].</span>
<span class="sd">        mask_token_id (`int`, *optional*):</span>
<span class="sd">            The index of the mask token in the vocabulary. This must be included in the config because of the</span>
<span class="sd">            &quot;mask-dropout&quot; scaling trick, which will scale the inputs depending on the number of masked tokens.</span>
<span class="sd">        pad_token_id (`int`, *optional*):</span>
<span class="sd">            The index of the padding token in the vocabulary. This must be included in the config because certain parts</span>
<span class="sd">            of the ESM code use this instead of the attention mask.</span>
<span class="sd">        hidden_size (`int`, *optional*, defaults to 768):</span>
<span class="sd">            Dimensionality of the encoder layers and the pooler layer.</span>
<span class="sd">        num_hidden_layers (`int`, *optional*, defaults to 12):</span>
<span class="sd">            Number of hidden layers in the Transformer encoder.</span>
<span class="sd">        num_attention_heads (`int`, *optional*, defaults to 12):</span>
<span class="sd">            Number of attention heads for each attention layer in the Transformer encoder.</span>
<span class="sd">        intermediate_size (`int`, *optional*, defaults to 3072):</span>
<span class="sd">            Dimensionality of the &quot;intermediate&quot; (often named feed-forward) layer in the Transformer encoder.</span>
<span class="sd">        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</span>
<span class="sd">        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The dropout ratio for the attention probabilities.</span>
<span class="sd">        max_position_embeddings (`int`, *optional*, defaults to 1026):</span>
<span class="sd">            The maximum sequence length that this model might ever be used with. Typically set this to something large</span>
<span class="sd">            just in case (e.g., 512 or 1024 or 2048).</span>
<span class="sd">        initializer_range (`float`, *optional*, defaults to 0.02):</span>
<span class="sd">            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</span>
<span class="sd">        layer_norm_eps (`float`, *optional*, defaults to 1e-12):</span>
<span class="sd">            The epsilon used by the layer normalization layers.</span>
<span class="sd">        position_embedding_type (`str`, *optional*, defaults to `&quot;absolute&quot;`):</span>
<span class="sd">            Type of position embedding. Choose one of `&quot;absolute&quot;`, `&quot;relative_key&quot;`, `&quot;relative_key_query&quot;, &quot;rotary&quot;`.</span>
<span class="sd">            For positional embeddings use `&quot;absolute&quot;`. For more information on `&quot;relative_key&quot;`, please refer to</span>
<span class="sd">            [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).</span>
<span class="sd">            For more information on `&quot;relative_key_query&quot;`, please refer to *Method 4* in [Improve Transformer Models</span>
<span class="sd">            with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).</span>
<span class="sd">        is_decoder (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.</span>
<span class="sd">        use_cache (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether or not the model should return the last key/values attentions (not used by all models). Only</span>
<span class="sd">            relevant if `config.is_decoder=True`.</span>
<span class="sd">        emb_layer_norm_before (`bool`, *optional*):</span>
<span class="sd">            Whether to apply layer normalization after embeddings but before the main stem of the network.</span>
<span class="sd">        token_dropout (`bool`, defaults to `False`):</span>
<span class="sd">            When this is enabled, masked tokens are treated as if they had been dropped out by input dropout.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import EsmModel, EsmConfig</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a ESM facebook/esm-1b style configuration &gt;&gt;&gt; configuration = EsmConfig()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a model from the configuration &gt;&gt;&gt; model = ESMModel(configuration)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Accessing the model configuration &gt;&gt;&gt; configuration = model.config</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;esm&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">mask_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
        <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">intermediate_size</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span>
        <span class="n">hidden_dropout_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">attention_probs_dropout_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">1026</span><span class="p">,</span>
        <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
        <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span>
        <span class="n">position_embedding_type</span><span class="o">=</span><span class="s2">&quot;absolute&quot;</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">emb_layer_norm_before</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">token_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">is_folding_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">esmfold_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">vocab_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the `EsmConfig` class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            vocab_size (int, optional): The size of the vocabulary. Defaults to None.</span>
<span class="sd">            mask_token_id (int, optional): The ID of the mask token. Defaults to None.</span>
<span class="sd">            pad_token_id (int, optional): The ID of the padding token. Defaults to None.</span>
<span class="sd">            hidden_size (int, optional): The size of the hidden layers. Defaults to 768.</span>
<span class="sd">            num_hidden_layers (int, optional): The number of hidden layers. Defaults to 12.</span>
<span class="sd">            num_attention_heads (int, optional): The number of attention heads. Defaults to 12.</span>
<span class="sd">            intermediate_size (int, optional): The size of the intermediate layers. Defaults to 3072.</span>
<span class="sd">            hidden_dropout_prob (float, optional): The dropout probability for hidden layers. Defaults to 0.1.</span>
<span class="sd">            attention_probs_dropout_prob (float, optional): The dropout probability for attention layers. Defaults to 0.1.</span>
<span class="sd">            max_position_embeddings (int, optional): The maximum position embeddings. Defaults to 1026.</span>
<span class="sd">            initializer_range (float, optional): The range for initializer values. Defaults to 0.02.</span>
<span class="sd">            layer_norm_eps (float, optional): The epsilon value for layer normalization. Defaults to 1e-12.</span>
<span class="sd">            position_embedding_type (str, optional): The type of position embedding. Defaults to &#39;absolute&#39;.</span>
<span class="sd">            use_cache (bool, optional): Whether to use cache. Defaults to True.</span>
<span class="sd">            emb_layer_norm_before (bool, optional): Whether to normalize embeddings before layers. Defaults to None.</span>
<span class="sd">            token_dropout (bool, optional): Whether to apply token dropout. Defaults to False.</span>
<span class="sd">            is_folding_model (bool, optional): Whether the model is a folding model. Defaults to False.</span>
<span class="sd">            esmfold_config (EsmFoldConfig, optional): The configuration for the folding model. Defaults to None.</span>
<span class="sd">            vocab_list (list, optional): The list of vocabulary tokens. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the HuggingFace port of ESMFold does not support `use_esm_attn_map`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">mask_token_id</span><span class="o">=</span><span class="n">mask_token_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">intermediate_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dropout_prob</span> <span class="o">=</span> <span class="n">hidden_dropout_prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_probs_dropout_prob</span> <span class="o">=</span> <span class="n">attention_probs_dropout_prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_eps</span> <span class="o">=</span> <span class="n">layer_norm_eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_type</span> <span class="o">=</span> <span class="n">position_embedding_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_layer_norm_before</span> <span class="o">=</span> <span class="n">emb_layer_norm_before</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_dropout</span> <span class="o">=</span> <span class="n">token_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_folding_model</span> <span class="o">=</span> <span class="n">is_folding_model</span>
        <span class="k">if</span> <span class="n">is_folding_model</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">esmfold_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;No esmfold_config supplied for folding model, using default values.&quot;</span><span class="p">)</span>
                <span class="n">esmfold_config</span> <span class="o">=</span> <span class="n">EsmFoldConfig</span><span class="p">()</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">esmfold_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">esmfold_config</span> <span class="o">=</span> <span class="n">EsmFoldConfig</span><span class="p">(</span><span class="o">**</span><span class="n">esmfold_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span> <span class="o">=</span> <span class="n">esmfold_config</span>
            <span class="k">if</span> <span class="n">vocab_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;No vocab_list supplied for folding model, assuming the ESM-2 vocabulary!&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab_list</span> <span class="o">=</span> <span class="n">get_default_vocab_list</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab_list</span> <span class="o">=</span> <span class="n">vocab_list</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocab_list</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span><span class="p">,</span> <span class="s2">&quot;use_esm_attn_map&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The HuggingFace port of ESMFold does not support use_esm_attn_map at this time!&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].</span>

<span class="sd">        Returns:</span>
<span class="sd">            `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span><span class="p">,</span> <span class="n">EsmFoldConfig</span><span class="p">):</span>
            <span class="n">output</span><span class="p">[</span><span class="s2">&quot;esmfold_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.configuration_esm.EsmConfig.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">configuration_esm</span><span class="o">.</span><span class="n">EsmConfig</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mask_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="n">hidden_dropout_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attention_probs_dropout_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">1026</span><span class="p">,</span> <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">position_embedding_type</span><span class="o">=</span><span class="s1">&#39;absolute&#39;</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">emb_layer_norm_before</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">token_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">is_folding_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">esmfold_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vocab_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.configuration_esm.EsmConfig.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the <code>EsmConfig</code> class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the vocabulary. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ID of the mask token. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ID of the padding token. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the hidden layers. Defaults to 768.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>768</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_hidden_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of hidden layers. Defaults to 12.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>12</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads. Defaults to 12.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>12</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>intermediate_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the intermediate layers. Defaults to 3072.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3072</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_dropout_prob</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for hidden layers. Defaults to 0.1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_probs_dropout_prob</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for attention layers. Defaults to 0.1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum position embeddings. Defaults to 1026.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1026</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>initializer_range</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The range for initializer values. Defaults to 0.02.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.02</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_norm_eps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The epsilon value for layer normalization. Defaults to 1e-12.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-12</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_embedding_type</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The type of position embedding. Defaults to 'absolute'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;absolute&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to use cache. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>emb_layer_norm_before</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to normalize embeddings before layers. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to apply token dropout. Defaults to False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_folding_model</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the model is a folding model. Defaults to False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>esmfold_config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration for the folding model. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig" href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig">EsmFoldConfig</a></code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_list</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The list of vocabulary tokens. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>list</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the HuggingFace port of ESMFold does not support <code>use_esm_attn_map</code>.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mask_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
    <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">intermediate_size</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span>
    <span class="n">hidden_dropout_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">attention_probs_dropout_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">1026</span><span class="p">,</span>
    <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span>
    <span class="n">position_embedding_type</span><span class="o">=</span><span class="s2">&quot;absolute&quot;</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">emb_layer_norm_before</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">token_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">is_folding_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">esmfold_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">vocab_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the `EsmConfig` class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        vocab_size (int, optional): The size of the vocabulary. Defaults to None.</span>
<span class="sd">        mask_token_id (int, optional): The ID of the mask token. Defaults to None.</span>
<span class="sd">        pad_token_id (int, optional): The ID of the padding token. Defaults to None.</span>
<span class="sd">        hidden_size (int, optional): The size of the hidden layers. Defaults to 768.</span>
<span class="sd">        num_hidden_layers (int, optional): The number of hidden layers. Defaults to 12.</span>
<span class="sd">        num_attention_heads (int, optional): The number of attention heads. Defaults to 12.</span>
<span class="sd">        intermediate_size (int, optional): The size of the intermediate layers. Defaults to 3072.</span>
<span class="sd">        hidden_dropout_prob (float, optional): The dropout probability for hidden layers. Defaults to 0.1.</span>
<span class="sd">        attention_probs_dropout_prob (float, optional): The dropout probability for attention layers. Defaults to 0.1.</span>
<span class="sd">        max_position_embeddings (int, optional): The maximum position embeddings. Defaults to 1026.</span>
<span class="sd">        initializer_range (float, optional): The range for initializer values. Defaults to 0.02.</span>
<span class="sd">        layer_norm_eps (float, optional): The epsilon value for layer normalization. Defaults to 1e-12.</span>
<span class="sd">        position_embedding_type (str, optional): The type of position embedding. Defaults to &#39;absolute&#39;.</span>
<span class="sd">        use_cache (bool, optional): Whether to use cache. Defaults to True.</span>
<span class="sd">        emb_layer_norm_before (bool, optional): Whether to normalize embeddings before layers. Defaults to None.</span>
<span class="sd">        token_dropout (bool, optional): Whether to apply token dropout. Defaults to False.</span>
<span class="sd">        is_folding_model (bool, optional): Whether the model is a folding model. Defaults to False.</span>
<span class="sd">        esmfold_config (EsmFoldConfig, optional): The configuration for the folding model. Defaults to None.</span>
<span class="sd">        vocab_list (list, optional): The list of vocabulary tokens. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the HuggingFace port of ESMFold does not support `use_esm_attn_map`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">mask_token_id</span><span class="o">=</span><span class="n">mask_token_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">intermediate_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dropout_prob</span> <span class="o">=</span> <span class="n">hidden_dropout_prob</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_probs_dropout_prob</span> <span class="o">=</span> <span class="n">attention_probs_dropout_prob</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">max_position_embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_eps</span> <span class="o">=</span> <span class="n">layer_norm_eps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_type</span> <span class="o">=</span> <span class="n">position_embedding_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">emb_layer_norm_before</span> <span class="o">=</span> <span class="n">emb_layer_norm_before</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">token_dropout</span> <span class="o">=</span> <span class="n">token_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">is_folding_model</span> <span class="o">=</span> <span class="n">is_folding_model</span>
    <span class="k">if</span> <span class="n">is_folding_model</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">esmfold_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;No esmfold_config supplied for folding model, using default values.&quot;</span><span class="p">)</span>
            <span class="n">esmfold_config</span> <span class="o">=</span> <span class="n">EsmFoldConfig</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">esmfold_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">esmfold_config</span> <span class="o">=</span> <span class="n">EsmFoldConfig</span><span class="p">(</span><span class="o">**</span><span class="n">esmfold_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span> <span class="o">=</span> <span class="n">esmfold_config</span>
        <span class="k">if</span> <span class="n">vocab_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;No vocab_list supplied for folding model, assuming the ESM-2 vocabulary!&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocab_list</span> <span class="o">=</span> <span class="n">get_default_vocab_list</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocab_list</span> <span class="o">=</span> <span class="n">vocab_list</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_list</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span><span class="p">,</span> <span class="s2">&quot;use_esm_attn_map&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The HuggingFace port of ESMFold does not support use_esm_attn_map at this time!&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.configuration_esm.EsmConfig.to_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">configuration_esm</span><span class="o">.</span><span class="n">EsmConfig</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.esm.configuration_esm.EsmConfig.to_dict" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Serializes this instance to a Python dictionary. Override the default [<code>~PretrainedConfig.to_dict</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>Dict[str, any]</code>: Dictionary of all the attributes that make up this configuration instance,</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].</span>

<span class="sd">    Returns:</span>
<span class="sd">        `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span><span class="p">,</span> <span class="n">EsmFoldConfig</span><span class="p">):</span>
        <span class="n">output</span><span class="p">[</span><span class="s2">&quot;esmfold_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>Represents the configuration of an ESM (Efficient Speech Model) fold instance.</p>
<p>This class provides methods to initialize the EsmFoldConfig instance and serialize it to a Python dictionary.</p>
<p>The EsmFoldConfig class inherits from a base class and includes methods for post-initialization and dictionary serialization.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.__post_init__" href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.__post_init__">__post_init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the EsmFoldConfig instance, setting defaults for any missing attributes.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.to_dict" href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.to_dict">to_dict</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Serializes the EsmFoldConfig instance to a Python dictionary, including the trunk configuration.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.trunk">trunk</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Represents the configuration of the trunk model used in the ESM fold.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig" href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig">TrunkConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <p>Ensure that the trunk attribute is either set to a TrunkConfig instance or a dictionary that can be converted to a TrunkConfig.</p>
</details>

<details class="return" open>
  <summary>Return</summary>
  <p>A Python dictionary containing all the attributes of the EsmFoldConfig instance, including the trunk configuration.</p>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">EsmFoldConfig</span><span class="p">:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents the configuration of an ESM (Efficient Speech Model) fold instance.</span>

<span class="sd">    This class provides methods to initialize the EsmFoldConfig instance and serialize it to a Python dictionary.</span>

<span class="sd">    The EsmFoldConfig class inherits from a base class and includes methods for post-initialization and dictionary serialization.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __post_init__(self): Initializes the EsmFoldConfig instance, setting defaults for any missing attributes.</span>
<span class="sd">        to_dict(self): Serializes the EsmFoldConfig instance to a Python dictionary, including the trunk configuration.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        trunk: Represents the configuration of the trunk model used in the ESM fold.</span>

<span class="sd">    Note:</span>
<span class="sd">        Ensure that the trunk attribute is either set to a TrunkConfig instance or a dictionary that can be converted to a TrunkConfig.</span>

<span class="sd">    Return:</span>
<span class="sd">        A Python dictionary containing all the attributes of the EsmFoldConfig instance, including the trunk configuration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">esm_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">fp16_esm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">use_esm_attn_map</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">esm_ablate_pairwise</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">esm_ablate_sequence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">esm_input_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">embed_aa</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">bypass_lm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">lddt_head_hid_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">trunk</span><span class="p">:</span> <span class="s2">&quot;TrunkConfig&quot;</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The &#39;__post_init__&#39; method is used in the &#39;EsmFoldConfig&#39; class to initialize the &#39;trunk&#39; attribute.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the &#39;EsmFoldConfig&#39; class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        Description:</span>
<span class="sd">            This method checks if the &#39;trunk&#39; attribute is None. If it is, a new instance of the &#39;TrunkConfig&#39; class</span>
<span class="sd">            is created and assigned to &#39;self.trunk&#39;. If the &#39;trunk&#39; attribute is of type dict, it is unpacked and</span>
<span class="sd">            passed as keyword arguments to create a new instance of the &#39;TrunkConfig&#39; class,  which is then assigned to</span>
<span class="sd">            &#39;self.trunk&#39;. This method is typically called after the object is initialized to ensure that the &#39;trunk&#39;</span>
<span class="sd">            attribute is properly set.</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; config = EsmFoldConfig()</span>
<span class="sd">            &gt;&gt;&gt; config.__post_init__()</span>
<span class="sd">            &gt;&gt;&gt; # The &#39;trunk&#39; attribute will be initialized with a new instance of the &#39;TrunkConfig&#39; class.</span>
<span class="sd">            ...</span>
<span class="sd">            &gt;&gt;&gt; config = EsmFoldConfig(trunk={&#39;option1&#39;: True, &#39;option2&#39;: False})</span>
<span class="sd">            &gt;&gt;&gt; config.__post_init__()</span>
<span class="sd">            &gt;&gt;&gt; # The &#39;trunk&#39; attribute will be initialized with a new instance of the &#39;TrunkConfig&#39; class,</span>
<span class="sd">            &gt;&gt;&gt; # with &#39;option1&#39; set to True and &#39;option2&#39; set to False.</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span> <span class="o">=</span> <span class="n">TrunkConfig</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span> <span class="o">=</span> <span class="n">TrunkConfig</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].</span>

<span class="sd">        Returns:</span>
<span class="sd">            `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">asdict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">output</span><span class="p">[</span><span class="s2">&quot;trunk&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.__post_init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">configuration_esm</span><span class="o">.</span><span class="n">EsmFoldConfig</span><span class="o">.</span><span class="n">__post_init__</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.__post_init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The '<strong>post_init</strong>' method is used in the 'EsmFoldConfig' class to initialize the 'trunk' attribute.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the 'EsmFoldConfig' class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="description" open>
  <summary>Description</summary>
  <p>This method checks if the 'trunk' attribute is None. If it is, a new instance of the 'TrunkConfig' class
is created and assigned to 'self.trunk'. If the 'trunk' attribute is of type dict, it is unpacked and
passed as keyword arguments to create a new instance of the 'TrunkConfig' class,  which is then assigned to
'self.trunk'. This method is typically called after the object is initialized to ensure that the 'trunk'
attribute is properly set.</p>
</details>

<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">config</span> <span class="o">=</span> <span class="n">EsmFoldConfig</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">__post_init__</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># The &#39;trunk&#39; attribute will be initialized with a new instance of the &#39;TrunkConfig&#39; class.</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">config</span> <span class="o">=</span> <span class="n">EsmFoldConfig</span><span class="p">(</span><span class="n">trunk</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;option1&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;option2&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">__post_init__</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># The &#39;trunk&#39; attribute will be initialized with a new instance of the &#39;TrunkConfig&#39; class,</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># with &#39;option1&#39; set to True and &#39;option2&#39; set to False.</span>
</code></pre></div>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The &#39;__post_init__&#39; method is used in the &#39;EsmFoldConfig&#39; class to initialize the &#39;trunk&#39; attribute.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: An instance of the &#39;EsmFoldConfig&#39; class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>

<span class="sd">    Description:</span>
<span class="sd">        This method checks if the &#39;trunk&#39; attribute is None. If it is, a new instance of the &#39;TrunkConfig&#39; class</span>
<span class="sd">        is created and assigned to &#39;self.trunk&#39;. If the &#39;trunk&#39; attribute is of type dict, it is unpacked and</span>
<span class="sd">        passed as keyword arguments to create a new instance of the &#39;TrunkConfig&#39; class,  which is then assigned to</span>
<span class="sd">        &#39;self.trunk&#39;. This method is typically called after the object is initialized to ensure that the &#39;trunk&#39;</span>
<span class="sd">        attribute is properly set.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; config = EsmFoldConfig()</span>
<span class="sd">        &gt;&gt;&gt; config.__post_init__()</span>
<span class="sd">        &gt;&gt;&gt; # The &#39;trunk&#39; attribute will be initialized with a new instance of the &#39;TrunkConfig&#39; class.</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; config = EsmFoldConfig(trunk={&#39;option1&#39;: True, &#39;option2&#39;: False})</span>
<span class="sd">        &gt;&gt;&gt; config.__post_init__()</span>
<span class="sd">        &gt;&gt;&gt; # The &#39;trunk&#39; attribute will be initialized with a new instance of the &#39;TrunkConfig&#39; class,</span>
<span class="sd">        &gt;&gt;&gt; # with &#39;option1&#39; set to True and &#39;option2&#39; set to False.</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span> <span class="o">=</span> <span class="n">TrunkConfig</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span> <span class="o">=</span> <span class="n">TrunkConfig</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.to_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">configuration_esm</span><span class="o">.</span><span class="n">EsmFoldConfig</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.esm.configuration_esm.EsmFoldConfig.to_dict" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Serializes this instance to a Python dictionary. Override the default [<code>~PretrainedConfig.to_dict</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>Dict[str, any]</code>: Dictionary of all the attributes that make up this configuration instance,</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].</span>

<span class="sd">    Returns:</span>
<span class="sd">        `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">asdict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">&quot;trunk&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">




<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sequence_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Single representation channel dimension</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>384</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pairwise_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pair representation channel dimension</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>128</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ipa_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>IPA hidden channel dimension</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resnet_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Angle resnet (Alg. 23 lines 11-14) hidden channel dimension</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>128</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_heads_ipa</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of IPA heads</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>12</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_qk_points</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of query/key points to generate during IPA</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_v_points</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of value points to generate during IPA</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dropout rate used throughout the layer</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_blocks</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of structure module blocks</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_transition_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of layers in the single representation transition (Alg. 23 lines 8-9)</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_resnet_blocks</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of blocks in the angle resnet</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_angles</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of angles to generate in the angle resnet</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>7</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>trans_scale_factor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Scale of single representation transition hidden dimension</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>epsilon</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Small number used in angle resnet normalization</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-08</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inf</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Large number used for attention masking</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>100000.0</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">StructureModuleConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        sequence_dim:</span>
<span class="sd">            Single representation channel dimension</span>
<span class="sd">        pairwise_dim:</span>
<span class="sd">            Pair representation channel dimension</span>
<span class="sd">        ipa_dim:</span>
<span class="sd">            IPA hidden channel dimension</span>
<span class="sd">        resnet_dim:</span>
<span class="sd">            Angle resnet (Alg. 23 lines 11-14) hidden channel dimension</span>
<span class="sd">        num_heads_ipa:</span>
<span class="sd">            Number of IPA heads</span>
<span class="sd">        num_qk_points:</span>
<span class="sd">            Number of query/key points to generate during IPA</span>
<span class="sd">        num_v_points:</span>
<span class="sd">            Number of value points to generate during IPA</span>
<span class="sd">        dropout_rate:</span>
<span class="sd">            Dropout rate used throughout the layer</span>
<span class="sd">        num_blocks:</span>
<span class="sd">            Number of structure module blocks</span>
<span class="sd">        num_transition_layers:</span>
<span class="sd">            Number of layers in the single representation transition (Alg. 23 lines 8-9)</span>
<span class="sd">        num_resnet_blocks:</span>
<span class="sd">            Number of blocks in the angle resnet</span>
<span class="sd">        num_angles:</span>
<span class="sd">            Number of angles to generate in the angle resnet</span>
<span class="sd">        trans_scale_factor:</span>
<span class="sd">            Scale of single representation transition hidden dimension</span>
<span class="sd">        epsilon:</span>
<span class="sd">            Small number used in angle resnet normalization</span>
<span class="sd">        inf:</span>
<span class="sd">            Large number used for attention masking</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sequence_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">384</span>
    <span class="n">pairwise_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">ipa_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">resnet_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">num_heads_ipa</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span>
    <span class="n">num_qk_points</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">num_v_points</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">num_transition_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">num_resnet_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">num_angles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">7</span>
    <span class="n">trans_scale_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span>
    <span class="n">inf</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e5</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts the current instance of the StructureModuleConfig class to a dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (StructureModuleConfig): The current instance of the StructureModuleConfig class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: A dictionary representation of the current StructureModuleConfig instance.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">asdict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig.to_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">configuration_esm</span><span class="o">.</span><span class="n">StructureModuleConfig</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig.to_dict" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Converts the current instance of the StructureModuleConfig class to a dictionary.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The current instance of the StructureModuleConfig class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig" href="#mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig">StructureModuleConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>dict</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A dictionary representation of the current StructureModuleConfig instance.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts the current instance of the StructureModuleConfig class to a dictionary.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (StructureModuleConfig): The current instance of the StructureModuleConfig class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary representation of the current StructureModuleConfig instance.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">asdict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.configuration_esm.TrunkConfig</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>Represents the configuration settings for the Trunk model.
This class defines the configuration attributes and their validations for the Trunk model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.structure_module">structure_module</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The configuration for the structure module.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig" href="#mindnlp.transformers.models.esm.configuration_esm.StructureModuleConfig">StructureModuleConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.max_recycles">max_recycles</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The maximum number of recycles, should be a positive integer.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.sequence_state_dim">sequence_state_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dimension of the sequence state.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.pairwise_state_dim">pairwise_state_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dimension of the pairwise state.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.sequence_head_width">sequence_head_width</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The width of the sequence head.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.pairwise_head_width">pairwise_head_width</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The width of the pairwise head.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.dropout">dropout</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dropout rate, should not be greater than 0.4.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If any of the following conditions are not met:</p>
<ul>
<li><code>max_recycles</code> is not a positive integer.</li>
<li><code>sequence_state_dim</code> is not a round multiple of itself.</li>
<li><code>pairwise_state_dim</code> is not a round multiple of itself.</li>
<li><code>sequence_state_dim</code> is not equal to <code>sequence_num_heads * sequence_head_width</code>.</li>
<li><code>pairwise_state_dim</code> is not equal to <code>pairwise_num_heads * pairwise_head_width</code>.</li>
<li><code>pairwise_state_dim</code> is not an even number.</li>
<li><code>dropout</code> is greater than 0.4.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.__post_init__" href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.__post_init__">__post_init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Performs post-initialization validations for the configuration attributes.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.to_dict" href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.to_dict">to_dict</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Serializes the instance to a Python dictionary, including the structure module configuration.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="overrides" open>
  <summary>Overrides</summary>
  <p><code>~PretrainedConfig.to_dict</code>: Overrides the default <code>to_dict</code> method to include the structure module
configuration in the dictionary output.</p>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">TrunkConfig</span><span class="p">:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents the configuration settings for the Trunk model.</span>
<span class="sd">    This class defines the configuration attributes and their validations for the Trunk model.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        structure_module (StructureModuleConfig): The configuration for the structure module.</span>
<span class="sd">        max_recycles (int): The maximum number of recycles, should be a positive integer.</span>
<span class="sd">        sequence_state_dim (int): The dimension of the sequence state.</span>
<span class="sd">        pairwise_state_dim (int): The dimension of the pairwise state.</span>
<span class="sd">        sequence_head_width (int): The width of the sequence head.</span>
<span class="sd">        pairwise_head_width (int): The width of the pairwise head.</span>
<span class="sd">        dropout (float): The dropout rate, should not be greater than 0.4.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError:</span>
<span class="sd">            If any of the following conditions are not met:</span>

<span class="sd">            - `max_recycles` is not a positive integer.</span>
<span class="sd">            - `sequence_state_dim` is not a round multiple of itself.</span>
<span class="sd">            - `pairwise_state_dim` is not a round multiple of itself.</span>
<span class="sd">            - `sequence_state_dim` is not equal to `sequence_num_heads * sequence_head_width`.</span>
<span class="sd">            - `pairwise_state_dim` is not equal to `pairwise_num_heads * pairwise_head_width`.</span>
<span class="sd">            - `pairwise_state_dim` is not an even number.</span>
<span class="sd">            - `dropout` is greater than 0.4.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __post_init__(self): Performs post-initialization validations for the configuration attributes.</span>
<span class="sd">        to_dict(self): Serializes the instance to a Python dictionary, including the structure module configuration.</span>

<span class="sd">    Overrides:</span>
<span class="sd">        `~PretrainedConfig.to_dict`: Overrides the default `to_dict` method to include the structure module</span>
<span class="sd">        configuration in the dictionary output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span>
    <span class="n">sequence_state_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span>
    <span class="n">pairwise_state_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">sequence_head_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">pairwise_head_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">position_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">layer_drop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">cpu_grad_checkpoint</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">max_recycles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">chunk_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">structure_module</span><span class="p">:</span> <span class="s2">&quot;StructureModuleConfig&quot;</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method initializes the TrunkConfig class after its instantiation.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the TrunkConfig class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If `max_recycles` is not a positive value.</span>
<span class="sd">            ValueError: If `sequence_state_dim` is not a round multiple of itself.</span>
<span class="sd">            ValueError: If `pairwise_state_dim` is not a round multiple of itself.</span>
<span class="sd">            ValueError: If `sequence_state_dim` is not equal to `sequence_num_heads * sequence_head_width`.</span>
<span class="sd">            ValueError: If `pairwise_state_dim` is not equal to `pairwise_num_heads * pairwise_head_width`.</span>
<span class="sd">            ValueError: If `pairwise_state_dim` is not an even number.</span>
<span class="sd">            ValueError: If `dropout` is greater than or equal to 0.4.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span> <span class="o">=</span> <span class="n">StructureModuleConfig</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span> <span class="o">=</span> <span class="n">StructureModuleConfig</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_recycles</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`max_recycles` should be positive, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_recycles</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`sequence_state_dim` should be a round multiple of `sequence_state_dim`, got&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`pairwise_state_dim` should be a round multiple of `pairwise_state_dim`, got&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="n">sequence_num_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_head_width</span>
        <span class="n">pairwise_num_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_head_width</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span> <span class="o">!=</span> <span class="n">sequence_num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_head_width</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`sequence_state_dim` should be equal to `sequence_num_heads * sequence_head_width, got&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">sequence_num_heads</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sequence_head_width</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span> <span class="o">!=</span> <span class="n">pairwise_num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_head_width</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`pairwise_state_dim` should be equal to `pairwise_num_heads * pairwise_head_width, got&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">pairwise_num_heads</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pairwise_head_width</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`pairwise_state_dim` should be even, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">&gt;=</span> <span class="mf">0.4</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`dropout` should not be greater than 0.4, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].</span>

<span class="sd">        Returns:</span>
<span class="sd">            `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">asdict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">output</span><span class="p">[</span><span class="s2">&quot;structure_module&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.__post_init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">configuration_esm</span><span class="o">.</span><span class="n">TrunkConfig</span><span class="o">.</span><span class="n">__post_init__</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.__post_init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method initializes the TrunkConfig class after its instantiation.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the TrunkConfig class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If <code>max_recycles</code> is not a positive value.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If <code>sequence_state_dim</code> is not a round multiple of itself.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If <code>pairwise_state_dim</code> is not a round multiple of itself.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If <code>sequence_state_dim</code> is not equal to <code>sequence_num_heads * sequence_head_width</code>.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If <code>pairwise_state_dim</code> is not equal to <code>pairwise_num_heads * pairwise_head_width</code>.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If <code>pairwise_state_dim</code> is not an even number.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If <code>dropout</code> is greater than or equal to 0.4.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method initializes the TrunkConfig class after its instantiation.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the TrunkConfig class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `max_recycles` is not a positive value.</span>
<span class="sd">        ValueError: If `sequence_state_dim` is not a round multiple of itself.</span>
<span class="sd">        ValueError: If `pairwise_state_dim` is not a round multiple of itself.</span>
<span class="sd">        ValueError: If `sequence_state_dim` is not equal to `sequence_num_heads * sequence_head_width`.</span>
<span class="sd">        ValueError: If `pairwise_state_dim` is not equal to `pairwise_num_heads * pairwise_head_width`.</span>
<span class="sd">        ValueError: If `pairwise_state_dim` is not an even number.</span>
<span class="sd">        ValueError: If `dropout` is greater than or equal to 0.4.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span> <span class="o">=</span> <span class="n">StructureModuleConfig</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span> <span class="o">=</span> <span class="n">StructureModuleConfig</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_recycles</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`max_recycles` should be positive, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_recycles</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;`sequence_state_dim` should be a round multiple of `sequence_state_dim`, got&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;`pairwise_state_dim` should be a round multiple of `pairwise_state_dim`, got&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="n">sequence_num_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_head_width</span>
    <span class="n">pairwise_num_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_head_width</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span> <span class="o">!=</span> <span class="n">sequence_num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_head_width</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;`sequence_state_dim` should be equal to `sequence_num_heads * sequence_head_width, got&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sequence_state_dim</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">sequence_num_heads</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sequence_head_width</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span> <span class="o">!=</span> <span class="n">pairwise_num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_head_width</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;`pairwise_state_dim` should be equal to `pairwise_num_heads * pairwise_head_width, got&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">pairwise_num_heads</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pairwise_head_width</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`pairwise_state_dim` should be even, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">&gt;=</span> <span class="mf">0.4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`dropout` should not be greater than 0.4, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.to_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">configuration_esm</span><span class="o">.</span><span class="n">TrunkConfig</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.esm.configuration_esm.TrunkConfig.to_dict" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Serializes this instance to a Python dictionary. Override the default [<code>~PretrainedConfig.to_dict</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>Dict[str, any]</code>: Dictionary of all the attributes that make up this configuration instance,</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].</span>

<span class="sd">    Returns:</span>
<span class="sd">        `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">asdict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">&quot;structure_module&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.configuration_esm.get_default_vocab_list" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">configuration_esm</span><span class="o">.</span><span class="n">get_default_vocab_list</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.esm.configuration_esm.get_default_vocab_list" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This function returns a list of default vocabulary items including special tokens and characters used in natural
language processing tasks.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>List</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A list of default vocabulary items including '<cls>', '<pad>', '<eos>', '<unk>', 'L', 'A', 'G', 'V', 'S',
'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C', 'X', 'B', 'U',
'Z', 'O', '.', '-', '<null_1>', '<mask>'.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\configuration_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_default_vocab_list</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This function returns a list of default vocabulary items including special tokens and characters used in natural</span>
<span class="sd">    language processing tasks.</span>

<span class="sd">    Args:</span>
<span class="sd">        None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        List:</span>
<span class="sd">            A list of default vocabulary items including &#39;&lt;cls&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;eos&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;L&#39;, &#39;A&#39;, &#39;G&#39;, &#39;V&#39;, &#39;S&#39;,</span>
<span class="sd">            &#39;E&#39;, &#39;R&#39;, &#39;T&#39;, &#39;I&#39;, &#39;D&#39;, &#39;P&#39;, &#39;K&#39;, &#39;Q&#39;, &#39;N&#39;, &#39;F&#39;, &#39;Y&#39;, &#39;M&#39;, &#39;H&#39;, &#39;W&#39;, &#39;C&#39;, &#39;X&#39;, &#39;B&#39;, &#39;U&#39;,</span>
<span class="sd">            &#39;Z&#39;, &#39;O&#39;, &#39;.&#39;, &#39;-&#39;, &#39;&lt;null_1&gt;&#39;, &#39;&lt;mask&gt;&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="s2">&quot;&lt;cls&gt;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&lt;eos&gt;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;L&quot;</span><span class="p">,</span>
        <span class="s2">&quot;A&quot;</span><span class="p">,</span>
        <span class="s2">&quot;G&quot;</span><span class="p">,</span>
        <span class="s2">&quot;V&quot;</span><span class="p">,</span>
        <span class="s2">&quot;S&quot;</span><span class="p">,</span>
        <span class="s2">&quot;E&quot;</span><span class="p">,</span>
        <span class="s2">&quot;R&quot;</span><span class="p">,</span>
        <span class="s2">&quot;T&quot;</span><span class="p">,</span>
        <span class="s2">&quot;I&quot;</span><span class="p">,</span>
        <span class="s2">&quot;D&quot;</span><span class="p">,</span>
        <span class="s2">&quot;P&quot;</span><span class="p">,</span>
        <span class="s2">&quot;K&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Q&quot;</span><span class="p">,</span>
        <span class="s2">&quot;N&quot;</span><span class="p">,</span>
        <span class="s2">&quot;F&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Y&quot;</span><span class="p">,</span>
        <span class="s2">&quot;M&quot;</span><span class="p">,</span>
        <span class="s2">&quot;H&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span>
        <span class="s2">&quot;C&quot;</span><span class="p">,</span>
        <span class="s2">&quot;X&quot;</span><span class="p">,</span>
        <span class="s2">&quot;B&quot;</span><span class="p">,</span>
        <span class="s2">&quot;U&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Z&quot;</span><span class="p">,</span>
        <span class="s2">&quot;O&quot;</span><span class="p">,</span>
        <span class="s2">&quot;.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;-&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&lt;null_1&gt;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&lt;mask&gt;&quot;</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM</code>


<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel" href="#mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel">EsmPreTrainedModel</a></code></p>


        <p>Represents an ESM (Evolutionary Scale Modeling) model for masked language modeling (MLM), inheriting from EsmPreTrainedModel.
This class provides the functionality to perform masked language modeling using the ESM model.</p>
<p>The EsmForMaskedLM class contains methods for initializing the model, getting and setting output embeddings,
forwarding the model, and predicting contacts.
The model architecture includes an ESM model and a language modeling head (lm_head).
The forward method takes input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states,
encoder_attention_mask, labels, output_attentions, output_hidden_states, and return_dict as input arguments and
returns the masked language modeling loss and other outputs.
The predict_contacts method takes tokens and attention_mask as input and returns the predicted contacts using the
ESM model.</p>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>If using <code>EsmForMaskedLM</code>, ensure <code>config.is_decoder=False</code> for bi-directional self-attention.</li>
<li>Labels for computing the masked language modeling loss should be indices in <code>[-100, 0, ..., config.vocab_size]</code>.
Tokens with indices set to <code>-100</code> are ignored (masked), and the loss is only computed for the tokens with labels
in <code>[0, ..., config.vocab_size]</code>.</li>
</ul>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmForMaskedLM</span><span class="p">(</span><span class="n">EsmPreTrainedModel</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents an ESM (Evolutionary Scale Modeling) model for masked language modeling (MLM), inheriting from EsmPreTrainedModel.</span>
<span class="sd">    This class provides the functionality to perform masked language modeling using the ESM model.</span>

<span class="sd">    The EsmForMaskedLM class contains methods for initializing the model, getting and setting output embeddings,</span>
<span class="sd">    forwarding the model, and predicting contacts.</span>
<span class="sd">    The model architecture includes an ESM model and a language modeling head (lm_head).</span>
<span class="sd">    The forward method takes input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states,</span>
<span class="sd">    encoder_attention_mask, labels, output_attentions, output_hidden_states, and return_dict as input arguments and</span>
<span class="sd">    returns the masked language modeling loss and other outputs.</span>
<span class="sd">    The predict_contacts method takes tokens and attention_mask as input and returns the predicted contacts using the</span>
<span class="sd">    ESM model.</span>

<span class="sd">    Note:</span>
<span class="sd">        - If using `EsmForMaskedLM`, ensure `config.is_decoder=False` for bi-directional self-attention.</span>
<span class="sd">        - Labels for computing the masked language modeling loss should be indices in `[-100, 0, ..., config.vocab_size]`.</span>
<span class="sd">        Tokens with indices set to `-100` are ignored (masked), and the loss is only computed for the tokens with labels</span>
<span class="sd">        in `[0, ..., config.vocab_size]`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;lm_head.decoder.weight&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of EsmForMaskedLM.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config (object): The configuration object containing model hyperparameters.</span>
<span class="sd">                It must have attributes like &#39;is_decoder&#39;, &#39;add_pooling_layer&#39;, and &#39;init_weights&#39;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;If you want to use `EsmForMaskedLM` make sure `config.is_decoder=False` for &quot;</span>
                <span class="s2">&quot;bi-directional self-attention.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">esm</span> <span class="o">=</span> <span class="n">EsmModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">EsmLMHead</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method returns the output embeddings for the language model head.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the EsmForMaskedLM class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            decoder: The method returns the output embeddings for the language model head.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">decoder</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the output embeddings for the ESM model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmForMaskedLM): The instance of the EsmForMaskedLM class.</span>
<span class="sd">            new_embeddings (torch.nn.Module): The new embeddings to be set as output embeddings for the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If the provided new_embeddings is not of type torch.nn.Module.</span>
<span class="sd">            AttributeError: If the lm_head.decoder attribute is not present in the EsmForMaskedLM instance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">MaskedLMOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,</span>
<span class="sd">                config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the</span>
<span class="sd">                loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`</span>
<span class="sd">            kwargs (`Dict[str, any]`, optional, defaults to *{}*):</span>
<span class="sd">                Used to hide legacy arguments that have been deprecated.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">prediction_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">prediction_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction_scores</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">MaskedLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">prediction_scores</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_contacts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method predicts contacts using the ESM (Evolutionary Scale Modeling) for Masked Language Modeling.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmForMaskedLM): The instance of the EsmForMaskedLM class.</span>
<span class="sd">            tokens (Tensor): The input tokens for prediction.</span>
<span class="sd">            attention_mask (Tensor): The attention mask for the input tokens.</span>
<span class="sd">                It masks the tokens that should not be attended to, specifying which tokens should be attended to</span>
<span class="sd">                and which should not.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">predict_contacts</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmForMaskedLM</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Initializes an instance of EsmForMaskedLM.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing model hyperparameters.
It must have attributes like 'is_decoder', 'add_pooling_layer', and 'init_weights'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of EsmForMaskedLM.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config (object): The configuration object containing model hyperparameters.</span>
<span class="sd">            It must have attributes like &#39;is_decoder&#39;, &#39;add_pooling_layer&#39;, and &#39;init_weights&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;If you want to use `EsmForMaskedLM` make sure `config.is_decoder=False` for &quot;</span>
            <span class="s2">&quot;bi-directional self-attention.&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">esm</span> <span class="o">=</span> <span class="n">EsmModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">EsmLMHead</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmForMaskedLM</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.forward" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ...,
config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Used to hide legacy arguments that have been deprecated.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, any]`, optional, defaults to *{}*</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">MaskedLMOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,</span>
<span class="sd">            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the</span>
<span class="sd">            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`</span>
<span class="sd">        kwargs (`Dict[str, any]`, optional, defaults to *{}*):</span>
<span class="sd">            Used to hide legacy arguments that have been deprecated.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">prediction_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

    <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">prediction_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction_scores</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

    <span class="k">return</span> <span class="n">MaskedLMOutput</span><span class="p">(</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">prediction_scores</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.get_output_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmForMaskedLM</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.get_output_embeddings" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This method returns the output embeddings for the language model head.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the EsmForMaskedLM class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>decoder</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The method returns the output embeddings for the language model head.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method returns the output embeddings for the language model head.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: An instance of the EsmForMaskedLM class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        decoder: The method returns the output embeddings for the language model head.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">decoder</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.predict_contacts" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmForMaskedLM</span><span class="o">.</span><span class="n">predict_contacts</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.predict_contacts" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This method predicts contacts using the ESM (Evolutionary Scale Modeling) for Masked Language Modeling.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmForMaskedLM class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM" href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM">EsmForMaskedLM</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tokens for prediction.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The attention mask for the input tokens.
It masks the tokens that should not be attended to, specifying which tokens should be attended to
and which should not.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict_contacts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method predicts contacts using the ESM (Evolutionary Scale Modeling) for Masked Language Modeling.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmForMaskedLM): The instance of the EsmForMaskedLM class.</span>
<span class="sd">        tokens (Tensor): The input tokens for prediction.</span>
<span class="sd">        attention_mask (Tensor): The attention mask for the input tokens.</span>
<span class="sd">            It masks the tokens that should not be attended to, specifying which tokens should be attended to</span>
<span class="sd">            and which should not.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">predict_contacts</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.set_output_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmForMaskedLM</span><span class="o">.</span><span class="n">set_output_embeddings</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM.set_output_embeddings" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Set the output embeddings for the ESM model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmForMaskedLM class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM" href="#mindnlp.transformers.models.esm.modeling_esm.EsmForMaskedLM">EsmForMaskedLM</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>new_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The new embeddings to be set as output embeddings for the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="torch.nn.Module">Module</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the provided new_embeddings is not of type torch.nn.Module.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AttributeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the lm_head.decoder attribute is not present in the EsmForMaskedLM instance.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set the output embeddings for the ESM model.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmForMaskedLM): The instance of the EsmForMaskedLM class.</span>
<span class="sd">        new_embeddings (torch.nn.Module): The new embeddings to be set as output embeddings for the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the provided new_embeddings is not of type torch.nn.Module.</span>
<span class="sd">        AttributeError: If the lm_head.decoder attribute is not present in the EsmForMaskedLM instance.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">new_embeddings</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification</code>


<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel" href="#mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel">EsmPreTrainedModel</a></code></p>


        <p>This class represents an ESM (Evoformer) model for sequence classification tasks.
It is a subclass of EsmPreTrainedModel, which provides the underlying architecture and functionality.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.num_labels">num_labels</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The number of labels for the classification task.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.config">config</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The configuration object for the ESM model.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.configuration_esm.EsmConfig" href="#mindnlp.transformers.models.esm.configuration_esm.EsmConfig">EsmConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.esm">esm</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The ESM model instance.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmModel" href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel">EsmModel</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.classifier">classifier</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The classification head for the ESM model.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.models.esm.modeling_esm.EsmClassificationHead">EsmClassificationHead</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.__init__" href="#mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the EsmForSequenceClassification instance.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.forward" href="#mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the ESM model for sequence classification.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmForSequenceClassification</span><span class="p">(</span><span class="n">EsmPreTrainedModel</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents an ESM (Evoformer) model for sequence classification tasks.</span>
<span class="sd">    It is a subclass of EsmPreTrainedModel, which provides the underlying architecture and functionality.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        num_labels (int): The number of labels for the classification task.</span>
<span class="sd">        config (EsmConfig): The configuration object for the ESM model.</span>
<span class="sd">        esm (EsmModel): The ESM model instance.</span>
<span class="sd">        classifier (EsmClassificationHead): The classification head for the ESM model.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the EsmForSequenceClassification instance.</span>
<span class="sd">        forward: Constructs the ESM model for sequence classification.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of EsmForSequenceClassification.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config:</span>
<span class="sd">                An object containing the configuration parameters for the model.</span>

<span class="sd">                - Type: object</span>
<span class="sd">                - Purpose: To configure the model and its components.</span>
<span class="sd">                - Restrictions: Must be a valid configuration object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">esm</span> <span class="o">=</span> <span class="n">EsmModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">EsmClassificationHead</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">SequenceClassifierOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            labels (`mindspore.Tensor` of shape `(batch_size,)`, *optional*):</span>
<span class="sd">                Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,</span>
<span class="sd">                config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If</span>
<span class="sd">                `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;regression&quot;</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">labels</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;single_label_classification&quot;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;multi_label_classification&quot;</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;single_label_classification&quot;</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;multi_label_classification&quot;</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">SequenceClassifierOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmForSequenceClassification</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Initializes an instance of EsmForSequenceClassification.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An object containing the configuration parameters for the model.</p>
<ul>
<li>Type: object</li>
<li>Purpose: To configure the model and its components.</li>
<li>Restrictions: Must be a valid configuration object.</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of EsmForSequenceClassification.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config:</span>
<span class="sd">            An object containing the configuration parameters for the model.</span>

<span class="sd">            - Type: object</span>
<span class="sd">            - Purpose: To configure the model and its components.</span>
<span class="sd">            - Restrictions: Must be a valid configuration object.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">esm</span> <span class="o">=</span> <span class="n">EsmModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">EsmClassificationHead</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmForSequenceClassification</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForSequenceClassification.forward" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,
config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size,)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">SequenceClassifierOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        labels (`mindspore.Tensor` of shape `(batch_size,)`, *optional*):</span>
<span class="sd">            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,</span>
<span class="sd">            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If</span>
<span class="sd">            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;regression&quot;</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">labels</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;single_label_classification&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;multi_label_classification&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;single_label_classification&quot;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;multi_label_classification&quot;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

    <span class="k">return</span> <span class="n">SequenceClassifierOutput</span><span class="p">(</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification</code>


<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel" href="#mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel">EsmPreTrainedModel</a></code></p>


        <p>EsmForTokenClassification is a class that represents a token classification model based on the ESM
(Evoformer Sequence Model) architecture. This class extends EsmPreTrainedModel to leverage pre-trained
weights and configurations for efficient token classification tasks. It includes methods for initializing the model,
forwarding the forward pass, and computing the token classification loss.</p>
<p>The <strong>init</strong> method initializes the EsmForTokenClassification model with configurable parameters such as the number
of labels, dropout probability, and hidden layer sizes. It also sets up the ESM model, dropout layer, and the
classifier for token classification.</p>
<p>The forward method defines the forward pass of the model, taking input tensors such as input_ids, attention_mask,
position_ids, etc., and returning the token classification output.
It computes the logits for token classification based on the sequence_output from the ESM model and calculates the
cross-entropy loss if labels are provided. The method allows for returning additional outputs like hidden states and
attentions based on the return_dict parameter.</p>


<details class="note" open>
  <summary>Note</summary>
  <p>This docstring is a high-level summary and does not include method signatures or implementation details.</p>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmForTokenClassification</span><span class="p">(</span><span class="n">EsmPreTrainedModel</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    EsmForTokenClassification is a class that represents a token classification model based on the ESM</span>
<span class="sd">    (Evoformer Sequence Model) architecture. This class extends EsmPreTrainedModel to leverage pre-trained</span>
<span class="sd">    weights and configurations for efficient token classification tasks. It includes methods for initializing the model,</span>
<span class="sd">    forwarding the forward pass, and computing the token classification loss.</span>

<span class="sd">    The __init__ method initializes the EsmForTokenClassification model with configurable parameters such as the number</span>
<span class="sd">    of labels, dropout probability, and hidden layer sizes. It also sets up the ESM model, dropout layer, and the</span>
<span class="sd">    classifier for token classification.</span>

<span class="sd">    The forward method defines the forward pass of the model, taking input tensors such as input_ids, attention_mask,</span>
<span class="sd">    position_ids, etc., and returning the token classification output.</span>
<span class="sd">    It computes the logits for token classification based on the sequence_output from the ESM model and calculates the</span>
<span class="sd">    cross-entropy loss if labels are provided. The method allows for returning additional outputs like hidden states and</span>
<span class="sd">    attentions based on the return_dict parameter.</span>

<span class="sd">    Note:</span>
<span class="sd">        This docstring is a high-level summary and does not include method signatures or implementation details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the EsmForTokenClassification class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the EsmForTokenClassification class.</span>
<span class="sd">            config:</span>
<span class="sd">                An instance of the configuration class containing the model configuration parameters.</span>

<span class="sd">                - Type: object</span>
<span class="sd">                - Purpose: Specifies the configuration settings for the model.</span>
<span class="sd">                - Restrictions: Must be a valid instance of the configuration class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If the config parameter is not of the correct type.</span>
<span class="sd">            ValueError: If the config.num_labels is not provided or is invalid.</span>
<span class="sd">            RuntimeError: If an error occurs during the initialization process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">esm</span> <span class="o">=</span> <span class="n">EsmModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">TokenClassifierOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">TokenClassifierOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmForTokenClassification</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmForTokenClassification class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmForTokenClassification class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the configuration class containing the model configuration parameters.</p>
<ul>
<li>Type: object</li>
<li>Purpose: Specifies the configuration settings for the model.</li>
<li>Restrictions: Must be a valid instance of the configuration class.</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the config parameter is not of the correct type.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the config.num_labels is not provided or is invalid.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If an error occurs during the initialization process.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the EsmForTokenClassification class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the EsmForTokenClassification class.</span>
<span class="sd">        config:</span>
<span class="sd">            An instance of the configuration class containing the model configuration parameters.</span>

<span class="sd">            - Type: object</span>
<span class="sd">            - Purpose: Specifies the configuration settings for the model.</span>
<span class="sd">            - Restrictions: Must be a valid instance of the configuration class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the config parameter is not of the correct type.</span>
<span class="sd">        ValueError: If the config.num_labels is not provided or is invalid.</span>
<span class="sd">        RuntimeError: If an error occurs during the initialization process.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">esm</span> <span class="o">=</span> <span class="n">EsmModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmForTokenClassification</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmForTokenClassification.forward" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">TokenClassifierOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">sequence_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

    <span class="k">return</span> <span class="n">TokenClassifierOutput</span><span class="p">(</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindnlp.transformers.models.esm.modeling_esm.EsmModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esm.EsmModel</code>


<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel" href="#mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel">EsmPreTrainedModel</a></code></p>


        <p>The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of
cross-attention is added between the self-attention layers, following the architecture described in <a href="https://arxiv.org/abs/1706.03762">Attention is
all you need</a> by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.</p>
<p>To behave as an decoder the model needs to be initialized with the <code>is_decoder</code> argument of the configuration set
to <code>True</code>. To be used in a Seq2Seq model, the model needs to initialized with both <code>is_decoder</code> argument and
<code>add_cross_attention</code> set to <code>True</code>; an <code>encoder_hidden_states</code> is then expected as an input to the forward pass.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmModel</span><span class="p">(</span><span class="n">EsmPreTrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of</span>
<span class="sd">    cross-attention is added between the self-attention layers, following the architecture described in [Attention is</span>
<span class="sd">    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,</span>
<span class="sd">    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.</span>

<span class="sd">    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set</span>
<span class="sd">    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and</span>
<span class="sd">    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the EsmModel class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config (object): The configuration object containing various settings for the model.</span>
<span class="sd">            add_pooling_layer (bool, optional): A flag indicating whether to include a pooling layer in the model.</span>
<span class="sd">                Default is True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">EsmEmbeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">EsmEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pooler</span> <span class="o">=</span> <span class="n">EsmPooler</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">if</span> <span class="n">add_pooling_layer</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">contact_head</span> <span class="o">=</span> <span class="n">EsmContactPredictionHead</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method returns the input embeddings for the ESMM model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the EsmModel class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            word_embeddings: This method returns the word embeddings for input data, represented as a tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the input embeddings for the EsmModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmModel): The instance of the EsmModel class.</span>
<span class="sd">            value: The input embeddings to be set. This should be of type `torch.Tensor`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads_to_prune</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base</span>
<span class="sd">        class PreTrainedModel</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">heads</span> <span class="ow">in</span> <span class="n">heads_to_prune</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">prune_heads</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">BaseModelOutputWithPoolingAndCrossAttentions</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            encoder_hidden_states  (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if</span>
<span class="sd">                the model is configured as a decoder.</span>
<span class="sd">            encoder_attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in</span>
<span class="sd">                the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:</span>

<span class="sd">                - 1 for tokens that are **not masked**,</span>
<span class="sd">                - 0 for tokens that are **masked**.</span>
<span class="sd">            past_key_values (`tuple(tuple(mindspore.Tensor))` of length `config.n_layers` with each tuple having 4 tensors</span>
<span class="sd">                of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):</span>
<span class="sd">                Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</span>

<span class="sd">                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that</span>
<span class="sd">                don&#39;t have their past key value states given to this model) of shape `(batch_size, 1)` instead of all</span>
<span class="sd">                `decoder_input_ids` of shape `(batch_size, sequence_length)`.</span>
<span class="sd">            use_cache (`bool`, *optional*):</span>
<span class="sd">                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see</span>
<span class="sd">                `past_key_values`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">warn_if_padding_and_no_attention_mask</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either input_ids or inputs_embeds&quot;</span><span class="p">)</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_shape</span>

        <span class="c1"># past_key_values_length</span>
        <span class="n">past_key_values_length</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="n">past_key_values_length</span><span class="p">)))</span>

        <span class="c1"># We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]</span>
        <span class="c1"># ourselves in which case we just need to make it broadcastable to all heads.</span>
        <span class="n">extended_attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_extended_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span>

        <span class="c1"># If a 2D or 3D attention mask is provided for the cross-attention</span>
        <span class="c1"># we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span> <span class="ow">and</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_batch_size</span><span class="p">,</span> <span class="n">encoder_sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">encoder_hidden_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">encoder_batch_size</span><span class="p">,</span> <span class="n">encoder_sequence_length</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">encoder_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">encoder_hidden_shape</span><span class="p">)</span>
            <span class="n">encoder_extended_attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">invert_attention_mask</span><span class="p">(</span><span class="n">encoder_attention_mask</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoder_extended_attention_mask</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Prepare head mask if needed</span>
        <span class="c1"># 1.0 in head_mask indicate we keep the head</span>
        <span class="c1"># attention_probs has shape bsz x n_heads x N x N</span>
        <span class="c1"># input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]</span>
        <span class="c1"># and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]</span>
        <span class="n">head_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_head_mask</span><span class="p">(</span><span class="n">head_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>

        <span class="n">embedding_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">past_key_values_length</span><span class="o">=</span><span class="n">past_key_values_length</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
            <span class="n">embedding_output</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">extended_attention_mask</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_extended_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">pooled_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooler</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">sequence_output</span><span class="p">,</span> <span class="n">pooled_output</span><span class="p">)</span> <span class="o">+</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="n">BaseModelOutputWithPoolingAndCrossAttentions</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">sequence_output</span><span class="p">,</span>
            <span class="n">pooler_output</span><span class="o">=</span><span class="n">pooled_output</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_contacts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predicts contacts using the EsmModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmModel): An instance of the EsmModel class.</span>
<span class="sd">            tokens (Tensor): The input tokens for prediction.</span>
<span class="sd">            attention_mask (Tensor): The attention mask for the input tokens.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">attns</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">attentions</span>
        <span class="n">attns</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">attns</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Matches the original model layout</span>
        <span class="c1"># In the original model, attentions for padding tokens are completely zeroed out.</span>
        <span class="c1"># This makes no difference most of the time because the other tokens won&#39;t attend to them,</span>
        <span class="c1"># but it does for the contact prediction task, which takes attentions as input,</span>
        <span class="c1"># so we have to mimic that here.</span>
        <span class="n">attns</span> <span class="o">*=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">attns</span> <span class="o">*=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">contact_head</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">attns</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmModel.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmModel</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmModel class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing various settings for the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_pooling_layer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A flag indicating whether to include a pooling layer in the model.
Default is True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the EsmModel class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config (object): The configuration object containing various settings for the model.</span>
<span class="sd">        add_pooling_layer (bool, optional): A flag indicating whether to include a pooling layer in the model.</span>
<span class="sd">            Default is True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">EsmEmbeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">EsmEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">pooler</span> <span class="o">=</span> <span class="n">EsmPooler</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">if</span> <span class="n">add_pooling_layer</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">contact_head</span> <span class="o">=</span> <span class="n">EsmContactPredictionHead</span><span class="p">(</span>
        <span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># Initialize weights and apply final processing</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmModel.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmModel</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.forward" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code> (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">BaseModelOutputWithPoolingAndCrossAttentions</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        encoder_hidden_states  (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if</span>
<span class="sd">            the model is configured as a decoder.</span>
<span class="sd">        encoder_attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in</span>
<span class="sd">            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:</span>

<span class="sd">            - 1 for tokens that are **not masked**,</span>
<span class="sd">            - 0 for tokens that are **masked**.</span>
<span class="sd">        past_key_values (`tuple(tuple(mindspore.Tensor))` of length `config.n_layers` with each tuple having 4 tensors</span>
<span class="sd">            of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):</span>
<span class="sd">            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</span>

<span class="sd">            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that</span>
<span class="sd">            don&#39;t have their past key value states given to this model) of shape `(batch_size, 1)` instead of all</span>
<span class="sd">            `decoder_input_ids` of shape `(batch_size, sequence_length)`.</span>
<span class="sd">        use_cache (`bool`, *optional*):</span>
<span class="sd">            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see</span>
<span class="sd">            `past_key_values`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warn_if_padding_and_no_attention_mask</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either input_ids or inputs_embeds&quot;</span><span class="p">)</span>

    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_shape</span>

    <span class="c1"># past_key_values_length</span>
    <span class="n">past_key_values_length</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="n">past_key_values_length</span><span class="p">)))</span>

    <span class="c1"># We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]</span>
    <span class="c1"># ourselves in which case we just need to make it broadcastable to all heads.</span>
    <span class="n">extended_attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_extended_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span>

    <span class="c1"># If a 2D or 3D attention mask is provided for the cross-attention</span>
    <span class="c1"># we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span> <span class="ow">and</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">encoder_batch_size</span><span class="p">,</span> <span class="n">encoder_sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">encoder_hidden_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">encoder_batch_size</span><span class="p">,</span> <span class="n">encoder_sequence_length</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">encoder_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">encoder_hidden_shape</span><span class="p">)</span>
        <span class="n">encoder_extended_attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">invert_attention_mask</span><span class="p">(</span><span class="n">encoder_attention_mask</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">encoder_extended_attention_mask</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Prepare head mask if needed</span>
    <span class="c1"># 1.0 in head_mask indicate we keep the head</span>
    <span class="c1"># attention_probs has shape bsz x n_heads x N x N</span>
    <span class="c1"># input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]</span>
    <span class="c1"># and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]</span>
    <span class="n">head_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_head_mask</span><span class="p">(</span><span class="n">head_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>

    <span class="n">embedding_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
        <span class="n">past_key_values_length</span><span class="o">=</span><span class="n">past_key_values_length</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
        <span class="n">embedding_output</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">extended_attention_mask</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_extended_attention_mask</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">pooled_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooler</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">sequence_output</span><span class="p">,</span> <span class="n">pooled_output</span><span class="p">)</span> <span class="o">+</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="k">return</span> <span class="n">BaseModelOutputWithPoolingAndCrossAttentions</span><span class="p">(</span>
        <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">sequence_output</span><span class="p">,</span>
        <span class="n">pooler_output</span><span class="o">=</span><span class="n">pooled_output</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="n">cross_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmModel.get_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmModel</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.get_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This method returns the input embeddings for the ESMM model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the EsmModel class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>word_embeddings</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method returns the word embeddings for input data, represented as a tensor.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method returns the input embeddings for the ESMM model.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: An instance of the EsmModel class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        word_embeddings: This method returns the word embeddings for input data, represented as a tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmModel.predict_contacts" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmModel</span><span class="o">.</span><span class="n">predict_contacts</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.predict_contacts" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Predicts contacts using the EsmModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the EsmModel class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmModel" href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel">EsmModel</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tokens for prediction.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The attention mask for the input tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict_contacts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predicts contacts using the EsmModel.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmModel): An instance of the EsmModel class.</span>
<span class="sd">        tokens (Tensor): The input tokens for prediction.</span>
<span class="sd">        attention_mask (Tensor): The attention mask for the input tokens.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">attns</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">attentions</span>
    <span class="n">attns</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">attns</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Matches the original model layout</span>
    <span class="c1"># In the original model, attentions for padding tokens are completely zeroed out.</span>
    <span class="c1"># This makes no difference most of the time because the other tokens won&#39;t attend to them,</span>
    <span class="c1"># but it does for the contact prediction task, which takes attentions as input,</span>
    <span class="c1"># so we have to mimic that here.</span>
    <span class="n">attns</span> <span class="o">*=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">attns</span> <span class="o">*=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">contact_head</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">attns</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esm.EsmModel.set_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esm</span><span class="o">.</span><span class="n">EsmModel</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">value</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel.set_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Sets the input embeddings for the EsmModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmModel class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmModel" href="#mindnlp.transformers.models.esm.modeling_esm.EsmModel">EsmModel</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input embeddings to be set. This should be of type <code>torch.Tensor</code>.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the input embeddings for the EsmModel.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmModel): The instance of the EsmModel class.</span>
<span class="sd">        value: The input embeddings to be set. This should be of type `torch.Tensor`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">value</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel</code>


<a href="#mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.modeling_utils.PreTrainedModel" href="../../modeling_utils/#mindnlp.transformers.modeling_utils.PreTrainedModel">PreTrainedModel</a></code></p>


        <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
models.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esm.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmPreTrainedModel</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained</span>
<span class="sd">    models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">EsmConfig</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;esm&quot;</span>
    <span class="n">supports_gradient_checkpointing</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;EsmLayer&quot;</span><span class="p">,</span> <span class="s2">&quot;EsmFoldTriangularSelfAttentionBlock&quot;</span><span class="p">,</span> <span class="s2">&quot;EsmEmbeddings&quot;</span><span class="p">]</span>

    <span class="c1"># Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights</span>
    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the weights&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="c1"># Slightly different from the TF version which uses truncated_normal for initialization</span>
            <span class="c1"># cf https://github.com/pytorch/pytorch/pull/5617</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">),</span>
                                                    <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">cell</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">:</span>
                <span class="n">weight</span><span class="p">[</span><span class="n">cell</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">):</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;ones&#39;</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.esm.modeling_esmfold" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>MindSpore ESMFold model</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>EsmCategoricalMixture represents a categorical mixture distribution for probability calculations based on given logits.</p>
<p>This class provides methods for initializing the distribution, calculating the log probability of a given value,
and computing the mean of the distribution.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.param">param</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The logits parameter for the categorical mixture distribution.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.bins">bins</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The number of bins for the distribution (default is 50).</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.start">start</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The starting value for the bins (default is 0).</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.end">end</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The ending value for the bins (default is 1).</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.__init__" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the categorical mixture distribution with the given parameters.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.log_prob" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.log_prob">log_prob</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Calculates the log probability of a given value within the distribution.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.mean" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.mean">mean</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Computes the mean of the categorical mixture distribution.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <p>This class inherits from an unspecified parent class.</p>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmCategoricalMixture</span><span class="p">:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    EsmCategoricalMixture represents a categorical mixture distribution for probability calculations based on given logits.</span>

<span class="sd">    This class provides methods for initializing the distribution, calculating the log probability of a given value,</span>
<span class="sd">    and computing the mean of the distribution.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        param: The logits parameter for the categorical mixture distribution.</span>
<span class="sd">        bins: The number of bins for the distribution (default is 50).</span>
<span class="sd">        start: The starting value for the bins (default is 0).</span>
<span class="sd">        end: The ending value for the bins (default is 1).</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the categorical mixture distribution with the given parameters.</span>
<span class="sd">        log_prob: Calculates the log probability of a given value within the distribution.</span>
<span class="sd">        mean: Computes the mean of the categorical mixture distribution.</span>

<span class="sd">    Note:</span>
<span class="sd">        This class inherits from an unspecified parent class.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the EsmCategoricalMixture class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: Instance of the EsmCategoricalMixture class.</span>
<span class="sd">            param: The logits parameter to be assigned to the instance.</span>
<span class="sd">            bins: Number of bins for creating the v_bins attribute. Default is 50.</span>
<span class="sd">            start: The starting value for the linspace function. Default is 0.</span>
<span class="sd">            end: The ending value for the linspace function. Default is 1.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the start value is greater than or equal to the end value.</span>
<span class="sd">            TypeError: If the param or bins parameter types are incompatible.</span>
<span class="sd">            ValueError: If the bins parameter is less than 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># All tensors are of shape ..., bins.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logits</span> <span class="o">=</span> <span class="n">param</span>
        <span class="n">bins</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">bins</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_bins</span> <span class="o">=</span> <span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">bins</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">2</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">true</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method calculates the log probability of a given true value in the context of a categorical mixture model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: EsmCategoricalMixture</span>
<span class="sd">                The instance of the EsmCategoricalMixture class.</span>
<span class="sd">            true: torch.Tensor</span>
<span class="sd">                The true value for which the log probability needs to be calculated.</span>
<span class="sd">                It should be a tensor of shape (batch_size,) where batch_size is the number of samples.</span>
<span class="sd">                The true values should be within the range of valid classes for the categorical mixture model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None:</span>
<span class="sd">                This method does not return any value. The log probability is calculated and stored internally within</span>
<span class="sd">                the EsmCategoricalMixture instance.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the true tensor does not have the expected shape or if it contains values outside the</span>
<span class="sd">                range of valid classes for the categorical mixture model.</span>
<span class="sd">            IndexError: If the true tensor index is out of bounds.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Shapes are:</span>
        <span class="c1">#     self.probs: ... x bins</span>
        <span class="c1">#     true      : ...</span>
        <span class="n">true_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">true</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_bins</span><span class="p">[[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">true</span><span class="o">.</span><span class="n">ndim</span><span class="p">])</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">nll</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather_elements</span><span class="p">(</span><span class="n">nll</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">true_index</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method &#39;mean&#39; calculates the mean value of the categorical mixture distribution in the EsmCategoricalMixture class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the EsmCategoricalMixture class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: If the method is called without implementing it in a subclass.</span>
<span class="sd">            ValueError: If the input data is not in the expected format.</span>
<span class="sd">            RuntimeError: If the operation fails due to unforeseen circumstances.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_bins</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmCategoricalMixture</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmCategoricalMixture class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Instance of the EsmCategoricalMixture class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>param</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The logits parameter to be assigned to the instance.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bins</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of bins for creating the v_bins attribute. Default is 50.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>50</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>start</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The starting value for the linspace function. Default is 0.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>end</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ending value for the linspace function. Default is 1.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the start value is greater than or equal to the end value.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the param or bins parameter types are incompatible.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the bins parameter is less than 1.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the EsmCategoricalMixture class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: Instance of the EsmCategoricalMixture class.</span>
<span class="sd">        param: The logits parameter to be assigned to the instance.</span>
<span class="sd">        bins: Number of bins for creating the v_bins attribute. Default is 50.</span>
<span class="sd">        start: The starting value for the linspace function. Default is 0.</span>
<span class="sd">        end: The ending value for the linspace function. Default is 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the start value is greater than or equal to the end value.</span>
<span class="sd">        TypeError: If the param or bins parameter types are incompatible.</span>
<span class="sd">        ValueError: If the bins parameter is less than 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># All tensors are of shape ..., bins.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logits</span> <span class="o">=</span> <span class="n">param</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">bins</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">v_bins</span> <span class="o">=</span> <span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">bins</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">2</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.log_prob" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmCategoricalMixture</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">true</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.log_prob" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method calculates the log probability of a given true value in the context of a categorical mixture model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>EsmCategoricalMixture
The instance of the EsmCategoricalMixture class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>true</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>torch.Tensor
The true value for which the log probability needs to be calculated.
It should be a tensor of shape (batch_size,) where batch_size is the number of samples.
The true values should be within the range of valid classes for the categorical mixture model.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method does not return any value. The log probability is calculated and stored internally within
the EsmCategoricalMixture instance.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the true tensor does not have the expected shape or if it contains values outside the
range of valid classes for the categorical mixture model.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>IndexError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the true tensor index is out of bounds.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">true</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method calculates the log probability of a given true value in the context of a categorical mixture model.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: EsmCategoricalMixture</span>
<span class="sd">            The instance of the EsmCategoricalMixture class.</span>
<span class="sd">        true: torch.Tensor</span>
<span class="sd">            The true value for which the log probability needs to be calculated.</span>
<span class="sd">            It should be a tensor of shape (batch_size,) where batch_size is the number of samples.</span>
<span class="sd">            The true values should be within the range of valid classes for the categorical mixture model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None:</span>
<span class="sd">            This method does not return any value. The log probability is calculated and stored internally within</span>
<span class="sd">            the EsmCategoricalMixture instance.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the true tensor does not have the expected shape or if it contains values outside the</span>
<span class="sd">            range of valid classes for the categorical mixture model.</span>
<span class="sd">        IndexError: If the true tensor index is out of bounds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Shapes are:</span>
    <span class="c1">#     self.probs: ... x bins</span>
    <span class="c1">#     true      : ...</span>
    <span class="n">true_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">true</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_bins</span><span class="p">[[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">true</span><span class="o">.</span><span class="n">ndim</span><span class="p">])</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">nll</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather_elements</span><span class="p">(</span><span class="n">nll</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">true_index</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.mean" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmCategoricalMixture</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmCategoricalMixture.mean" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Method 'mean' calculates the mean value of the categorical mixture distribution in the EsmCategoricalMixture class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmCategoricalMixture class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>NotImplementedError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the method is called without implementing it in a subclass.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input data is not in the expected format.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the operation fails due to unforeseen circumstances.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method &#39;mean&#39; calculates the mean value of the categorical mixture distribution in the EsmCategoricalMixture class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the EsmCategoricalMixture class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        NotImplementedError: If the method is called without implementing it in a subclass.</span>
<span class="sd">        ValueError: If the input data is not in the expected format.</span>
<span class="sd">        RuntimeError: If the operation fails due to unforeseen circumstances.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_bins</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Implements Algorithm 20, lines 11-14</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldAngleResnet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements Algorithm 20, lines 11-14</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes the EsmFoldAngleResnet class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldAngleResnet): The instance of the EsmFoldAngleResnet class.</span>
<span class="sd">            config:</span>
<span class="sd">                The configuration object containing parameters for the EsmFoldAngleResnet initialization.</span>

<span class="sd">                - Type: object</span>
<span class="sd">                - Purpose: Specifies the configuration settings for the EsmFoldAngleResnet class.</span>
<span class="sd">                - Restrictions: Must be a valid configuration object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_in</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_initial</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_resnet_blocks</span><span class="p">):</span>
            <span class="n">layer</span> <span class="o">=</span> <span class="n">EsmFoldAngleResnetBlock</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_angles</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">s_initial</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            s:</span>
<span class="sd">                [*, C_hidden] single embedding</span>
<span class="sd">            s_initial:</span>
<span class="sd">                [*, C_hidden] single embedding as of the start of the StructureModule</span>
<span class="sd">        Returns:</span>
<span class="sd">            [*, no_angles, 2] predicted angles</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># NOTE: The ReLU&#39;s applied to the inputs are absent from the supplement</span>
        <span class="c1"># pseudocode but present in the source. For maximal compatibility with</span>
        <span class="c1"># the pretrained weights, I&#39;m going with the source.</span>

        <span class="c1"># [*, C_hidden]</span>
        <span class="n">s_initial</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">s_initial</span><span class="p">)</span>
        <span class="n">s_initial</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_initial</span><span class="p">(</span><span class="n">s_initial</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_in</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">+</span> <span class="n">s_initial</span>

        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="c1"># [*, no_angles * 2]</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="c1"># [*, no_angles, 2]</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="n">unnormalized_s</span> <span class="o">=</span> <span class="n">s</span>
        <span class="n">norm_denom</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                <span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">/</span> <span class="n">norm_denom</span>

        <span class="k">return</span> <span class="n">unnormalized_s</span><span class="p">,</span> <span class="n">s</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldAngleResnet</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes the EsmFoldAngleResnet class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmFoldAngleResnet class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet">EsmFoldAngleResnet</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing parameters for the EsmFoldAngleResnet initialization.</p>
<ul>
<li>Type: object</li>
<li>Purpose: Specifies the configuration settings for the EsmFoldAngleResnet class.</li>
<li>Restrictions: Must be a valid configuration object.</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Initializes the EsmFoldAngleResnet class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmFoldAngleResnet): The instance of the EsmFoldAngleResnet class.</span>
<span class="sd">        config:</span>
<span class="sd">            The configuration object containing parameters for the EsmFoldAngleResnet initialization.</span>

<span class="sd">            - Type: object</span>
<span class="sd">            - Purpose: Specifies the configuration settings for the EsmFoldAngleResnet class.</span>
<span class="sd">            - Restrictions: Must be a valid configuration object.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">linear_in</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_initial</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_resnet_blocks</span><span class="p">):</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="n">EsmFoldAngleResnetBlock</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_angles</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldAngleResnet</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s_initial</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnet.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>s</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, C_hidden] single embedding</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s_initial</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, C_hidden] single embedding as of the start of the StructureModule</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">s_initial</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        s:</span>
<span class="sd">            [*, C_hidden] single embedding</span>
<span class="sd">        s_initial:</span>
<span class="sd">            [*, C_hidden] single embedding as of the start of the StructureModule</span>
<span class="sd">    Returns:</span>
<span class="sd">        [*, no_angles, 2] predicted angles</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># NOTE: The ReLU&#39;s applied to the inputs are absent from the supplement</span>
    <span class="c1"># pseudocode but present in the source. For maximal compatibility with</span>
    <span class="c1"># the pretrained weights, I&#39;m going with the source.</span>

    <span class="c1"># [*, C_hidden]</span>
    <span class="n">s_initial</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">s_initial</span><span class="p">)</span>
    <span class="n">s_initial</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_initial</span><span class="p">(</span><span class="n">s_initial</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_in</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">+</span> <span class="n">s_initial</span>

    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># [*, no_angles * 2]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># [*, no_angles, 2]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="n">unnormalized_s</span> <span class="o">=</span> <span class="n">s</span>
    <span class="n">norm_denom</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">/</span> <span class="n">norm_denom</span>

    <span class="k">return</span> <span class="n">unnormalized_s</span><span class="p">,</span> <span class="n">s</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents an EsmFoldAngleResnetBlock, which is a block used in the forwardion of an EsmFold model.
It inherits from the nn.Module class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.linear_1">linear_1</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A linear layer used in the block, initialized with a rectified linear unit (ReLU) activation function.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear">EsmFoldLinear</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.linear_2">linear_2</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Another linear layer used in the block, initialized with a final activation function.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear">EsmFoldLinear</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.relu">relu</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the ReLU activation function.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.ReLU">ReLU</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.__init__" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the EsmFoldAngleResnetBlock with the given configuration.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.forward" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the EsmFoldAngleResnetBlock using the input tensor.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldAngleResnetBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents an EsmFoldAngleResnetBlock, which is a block used in the forwardion of an EsmFold model.</span>
<span class="sd">    It inherits from the nn.Module class.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        linear_1 (EsmFoldLinear):</span>
<span class="sd">            A linear layer used in the block, initialized with a rectified linear unit (ReLU) activation function.</span>
<span class="sd">        linear_2 (EsmFoldLinear):</span>
<span class="sd">            Another linear layer used in the block, initialized with a final activation function.</span>
<span class="sd">        relu (nn.ReLU): An instance of the ReLU activation function.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the EsmFoldAngleResnetBlock with the given configuration.</span>
<span class="sd">        forward: Constructs the EsmFoldAngleResnetBlock using the input tensor.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an EsmFoldAngleResnetBlock object.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldAngleResnetBlock): The current instance of the EsmFoldAngleResnetBlock class.</span>
<span class="sd">            config (object):</span>
<span class="sd">                A configuration object containing the parameters for initializing the EsmFoldAngleResnetBlock.</span>

<span class="sd">                - resnet_dim (int): The dimension of the resnet block.</span>
<span class="sd">                - init (str): The initialization method for the linear layers. Possible values are &#39;relu&#39; and &#39;final&#39;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If the provided config object is not of the expected type.</span>
<span class="sd">            ValueError: If the config object does not contain the required parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method forwards a computation graph for the EsmFoldAngleResnetBlock.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldAngleResnetBlock): The instance of the EsmFoldAngleResnetBlock class.</span>
<span class="sd">            a (mindspore.Tensor): The input tensor for the computation graph.</span>

<span class="sd">        Returns:</span>
<span class="sd">            mindspore.Tensor: The output tensor resulting from the computation graph.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">s_initial</span> <span class="o">=</span> <span class="n">a</span>

        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">s_initial</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldAngleResnetBlock</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an EsmFoldAngleResnetBlock object.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The current instance of the EsmFoldAngleResnetBlock class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock">EsmFoldAngleResnetBlock</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A configuration object containing the parameters for initializing the EsmFoldAngleResnetBlock.</p>
<ul>
<li>resnet_dim (int): The dimension of the resnet block.</li>
<li>init (str): The initialization method for the linear layers. Possible values are 'relu' and 'final'.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the provided config object is not of the expected type.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the config object does not contain the required parameters.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an EsmFoldAngleResnetBlock object.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmFoldAngleResnetBlock): The current instance of the EsmFoldAngleResnetBlock class.</span>
<span class="sd">        config (object):</span>
<span class="sd">            A configuration object containing the parameters for initializing the EsmFoldAngleResnetBlock.</span>

<span class="sd">            - resnet_dim (int): The dimension of the resnet block.</span>
<span class="sd">            - init (str): The initialization method for the linear layers. Possible values are &#39;relu&#39; and &#39;final&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the provided config object is not of the expected type.</span>
<span class="sd">        ValueError: If the config object does not contain the required parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">resnet_dim</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldAngleResnetBlock</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method forwards a computation graph for the EsmFoldAngleResnetBlock.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmFoldAngleResnetBlock class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAngleResnetBlock">EsmFoldAngleResnetBlock</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>a</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor for the computation graph.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindspore.Tensor">Tensor</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor: The output tensor resulting from the computation graph.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method forwards a computation graph for the EsmFoldAngleResnetBlock.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmFoldAngleResnetBlock): The instance of the EsmFoldAngleResnetBlock class.</span>
<span class="sd">        a (mindspore.Tensor): The input tensor for the computation graph.</span>

<span class="sd">    Returns:</span>
<span class="sd">        mindspore.Tensor: The output tensor resulting from the computation graph.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s_initial</span> <span class="o">=</span> <span class="n">a</span>

    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">s_initial</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Standard multi-head attention using AlphaFold's default layer initialization. Allows multiple bias vectors.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Standard multi-head attention using AlphaFold&#39;s default layer initialization. Allows multiple bias vectors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">c_q</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">c_k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">c_v</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">c_hidden</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">no_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">gating</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            c_q:</span>
<span class="sd">                Input dimension of query data</span>
<span class="sd">            c_k:</span>
<span class="sd">                Input dimension of key data</span>
<span class="sd">            c_v:</span>
<span class="sd">                Input dimension of value data</span>
<span class="sd">            c_hidden:</span>
<span class="sd">                Per-head hidden dimension</span>
<span class="sd">            no_heads:</span>
<span class="sd">                Number of attention heads</span>
<span class="sd">            gating:</span>
<span class="sd">                Whether the output should be gated using query data</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c_q</span> <span class="o">=</span> <span class="n">c_q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_k</span> <span class="o">=</span> <span class="n">c_k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_v</span> <span class="o">=</span> <span class="n">c_v</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">=</span> <span class="n">c_hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span> <span class="o">=</span> <span class="n">no_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gating</span> <span class="o">=</span> <span class="n">gating</span>

        <span class="c1"># DISCREPANCY: c_hidden is not the per-head channel dimension, as</span>
        <span class="c1"># stated in the supplement, but the overall channel dimension.</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;glorot&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;glorot&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;glorot&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_o</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_q</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;gating&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_prep_qkv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">kv_x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares the query, key, and value tensors for the EsmFoldAttention module.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldAttention): The instance of the EsmFoldAttention module.</span>
<span class="sd">            q_x (mindspore.Tensor): The query tensor.</span>
<span class="sd">                It should have a shape of (batch_size, seq_length, hidden_size).</span>
<span class="sd">            kv_x (mindspore.Tensor): The key-value tensor.</span>
<span class="sd">                It should have a shape of (batch_size, seq_length, hidden_size).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[mindspore.Tensor, mindspore.Tensor, mindspore.Tensor]:</span>
<span class="sd">                A tuple containing the query, key, and value tensors.</span>

<span class="sd">                - q: The transformed query tensor with a shape of (batch_size, seq_length, no_heads, hidden_size//no_heads).</span>
<span class="sd">                - k: The transformed key tensor with a shape of (batch_size, seq_length, no_heads, hidden_size//no_heads).</span>
<span class="sd">                - v: The transformed value tensor with a shape of (batch_size, seq_length, no_heads, hidden_size//no_heads).</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># [*, Q/K/V, H * C_hidden]</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">q_x</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">kv_x</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">kv_x</span><span class="p">)</span>

        <span class="c1"># [*, Q/K, H, C_hidden]</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># [*, H, Q/K, C_hidden]</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">/=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span>

    <span class="k">def</span> <span class="nf">_wrap_up</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">o</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">q_x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method &#39;_wrap_up&#39; in the class &#39;EsmFoldAttention&#39; performs a wrapping up operation on the input tensors.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the &#39;EsmFoldAttention&#39; class.</span>
<span class="sd">            o (mindspore.Tensor): Input tensor representing the output from previous layers.</span>
<span class="sd">                Shape should be compatible with the subsequent operations.</span>
<span class="sd">            q_x (mindspore.Tensor): Input tensor representing the query tensor.</span>
<span class="sd">                Shape should be compatible with the subsequent operations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            mindspore.Tensor: A tensor resulting from the wrapping up operation.</span>
<span class="sd">                The shape and content of the tensor depend on the operations performed within the method.</span>

<span class="sd">        Raises:</span>
<span class="sd">            No specific exceptions are documented to be raised by this method under normal operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">q_x</span><span class="p">))</span>

            <span class="c1"># [*, Q, H, C_hidden]</span>
            <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">o</span> <span class="o">=</span> <span class="n">o</span> <span class="o">*</span> <span class="n">g</span>

        <span class="c1"># [*, Q, H * C_hidden]</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">flatten_final_dims</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># [*, Q, C_q]</span>
        <span class="n">o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_o</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">o</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">q_x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">kv_x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">biases</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_memory_efficient_kernel</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_lma</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">lma_q_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
        <span class="n">lma_kv_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
        <span class="n">use_flash</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">flash_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            q_x:</span>
<span class="sd">                [*, Q, C_q] query data</span>
<span class="sd">            kv_x:</span>
<span class="sd">                [*, K, C_k] key data</span>
<span class="sd">            biases:</span>
<span class="sd">                List of biases that broadcast to [*, H, Q, K]</span>
<span class="sd">            use_memory_efficient_kernel:</span>
<span class="sd">                Whether to use a custom memory-efficient attention kernel. This should be the default choice for most.</span>
<span class="sd">                If none of the &quot;use_&lt;...&gt;&quot; flags are True, a stock PyTorch implementation is used instead</span>
<span class="sd">            use_lma:</span>
<span class="sd">                Whether to use low-memory attention (Staats &amp; Rabe 2021). If none of the &quot;use_&lt;...&gt;&quot; flags are True, a</span>
<span class="sd">                stock PyTorch implementation is used instead</span>
<span class="sd">            lma_q_chunk_size:</span>
<span class="sd">                Query chunk size (for LMA)</span>
<span class="sd">            lma_kv_chunk_size:</span>
<span class="sd">                Key/Value chunk size (for LMA)</span>
<span class="sd">        Returns</span>
<span class="sd">            [*, Q, C_q] attention update</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">use_lma</span> <span class="ow">and</span> <span class="p">(</span><span class="n">lma_q_chunk_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">lma_kv_chunk_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;If use_lma is specified, lma_q_chunk_size and lma_kv_chunk_size must be provided&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_flash</span> <span class="ow">and</span> <span class="n">biases</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;use_flash is incompatible with the bias option. For masking, use flash_mask instead&quot;</span><span class="p">)</span>

        <span class="n">attn_options</span> <span class="o">=</span> <span class="p">[</span><span class="n">use_memory_efficient_kernel</span><span class="p">,</span> <span class="n">use_lma</span><span class="p">,</span> <span class="n">use_flash</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">attn_options</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Choose at most one alternative attention algorithm&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">biases</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># [*, H, Q/K, C_hidden]</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prep_qkv</span><span class="p">(</span><span class="n">q_x</span><span class="p">,</span> <span class="n">kv_x</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

        <span class="c1"># [*, H, Q, K]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">biases</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">+=</span> <span class="n">b</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">softmax_no_cast</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># [*, H, Q, C_hidden]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrap_up</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">q_x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldAttention</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">c_q</span><span class="p">,</span> <span class="n">c_k</span><span class="p">,</span> <span class="n">c_v</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">no_heads</span><span class="p">,</span> <span class="n">gating</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input dimension of query data</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_k</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input dimension of key data</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_v</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input dimension of value data</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_hidden</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Per-head hidden dimension</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>no_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>gating</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the output should be gated using query data</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">c_q</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">c_k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">c_v</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">c_hidden</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">no_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">gating</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        c_q:</span>
<span class="sd">            Input dimension of query data</span>
<span class="sd">        c_k:</span>
<span class="sd">            Input dimension of key data</span>
<span class="sd">        c_v:</span>
<span class="sd">            Input dimension of value data</span>
<span class="sd">        c_hidden:</span>
<span class="sd">            Per-head hidden dimension</span>
<span class="sd">        no_heads:</span>
<span class="sd">            Number of attention heads</span>
<span class="sd">        gating:</span>
<span class="sd">            Whether the output should be gated using query data</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">c_q</span> <span class="o">=</span> <span class="n">c_q</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">c_k</span> <span class="o">=</span> <span class="n">c_k</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">c_v</span> <span class="o">=</span> <span class="n">c_v</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">=</span> <span class="n">c_hidden</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span> <span class="o">=</span> <span class="n">no_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gating</span> <span class="o">=</span> <span class="n">gating</span>

    <span class="c1"># DISCREPANCY: c_hidden is not the per-head channel dimension, as</span>
    <span class="c1"># stated in the supplement, but the overall channel dimension.</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;glorot&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;glorot&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;glorot&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_o</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_q</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;gating&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldAttention</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">q_x</span><span class="p">,</span> <span class="n">kv_x</span><span class="p">,</span> <span class="n">biases</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_memory_efficient_kernel</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_lma</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lma_q_chunk_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">lma_kv_chunk_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">use_flash</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">flash_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldAttention.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>q_x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, Q, C_q] query data</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kv_x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, K, C_k] key data</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>biases</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of biases that broadcast to [*, H, Q, K]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="mindspore.Tensor">Tensor</span>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_memory_efficient_kernel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to use a custom memory-efficient attention kernel. This should be the default choice for most.
If none of the "use_&lt;...&gt;" flags are True, a stock PyTorch implementation is used instead</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_lma</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to use low-memory attention (Staats &amp; Rabe 2021). If none of the "use_&lt;...&gt;" flags are True, a
stock PyTorch implementation is used instead</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lma_q_chunk_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Query chunk size (for LMA)</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1024</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lma_kv_chunk_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Key/Value chunk size (for LMA)</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Returns
    [*, Q, C_q] attention update</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">q_x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">kv_x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">biases</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_memory_efficient_kernel</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_lma</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lma_q_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
    <span class="n">lma_kv_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
    <span class="n">use_flash</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">flash_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        q_x:</span>
<span class="sd">            [*, Q, C_q] query data</span>
<span class="sd">        kv_x:</span>
<span class="sd">            [*, K, C_k] key data</span>
<span class="sd">        biases:</span>
<span class="sd">            List of biases that broadcast to [*, H, Q, K]</span>
<span class="sd">        use_memory_efficient_kernel:</span>
<span class="sd">            Whether to use a custom memory-efficient attention kernel. This should be the default choice for most.</span>
<span class="sd">            If none of the &quot;use_&lt;...&gt;&quot; flags are True, a stock PyTorch implementation is used instead</span>
<span class="sd">        use_lma:</span>
<span class="sd">            Whether to use low-memory attention (Staats &amp; Rabe 2021). If none of the &quot;use_&lt;...&gt;&quot; flags are True, a</span>
<span class="sd">            stock PyTorch implementation is used instead</span>
<span class="sd">        lma_q_chunk_size:</span>
<span class="sd">            Query chunk size (for LMA)</span>
<span class="sd">        lma_kv_chunk_size:</span>
<span class="sd">            Key/Value chunk size (for LMA)</span>
<span class="sd">    Returns</span>
<span class="sd">        [*, Q, C_q] attention update</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">use_lma</span> <span class="ow">and</span> <span class="p">(</span><span class="n">lma_q_chunk_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">lma_kv_chunk_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;If use_lma is specified, lma_q_chunk_size and lma_kv_chunk_size must be provided&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">use_flash</span> <span class="ow">and</span> <span class="n">biases</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;use_flash is incompatible with the bias option. For masking, use flash_mask instead&quot;</span><span class="p">)</span>

    <span class="n">attn_options</span> <span class="o">=</span> <span class="p">[</span><span class="n">use_memory_efficient_kernel</span><span class="p">,</span> <span class="n">use_lma</span><span class="p">,</span> <span class="n">use_flash</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">attn_options</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Choose at most one alternative attention algorithm&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">biases</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># [*, H, Q/K, C_hidden]</span>
    <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prep_qkv</span><span class="p">(</span><span class="n">q_x</span><span class="p">,</span> <span class="n">kv_x</span><span class="p">)</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="c1"># [*, H, Q, K]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">biases</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="n">b</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">softmax_no_cast</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># [*, H, Q, C_hidden]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrap_up</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">q_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Implements part of Algorithm 23.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldBackboneUpdate</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements part of Algorithm 23.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the EsmFoldBackboneUpdate class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config: A dictionary containing configuration parameters for the backbone update.</span>
<span class="sd">                It should include the &#39;sequence_dim&#39; parameter representing the dimension of the input sequence.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If the config parameter is not provided or is not a dictionary.</span>
<span class="sd">            ValueError: If the &#39;sequence_dim&#39; parameter is missing in the config dictionary.</span>
<span class="sd">            ValueError: If the &#39;sequence_dim&#39; parameter in the config dictionary is not a positive integer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            [*, N_res, C_s] single representation</span>
<span class="sd">        Returns:</span>
<span class="sd">            [*, N_res, 6] update vector</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># [*, 6]</span>
        <span class="n">update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">update</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldBackboneUpdate</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes the EsmFoldBackboneUpdate class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary containing configuration parameters for the backbone update.
It should include the 'sequence_dim' parameter representing the dimension of the input sequence.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the config parameter is not provided or is not a dictionary.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the 'sequence_dim' parameter is missing in the config dictionary.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the 'sequence_dim' parameter in the config dictionary is not a positive integer.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the EsmFoldBackboneUpdate class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config: A dictionary containing configuration parameters for the backbone update.</span>
<span class="sd">            It should include the &#39;sequence_dim&#39; parameter representing the dimension of the input sequence.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the config parameter is not provided or is not a dictionary.</span>
<span class="sd">        ValueError: If the &#39;sequence_dim&#39; parameter is missing in the config dictionary.</span>
<span class="sd">        ValueError: If the &#39;sequence_dim&#39; parameter in the config dictionary is not a positive integer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldBackboneUpdate</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">s</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldBackboneUpdate.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        [*, N_res, C_s] single representation</span>
<span class="sd">    Returns:</span>
<span class="sd">        [*, N_res, 6] update vector</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># [*, 6]</span>
    <span class="n">update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">update</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Implementation of dropout with the ability to share the dropout mask along a particular dimension.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldDropout</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of dropout with the ability to share the dropout mask along a particular dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">batch_dim</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the EsmFoldDropout class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            r (float): The dropout rate value.</span>
<span class="sd">            batch_dim (Union[int, List[int]]):</span>
<span class="sd">                The dimension(s) of the input batch.</span>
<span class="sd">                If an integer is provided, it will be converted to a list with that integer as the only element.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">batch_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_dim</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span> <span class="o">=</span> <span class="n">batch_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method forwards a modified tensor with dropout for the EsmFoldDropout class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the EsmFoldDropout class.</span>
<span class="sd">            x (mindspore.Tensor): The input tensor for which the modified tensor is forwarded.</span>

<span class="sd">        Returns:</span>
<span class="sd">            mindspore.Tensor: Returns a new tensor, which is the result of applying dropout to the input tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If the input x is not of type mindspore.Tensor.</span>
<span class="sd">            ValueError: If the shape manipulation encounters an error during the forwardion process.</span>
<span class="sd">            RuntimeError: If there is a runtime issue during the execution of the method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">bd</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span><span class="p">:</span>
                <span class="n">shape</span><span class="p">[</span><span class="n">bd</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldDropout</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">batch_dim</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmFoldDropout class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>r</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate value.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>batch_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension(s) of the input batch.
If an integer is provided, it will be converted to a list with that integer as the only element.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Union">Union</span>[int, <span title="typing.List">List</span>[int]]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">batch_dim</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the EsmFoldDropout class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        r (float): The dropout rate value.</span>
<span class="sd">        batch_dim (Union[int, List[int]]):</span>
<span class="sd">            The dimension(s) of the input batch.</span>
<span class="sd">            If an integer is provided, it will be converted to a list with that integer as the only element.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">r</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">batch_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_dim</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span> <span class="o">=</span> <span class="n">batch_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldDropout</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method forwards a modified tensor with dropout for the EsmFoldDropout class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the EsmFoldDropout class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor for which the modified tensor is forwarded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindspore.Tensor">Tensor</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor: Returns a new tensor, which is the result of applying dropout to the input tensor.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input x is not of type mindspore.Tensor.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the shape manipulation encounters an error during the forwardion process.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If there is a runtime issue during the execution of the method.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method forwards a modified tensor with dropout for the EsmFoldDropout class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: An instance of the EsmFoldDropout class.</span>
<span class="sd">        x (mindspore.Tensor): The input tensor for which the modified tensor is forwarded.</span>

<span class="sd">    Returns:</span>
<span class="sd">        mindspore.Tensor: Returns a new tensor, which is the result of applying dropout to the input tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input x is not of type mindspore.Tensor.</span>
<span class="sd">        ValueError: If the shape manipulation encounters an error during the forwardion process.</span>
<span class="sd">        RuntimeError: If there is a runtime issue during the execution of the method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">bd</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span><span class="p">:</span>
            <span class="n">shape</span><span class="p">[</span><span class="n">bd</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Implements Algorithm 22.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldInvariantPointAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements Algorithm 22.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes an instance of the EsmFoldInvariantPointAttention class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config: An object containing the configuration settings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>

<span class="sd">        Description:</span>
<span class="sd">            This method initializes the EsmFoldInvariantPointAttention instance by setting various parameters and</span>
<span class="sd">            creating necessary objects.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config: An object containing the configuration settings.</span>

<span class="sd">        The config object must have the following attributes:</span>

<span class="sd">        - sequence_dim: An integer representing the dimension of the sequence.</span>
<span class="sd">        - pairwise_dim: An integer representing the dimension of the pairwise data.</span>
<span class="sd">        - ipa_dim: An integer representing the dimension of the ipa data.</span>
<span class="sd">        - num_heads_ipa: An integer representing the number of heads for the ipa.</span>
<span class="sd">        - num_qk_points: An integer representing the number of query and key points.</span>
<span class="sd">        - num_v_points: An integer representing the number of value points.</span>

<span class="sd">        Attributes:</span>
<span class="sd">            hidden_dim: An integer representing the ipa dimension.</span>
<span class="sd">            num_heads: An integer representing the number of ipa heads.</span>
<span class="sd">            num_qk_points: An integer representing the number of query and key points.</span>
<span class="sd">            num_v_points: An integer representing the number of value points.</span>
<span class="sd">            linear_q: An instance of the EsmFoldLinear class with input dimension c_s and output dimension hc.</span>
<span class="sd">            linear_kv: An instance of the EsmFoldLinear class with input dimension c_s and output dimension 2 * hc.</span>
<span class="sd">            linear_q_points: An instance of the EsmFoldLinear class with input dimension c_s and output dimension hpq.</span>
<span class="sd">            linear_kv_points: An instance of the EsmFoldLinear class with input dimension c_s and output dimension hpkv.</span>
<span class="sd">            linear_b: An instance of the EsmFoldLinear class with input dimension c_z and output dimension num_heads_ipa.</span>
<span class="sd">            head_weights: A Parameter object representing the weights of the ipa heads.</span>
<span class="sd">            linear_out: An instance of the EsmFoldLinear class with input dimension concat_out_dim and output dimension c_s.</span>
<span class="sd">            softmax: An instance of the Softmax class used for softmax activation.</span>
<span class="sd">            softplus: An instance of the Softplus class used for softplus activation.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="n">c_s</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span>
        <span class="n">c_z</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">ipa_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_qk_points</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_qk_points</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_v_points</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_v_points</span>

        <span class="c1"># These linear layers differ from their specifications in the</span>
        <span class="c1"># supplement. There, they lack bias and use Glorot initialization.</span>
        <span class="c1"># Here as in the official source, they have bias and use the default</span>
        <span class="c1"># Lecun initialization.</span>
        <span class="n">hc</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">ipa_dim</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">hc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_kv</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hc</span><span class="p">)</span>

        <span class="n">hpq</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_qk_points</span> <span class="o">*</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_q_points</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">hpq</span><span class="p">)</span>

        <span class="n">hpkv</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span> <span class="o">*</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_qk_points</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">num_v_points</span><span class="p">)</span> <span class="o">*</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_kv_points</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">hpkv</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_z</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">head_weights</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span><span class="p">)))</span>

        <span class="n">concat_out_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span> <span class="o">*</span> <span class="p">(</span><span class="n">c_z</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">ipa_dim</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">num_v_points</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">concat_out_dim</span><span class="p">,</span> <span class="n">c_s</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">s</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">z</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">r</span><span class="p">:</span> <span class="n">Rigid</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            s:</span>
<span class="sd">                [*, N_res, C_s] single representation</span>
<span class="sd">            z:</span>
<span class="sd">                [*, N_res, N_res, C_z] pair representation</span>
<span class="sd">            r:</span>
<span class="sd">                [*, N_res] transformation object</span>
<span class="sd">            mask:</span>
<span class="sd">                [*, N_res] mask</span>
<span class="sd">        Returns:</span>
<span class="sd">            [*, N_res, C_s] single representation update</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="n">z</span><span class="p">]</span>

        <span class="c1">#######################################</span>
        <span class="c1"># Generate scalar and point activations</span>
        <span class="c1">#######################################</span>
        <span class="c1"># [*, N_res, H * C_hidden]</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_kv</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="c1"># [*, N_res, H, C_hidden]</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># [*, N_res, H, 2 * C_hidden]</span>
        <span class="n">kv</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># [*, N_res, H, C_hidden]</span>
        <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># [*, N_res, H * P_q * 3]</span>
        <span class="n">q_pts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q_points</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="c1"># This is kind of clunky, but it&#39;s how the original does it</span>
        <span class="c1"># [*, N_res, H * P_q, 3]</span>
        <span class="n">q_pts</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">q_pts</span><span class="p">,</span> <span class="n">q_pts</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q_pts</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">q_pts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q_pts</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">q_pts</span><span class="p">)</span>

        <span class="c1"># [*, N_res, H, P_q, 3]</span>
        <span class="n">q_pts</span> <span class="o">=</span> <span class="n">q_pts</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">q_pts</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_qk_points</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

        <span class="c1"># [*, N_res, H * (P_q + P_v) * 3]</span>
        <span class="n">kv_pts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_kv_points</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="c1"># [*, N_res, H * (P_q + P_v), 3]</span>
        <span class="n">kv_pts</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv_pts</span><span class="p">,</span> <span class="n">kv_pts</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">kv_pts</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">kv_pts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">kv_pts</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">kv_pts</span><span class="p">)</span>

        <span class="c1"># [*, N_res, H, (P_q + P_v), 3]</span>
        <span class="n">kv_pts</span> <span class="o">=</span> <span class="n">kv_pts</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">kv_pts</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

        <span class="c1"># [*, N_res, H, P_q/P_v, 3]</span>
        <span class="n">k_pts</span><span class="p">,</span> <span class="n">v_pts</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv_pts</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_qk_points</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_v_points</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1">##########################</span>
        <span class="c1"># Compute attention scores</span>
        <span class="c1">##########################</span>
        <span class="c1"># [*, N_res, N_res, H]</span>
        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="c1"># [*, H, N_res, N_res]</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
            <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>  <span class="c1"># [*, H, N_res, C_hidden]</span>
            <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>  <span class="c1"># [*, H, C_hidden, N_res]</span>
        <span class="p">)</span>

        <span class="n">a</span> <span class="o">*=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">))</span>
        <span class="n">a</span> <span class="o">+=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># [*, N_res, N_res, H, P_q, 3]</span>
        <span class="n">pt_att</span> <span class="o">=</span> <span class="n">q_pts</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_pts</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">pt_att</span> <span class="o">=</span> <span class="n">pt_att</span><span class="o">**</span><span class="mi">2</span>

        <span class="c1"># [*, N_res, N_res, H, P_q]</span>
        <span class="n">pt_att</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">pt_att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">head_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_weights</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="p">((</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">pt_att</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">head_weights</span> <span class="o">=</span> <span class="n">head_weights</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_qk_points</span> <span class="o">*</span> <span class="mf">9.0</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)))</span>
        <span class="n">pt_att</span> <span class="o">=</span> <span class="n">pt_att</span> <span class="o">*</span> <span class="n">head_weights</span>

        <span class="c1"># [*, N_res, N_res, H]</span>
        <span class="n">pt_att</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pt_att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="c1"># [*, N_res, N_res]</span>
        <span class="n">square_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">square_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">inf</span> <span class="o">*</span> <span class="p">(</span><span class="n">square_mask</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># [*, H, N_res, N_res]</span>
        <span class="n">pt_att</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">pt_att</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">pt_att</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">square_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

        <span class="c1">################</span>
        <span class="c1"># Compute output</span>
        <span class="c1">################</span>
        <span class="c1"># [*, N_res, H, C_hidden]</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>

        <span class="c1"># [*, N_res, H * C_hidden]</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">flatten_final_dims</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># [*, H, 3, N_res, P_v]</span>
        <span class="n">o_pt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
            <span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">v_pts</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]),</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># [*, N_res, H, P_v, 3]</span>
        <span class="n">o_pt</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">o_pt</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">o_pt</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">invert_apply</span><span class="p">(</span><span class="n">o_pt</span><span class="p">)</span>

        <span class="c1"># [*, N_res, H * P_v]</span>
        <span class="n">o_pt_norm</span> <span class="o">=</span> <span class="n">flatten_final_dims</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">o_pt</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">epsilon</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># [*, N_res, H * P_v, 3]</span>
        <span class="n">o_pt</span> <span class="o">=</span> <span class="n">o_pt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">o_pt</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="c1"># [*, N_res, H, C_z]</span>
        <span class="n">o_pair</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">),</span> <span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

        <span class="c1"># [*, N_res, H * C_z]</span>
        <span class="n">o_pair</span> <span class="o">=</span> <span class="n">flatten_final_dims</span><span class="p">(</span><span class="n">o_pair</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># [*, N_res, C_s]</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span><span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">o</span><span class="p">,</span> <span class="o">*</span><span class="n">ops</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">o_pt</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">o_pt_norm</span><span class="p">,</span> <span class="n">o_pair</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">s</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldInvariantPointAttention</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmFoldInvariantPointAttention class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An object containing the configuration settings.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="description" open>
  <summary>Description</summary>
  <p>This method initializes the EsmFoldInvariantPointAttention instance by setting various parameters and
creating necessary objects.</p>
</details>

<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An object containing the configuration settings.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <p>The config object must have the following attributes:</p>
<ul>
<li>sequence_dim: An integer representing the dimension of the sequence.</li>
<li>pairwise_dim: An integer representing the dimension of the pairwise data.</li>
<li>ipa_dim: An integer representing the dimension of the ipa data.</li>
<li>num_heads_ipa: An integer representing the number of heads for the ipa.</li>
<li>num_qk_points: An integer representing the number of query and key points.</li>
<li>num_v_points: An integer representing the number of value points.</li>
</ul>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.hidden_dim">hidden_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An integer representing the ipa dimension.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.num_heads">num_heads</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An integer representing the number of ipa heads.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.num_qk_points">num_qk_points</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An integer representing the number of query and key points.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.num_v_points">num_v_points</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An integer representing the number of value points.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.linear_q">linear_q</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the EsmFoldLinear class with input dimension c_s and output dimension hc.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.linear_kv">linear_kv</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the EsmFoldLinear class with input dimension c_s and output dimension 2 * hc.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.linear_q_points">linear_q_points</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the EsmFoldLinear class with input dimension c_s and output dimension hpq.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.linear_kv_points">linear_kv_points</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the EsmFoldLinear class with input dimension c_s and output dimension hpkv.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.linear_b">linear_b</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the EsmFoldLinear class with input dimension c_z and output dimension num_heads_ipa.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.head_weights">head_weights</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A Parameter object representing the weights of the ipa heads.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.linear_out">linear_out</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the EsmFoldLinear class with input dimension concat_out_dim and output dimension c_s.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.softmax">softmax</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the Softmax class used for softmax activation.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.__init__.softplus">softplus</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the Softplus class used for softplus activation.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Initializes an instance of the EsmFoldInvariantPointAttention class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config: An object containing the configuration settings.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>

<span class="sd">    Description:</span>
<span class="sd">        This method initializes the EsmFoldInvariantPointAttention instance by setting various parameters and</span>
<span class="sd">        creating necessary objects.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config: An object containing the configuration settings.</span>

<span class="sd">    The config object must have the following attributes:</span>

<span class="sd">    - sequence_dim: An integer representing the dimension of the sequence.</span>
<span class="sd">    - pairwise_dim: An integer representing the dimension of the pairwise data.</span>
<span class="sd">    - ipa_dim: An integer representing the dimension of the ipa data.</span>
<span class="sd">    - num_heads_ipa: An integer representing the number of heads for the ipa.</span>
<span class="sd">    - num_qk_points: An integer representing the number of query and key points.</span>
<span class="sd">    - num_v_points: An integer representing the number of value points.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        hidden_dim: An integer representing the ipa dimension.</span>
<span class="sd">        num_heads: An integer representing the number of ipa heads.</span>
<span class="sd">        num_qk_points: An integer representing the number of query and key points.</span>
<span class="sd">        num_v_points: An integer representing the number of value points.</span>
<span class="sd">        linear_q: An instance of the EsmFoldLinear class with input dimension c_s and output dimension hc.</span>
<span class="sd">        linear_kv: An instance of the EsmFoldLinear class with input dimension c_s and output dimension 2 * hc.</span>
<span class="sd">        linear_q_points: An instance of the EsmFoldLinear class with input dimension c_s and output dimension hpq.</span>
<span class="sd">        linear_kv_points: An instance of the EsmFoldLinear class with input dimension c_s and output dimension hpkv.</span>
<span class="sd">        linear_b: An instance of the EsmFoldLinear class with input dimension c_z and output dimension num_heads_ipa.</span>
<span class="sd">        head_weights: A Parameter object representing the weights of the ipa heads.</span>
<span class="sd">        linear_out: An instance of the EsmFoldLinear class with input dimension concat_out_dim and output dimension c_s.</span>
<span class="sd">        softmax: An instance of the Softmax class used for softmax activation.</span>
<span class="sd">        softplus: An instance of the Softplus class used for softplus activation.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="n">c_s</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span>
    <span class="n">c_z</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">ipa_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_qk_points</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_qk_points</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_v_points</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_v_points</span>

    <span class="c1"># These linear layers differ from their specifications in the</span>
    <span class="c1"># supplement. There, they lack bias and use Glorot initialization.</span>
    <span class="c1"># Here as in the official source, they have bias and use the default</span>
    <span class="c1"># Lecun initialization.</span>
    <span class="n">hc</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">ipa_dim</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">hc</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_kv</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hc</span><span class="p">)</span>

    <span class="n">hpq</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_qk_points</span> <span class="o">*</span> <span class="mi">3</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_q_points</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">hpq</span><span class="p">)</span>

    <span class="n">hpkv</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span> <span class="o">*</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_qk_points</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">num_v_points</span><span class="p">)</span> <span class="o">*</span> <span class="mi">3</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_kv_points</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">hpkv</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_z</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">head_weights</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span><span class="p">)))</span>

    <span class="n">concat_out_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads_ipa</span> <span class="o">*</span> <span class="p">(</span><span class="n">c_z</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">ipa_dim</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">num_v_points</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">concat_out_dim</span><span class="p">,</span> <span class="n">c_s</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldInvariantPointAttention</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldInvariantPointAttention.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>s</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, N_res, C_s] single representation</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>z</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, N_res, N_res, C_z] pair representation</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>r</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, N_res] transformation object</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.models.esm.openfold_utils.Rigid">Rigid</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, N_res] mask</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">s</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">r</span><span class="p">:</span> <span class="n">Rigid</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        s:</span>
<span class="sd">            [*, N_res, C_s] single representation</span>
<span class="sd">        z:</span>
<span class="sd">            [*, N_res, N_res, C_z] pair representation</span>
<span class="sd">        r:</span>
<span class="sd">            [*, N_res] transformation object</span>
<span class="sd">        mask:</span>
<span class="sd">            [*, N_res] mask</span>
<span class="sd">    Returns:</span>
<span class="sd">        [*, N_res, C_s] single representation update</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="n">z</span><span class="p">]</span>

    <span class="c1">#######################################</span>
    <span class="c1"># Generate scalar and point activations</span>
    <span class="c1">#######################################</span>
    <span class="c1"># [*, N_res, H * C_hidden]</span>
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_kv</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># [*, N_res, H, C_hidden]</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># [*, N_res, H, 2 * C_hidden]</span>
    <span class="n">kv</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># [*, N_res, H, C_hidden]</span>
    <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># [*, N_res, H * P_q * 3]</span>
    <span class="n">q_pts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q_points</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># This is kind of clunky, but it&#39;s how the original does it</span>
    <span class="c1"># [*, N_res, H * P_q, 3]</span>
    <span class="n">q_pts</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">q_pts</span><span class="p">,</span> <span class="n">q_pts</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">q_pts</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">q_pts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">q_pts</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">q_pts</span><span class="p">)</span>

    <span class="c1"># [*, N_res, H, P_q, 3]</span>
    <span class="n">q_pts</span> <span class="o">=</span> <span class="n">q_pts</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">q_pts</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_qk_points</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="c1"># [*, N_res, H * (P_q + P_v) * 3]</span>
    <span class="n">kv_pts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_kv_points</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># [*, N_res, H * (P_q + P_v), 3]</span>
    <span class="n">kv_pts</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv_pts</span><span class="p">,</span> <span class="n">kv_pts</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">kv_pts</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">kv_pts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">kv_pts</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">kv_pts</span><span class="p">)</span>

    <span class="c1"># [*, N_res, H, (P_q + P_v), 3]</span>
    <span class="n">kv_pts</span> <span class="o">=</span> <span class="n">kv_pts</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">kv_pts</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="c1"># [*, N_res, H, P_q/P_v, 3]</span>
    <span class="n">k_pts</span><span class="p">,</span> <span class="n">v_pts</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv_pts</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_qk_points</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_v_points</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1">##########################</span>
    <span class="c1"># Compute attention scores</span>
    <span class="c1">##########################</span>
    <span class="c1"># [*, N_res, N_res, H]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># [*, H, N_res, N_res]</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
        <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>  <span class="c1"># [*, H, N_res, C_hidden]</span>
        <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>  <span class="c1"># [*, H, C_hidden, N_res]</span>
    <span class="p">)</span>

    <span class="n">a</span> <span class="o">*=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">))</span>
    <span class="n">a</span> <span class="o">+=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># [*, N_res, N_res, H, P_q, 3]</span>
    <span class="n">pt_att</span> <span class="o">=</span> <span class="n">q_pts</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span> <span class="o">-</span> <span class="n">k_pts</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">pt_att</span> <span class="o">=</span> <span class="n">pt_att</span><span class="o">**</span><span class="mi">2</span>

    <span class="c1"># [*, N_res, N_res, H, P_q]</span>
    <span class="n">pt_att</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">pt_att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">head_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_weights</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="p">((</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">pt_att</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="n">head_weights</span> <span class="o">=</span> <span class="n">head_weights</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_qk_points</span> <span class="o">*</span> <span class="mf">9.0</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">pt_att</span> <span class="o">=</span> <span class="n">pt_att</span> <span class="o">*</span> <span class="n">head_weights</span>

    <span class="c1"># [*, N_res, N_res, H]</span>
    <span class="n">pt_att</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pt_att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="c1"># [*, N_res, N_res]</span>
    <span class="n">square_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">square_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">inf</span> <span class="o">*</span> <span class="p">(</span><span class="n">square_mask</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># [*, H, N_res, N_res]</span>
    <span class="n">pt_att</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">pt_att</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">pt_att</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">square_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="c1">################</span>
    <span class="c1"># Compute output</span>
    <span class="c1">################</span>
    <span class="c1"># [*, N_res, H, C_hidden]</span>
    <span class="n">o</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>

    <span class="c1"># [*, N_res, H * C_hidden]</span>
    <span class="n">o</span> <span class="o">=</span> <span class="n">flatten_final_dims</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># [*, H, 3, N_res, P_v]</span>
    <span class="n">o_pt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">v_pts</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]),</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># [*, N_res, H, P_v, 3]</span>
    <span class="n">o_pt</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">o_pt</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">o_pt</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">invert_apply</span><span class="p">(</span><span class="n">o_pt</span><span class="p">)</span>

    <span class="c1"># [*, N_res, H * P_v]</span>
    <span class="n">o_pt_norm</span> <span class="o">=</span> <span class="n">flatten_final_dims</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">o_pt</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">epsilon</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># [*, N_res, H * P_v, 3]</span>
    <span class="n">o_pt</span> <span class="o">=</span> <span class="n">o_pt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">o_pt</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="c1"># [*, N_res, H, C_z]</span>
    <span class="n">o_pair</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">),</span> <span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="c1"># [*, N_res, H * C_z]</span>
    <span class="n">o_pair</span> <span class="o">=</span> <span class="n">flatten_final_dims</span><span class="p">(</span><span class="n">o_pair</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># [*, N_res, C_s]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">o</span><span class="p">,</span> <span class="o">*</span><span class="n">ops</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">o_pt</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">o_pt_norm</span><span class="p">,</span> <span class="n">o_pair</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">s</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Linear">Linear</span></code></p>


        <p>A Linear layer with built-in nonstandard initializations. Called just like torch.nn.Linear.</p>
<p>Implements the initializers in 1.11.4, plus some additional ones found in the code.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Linear layer with built-in nonstandard initializations. Called just like torch.nn.Linear.</span>

<span class="sd">    Implements the initializers in 1.11.4, plus some additional ones found in the code.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">init</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span>
        <span class="n">init_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="kc">None</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            in_dim:</span>
<span class="sd">                The final dimension of inputs to the layer</span>
<span class="sd">            out_dim:</span>
<span class="sd">                The final dimension of layer outputs</span>
<span class="sd">            bias:</span>
<span class="sd">                Whether to learn an additive bias. True by default</span>
<span class="sd">            init:</span>
<span class="sd">                The initializer to use. Choose from:</span>

<span class="sd">                &quot;default&quot;: LeCun fan-in truncated normal initialization &quot;relu&quot;: He initialization w/ truncated normal</span>
<span class="sd">                distribution &quot;glorot&quot;: Fan-average Glorot uniform initialization &quot;gating&quot;: Weights=0, Bias=1 &quot;normal&quot;:</span>
<span class="sd">                Normal initialization with std=1/sqrt(fan_in) &quot;final&quot;: Weights=0, Bias=0</span>

<span class="sd">                Overridden by init_fn if the latter is not None.</span>
<span class="sd">            init_fn:</span>
<span class="sd">                A custom initializer taking weight and bias as inputs. Overrides init if not None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_fn</span> <span class="o">=</span> <span class="n">init_fn</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">init</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="s2">&quot;glorot&quot;</span><span class="p">,</span> <span class="s2">&quot;gating&quot;</span><span class="p">,</span> <span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;final&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid init string.&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldLinear</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;default&#39;</span><span class="p">,</span> <span class="n">init_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>in_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The final dimension of inputs to the layer</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The final dimension of layer outputs</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to learn an additive bias. True by default</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The initializer to use. Choose from:</p>
<p>"default": LeCun fan-in truncated normal initialization "relu": He initialization w/ truncated normal
distribution "glorot": Fan-average Glorot uniform initialization "gating": Weights=0, Bias=1 "normal":
Normal initialization with std=1/sqrt(fan_in) "final": Weights=0, Bias=0</p>
<p>Overridden by init_fn if the latter is not None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;default&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_fn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A custom initializer taking weight and bias as inputs. Overrides init if not None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>[[<span title="mindspore.Tensor">Tensor</span>, <span title="mindspore.Tensor">Tensor</span>], None]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">init</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span>
    <span class="n">init_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="kc">None</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        in_dim:</span>
<span class="sd">            The final dimension of inputs to the layer</span>
<span class="sd">        out_dim:</span>
<span class="sd">            The final dimension of layer outputs</span>
<span class="sd">        bias:</span>
<span class="sd">            Whether to learn an additive bias. True by default</span>
<span class="sd">        init:</span>
<span class="sd">            The initializer to use. Choose from:</span>

<span class="sd">            &quot;default&quot;: LeCun fan-in truncated normal initialization &quot;relu&quot;: He initialization w/ truncated normal</span>
<span class="sd">            distribution &quot;glorot&quot;: Fan-average Glorot uniform initialization &quot;gating&quot;: Weights=0, Bias=1 &quot;normal&quot;:</span>
<span class="sd">            Normal initialization with std=1/sqrt(fan_in) &quot;final&quot;: Weights=0, Bias=0</span>

<span class="sd">            Overridden by init_fn if the latter is not None.</span>
<span class="sd">        init_fn:</span>
<span class="sd">            A custom initializer taking weight and bias as inputs. Overrides init if not None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_fn</span> <span class="o">=</span> <span class="n">init_fn</span>
    <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">init</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="s2">&quot;glorot&quot;</span><span class="p">,</span> <span class="s2">&quot;gating&quot;</span><span class="p">,</span> <span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;final&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid init string.&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>EsmFoldPairToSequence class represents a neural network module for converting pairwise features to sequence features
using self-attention mechanism.</p>
<p>This class inherits from nn.Module and includes methods for initializing the module and forwarding the forward pass.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.pairwise_state_dim">pairwise_state_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Dimension of the pairwise state features.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.num_heads">num_heads</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Number of attention heads.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.__init__" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the EsmFoldPairToSequence module with the given pairwise_state_dim and num_heads.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.forward" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Applies self-attention mechanism to the input pairwise_state tensor to generate pairwise_bias tensor.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pairwise_state_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the pairwise state features.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="inputs" open>
  <summary>Inputs</summary>
  <p>pairwise_state (tensor): Input tensor of shape B x L x L x pairwise_state_dim.</p>
</details>

<details class="outputs" open>
  <summary>Outputs</summary>
  <p>pairwise_bias (tensor): Output tensor of shape B x L x L x num_heads.</p>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldPairToSequence</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    EsmFoldPairToSequence class represents a neural network module for converting pairwise features to sequence features</span>
<span class="sd">    using self-attention mechanism.</span>

<span class="sd">    This class inherits from nn.Module and includes methods for initializing the module and forwarding the forward pass.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        pairwise_state_dim (int): Dimension of the pairwise state features.</span>
<span class="sd">        num_heads (int): Number of attention heads.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__:</span>
<span class="sd">            Initializes the EsmFoldPairToSequence module with the given pairwise_state_dim and num_heads.</span>

<span class="sd">        forward:</span>
<span class="sd">            Applies self-attention mechanism to the input pairwise_state tensor to generate pairwise_bias tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        pairwise_state_dim (int): Dimension of the pairwise state features.</span>
<span class="sd">        num_heads (int): Number of attention heads.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        pairwise_state (tensor): Input tensor of shape B x L x L x pairwise_state_dim.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        pairwise_bias (tensor): Output tensor of shape B x L x L x num_heads.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the EsmFoldPairToSequence class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            pairwise_state_dim (int): The dimension of the pairwise state.</span>
<span class="sd">            num_heads (int): The number of attention heads to use.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If pairwise_state_dim or num_heads is not a positive integer.</span>
<span class="sd">            AttributeError: If the attributes layernorm or linear cannot be initialized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">pairwise_state_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pairwise_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            pairwise_state: B x L x L x pairwise_state_dim</span>

<span class="sd">        Output:</span>
<span class="sd">            pairwise_bias: B x L x L x num_heads</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">)</span>
        <span class="n">pairwise_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pairwise_bias</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldPairToSequence</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmFoldPairToSequence class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pairwise_state_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the pairwise state.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads to use.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If pairwise_state_dim or num_heads is not a positive integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AttributeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the attributes layernorm or linear cannot be initialized.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the EsmFoldPairToSequence class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        pairwise_state_dim (int): The dimension of the pairwise state.</span>
<span class="sd">        num_heads (int): The number of attention heads to use.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If pairwise_state_dim or num_heads is not a positive integer.</span>
<span class="sd">        AttributeError: If the attributes layernorm or linear cannot be initialized.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">pairwise_state_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldPairToSequence</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<details class="inputs" open>
  <summary>Inputs</summary>
  
</details>

<details class="output" open>
  <summary>Output</summary>
  
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pairwise_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inputs:</span>
<span class="sd">        pairwise_state: B x L x L x pairwise_state_dim</span>

<span class="sd">    Output:</span>
<span class="sd">        pairwise_bias: B x L x L x num_heads</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span>
    <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">)</span>
    <span class="n">pairwise_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pairwise_bias</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPreTrainedModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPreTrainedModel</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPreTrainedModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel" href="#mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel">EsmPreTrainedModel</a></code></p>


        <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
models.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldPreTrainedModel</span><span class="p">(</span><span class="n">EsmPreTrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained</span>
<span class="sd">    models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Subclass `EsMPreTrainedModel` to deal with special init</span>
    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the weights&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">EsmFoldLinear</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">cell</span><span class="o">.</span><span class="n">init_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">init_fn</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">cell</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
                <span class="n">trunc_normal_init_</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">cell</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span>
                <span class="n">trunc_normal_init_</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">cell</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;glorot&quot;</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">XavierUniform</span><span class="p">(),</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="k">elif</span> <span class="n">cell</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;gating&quot;</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">if</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="n">cell</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;normal&quot;</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">HeNormal</span><span class="p">(</span><span class="n">nonlinearity</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">),</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="k">elif</span> <span class="n">cell</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;final&quot;</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">EsmFoldInvariantPointAttention</span><span class="p">):</span>
            <span class="n">ipa_point_weights_init_</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">head_weights</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">EsmFoldTriangularSelfAttentionBlock</span><span class="p">):</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">tri_mul_in</span><span class="o">.</span><span class="n">linear_z</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">tri_mul_in</span><span class="o">.</span><span class="n">linear_z</span><span class="o">.</span><span class="n">bias</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">tri_mul_out</span><span class="o">.</span><span class="n">linear_z</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">tri_mul_out</span><span class="o">.</span><span class="n">linear_z</span><span class="o">.</span><span class="n">bias</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">tri_att_start</span><span class="o">.</span><span class="n">mha</span><span class="o">.</span><span class="n">linear_o</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">tri_att_start</span><span class="o">.</span><span class="n">mha</span><span class="o">.</span><span class="n">linear_o</span><span class="o">.</span><span class="n">bias</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">tri_att_end</span><span class="o">.</span><span class="n">mha</span><span class="o">.</span><span class="n">linear_o</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">tri_att_end</span><span class="o">.</span><span class="n">mha</span><span class="o">.</span><span class="n">linear_o</span><span class="o">.</span><span class="n">bias</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">cell</span><span class="o">.</span><span class="n">sequence_to_pair</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">sequence_to_pair</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">pair_to_sequence</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">seq_attention</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">seq_attention</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">mlp_seq</span><span class="o">.</span><span class="n">mlp</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">mlp_seq</span><span class="o">.</span><span class="n">mlp</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">mlp_pair</span><span class="o">.</span><span class="n">mlp</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">mlp_pair</span><span class="o">.</span><span class="n">mlp</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="n">cell</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Represents a class for forwarding relative position embeddings for protein folding using the ESM
(Evolutionary Scale Modeling) framework.</p>
<p>This class inherits from the nn.Module class and provides methods for initializing the class and forwarding pairwise
state embeddings based on residue indices and optional masking.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.bins">bins</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An integer representing the number of position bins used for forwarding the embeddings.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.embedding">embedding</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of nn.Embedding used for creating the embeddings based on the position differences.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.__init__" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the EsmFoldRelativePosition class with the provided configuration.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.forward" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs pairwise state embeddings based on the given residue indices and optional mask.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An object containing configuration parameters for initializing the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>residue_index</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A B x L tensor of indices (dtype=torch.long) representing the residue indices.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A B x L tensor of booleans representing an optional mask.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pairwise_state</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A B x L x L x pairwise_state_dim tensor of embeddings based on the input residue indices and mask.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the dtype of residue_index is not torch.long or if the shapes of residue_index and mask are inconsistent.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldRelativePosition</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents a class for forwarding relative position embeddings for protein folding using the ESM</span>
<span class="sd">    (Evolutionary Scale Modeling) framework.</span>

<span class="sd">    This class inherits from the nn.Module class and provides methods for initializing the class and forwarding pairwise</span>
<span class="sd">    state embeddings based on residue indices and optional masking.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        bins: An integer representing the number of position bins used for forwarding the embeddings.</span>
<span class="sd">        embedding: An instance of nn.Embedding used for creating the embeddings based on the position differences.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the EsmFoldRelativePosition class with the provided configuration.</span>
<span class="sd">        forward: Constructs pairwise state embeddings based on the given residue indices and optional mask.</span>

<span class="sd">    Args:</span>
<span class="sd">        config: An object containing configuration parameters for initializing the class.</span>
<span class="sd">        residue_index: A B x L tensor of indices (dtype=torch.long) representing the residue indices.</span>
<span class="sd">        mask: A B x L tensor of booleans representing an optional mask.</span>

<span class="sd">    Returns:</span>
<span class="sd">        pairwise_state: A B x L x L x pairwise_state_dim tensor of embeddings based on the input residue indices and mask.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError:</span>
<span class="sd">            If the dtype of residue_index is not torch.long or if the shapes of residue_index and mask are inconsistent.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the EsmFoldRelativePosition class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldRelativePosition): The current instance of the class.</span>
<span class="sd">            config: The configuration object containing the necessary parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bins</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">position_bins</span>

        <span class="c1"># Note an additional offset is used so that the 0th position</span>
        <span class="c1"># is reserved for masked pairs.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">residue_index</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Input:</span>
<span class="sd">            residue_index: B x L tensor of indices (dytpe=torch.long) mask: B x L tensor of booleans</span>

<span class="sd">        Output:</span>
<span class="sd">            pairwise_state: B x L x L x pairwise_state_dim tensor of embeddings</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">residue_index</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`residue_index` has dtype </span><span class="si">{</span><span class="n">residue_index</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, it should be `torch.long`.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">residue_index</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`residue_index` and `mask` have inconsistent shapes: </span><span class="si">{</span><span class="n">residue_index</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="n">diff</span> <span class="o">=</span> <span class="n">residue_index</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">residue_index</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">)</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Add 1 to adjust for padding index.</span>

        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">diff</span><span class="p">[</span><span class="n">mask</span> <span class="o">==</span> <span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># noqa: E712</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldRelativePosition</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmFoldRelativePosition class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The current instance of the class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition">EsmFoldRelativePosition</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing the necessary parameters.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the EsmFoldRelativePosition class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmFoldRelativePosition): The current instance of the class.</span>
<span class="sd">        config: The configuration object containing the necessary parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bins</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">position_bins</span>

    <span class="c1"># Note an additional offset is used so that the 0th position</span>
    <span class="c1"># is reserved for masked pairs.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldRelativePosition</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">residue_index</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldRelativePosition.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<details class="input" open>
  <summary>Input</summary>
  
</details>

<details class="output" open>
  <summary>Output</summary>
  
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">residue_index</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Input:</span>
<span class="sd">        residue_index: B x L tensor of indices (dytpe=torch.long) mask: B x L tensor of booleans</span>

<span class="sd">    Output:</span>
<span class="sd">        pairwise_state: B x L x L x pairwise_state_dim tensor of embeddings</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">residue_index</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`residue_index` has dtype </span><span class="si">{</span><span class="n">residue_index</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, it should be `torch.long`.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">residue_index</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;`residue_index` and `mask` have inconsistent shapes: </span><span class="si">{</span><span class="n">residue_index</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="n">diff</span> <span class="o">=</span> <span class="n">residue_index</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">residue_index</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">)</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Add 1 to adjust for padding index.</span>

    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">diff</span><span class="p">[</span><span class="n">mask</span> <span class="o">==</span> <span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># noqa: E712</span>

    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents a multi-layer perceptron (MLP) used for folding residues in the ESM
(Evolutionary Scale Modeling) framework. It inherits from the nn.Module class.</p>
<p>The EsmFoldResidueMLP class implements a MLP architecture with layer normalization, dense layers, ReLU activation,
and dropout. The MLP takes an input tensor and applies a series of linear transformations to produce an output
tensor. The output tensor is then added element-wise to the input tensor, resulting in the folded residue
representation.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.embed_dim">embed_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dimensionality of the input and output tensors.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.inner_dim">inner_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dimensionality of the intermediate hidden layer in the MLP.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.dropout">dropout</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dropout probability applied after the ReLU activation. Defaults to 0.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.__init__" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes an instance of the EsmFoldResidueMLP class.</p>
<ul>
<li>embed_dim (int): The dimensionality of the input and output tensors.</li>
<li>inner_dim (int): The dimensionality of the intermediate hidden layer in the MLP.</li>
<li>dropout (float, optional): The dropout probability applied after the ReLU activation. Defaults to 0.</li>
</ul>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.forward" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Applies the MLP to the input tensor x and returns the folded residue representation.</p>
<ul>
<li>x (Tensor): The input tensor of shape (batch_size, embed_dim).</li>
</ul>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">128</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inner_dim</span> <span class="o">=</span> <span class="mi">256</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mlp</span> <span class="o">=</span> <span class="n">EsmFoldResidueMLP</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</code></pre></div>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldResidueMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a multi-layer perceptron (MLP) used for folding residues in the ESM</span>
<span class="sd">    (Evolutionary Scale Modeling) framework. It inherits from the nn.Module class.</span>

<span class="sd">    The EsmFoldResidueMLP class implements a MLP architecture with layer normalization, dense layers, ReLU activation,</span>
<span class="sd">    and dropout. The MLP takes an input tensor and applies a series of linear transformations to produce an output</span>
<span class="sd">    tensor. The output tensor is then added element-wise to the input tensor, resulting in the folded residue</span>
<span class="sd">    representation.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        embed_dim (int): The dimensionality of the input and output tensors.</span>
<span class="sd">        inner_dim (int): The dimensionality of the intermediate hidden layer in the MLP.</span>
<span class="sd">        dropout (float, optional): The dropout probability applied after the ReLU activation. Defaults to 0.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__:</span>
<span class="sd">            Initializes an instance of the EsmFoldResidueMLP class.</span>

<span class="sd">            - embed_dim (int): The dimensionality of the input and output tensors.</span>
<span class="sd">            - inner_dim (int): The dimensionality of the intermediate hidden layer in the MLP.</span>
<span class="sd">            - dropout (float, optional): The dropout probability applied after the ReLU activation. Defaults to 0.</span>

<span class="sd">        forward(self, x):</span>
<span class="sd">            Applies the MLP to the input tensor x and returns the folded residue representation.</span>

<span class="sd">            - x (Tensor): The input tensor of shape (batch_size, embed_dim).</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; embed_dim = 128</span>
<span class="sd">        &gt;&gt;&gt; inner_dim = 256</span>
<span class="sd">        &gt;&gt;&gt; dropout = 0.2</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; mlp = EsmFoldResidueMLP(embed_dim, inner_dim, dropout)</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = torch.randn(batch_size, embed_dim)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; output = mlp.forward(input_tensor)</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the EsmFoldResidueMLP class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (object): The instance of the class.</span>
<span class="sd">            embed_dim (int): The dimension of the input embeddings.</span>
<span class="sd">            inner_dim (int): The dimension of the inner layer.</span>
<span class="sd">            dropout (float, optional): The dropout probability. Defaults to 0.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If embed_dim or inner_dim is not an integer, or if dropout is not a float.</span>
<span class="sd">            ValueError: If embed_dim or inner_dim is less than or equal to 0, or if dropout is not within the range [0, 1].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs an output value by adding the input value with the result of the multi-layer perceptron (MLP) operation.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldResidueMLP): Instance of the EsmFoldResidueMLP class.</span>
<span class="sd">            x (any): Input value to be used in the forwardion process.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: The forwarded value is returned as the result of adding the input value with the MLP operation.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If the input value &#39;x&#39; is not compatible for addition with the MLP operation.</span>
<span class="sd">            ValueError: If the MLP operation encounters any unexpected issues during computation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldResidueMLP</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes the EsmFoldResidueMLP class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the input embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inner_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the inner layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability. Defaults to 0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If embed_dim or inner_dim is not an integer, or if dropout is not a float.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If embed_dim or inner_dim is less than or equal to 0, or if dropout is not within the range [0, 1].</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the EsmFoldResidueMLP class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (object): The instance of the class.</span>
<span class="sd">        embed_dim (int): The dimension of the input embeddings.</span>
<span class="sd">        inner_dim (int): The dimension of the inner layer.</span>
<span class="sd">        dropout (float, optional): The dropout probability. Defaults to 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If embed_dim or inner_dim is not an integer, or if dropout is not a float.</span>
<span class="sd">        ValueError: If embed_dim or inner_dim is less than or equal to 0, or if dropout is not within the range [0, 1].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldResidueMLP</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs an output value by adding the input value with the result of the multi-layer perceptron (MLP) operation.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Instance of the EsmFoldResidueMLP class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP">EsmFoldResidueMLP</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input value to be used in the forwardion process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>any</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The forwarded value is returned as the result of adding the input value with the MLP operation.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input value 'x' is not compatible for addition with the MLP operation.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the MLP operation encounters any unexpected issues during computation.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs an output value by adding the input value with the result of the multi-layer perceptron (MLP) operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmFoldResidueMLP): Instance of the EsmFoldResidueMLP class.</span>
<span class="sd">        x (any): Input value to be used in the forwardion process.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: The forwarded value is returned as the result of adding the input value with the MLP operation.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input value &#39;x&#39; is not compatible for addition with the MLP operation.</span>
<span class="sd">        ValueError: If the MLP operation encounters any unexpected issues during computation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents a self-attention mechanism for processing sequences, specifically designed for handling
sequences of varying lengths.
It implements a multi-head self-attention mechanism with optional gating, bias, and masking capabilities.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.embed_dim">embed_dim</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dimension of the input embedding.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.num_heads">num_heads</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The number of attention heads.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.head_width">head_width</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The width of each attention head.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.gated">gated</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Indicates whether the attention mechanism uses gating.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.proj">proj</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Linear projection layer for processing input sequences.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Linear">Linear</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.o_proj">o_proj</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Output projection layer.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Linear">Linear</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.g_proj">g_proj</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Gating projection layer (if gated is True).</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Linear">Linear</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.rescale_factor">rescale_factor</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Scaling factor for the attention weights.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.forward" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Performs self-attention on the input batch of sequences with optional mask and external pairwise bias.</p>
<p>Inputs:</p>
<ul>
<li>x (Tensor): Batch of input sequences of shape (B x L x C).</li>
<li>mask (Tensor, optional): Batch of boolean masks where 1 denotes valid positions and 0 denotes padding positions of shape (B x L_k).</li>
<li>bias (Tensor, optional): Batch of scalar pairwise attention biases of shape (B x Lq x Lk x num_heads).</li>
<li>indices (Tensor, optional): Additional indices for attention computation.</li>
</ul>
<p>Outputs:</p>
<ul>
<li>y (Tensor): Sequence projection of shape (B x L x embed_dim).</li>
<li>attention_maps (Tensor): Attention maps of shape (B x L x L x num_heads).</li>
</ul>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>Gating mechanism is applied if 'gated' is set to True.</li>
<li>The attention weights are softmax normalized.</li>
<li>The attention computation is based on the query, key, and value projections.</li>
<li>Masking is supported to handle sequences of different lengths.</li>
</ul>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a self-attention mechanism for processing sequences, specifically designed for handling</span>
<span class="sd">    sequences of varying lengths.</span>
<span class="sd">    It implements a multi-head self-attention mechanism with optional gating, bias, and masking capabilities.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        embed_dim (int): The dimension of the input embedding.</span>
<span class="sd">        num_heads (int): The number of attention heads.</span>
<span class="sd">        head_width (int): The width of each attention head.</span>
<span class="sd">        gated (bool): Indicates whether the attention mechanism uses gating.</span>
<span class="sd">        proj (nn.Linear): Linear projection layer for processing input sequences.</span>
<span class="sd">        o_proj (nn.Linear): Output projection layer.</span>
<span class="sd">        g_proj (nn.Linear): Gating projection layer (if gated is True).</span>
<span class="sd">        rescale_factor (float): Scaling factor for the attention weights.</span>

<span class="sd">    Methods:</span>
<span class="sd">        forward(self, x, mask=None, bias=None, indices=None):</span>
<span class="sd">            Performs self-attention on the input batch of sequences with optional mask and external pairwise bias.</span>

<span class="sd">            Inputs:</span>

<span class="sd">            - x (Tensor): Batch of input sequences of shape (B x L x C).</span>
<span class="sd">            - mask (Tensor, optional): Batch of boolean masks where 1 denotes valid positions and 0 denotes padding positions of shape (B x L_k).</span>
<span class="sd">            - bias (Tensor, optional): Batch of scalar pairwise attention biases of shape (B x Lq x Lk x num_heads).</span>
<span class="sd">            - indices (Tensor, optional): Additional indices for attention computation.</span>

<span class="sd">            Outputs:</span>

<span class="sd">            - y (Tensor): Sequence projection of shape (B x L x embed_dim).</span>
<span class="sd">            - attention_maps (Tensor): Attention maps of shape (B x L x L x num_heads).</span>

<span class="sd">    Note:</span>
<span class="sd">        - Gating mechanism is applied if &#39;gated&#39; is set to True.</span>
<span class="sd">        - The attention weights are softmax normalized.</span>
<span class="sd">        - The attention computation is based on the query, key, and value projections.</span>
<span class="sd">        - Masking is supported to handle sequences of different lengths.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_width</span><span class="p">,</span> <span class="n">gated</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the EsmFoldSelfAttention class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            embed_dim (int): The dimension of the input embeddings.</span>
<span class="sd">            num_heads (int): The number of attention heads.</span>
<span class="sd">            head_width (int): The width of each attention head.</span>
<span class="sd">            gated (bool, optional): Specifies whether the attention mechanism is gated. Defaults to False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            AssertionError: If embed_dim is not equal to the product of num_heads and head_width.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_width</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_width</span> <span class="o">=</span> <span class="n">head_width</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gated</span> <span class="o">=</span> <span class="n">gated</span>
        <span class="k">if</span> <span class="n">gated</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rescale_factor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_width</span><span class="o">**-</span><span class="mf">0.5</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Basic self attention with optional mask and external pairwise bias. To handle sequences of different lengths,</span>
<span class="sd">        use mask.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            x: batch of input sequneces (.. x L x C) mask: batch of boolean masks where 1=valid, 0=padding position (..</span>
<span class="sd">                x L_k) bias: batch of scalar pairwise attention biases (.. x Lq x Lk x num_heads)</span>

<span class="sd">        Outputs:</span>
<span class="sd">            sequence projection (B x L x embed_dim), attention maps (B x L x L x num_heads)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rescale_factor</span> <span class="o">*</span> <span class="n">q</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;...qc, ...kc -&gt; ...qk&quot;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

        <span class="c1"># Add external attention bias.</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">bias</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Do not attend to padding tokens.</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="kc">False</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>

        <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;...hqk,...hkc-&gt;...qhc&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gated</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="o">*</span> <span class="n">y</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldSelfAttention</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_width</span><span class="p">,</span> <span class="n">gated</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes the EsmFoldSelfAttention class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the input embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_width</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The width of each attention head.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>gated</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Specifies whether the attention mechanism is gated. Defaults to False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AssertionError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If embed_dim is not equal to the product of num_heads and head_width.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_width</span><span class="p">,</span> <span class="n">gated</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the EsmFoldSelfAttention class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        embed_dim (int): The dimension of the input embeddings.</span>
<span class="sd">        num_heads (int): The number of attention heads.</span>
<span class="sd">        head_width (int): The width of each attention head.</span>
<span class="sd">        gated (bool, optional): Specifies whether the attention mechanism is gated. Defaults to False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AssertionError: If embed_dim is not equal to the product of num_heads and head_width.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_width</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_width</span> <span class="o">=</span> <span class="n">head_width</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gated</span> <span class="o">=</span> <span class="n">gated</span>
    <span class="k">if</span> <span class="n">gated</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">rescale_factor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_width</span><span class="o">**-</span><span class="mf">0.5</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldSelfAttention</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Basic self attention with optional mask and external pairwise bias. To handle sequences of different lengths,
use mask.</p>


<details class="inputs" open>
  <summary>Inputs</summary>
  
</details>

<details class="outputs" open>
  <summary>Outputs</summary>
  <p>sequence projection (B x L x embed_dim), attention maps (B x L x L x num_heads)</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Basic self attention with optional mask and external pairwise bias. To handle sequences of different lengths,</span>
<span class="sd">    use mask.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        x: batch of input sequneces (.. x L x C) mask: batch of boolean masks where 1=valid, 0=padding position (..</span>
<span class="sd">            x L_k) bias: batch of scalar pairwise attention biases (.. x Lq x Lk x num_heads)</span>

<span class="sd">    Outputs:</span>
<span class="sd">        sequence projection (B x L x embed_dim), attention maps (B x L x L x num_heads)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rescale_factor</span> <span class="o">*</span> <span class="n">q</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;...qc, ...kc -&gt; ...qk&quot;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

    <span class="c1"># Add external attention bias.</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">bias</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Do not attend to padding tokens.</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="kc">False</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;...hqk,...hkc-&gt;...qhc&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gated</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">g_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="o">*</span> <span class="n">y</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents a neural network model for transforming sequence states into pairwise states
using an attention mechanism.</p>
<p>This class inherits from nn.Module and includes methods for initialization and forwarding the pairwise states
from sequence states.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.layernorm">layernorm</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A layer normalization module for normalizing the sequence state dimensions.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.LayerNorm">LayerNorm</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.proj">proj</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A fully connected layer for projecting the sequence state into an inner dimension space.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Linear">Linear</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.o_proj">o_proj</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A fully connected layer for projecting the inner dimension space into pairwise state dimensions.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Linear">Linear</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.__init__" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the EsmFoldSequenceToPair instance with the specified dimensions.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.forward" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Transforms the input sequence state tensor into pairwise state tensor.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sequence_state_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the input sequence state.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inner_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the inner representation used in the transformation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pairwise_state_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the output pairwise state.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="inputs" open>
  <summary>Inputs</summary>
  <p>sequence_state (Tensor): Input sequence state tensor with shape B x L x sequence_state_dim.</p>
</details>

<details class="output" open>
  <summary>Output</summary>
  <p>pairwise_state (Tensor): Output pairwise state tensor with shape B x L x L x pairwise_state_dim.</p>
</details>

<details class="intermediate-state" open>
  <summary>Intermediate state</summary>
  <p>Intermediate state tensor with shape B x L x L x 2*inner_dim, used during the transformation process.</p>
</details>

<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>Tensor</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Pairwise state tensor representing the relationships between elements in the input sequence state.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AssertionError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input sequence state tensor does not have the expected shape.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldSequenceToPair</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a neural network model for transforming sequence states into pairwise states</span>
<span class="sd">    using an attention mechanism.</span>

<span class="sd">    This class inherits from nn.Module and includes methods for initialization and forwarding the pairwise states</span>
<span class="sd">    from sequence states.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        layernorm (nn.LayerNorm): A layer normalization module for normalizing the sequence state dimensions.</span>
<span class="sd">        proj (nn.Linear): A fully connected layer for projecting the sequence state into an inner dimension space.</span>
<span class="sd">        o_proj (nn.Linear): A fully connected layer for projecting the inner dimension space into pairwise state dimensions.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the EsmFoldSequenceToPair instance with the specified dimensions.</span>

<span class="sd">        forward: Transforms the input sequence state tensor into pairwise state tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        sequence_state_dim (int): Dimension of the input sequence state.</span>
<span class="sd">        inner_dim (int): Dimension of the inner representation used in the transformation.</span>
<span class="sd">        pairwise_state_dim (int): Dimension of the output pairwise state.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        sequence_state (Tensor): Input sequence state tensor with shape B x L x sequence_state_dim.</span>

<span class="sd">    Output:</span>
<span class="sd">        pairwise_state (Tensor): Output pairwise state tensor with shape B x L x L x pairwise_state_dim.</span>

<span class="sd">    Intermediate state:</span>
<span class="sd">        Intermediate state tensor with shape B x L x L x 2*inner_dim, used during the transformation process.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Pairwise state tensor representing the relationships between elements in the input sequence state.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AssertionError: If the input sequence state tensor does not have the expected shape.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence_state_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">pairwise_state_dim</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the EsmFoldSequenceToPair class.</span>

<span class="sd">        Args:</span>
<span class="sd">            sequence_state_dim (int): The dimension of the input sequence state.</span>
<span class="sd">            inner_dim (int): The inner dimension used for projection.</span>
<span class="sd">            pairwise_state_dim (int): The dimension of the pairwise state.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">sequence_state_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">sequence_state_dim</span><span class="p">,</span> <span class="n">inner_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            sequence_state: B x L x sequence_state_dim</span>

<span class="sd">        Output:</span>
<span class="sd">            pairwise_state: B x L x L x pairwise_state_dim</span>

<span class="sd">        Intermediate state:</span>
<span class="sd">          B x L x L x 2*inner_dim</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>

        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">sequence_state</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">prod</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">prod</span><span class="p">,</span> <span class="n">diff</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldSequenceToPair</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">sequence_state_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">pairwise_state_dim</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes the EsmFoldSequenceToPair class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sequence_state_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the input sequence state.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inner_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The inner dimension used for projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pairwise_state_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the pairwise state.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence_state_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">pairwise_state_dim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the EsmFoldSequenceToPair class.</span>

<span class="sd">    Args:</span>
<span class="sd">        sequence_state_dim (int): The dimension of the input sequence state.</span>
<span class="sd">        inner_dim (int): The inner dimension used for projection.</span>
<span class="sd">        pairwise_state_dim (int): The dimension of the pairwise state.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">sequence_state_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">sequence_state_dim</span><span class="p">,</span> <span class="n">inner_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldSequenceToPair</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">sequence_state</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<details class="inputs" open>
  <summary>Inputs</summary>
  
</details>

<details class="output" open>
  <summary>Output</summary>
  
</details>

<details class="intermediate-state" open>
  <summary>Intermediate state</summary>
  <p>B x L x L x 2*inner_dim</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inputs:</span>
<span class="sd">        sequence_state: B x L x sequence_state_dim</span>

<span class="sd">    Output:</span>
<span class="sd">        pairwise_state: B x L x L x pairwise_state_dim</span>

<span class="sd">    Intermediate state:</span>
<span class="sd">      B x L x L x 2*inner_dim</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>

    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">sequence_state</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">prod</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">prod</span><span class="p">,</span> <span class="n">diff</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>The EsmFoldStructureModule class represents a module for predicting protein structure using Evolutionary Structure
Model (ESM) and folding techniques. It inherits from the nn.Module class.</p>
<p>The class includes methods for initializing the module, forwarding the protein structure prediction, and
converting torsion angles to frames and literature positions to atom14 positions.
The forward method takes evolutionary formers' output, amino acid indices, and optional sequence mask as input and
returns a dictionary of predicted outputs. The _init_residue_constants method initializes constants used
in the module for calculating torsion angles to frames and literature positions to atom14 positions.</p>
<p>The class also includes the code for initializing the default frames, group indices, atom masks, and literature
positions, and for converting torsion angles to frames and frames and literature positions to atom14 positions.</p>
<p>Please note that the detailed implementation and usage of the class methods are described in the code provided.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2427</span>
<span class="normal">2428</span>
<span class="normal">2429</span>
<span class="normal">2430</span>
<span class="normal">2431</span>
<span class="normal">2432</span>
<span class="normal">2433</span>
<span class="normal">2434</span>
<span class="normal">2435</span>
<span class="normal">2436</span>
<span class="normal">2437</span>
<span class="normal">2438</span>
<span class="normal">2439</span>
<span class="normal">2440</span>
<span class="normal">2441</span>
<span class="normal">2442</span>
<span class="normal">2443</span>
<span class="normal">2444</span>
<span class="normal">2445</span>
<span class="normal">2446</span>
<span class="normal">2447</span>
<span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span>
<span class="normal">2485</span>
<span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span>
<span class="normal">2559</span>
<span class="normal">2560</span>
<span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span>
<span class="normal">2569</span>
<span class="normal">2570</span>
<span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span>
<span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldStructureModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The EsmFoldStructureModule class represents a module for predicting protein structure using Evolutionary Structure</span>
<span class="sd">    Model (ESM) and folding techniques. It inherits from the nn.Module class.</span>

<span class="sd">    The class includes methods for initializing the module, forwarding the protein structure prediction, and</span>
<span class="sd">    converting torsion angles to frames and literature positions to atom14 positions.</span>
<span class="sd">    The forward method takes evolutionary formers&#39; output, amino acid indices, and optional sequence mask as input and</span>
<span class="sd">    returns a dictionary of predicted outputs. The _init_residue_constants method initializes constants used</span>
<span class="sd">    in the module for calculating torsion angles to frames and literature positions to atom14 positions.</span>

<span class="sd">    The class also includes the code for initializing the default frames, group indices, atom masks, and literature</span>
<span class="sd">    positions, and for converting torsion angles to frames and frames and literature positions to atom14 positions.</span>

<span class="sd">    Please note that the detailed implementation and usage of the class methods are described in the code provided.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes an instance of the EsmFoldStructureModule class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldStructureModule): The instance of the class itself.</span>
<span class="sd">            config:</span>
<span class="sd">                A configuration object containing parameters for initializing the module.</span>

<span class="sd">                - Type: Custom configuration object</span>
<span class="sd">                - Purpose: Stores various configuration parameters for the module.</span>
<span class="sd">                - Restrictions: Must be a valid configuration object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="c1"># Buffers to be lazily initialized later</span>
        <span class="c1"># self.default_frames</span>
        <span class="c1"># self.group_idx</span>
        <span class="c1"># self.atom_mask</span>
        <span class="c1"># self.lit_positions</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_s</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_z</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">pairwise_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_in</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ipa</span> <span class="o">=</span> <span class="n">EsmFoldInvariantPointAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ipa_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_ipa</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">transition</span> <span class="o">=</span> <span class="n">EsmFoldStructureModuleTransition</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bb_update</span> <span class="o">=</span> <span class="n">EsmFoldBackboneUpdate</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">angle_resnet</span> <span class="o">=</span> <span class="n">EsmFoldAngleResnet</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">evoformer_output_dict</span><span class="p">,</span>
        <span class="n">aatype</span><span class="p">,</span>
        <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_offload_inference</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            evoformer_output_dict:</span>
<span class="sd">                Dictionary containing:</span>

<span class="sd">                - &quot;single&quot;: [*, N_res, C_s] single representation</span>
<span class="sd">                - &quot;pair&quot;: [*, N_res, N_res, C_z] pair representation</span>
<span class="sd">            aatype:</span>
<span class="sd">                [*, N_res] amino acid indices</span>
<span class="sd">            mask:</span>
<span class="sd">                Optional [*, N_res] sequence mask</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of outputs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">evoformer_output_dict</span><span class="p">[</span><span class="s2">&quot;single&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># [*, N]</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># [*, N, C_s]</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_s</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="c1"># [*, N, N, C_z]</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_z</span><span class="p">(</span><span class="n">evoformer_output_dict</span><span class="p">[</span><span class="s2">&quot;pair&quot;</span><span class="p">])</span>

        <span class="n">z_reference_list</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">_offload_inference</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">sys</span><span class="o">.</span><span class="n">getrefcount</span><span class="p">(</span><span class="n">evoformer_output_dict</span><span class="p">[</span><span class="s2">&quot;pair&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="n">evoformer_output_dict</span><span class="p">[</span><span class="s2">&quot;pair&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">evoformer_output_dict</span><span class="p">[</span><span class="s2">&quot;pair&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="n">z_reference_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">z</span><span class="p">]</span>
            <span class="n">z</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># [*, N, C_s]</span>
        <span class="n">s_initial</span> <span class="o">=</span> <span class="n">s</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_in</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="c1"># [*, N]</span>
        <span class="n">rigids</span> <span class="o">=</span> <span class="n">Rigid</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span>
            <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">s</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;quat&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_blocks</span><span class="p">):</span>
            <span class="c1"># [*, N, C_s]</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ipa</span><span class="p">(</span>
                <span class="n">s</span><span class="p">,</span>
                <span class="n">z</span><span class="p">,</span>
                <span class="n">rigids</span><span class="p">,</span>
                <span class="n">mask</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ipa_dropout</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_ipa</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transition</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

            <span class="c1"># [*, N]</span>
            <span class="n">rigids</span> <span class="o">=</span> <span class="n">rigids</span><span class="o">.</span><span class="n">compose_q_update_vec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bb_update</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>

            <span class="c1"># To hew as closely as possible to AlphaFold, we convert our</span>
            <span class="c1"># quaternion-based transformations to rotation-matrix ones</span>
            <span class="c1"># here</span>
            <span class="n">backb_to_global</span> <span class="o">=</span> <span class="n">Rigid</span><span class="p">(</span>
                <span class="n">Rotation</span><span class="p">(</span><span class="n">rot_mats</span><span class="o">=</span><span class="n">rigids</span><span class="o">.</span><span class="n">get_rots</span><span class="p">()</span><span class="o">.</span><span class="n">get_rot_mats</span><span class="p">(),</span> <span class="n">quats</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
                <span class="n">rigids</span><span class="o">.</span><span class="n">get_trans</span><span class="p">(),</span>
            <span class="p">)</span>

            <span class="n">backb_to_global</span> <span class="o">=</span> <span class="n">backb_to_global</span><span class="o">.</span><span class="n">scale_translation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">trans_scale_factor</span><span class="p">)</span>

            <span class="c1"># [*, N, 7, 2]</span>
            <span class="n">unnormalized_angles</span><span class="p">,</span> <span class="n">angles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">angle_resnet</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s_initial</span><span class="p">)</span>

            <span class="n">all_frames_to_global</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">torsion_angles_to_frames</span><span class="p">(</span><span class="n">backb_to_global</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="n">aatype</span><span class="p">)</span>

            <span class="n">pred_xyz</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">frames_and_literature_positions_to_atom14_pos</span><span class="p">(</span><span class="n">all_frames_to_global</span><span class="p">,</span> <span class="n">aatype</span><span class="p">)</span>

            <span class="n">scaled_rigids</span> <span class="o">=</span> <span class="n">rigids</span><span class="o">.</span><span class="n">scale_translation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">trans_scale_factor</span><span class="p">)</span>

            <span class="n">preds</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;frames&quot;</span><span class="p">:</span> <span class="n">scaled_rigids</span><span class="o">.</span><span class="n">to_tensor_7</span><span class="p">(),</span>
                <span class="s2">&quot;sidechain_frames&quot;</span><span class="p">:</span> <span class="n">all_frames_to_global</span><span class="o">.</span><span class="n">to_tensor_4x4</span><span class="p">(),</span>
                <span class="s2">&quot;unnormalized_angles&quot;</span><span class="p">:</span> <span class="n">unnormalized_angles</span><span class="p">,</span>
                <span class="s2">&quot;angles&quot;</span><span class="p">:</span> <span class="n">angles</span><span class="p">,</span>
                <span class="s2">&quot;positions&quot;</span><span class="p">:</span> <span class="n">pred_xyz</span><span class="p">,</span>
                <span class="s2">&quot;states&quot;</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span>
            <span class="p">}</span>

            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>

            <span class="n">rigids</span> <span class="o">=</span> <span class="n">rigids</span><span class="o">.</span><span class="n">stop_rot_gradient</span><span class="p">()</span>

        <span class="k">del</span> <span class="n">z</span><span class="p">,</span> <span class="n">z_reference_list</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">dict_multimap</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;single&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

        <span class="k">return</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">_init_residue_constants</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">float_dtype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the residue constants required for EsmFoldStructureModule.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldStructureModule): An instance of the EsmFoldStructureModule class.</span>
<span class="sd">            float_dtype (dtype): The data type of the floating point values.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>

<span class="sd">        Description:</span>
<span class="sd">            This method initializes the following residue constants:</span>

<span class="sd">            - default_frames: A tensor containing the default frames for rigid groups.</span>
<span class="sd">            If not already initialized, it is created using the &#39;restype_rigid_group_default_frame&#39; constant and the</span>
<span class="sd">            provided float_dtype.</span>
<span class="sd">            - group_idx: A tensor mapping atom14 indices to rigid group indices.</span>
<span class="sd">            If not already initialized, it is created using the &#39;restype_atom14_to_rigid_group&#39; constant.</span>
<span class="sd">            - atom_mask: A tensor containing the atom14 mask.</span>
<span class="sd">            If not already initialized, it is created using the &#39;restype_atom14_mask&#39; constant and the provided</span>
<span class="sd">            float_dtype.</span>
<span class="sd">            - lit_positions: A tensor containing the positions of atom14 rigid groups.</span>
<span class="sd">            If not already initialized, it is created using the &#39;restype_atom14_rigid_group_positions&#39; constant and</span>
<span class="sd">            the provided float_dtype.</span>

<span class="sd">        Note:</span>
<span class="sd">            - This method should be called before using any other functionality of the EsmFoldStructureModule class.</span>
<span class="sd">            - The &#39;float_dtype&#39; parameter determines the precision of the floating point values used in the tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;default_frames&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">default_frames</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="n">residue_constants</span><span class="o">.</span><span class="n">restype_rigid_group_default_frame</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">float_dtype</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;group_idx&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">group_idx</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="n">residue_constants</span><span class="o">.</span><span class="n">restype_atom14_to_rigid_group</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;atom_mask&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">atom_mask</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="n">residue_constants</span><span class="o">.</span><span class="n">restype_atom14_mask</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">float_dtype</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;lit_positions&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lit_positions</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="n">residue_constants</span><span class="o">.</span><span class="n">restype_atom14_rigid_group_positions</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">float_dtype</span><span class="p">,</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">torsion_angles_to_frames</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts torsion angles to frames using the given parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldStructureModule): The instance of the EsmFoldStructureModule class.</span>
<span class="sd">            r (numpy.ndarray): The input array of shape (N, 3) containing the residue atoms&#39; coordinates in angstroms.</span>
<span class="sd">            alpha (numpy.ndarray): The input array of shape (N, 3) containing the residue angles in radians.</span>
<span class="sd">            f (numpy.ndarray): The input array of shape (N, 3, 3) containing the reference frames.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the input arrays have incompatible shapes or types.</span>
<span class="sd">            TypeError: If the input parameters are not of the expected types.</span>
<span class="sd">            RuntimeError: If an unexpected error occurs during the conversion process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Lazily initialize the residue constants on the correct device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_residue_constants</span><span class="p">(</span><span class="n">alpha</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># Separated purely to make testing less annoying</span>
        <span class="k">return</span> <span class="n">torsion_angles_to_frames</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_frames</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">frames_and_literature_positions_to_atom14_pos</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>  <span class="c1"># [*, N, 8]  # [*, N]</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts frames and literature positions to atom14 positions.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldStructureModule): The instance of the EsmFoldStructureModule class.</span>
<span class="sd">            r (object): The &#39;r&#39; parameter representing some variable.</span>
<span class="sd">            f (object): The &#39;f&#39; parameter representing some variable.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Lazily initialize the residue constants on the correct device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_residue_constants</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">get_rots</span><span class="p">()</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">frames_and_literature_positions_to_atom14_pos</span><span class="p">(</span>
            <span class="n">r</span><span class="p">,</span>
            <span class="n">f</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">default_frames</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">group_idx</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">atom_mask</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lit_positions</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldStructureModule</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmFoldStructureModule class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class itself.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule">EsmFoldStructureModule</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A configuration object containing parameters for initializing the module.</p>
<ul>
<li>Type: Custom configuration object</li>
<li>Purpose: Stores various configuration parameters for the module.</li>
<li>Restrictions: Must be a valid configuration object.</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2444</span>
<span class="normal">2445</span>
<span class="normal">2446</span>
<span class="normal">2447</span>
<span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Initializes an instance of the EsmFoldStructureModule class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmFoldStructureModule): The instance of the class itself.</span>
<span class="sd">        config:</span>
<span class="sd">            A configuration object containing parameters for initializing the module.</span>

<span class="sd">            - Type: Custom configuration object</span>
<span class="sd">            - Purpose: Stores various configuration parameters for the module.</span>
<span class="sd">            - Restrictions: Must be a valid configuration object.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="c1"># Buffers to be lazily initialized later</span>
    <span class="c1"># self.default_frames</span>
    <span class="c1"># self.group_idx</span>
    <span class="c1"># self.atom_mask</span>
    <span class="c1"># self.lit_positions</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_s</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_z</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">pairwise_dim</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">linear_in</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">ipa</span> <span class="o">=</span> <span class="n">EsmFoldInvariantPointAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">ipa_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_ipa</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">transition</span> <span class="o">=</span> <span class="n">EsmFoldStructureModuleTransition</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bb_update</span> <span class="o">=</span> <span class="n">EsmFoldBackboneUpdate</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">angle_resnet</span> <span class="o">=</span> <span class="n">EsmFoldAngleResnet</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldStructureModule</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">evoformer_output_dict</span><span class="p">,</span> <span class="n">aatype</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_offload_inference</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>evoformer_output_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dictionary containing:</p>
<ul>
<li>"single": [*, N_res, C_s] single representation</li>
<li>"pair": [*, N_res, N_res, C_z] pair representation</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>aatype</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, N_res] amino acid indices</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional [*, N_res] sequence mask</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A dictionary of outputs</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span>
<span class="normal">2559</span>
<span class="normal">2560</span>
<span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span>
<span class="normal">2569</span>
<span class="normal">2570</span>
<span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">evoformer_output_dict</span><span class="p">,</span>
    <span class="n">aatype</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_offload_inference</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Args:</span>
<span class="sd">        evoformer_output_dict:</span>
<span class="sd">            Dictionary containing:</span>

<span class="sd">            - &quot;single&quot;: [*, N_res, C_s] single representation</span>
<span class="sd">            - &quot;pair&quot;: [*, N_res, N_res, C_z] pair representation</span>
<span class="sd">        aatype:</span>
<span class="sd">            [*, N_res] amino acid indices</span>
<span class="sd">        mask:</span>
<span class="sd">            Optional [*, N_res] sequence mask</span>

<span class="sd">    Returns:</span>
<span class="sd">        A dictionary of outputs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">evoformer_output_dict</span><span class="p">[</span><span class="s2">&quot;single&quot;</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># [*, N]</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># [*, N, C_s]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_s</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># [*, N, N, C_z]</span>
    <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_z</span><span class="p">(</span><span class="n">evoformer_output_dict</span><span class="p">[</span><span class="s2">&quot;pair&quot;</span><span class="p">])</span>

    <span class="n">z_reference_list</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">_offload_inference</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">sys</span><span class="o">.</span><span class="n">getrefcount</span><span class="p">(</span><span class="n">evoformer_output_dict</span><span class="p">[</span><span class="s2">&quot;pair&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="n">evoformer_output_dict</span><span class="p">[</span><span class="s2">&quot;pair&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">evoformer_output_dict</span><span class="p">[</span><span class="s2">&quot;pair&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="n">z_reference_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">z</span><span class="p">]</span>
        <span class="n">z</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># [*, N, C_s]</span>
    <span class="n">s_initial</span> <span class="o">=</span> <span class="n">s</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_in</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># [*, N]</span>
    <span class="n">rigids</span> <span class="o">=</span> <span class="n">Rigid</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span>
        <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">s</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;quat&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_blocks</span><span class="p">):</span>
        <span class="c1"># [*, N, C_s]</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ipa</span><span class="p">(</span>
            <span class="n">s</span><span class="p">,</span>
            <span class="n">z</span><span class="p">,</span>
            <span class="n">rigids</span><span class="p">,</span>
            <span class="n">mask</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ipa_dropout</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_ipa</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transition</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="c1"># [*, N]</span>
        <span class="n">rigids</span> <span class="o">=</span> <span class="n">rigids</span><span class="o">.</span><span class="n">compose_q_update_vec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bb_update</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>

        <span class="c1"># To hew as closely as possible to AlphaFold, we convert our</span>
        <span class="c1"># quaternion-based transformations to rotation-matrix ones</span>
        <span class="c1"># here</span>
        <span class="n">backb_to_global</span> <span class="o">=</span> <span class="n">Rigid</span><span class="p">(</span>
            <span class="n">Rotation</span><span class="p">(</span><span class="n">rot_mats</span><span class="o">=</span><span class="n">rigids</span><span class="o">.</span><span class="n">get_rots</span><span class="p">()</span><span class="o">.</span><span class="n">get_rot_mats</span><span class="p">(),</span> <span class="n">quats</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
            <span class="n">rigids</span><span class="o">.</span><span class="n">get_trans</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="n">backb_to_global</span> <span class="o">=</span> <span class="n">backb_to_global</span><span class="o">.</span><span class="n">scale_translation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">trans_scale_factor</span><span class="p">)</span>

        <span class="c1"># [*, N, 7, 2]</span>
        <span class="n">unnormalized_angles</span><span class="p">,</span> <span class="n">angles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">angle_resnet</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s_initial</span><span class="p">)</span>

        <span class="n">all_frames_to_global</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">torsion_angles_to_frames</span><span class="p">(</span><span class="n">backb_to_global</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="n">aatype</span><span class="p">)</span>

        <span class="n">pred_xyz</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">frames_and_literature_positions_to_atom14_pos</span><span class="p">(</span><span class="n">all_frames_to_global</span><span class="p">,</span> <span class="n">aatype</span><span class="p">)</span>

        <span class="n">scaled_rigids</span> <span class="o">=</span> <span class="n">rigids</span><span class="o">.</span><span class="n">scale_translation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">trans_scale_factor</span><span class="p">)</span>

        <span class="n">preds</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;frames&quot;</span><span class="p">:</span> <span class="n">scaled_rigids</span><span class="o">.</span><span class="n">to_tensor_7</span><span class="p">(),</span>
            <span class="s2">&quot;sidechain_frames&quot;</span><span class="p">:</span> <span class="n">all_frames_to_global</span><span class="o">.</span><span class="n">to_tensor_4x4</span><span class="p">(),</span>
            <span class="s2">&quot;unnormalized_angles&quot;</span><span class="p">:</span> <span class="n">unnormalized_angles</span><span class="p">,</span>
            <span class="s2">&quot;angles&quot;</span><span class="p">:</span> <span class="n">angles</span><span class="p">,</span>
            <span class="s2">&quot;positions&quot;</span><span class="p">:</span> <span class="n">pred_xyz</span><span class="p">,</span>
            <span class="s2">&quot;states&quot;</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>

        <span class="n">rigids</span> <span class="o">=</span> <span class="n">rigids</span><span class="o">.</span><span class="n">stop_rot_gradient</span><span class="p">()</span>

    <span class="k">del</span> <span class="n">z</span><span class="p">,</span> <span class="n">z_reference_list</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">dict_multimap</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
    <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;single&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.frames_and_literature_positions_to_atom14_pos" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldStructureModule</span><span class="o">.</span><span class="n">frames_and_literature_positions_to_atom14_pos</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.frames_and_literature_positions_to_atom14_pos" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Converts frames and literature positions to atom14 positions.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmFoldStructureModule class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule">EsmFoldStructureModule</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>r</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The 'r' parameter representing some variable.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>f</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The 'f' parameter representing some variable.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span>
<span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">frames_and_literature_positions_to_atom14_pos</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>  <span class="c1"># [*, N, 8]  # [*, N]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts frames and literature positions to atom14 positions.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmFoldStructureModule): The instance of the EsmFoldStructureModule class.</span>
<span class="sd">        r (object): The &#39;r&#39; parameter representing some variable.</span>
<span class="sd">        f (object): The &#39;f&#39; parameter representing some variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Lazily initialize the residue constants on the correct device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_init_residue_constants</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">get_rots</span><span class="p">()</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">frames_and_literature_positions_to_atom14_pos</span><span class="p">(</span>
        <span class="n">r</span><span class="p">,</span>
        <span class="n">f</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">default_frames</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group_idx</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">atom_mask</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lit_positions</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.torsion_angles_to_frames" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldStructureModule</span><span class="o">.</span><span class="n">torsion_angles_to_frames</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule.torsion_angles_to_frames" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Converts torsion angles to frames using the given parameters.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmFoldStructureModule class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModule">EsmFoldStructureModule</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>r</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input array of shape (N, 3) containing the residue atoms' coordinates in angstroms.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="numpy.ndarray">ndarray</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>alpha</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input array of shape (N, 3) containing the residue angles in radians.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="numpy.ndarray">ndarray</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>f</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input array of shape (N, 3, 3) containing the reference frames.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="numpy.ndarray">ndarray</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input arrays have incompatible shapes or types.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input parameters are not of the expected types.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If an unexpected error occurs during the conversion process.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">torsion_angles_to_frames</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts torsion angles to frames using the given parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmFoldStructureModule): The instance of the EsmFoldStructureModule class.</span>
<span class="sd">        r (numpy.ndarray): The input array of shape (N, 3) containing the residue atoms&#39; coordinates in angstroms.</span>
<span class="sd">        alpha (numpy.ndarray): The input array of shape (N, 3) containing the residue angles in radians.</span>
<span class="sd">        f (numpy.ndarray): The input array of shape (N, 3, 3) containing the reference frames.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the input arrays have incompatible shapes or types.</span>
<span class="sd">        TypeError: If the input parameters are not of the expected types.</span>
<span class="sd">        RuntimeError: If an unexpected error occurs during the conversion process.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Lazily initialize the residue constants on the correct device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_init_residue_constants</span><span class="p">(</span><span class="n">alpha</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># Separated purely to make testing less annoying</span>
    <span class="k">return</span> <span class="n">torsion_angles_to_frames</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_frames</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>The EsmFoldStructureModuleTransition class represents a module for transitioning the fold structure in a neural network.
This class inherits from the nn.Module class and is used to forward transition layers for the fold structure module.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.config">config</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A configuration object containing parameters for the module.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.layers">layers</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A CellList containing the transition layers for the module.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.dropout">dropout</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A dropout layer with a specified dropout rate.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.layer_norm">layer_norm</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A layer normalization layer for normalizing the output.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.__init__" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the EsmFoldStructureModuleTransition with the given configuration.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.forward" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the transition layers for the fold structure module using the input s.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span>
<span class="normal">2401</span>
<span class="normal">2402</span>
<span class="normal">2403</span>
<span class="normal">2404</span>
<span class="normal">2405</span>
<span class="normal">2406</span>
<span class="normal">2407</span>
<span class="normal">2408</span>
<span class="normal">2409</span>
<span class="normal">2410</span>
<span class="normal">2411</span>
<span class="normal">2412</span>
<span class="normal">2413</span>
<span class="normal">2414</span>
<span class="normal">2415</span>
<span class="normal">2416</span>
<span class="normal">2417</span>
<span class="normal">2418</span>
<span class="normal">2419</span>
<span class="normal">2420</span>
<span class="normal">2421</span>
<span class="normal">2422</span>
<span class="normal">2423</span>
<span class="normal">2424</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldStructureModuleTransition</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The EsmFoldStructureModuleTransition class represents a module for transitioning the fold structure in a neural network.</span>
<span class="sd">    This class inherits from the nn.Module class and is used to forward transition layers for the fold structure module.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        config: A configuration object containing parameters for the module.</span>
<span class="sd">        layers: A CellList containing the transition layers for the module.</span>
<span class="sd">        dropout: A dropout layer with a specified dropout rate.</span>
<span class="sd">        layer_norm: A layer normalization layer for normalizing the output.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the EsmFoldStructureModuleTransition with the given configuration.</span>
<span class="sd">        forward: Constructs the transition layers for the fold structure module using the input s.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the EsmFoldStructureModuleTransition class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config: An object of type &#39;Config&#39; that holds the configuration settings for the module.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_transition_layers</span><span class="p">):</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">EsmFoldStructureModuleTransitionLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the EsmFoldStructureModuleTransition.</span>

<span class="sd">        This method takes in two parameters: self and s.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldStructureModuleTransition): An instance of the EsmFoldStructureModuleTransition class.</span>
<span class="sd">            s (unknown type): The input data.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">s</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldStructureModuleTransition</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmFoldStructureModuleTransition class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An object of type 'Config' that holds the configuration settings for the module.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the EsmFoldStructureModuleTransition class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config: An object of type &#39;Config&#39; that holds the configuration settings for the module.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_transition_layers</span><span class="p">):</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">EsmFoldStructureModuleTransitionLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldStructureModuleTransition</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">s</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the EsmFoldStructureModuleTransition.</p>
<p>This method takes in two parameters: self and s.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the EsmFoldStructureModuleTransition class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransition">EsmFoldStructureModuleTransition</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input data.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>unknown type</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2402</span>
<span class="normal">2403</span>
<span class="normal">2404</span>
<span class="normal">2405</span>
<span class="normal">2406</span>
<span class="normal">2407</span>
<span class="normal">2408</span>
<span class="normal">2409</span>
<span class="normal">2410</span>
<span class="normal">2411</span>
<span class="normal">2412</span>
<span class="normal">2413</span>
<span class="normal">2414</span>
<span class="normal">2415</span>
<span class="normal">2416</span>
<span class="normal">2417</span>
<span class="normal">2418</span>
<span class="normal">2419</span>
<span class="normal">2420</span>
<span class="normal">2421</span>
<span class="normal">2422</span>
<span class="normal">2423</span>
<span class="normal">2424</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the EsmFoldStructureModuleTransition.</span>

<span class="sd">    This method takes in two parameters: self and s.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmFoldStructureModuleTransition): An instance of the EsmFoldStructureModuleTransition class.</span>
<span class="sd">        s (unknown type): The input data.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">s</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>EsmFoldStructureModuleTransitionLayer</p>
<p>Represents a transition layer for the EsmFold structure module, inheriting from nn.Module.</p>
<p>This class initializes with the provided configuration and forwards a transition layer for the EsmFold structure
module using the specified linear layers and activation functions.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.linear_1">linear_1</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The first linear layer for the transition.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear">EsmFoldLinear</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.linear_2">linear_2</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The second linear layer for the transition.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear">EsmFoldLinear</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.linear_3">linear_3</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The third linear layer for the transition.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldLinear">EsmFoldLinear</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.relu">relu</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The rectified linear unit activation function.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.ReLU">ReLU</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.forward" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the transition layer for the EsmFold structure module using the input tensor 's'.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The output tensor after applying the transition layer to the input tensor 's'.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldStructureModuleTransitionLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    EsmFoldStructureModuleTransitionLayer</span>

<span class="sd">    Represents a transition layer for the EsmFold structure module, inheriting from nn.Module.</span>

<span class="sd">    This class initializes with the provided configuration and forwards a transition layer for the EsmFold structure</span>
<span class="sd">    module using the specified linear layers and activation functions.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        linear_1 (EsmFoldLinear): The first linear layer for the transition.</span>
<span class="sd">        linear_2 (EsmFoldLinear): The second linear layer for the transition.</span>
<span class="sd">        linear_3 (EsmFoldLinear): The third linear layer for the transition.</span>
<span class="sd">        relu (nn.ReLU): The rectified linear unit activation function.</span>

<span class="sd">    Methods:</span>
<span class="sd">        forward(s): Constructs the transition layer for the EsmFold structure module using the input tensor &#39;s&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The output tensor after applying the transition layer to the input tensor &#39;s&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a new instance of the EsmFoldStructureModuleTransitionLayer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config:</span>
<span class="sd">                The configuration object containing the parameters for initializing the transition layer.</span>

<span class="sd">                - Type: object</span>
<span class="sd">                - Purpose: Specifies the configuration parameters required for initializing the transition layer.</span>
<span class="sd">                - Restrictions: Must be a valid configuration object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_3</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Constructs a new EsmFoldStructureModuleTransitionLayer.</span>

<span class="sd">        This method takes in two parameters, self and s.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldStructureModuleTransitionLayer): An instance of the EsmFoldStructureModuleTransitionLayer class.</span>
<span class="sd">            s (Tensor): The input tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: The output tensor after applying linear transformations and element-wise addition.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">s_initial</span> <span class="o">=</span> <span class="n">s</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_3</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">+</span> <span class="n">s_initial</span>

        <span class="k">return</span> <span class="n">s</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldStructureModuleTransitionLayer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a new instance of the EsmFoldStructureModuleTransitionLayer class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing the parameters for initializing the transition layer.</p>
<ul>
<li>Type: object</li>
<li>Purpose: Specifies the configuration parameters required for initializing the transition layer.</li>
<li>Restrictions: Must be a valid configuration object.</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a new instance of the EsmFoldStructureModuleTransitionLayer class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config:</span>
<span class="sd">            The configuration object containing the parameters for initializing the transition layer.</span>

<span class="sd">            - Type: object</span>
<span class="sd">            - Purpose: Specifies the configuration parameters required for initializing the transition layer.</span>
<span class="sd">            - Restrictions: Must be a valid configuration object.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_3</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldStructureModuleTransitionLayer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">s</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs a new EsmFoldStructureModuleTransitionLayer.</p>
<p>This method takes in two parameters, self and s.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the EsmFoldStructureModuleTransitionLayer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldStructureModuleTransitionLayer">EsmFoldStructureModuleTransitionLayer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>Tensor</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The output tensor after applying linear transformations and element-wise addition.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Constructs a new EsmFoldStructureModuleTransitionLayer.</span>

<span class="sd">    This method takes in two parameters, self and s.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmFoldStructureModuleTransitionLayer): An instance of the EsmFoldStructureModuleTransitionLayer class.</span>
<span class="sd">        s (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: The output tensor after applying linear transformations and element-wise addition.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s_initial</span> <span class="o">=</span> <span class="n">s</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_3</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">+</span> <span class="n">s_initial</span>

    <span class="k">return</span> <span class="n">s</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents an attention mechanism called EsmFoldTriangleAttention, which is used in the ESMFold model.
It is designed to calculate attention weights between pairs of elements in a tensor.</p>
<p>The EsmFoldTriangleAttention class inherits from the nn.Module class and has the following attributes:</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.c_in">c_in</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Input channel dimension.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.c_hidden">c_hidden</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Overall hidden channel dimension (not per-head).</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.no_heads">no_heads</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Number of attention heads.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.starting">starting</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Flag indicating if the attention is applied to the starting point of a pair.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.inf">inf</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Value used as infinity for masking.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.layer_norm">layer_norm</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Layer normalization module applied to the input tensor.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.linear">linear</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Linear transformation layer used for computing triangle biases.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.mha">mha</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>EsmFoldAttention module used for calculating attention weights.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.__init__" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes an instance of the EsmFoldTriangleAttention class.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention._chunk">_chunk</span></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Splits the input tensor into chunks and applies the EsmFoldAttention module to each chunk.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.forward" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Applies the attention mechanism to the input tensor and returns the output tensor.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldTriangleAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents an attention mechanism called EsmFoldTriangleAttention, which is used in the ESMFold model.</span>
<span class="sd">    It is designed to calculate attention weights between pairs of elements in a tensor.</span>

<span class="sd">    The EsmFoldTriangleAttention class inherits from the nn.Module class and has the following attributes:</span>

<span class="sd">    Attributes:</span>
<span class="sd">        c_in:</span>
<span class="sd">            Input channel dimension.</span>
<span class="sd">        c_hidden:</span>
<span class="sd">            Overall hidden channel dimension (not per-head).</span>
<span class="sd">        no_heads:</span>
<span class="sd">            Number of attention heads.</span>
<span class="sd">        starting:</span>
<span class="sd">            Flag indicating if the attention is applied to the starting point of a pair.</span>
<span class="sd">        inf:</span>
<span class="sd">            Value used as infinity for masking.</span>
<span class="sd">        layer_norm:</span>
<span class="sd">            Layer normalization module applied to the input tensor.</span>
<span class="sd">        linear:</span>
<span class="sd">            Linear transformation layer used for computing triangle biases.</span>
<span class="sd">        mha:</span>
<span class="sd">            EsmFoldAttention module used for calculating attention weights.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__:</span>
<span class="sd">            Initializes an instance of the EsmFoldTriangleAttention class.</span>

<span class="sd">        _chunk:</span>
<span class="sd">            Splits the input tensor into chunks and applies the EsmFoldAttention module to each chunk.</span>

<span class="sd">        forward:</span>
<span class="sd">            Applies the attention mechanism to the input tensor and returns the output tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c_in</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">no_heads</span><span class="p">,</span> <span class="n">starting</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inf</span><span class="o">=</span><span class="mf">1e9</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            c_in:</span>
<span class="sd">                Input channel dimension</span>
<span class="sd">            c_hidden:</span>
<span class="sd">                Overall hidden channel dimension (not per-head)</span>
<span class="sd">            no_heads:</span>
<span class="sd">                Number of attention heads</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c_in</span> <span class="o">=</span> <span class="n">c_in</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">=</span> <span class="n">c_hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span> <span class="o">=</span> <span class="n">no_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">starting</span> <span class="o">=</span> <span class="n">starting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inf</span> <span class="o">=</span> <span class="n">inf</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_in</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;normal&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">EsmFoldAttention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_chunk</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">biases</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">use_memory_efficient_kernel</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_lma</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">inplace_safe</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s2">&quot;triangle! triangle!&quot;</span>
        <span class="n">mha_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;q_x&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
            <span class="s2">&quot;kv_x&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
            <span class="s2">&quot;biases&quot;</span><span class="p">:</span> <span class="n">biases</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">chunk_layer</span><span class="p">(</span>
            <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">,</span> <span class="n">use_memory_efficient_kernel</span><span class="o">=</span><span class="n">use_memory_efficient_kernel</span><span class="p">,</span> <span class="n">use_lma</span><span class="o">=</span><span class="n">use_lma</span><span class="p">),</span>
            <span class="n">mha_inputs</span><span class="p">,</span>
            <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span>
            <span class="n">no_batch_dims</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]),</span>
            <span class="n">_out</span><span class="o">=</span><span class="n">x</span> <span class="k">if</span> <span class="n">inplace_safe</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_memory_efficient_kernel</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_lma</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">inplace_safe</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            x:</span>
<span class="sd">                [*, I, J, C_in] input tensor (e.g. the pair representation)</span>
<span class="sd">        Returns:</span>
<span class="sd">            [*, I, J, C_in] output tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># [*, I, J]</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">starting</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># [*, I, J, C_in]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># [*, I, 1, 1, J]</span>
        <span class="n">mask_bias</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inf</span> <span class="o">*</span> <span class="p">(</span><span class="n">mask</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># [*, H, I, J]</span>
        <span class="n">triangle_bias</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># [*, 1, H, I, J]</span>
        <span class="n">triangle_bias</span> <span class="o">=</span> <span class="n">triangle_bias</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span>

        <span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">mask_bias</span><span class="p">,</span> <span class="n">triangle_bias</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">chunk_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_chunk</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">biases</span><span class="p">,</span>
                <span class="n">chunk_size</span><span class="p">,</span>
                <span class="n">use_memory_efficient_kernel</span><span class="o">=</span><span class="n">use_memory_efficient_kernel</span><span class="p">,</span>
                <span class="n">use_lma</span><span class="o">=</span><span class="n">use_lma</span><span class="p">,</span>
                <span class="n">inplace_safe</span><span class="o">=</span><span class="n">inplace_safe</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">(</span>
                <span class="n">q_x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">kv_x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">biases</span><span class="o">=</span><span class="n">biases</span><span class="p">,</span> <span class="n">use_memory_efficient_kernel</span><span class="o">=</span><span class="n">use_memory_efficient_kernel</span><span class="p">,</span> <span class="n">use_lma</span><span class="o">=</span><span class="n">use_lma</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">starting</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldTriangleAttention</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">no_heads</span><span class="p">,</span> <span class="n">starting</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inf</span><span class="o">=</span><span class="mf">1000000000.0</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c_in</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input channel dimension</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_hidden</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Overall hidden channel dimension (not per-head)</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>no_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c_in</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">no_heads</span><span class="p">,</span> <span class="n">starting</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inf</span><span class="o">=</span><span class="mf">1e9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        c_in:</span>
<span class="sd">            Input channel dimension</span>
<span class="sd">        c_hidden:</span>
<span class="sd">            Overall hidden channel dimension (not per-head)</span>
<span class="sd">        no_heads:</span>
<span class="sd">            Number of attention heads</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">c_in</span> <span class="o">=</span> <span class="n">c_in</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span> <span class="o">=</span> <span class="n">c_hidden</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span> <span class="o">=</span> <span class="n">no_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">starting</span> <span class="o">=</span> <span class="n">starting</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inf</span> <span class="o">=</span> <span class="n">inf</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_in</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;normal&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">EsmFoldAttention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_hidden</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_heads</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldTriangleAttention</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_memory_efficient_kernel</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_lma</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace_safe</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, I, J, C_in] input tensor (e.g. the pair representation)</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">chunk_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_memory_efficient_kernel</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_lma</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">inplace_safe</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x:</span>
<span class="sd">            [*, I, J, C_in] input tensor (e.g. the pair representation)</span>
<span class="sd">    Returns:</span>
<span class="sd">        [*, I, J, C_in] output tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># [*, I, J]</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span>
            <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">starting</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># [*, I, J, C_in]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># [*, I, 1, 1, J]</span>
    <span class="n">mask_bias</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inf</span> <span class="o">*</span> <span class="p">(</span><span class="n">mask</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>

    <span class="c1"># [*, H, I, J]</span>
    <span class="n">triangle_bias</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># [*, 1, H, I, J]</span>
    <span class="n">triangle_bias</span> <span class="o">=</span> <span class="n">triangle_bias</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span>

    <span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">mask_bias</span><span class="p">,</span> <span class="n">triangle_bias</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">chunk_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_chunk</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">biases</span><span class="p">,</span>
            <span class="n">chunk_size</span><span class="p">,</span>
            <span class="n">use_memory_efficient_kernel</span><span class="o">=</span><span class="n">use_memory_efficient_kernel</span><span class="p">,</span>
            <span class="n">use_lma</span><span class="o">=</span><span class="n">use_lma</span><span class="p">,</span>
            <span class="n">inplace_safe</span><span class="o">=</span><span class="n">inplace_safe</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">(</span>
            <span class="n">q_x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">kv_x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">biases</span><span class="o">=</span><span class="n">biases</span><span class="p">,</span> <span class="n">use_memory_efficient_kernel</span><span class="o">=</span><span class="n">use_memory_efficient_kernel</span><span class="p">,</span> <span class="n">use_lma</span><span class="o">=</span><span class="n">use_lma</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">starting</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Implements Algorithms 11 and 12.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldTriangleMultiplicativeUpdate</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements Algorithms 11 and 12.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">_outgoing</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the EsmFoldTriangleMultiplicativeUpdate class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config: An object containing configuration parameters.</span>
<span class="sd">            _outgoing (bool): A boolean indicating whether the update is outgoing (default is True).</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If config is not provided or is not of the expected type.</span>
<span class="sd">            ValueError: If config.pairwise_state_dim is not accessible or does not have the expected value.</span>
<span class="sd">            RuntimeError: If an issue occurs during the initialization of linear layers or normalization layers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">c_hidden</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_outgoing</span> <span class="o">=</span> <span class="n">_outgoing</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_a_p</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_a_g</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;gating&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_b_p</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_b_g</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;gating&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;gating&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_z</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_in</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_combine_projections</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">_inplace_chunk_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Combines two projections using a multiplicative update method.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmFoldTriangleMultiplicativeUpdate): The instance of the EsmFoldTriangleMultiplicativeUpdate class.</span>
<span class="sd">            a (mindspore.Tensor): The first projection tensor.</span>
<span class="sd">            b (mindspore.Tensor): The second projection tensor.</span>
<span class="sd">            _inplace_chunk_size (Optional[int], optional): The size of the chunk for in-place computation.</span>
<span class="sd">                Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            mindspore.Tensor: The combined projection tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outgoing</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">_inplace_chunk_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># To be replaced by torch vmap</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="n">_inplace_chunk_size</span><span class="p">):</span>
                <span class="n">a_chunk</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">_inplace_chunk_size</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
                <span class="n">b_chunk</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">_inplace_chunk_size</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
                <span class="n">a</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">_inplace_chunk_size</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
                    <span class="n">a_chunk</span><span class="p">,</span>
                    <span class="n">b_chunk</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">p</span> <span class="o">=</span> <span class="n">a</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_inference_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">z</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inplace_chunk_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">with_add</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            z:</span>
<span class="sd">                A [*, N, N, C_z] pair representation</span>
<span class="sd">            mask:</span>
<span class="sd">                A [*, N, N] pair mask</span>
<span class="sd">            inplace_chunk_size:</span>
<span class="sd">                Size of chunks used in the main computation. Increase to trade memory for speed.</span>
<span class="sd">            with_add:</span>
<span class="sd">                If True, z is overwritten with (z + update). Otherwise, it is overwritten with (update).</span>
<span class="sd">        Returns:</span>
<span class="sd">            A reference to the overwritten z</span>

<span class="sd">        More memory-efficient, inference-only version of the forward function. Uses in-place operations, fusion of the</span>
<span class="sd">        addition that happens after this module in the Evoformer, a smidge of recomputation, and a cache of overwritten</span>
<span class="sd">        values to lower peak memory consumption of this module from 5x the size of the input tensor z to 2.5x its size.</span>
<span class="sd">        Useful for inference on extremely long sequences.</span>

<span class="sd">        It works as follows. We will make reference to variables used in the default forward implementation below.</span>
<span class="sd">        Naively, triangle multiplication attention requires the manifestation of 5 tensors the size of z: 1) z, the</span>
<span class="sd">        &quot;square&quot; input tensor, 2) a, the first projection of z, 3) b, the second projection of b, 4) g, a z-sized mask,</span>
<span class="sd">        and 5) a z-sized tensor for intermediate computations. For large N, this is prohibitively expensive; for</span>
<span class="sd">        N=4000, for example, z is more than 8GB alone. To avoid this problem, we compute b, g, and all intermediate</span>
<span class="sd">        tensors in small chunks, noting that the chunks required to compute a chunk of the output depend only on the</span>
<span class="sd">        tensor a and corresponding vertical and horizontal chunks of z. This suggests an algorithm that loops over</span>
<span class="sd">        pairs of chunks of z: hereafter &quot;columns&quot; and &quot;rows&quot; of z, even though each &quot;column&quot; and &quot;row&quot; in fact contains</span>
<span class="sd">        inplace_chunk_size contiguous true columns and rows of z. Writing output chunks to a new tensor would bring</span>
<span class="sd">        total memory consumption down to 3x the size of z. However, more memory can be saved by writing output chunks</span>
<span class="sd">        directly to z in-place. WLOG, we choose to write output chunks vertically, overwriting the ith &quot;column&quot; of z at</span>
<span class="sd">        the end of the ith iteration of the main loop. Despite this overwriting, the ith column is always one column</span>
<span class="sd">        ahead of previously overwritten columns and can be recovered directly from z. After the first iteration,</span>
<span class="sd">        however, the ith row of z is always at least partially overwritten. For this reason, we introduce the z-cache,</span>
<span class="sd">        a tensor one-half the size of z. The z-cache initially contains the left half (2nd and 3rd quadrants) of z. For</span>
<span class="sd">        0 &lt; i &lt; N/2, the missing left part of the ith row of z is recovered from this cache at the beginning of the ith</span>
<span class="sd">        iteration. Once i exceeds n/2, the cache is &quot;reoriented&quot; to encompass the 3rd and 4th quadrants of z instead.</span>
<span class="sd">        Though the 3rd quadrant of the original z is entirely overwritten at this point, it can be recovered from the</span>
<span class="sd">        z-cache itself. Thereafter, the ith row of z can be recovered in its entirety from the reoriented z-cache.</span>
<span class="sd">        After the final iteration, z has been completely overwritten and contains the triangular multiplicative update.</span>
<span class="sd">        If with_add is True, it instead contains the sum of z and the triangular multiplicative update. In either case,</span>
<span class="sd">        peak memory consumption is just 2.5x the size of z, disregarding memory used for chunks and other small</span>
<span class="sd">        variables.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">compute_projection_helper</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">a</span><span class="p">:</span>
                <span class="n">linear_g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_a_g</span>
                <span class="n">linear_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_a_p</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">linear_g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_b_g</span>
                <span class="n">linear_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_b_p</span>

            <span class="n">pair</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_in</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">linear_g</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
            <span class="n">p</span> <span class="o">*=</span> <span class="n">linear_p</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>
            <span class="n">p</span> <span class="o">*=</span> <span class="n">mask</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">p</span>

        <span class="k">def</span> <span class="nf">compute_projection</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">chunked</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">need_transpose</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outgoing</span> <span class="o">^</span> <span class="n">a</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">chunked</span><span class="p">:</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">compute_projection_helper</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">need_transpose</span><span class="p">:</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># This computation is chunked so as not to exceed our 2.5x</span>
                <span class="c1"># budget with a large intermediate tensor</span>
                <span class="n">linear_g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_a_g</span> <span class="k">if</span> <span class="n">a</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_b_g</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">linear_g</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">out_shape</span> <span class="o">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span><span class="p">,)</span> <span class="o">+</span> <span class="n">pair</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">out_shape</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pair</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="n">inplace_chunk_size</span><span class="p">):</span>
                    <span class="n">pair_chunk</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">inplace_chunk_size</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
                    <span class="n">pair_chunk</span> <span class="o">=</span> <span class="n">compute_projection_helper</span><span class="p">(</span>
                        <span class="n">pair</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">inplace_chunk_size</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span>
                        <span class="n">mask</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">inplace_chunk_size</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span>
                        <span class="n">a</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">need_transpose</span><span class="p">:</span>
                        <span class="n">pair_chunk</span> <span class="o">=</span> <span class="n">pair_chunk</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
                        <span class="n">p</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">inplace_chunk_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">pair_chunk</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">p</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">inplace_chunk_size</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">pair_chunk</span>

                    <span class="k">del</span> <span class="n">pair_chunk</span>

            <span class="k">return</span> <span class="n">p</span>

        <span class="c1"># We start by fully manifesting a. In addition to the input, this</span>
        <span class="c1"># brings total memory consumption to 2x z (disregarding size of chunks)</span>
        <span class="c1"># [*, N, N, c]</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">compute_projection</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">chunked</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inplace_chunk_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">half_n</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">2</span>
            <span class="n">row_dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span>
            <span class="n">col_dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
            <span class="n">b_chunk_dim</span> <span class="o">=</span> <span class="n">row_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outgoing</span> <span class="k">else</span> <span class="n">col_dim</span>

            <span class="k">def</span> <span class="nf">empty_slicer</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">[</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">]</span>

            <span class="k">def</span> <span class="nf">slice_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
                <span class="c1"># Slices start:end from the dim dimension of t</span>
                <span class="n">s</span> <span class="o">=</span> <span class="n">empty_slicer</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
                <span class="n">s</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">t</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>

            <span class="k">def</span> <span class="nf">flip_z_cache_</span><span class="p">(</span><span class="n">z_cache</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
                <span class="c1"># &quot;Reorient&quot; the z_cache (see below), filling it with quadrants</span>
                <span class="c1"># 3---recovered from the z_cache---and 4---recovered from z---</span>
                <span class="c1"># of the input tensor z.</span>
                <span class="n">quadrant_3</span> <span class="o">=</span> <span class="n">slice_tensor</span><span class="p">(</span><span class="n">z_cache</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">row_dim</span><span class="p">)</span>
                <span class="n">z_cache</span> <span class="o">=</span> <span class="n">z_cache</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">row_dim</span><span class="p">,</span> <span class="n">col_dim</span><span class="p">)</span>

                <span class="c1"># If n is odd, we need to shrink the z_cache by one row</span>
                <span class="n">z_cache</span> <span class="o">=</span> <span class="n">z_cache</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="p">(</span><span class="n">n</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="p">:,</span> <span class="p">:]</span>

                <span class="c1"># Move the 3rd quadrant of z into the</span>
                <span class="n">first_half_slicer</span> <span class="o">=</span> <span class="n">empty_slicer</span><span class="p">(</span><span class="n">z_cache</span><span class="p">)</span>
                <span class="n">first_half_slicer</span><span class="p">[</span><span class="n">col_dim</span><span class="p">]</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">)</span>
                <span class="n">z_cache</span><span class="p">[</span><span class="n">first_half_slicer</span><span class="p">]</span> <span class="o">=</span> <span class="n">quadrant_3</span>

                <span class="c1"># Get the fourth quadrant of z</span>
                <span class="n">quadrant_4</span> <span class="o">=</span> <span class="n">slice_tensor</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">row_dim</span><span class="p">)</span>
                <span class="n">quadrant_4</span> <span class="o">=</span> <span class="n">slice_tensor</span><span class="p">(</span><span class="n">quadrant_4</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">col_dim</span><span class="p">)</span>

                <span class="c1"># Insert said quadrant into the rotated z-cache</span>
                <span class="n">quadrant_3_slicer</span> <span class="o">=</span> <span class="n">empty_slicer</span><span class="p">(</span><span class="n">z_cache</span><span class="p">)</span>
                <span class="n">quadrant_3_slicer</span><span class="p">[</span><span class="n">col_dim</span><span class="p">]</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">half_n</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

                <span class="n">z_cache</span><span class="p">[</span><span class="n">quadrant_3_slicer</span><span class="p">]</span> <span class="o">=</span> <span class="n">quadrant_4</span>

                <span class="k">return</span> <span class="n">z_cache</span>

            <span class="c1"># Initialize the z cache to the left half of z.</span>
            <span class="n">z_cache_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">z_cache_shape</span><span class="p">[</span><span class="n">col_dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">half_n</span>
            <span class="n">z_cache</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">z_cache_shape</span><span class="p">)</span>
            <span class="n">z_cache_slicer</span> <span class="o">=</span> <span class="n">empty_slicer</span><span class="p">(</span><span class="n">z_cache</span><span class="p">)</span>
            <span class="n">z_cache_slicer</span><span class="p">[</span><span class="n">col_dim</span><span class="p">]</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">)</span>
            <span class="n">z_cache</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="n">z_cache_slicer</span><span class="p">]</span>
            <span class="n">z_cache_rotated</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="c1"># We need to reorient the z-cache at the halfway point, and we</span>
            <span class="c1"># don&#39;t want a single chunk to straddle that point. We contract one</span>
            <span class="c1"># of the chunks in the middle to address that problem.</span>
            <span class="n">i_range</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="n">inplace_chunk_size</span><span class="p">))</span>
            <span class="n">initial_offsets</span> <span class="o">=</span> <span class="p">[</span><span class="n">i_2</span> <span class="o">-</span> <span class="n">i_1</span> <span class="k">for</span> <span class="n">i_1</span><span class="p">,</span> <span class="n">i_2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">i_range</span><span class="p">,</span> <span class="n">i_range</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">half_n</span><span class="p">])]</span>
            <span class="n">after_half</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">half_n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">inplace_chunk_size</span><span class="p">))</span>
            <span class="n">after_half_offsets</span> <span class="o">=</span> <span class="p">[</span><span class="n">inplace_chunk_size</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">after_half</span><span class="p">]</span>
            <span class="n">combined_range_with_offsets</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">i_range</span> <span class="o">+</span> <span class="n">after_half</span><span class="p">,</span> <span class="n">initial_offsets</span> <span class="o">+</span> <span class="n">after_half_offsets</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">offset</span> <span class="ow">in</span> <span class="n">combined_range_with_offsets</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">z_cache_rotated</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">half_n</span><span class="p">:</span>
                    <span class="n">z_cache</span> <span class="o">=</span> <span class="n">flip_z_cache_</span><span class="p">(</span><span class="n">z_cache</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
                    <span class="n">z_cache_rotated</span> <span class="o">=</span> <span class="kc">True</span>

                <span class="n">z_chunk_b</span> <span class="o">=</span> <span class="n">slice_tensor</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">offset</span><span class="p">,</span> <span class="n">b_chunk_dim</span><span class="p">)</span>
                <span class="n">mask_chunk</span> <span class="o">=</span> <span class="n">slice_tensor</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">offset</span><span class="p">,</span> <span class="n">b_chunk_dim</span><span class="p">)</span>

                <span class="n">z_chunk_b</span> <span class="o">=</span> <span class="n">z_chunk_b</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">b_chunk_dim</span> <span class="o">==</span> <span class="n">col_dim</span><span class="p">:</span>
                    <span class="n">z_chunk_b</span> <span class="o">=</span> <span class="n">slice_tensor</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">offset</span><span class="p">,</span> <span class="n">col_dim</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>  <span class="c1"># b_chunk_dim == row_dim</span>
                    <span class="c1"># In this case, the b-dimension (b_chunk_dim) is partially</span>
                    <span class="c1"># overwritten at the end of each iteration. We need to</span>
                    <span class="c1"># restore the missing component from the z-cache.</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">z_cache_rotated</span><span class="p">:</span>
                        <span class="n">z_chunk_slicer</span> <span class="o">=</span> <span class="n">empty_slicer</span><span class="p">(</span><span class="n">z_chunk_b</span><span class="p">)</span>
                        <span class="n">z_chunk_slicer</span><span class="p">[</span><span class="n">col_dim</span><span class="p">]</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">)</span>
                        <span class="n">z_chunk_b</span><span class="p">[</span><span class="n">z_chunk_slicer</span><span class="p">]</span> <span class="o">=</span> <span class="n">slice_tensor</span><span class="p">(</span><span class="n">z_cache</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">offset</span><span class="p">,</span> <span class="n">row_dim</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">z_cache_offset</span> <span class="o">=</span> <span class="n">i</span> <span class="o">-</span> <span class="n">half_n</span>
                        <span class="n">z_chunk_b</span> <span class="o">=</span> <span class="n">slice_tensor</span><span class="p">(</span><span class="n">z_cache</span><span class="p">,</span> <span class="n">z_cache_offset</span><span class="p">,</span> <span class="n">z_cache_offset</span> <span class="o">+</span> <span class="n">offset</span><span class="p">,</span> <span class="n">row_dim</span><span class="p">)</span>

                <span class="n">b_chunk</span> <span class="o">=</span> <span class="n">compute_projection</span><span class="p">(</span><span class="n">z_chunk_b</span><span class="p">,</span> <span class="n">mask_chunk</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">chunked</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">del</span> <span class="n">z_chunk_b</span>

                <span class="n">x_chunk</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b_chunk</span><span class="p">)</span>
                <span class="n">x_chunk</span> <span class="o">=</span> <span class="n">permute_final_dims</span><span class="p">(</span><span class="n">x_chunk</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
                <span class="n">x_chunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_out</span><span class="p">(</span><span class="n">x_chunk</span><span class="p">)</span>
                <span class="n">x_chunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_z</span><span class="p">(</span><span class="n">x_chunk</span><span class="p">)</span>

                <span class="c1"># The g dimension (col_dim) is parallel to and ahead of the</span>
                <span class="c1"># overwrites in z. We can extract the g chunk normally.</span>
                <span class="n">z_chunk_g</span> <span class="o">=</span> <span class="n">slice_tensor</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">offset</span><span class="p">,</span> <span class="n">col_dim</span><span class="p">)</span>
                <span class="n">g_chunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_in</span><span class="p">(</span><span class="n">z_chunk_g</span><span class="p">))</span>
                <span class="n">g_chunk</span> <span class="o">=</span> <span class="n">g_chunk</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
                <span class="k">del</span> <span class="n">z_chunk_g</span>

                <span class="n">x_chunk</span> <span class="o">*=</span> <span class="n">g_chunk</span>

                <span class="c1"># Write the columns into z in-place</span>
                <span class="n">z_slicer</span> <span class="o">=</span> <span class="n">empty_slicer</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
                <span class="n">z_slicer</span><span class="p">[</span><span class="n">col_dim</span><span class="p">]</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">with_add</span><span class="p">:</span>
                    <span class="n">z</span><span class="p">[</span><span class="n">z_slicer</span><span class="p">]</span> <span class="o">+=</span> <span class="n">x_chunk</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">z</span><span class="p">[</span><span class="n">z_slicer</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_chunk</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">compute_projection</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_z</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
            <span class="n">x</span> <span class="o">*=</span> <span class="n">g</span>
            <span class="k">if</span> <span class="n">with_add</span><span class="p">:</span>
                <span class="n">z</span> <span class="o">+=</span> <span class="n">x</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">z</span> <span class="o">=</span> <span class="n">x</span>

        <span class="k">return</span> <span class="n">z</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">z</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inplace_safe</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">_add_with_inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">_inplace_chunk_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            x:</span>
<span class="sd">                [*, N_res, N_res, C_z] input tensor</span>
<span class="sd">            mask:</span>
<span class="sd">                [*, N_res, N_res] input mask</span>
<span class="sd">        Returns:</span>
<span class="sd">            [*, N_res, N_res, C_z] output tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">inplace_safe</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_forward</span><span class="p">(</span>
                <span class="n">z</span><span class="p">,</span>
                <span class="n">mask</span><span class="p">,</span>
                <span class="n">inplace_chunk_size</span><span class="o">=</span><span class="n">_inplace_chunk_size</span><span class="p">,</span>
                <span class="n">with_add</span><span class="o">=</span><span class="n">_add_with_inplace</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">x</span>

        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_in</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">mask</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_a_g</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_a_p</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">mask</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_b_g</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_b_p</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_combine_projections</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

        <span class="k">del</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_z</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">g</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldTriangleMultiplicativeUpdate</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">_outgoing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmFoldTriangleMultiplicativeUpdate class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An object containing configuration parameters.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>_outgoing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A boolean indicating whether the update is outgoing (default is True).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If config is not provided or is not of the expected type.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If config.pairwise_state_dim is not accessible or does not have the expected value.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If an issue occurs during the initialization of linear layers or normalization layers.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">_outgoing</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the EsmFoldTriangleMultiplicativeUpdate class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config: An object containing configuration parameters.</span>
<span class="sd">        _outgoing (bool): A boolean indicating whether the update is outgoing (default is True).</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If config is not provided or is not of the expected type.</span>
<span class="sd">        ValueError: If config.pairwise_state_dim is not accessible or does not have the expected value.</span>
<span class="sd">        RuntimeError: If an issue occurs during the initialization of linear layers or normalization layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">c_hidden</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_outgoing</span> <span class="o">=</span> <span class="n">_outgoing</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">linear_a_p</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_a_g</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;gating&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_b_p</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_b_g</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;gating&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;gating&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_z</span> <span class="o">=</span> <span class="n">EsmFoldLinear</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">,</span> <span class="n">c_hidden</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;final&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_in</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_hidden</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldTriangleMultiplicativeUpdate</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inplace_safe</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">_add_with_inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">_inplace_chunk_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, N_res, N_res, C_z] input tensor</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>[*, N_res, N_res] input mask</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inplace_safe</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">_add_with_inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">_inplace_chunk_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x:</span>
<span class="sd">            [*, N_res, N_res, C_z] input tensor</span>
<span class="sd">        mask:</span>
<span class="sd">            [*, N_res, N_res] input mask</span>
<span class="sd">    Returns:</span>
<span class="sd">        [*, N_res, N_res, C_z] output tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace_safe</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_forward</span><span class="p">(</span>
            <span class="n">z</span><span class="p">,</span>
            <span class="n">mask</span><span class="p">,</span>
            <span class="n">inplace_chunk_size</span><span class="o">=</span><span class="n">_inplace_chunk_size</span><span class="p">,</span>
            <span class="n">with_add</span><span class="o">=</span><span class="n">_add_with_inplace</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_in</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">mask</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_a_g</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_a_p</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">mask</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_b_g</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_b_p</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_combine_projections</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">del</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_z</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">g</span>

    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents a block of Triangular Self-Attention for the EsmFold model.
It is used to process sequence and pairwise states in the EsmFold model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.layernorm_1">layernorm_1</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A layer normalization module for the sequence state dimension.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.LayerNorm">LayerNorm</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.sequence_to_pair">sequence_to_pair</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A module that converts the sequence state to pairwise state.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSequenceToPair">EsmFoldSequenceToPair</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.pair_to_sequence">pair_to_sequence</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A module that converts the pairwise state to sequence state.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldPairToSequence">EsmFoldPairToSequence</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.seq_attention">seq_attention</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A self-attention module for the sequence state.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldSelfAttention">EsmFoldSelfAttention</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.tri_mul_out">tri_mul_out</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A module that performs triangular multiplicative update on the pairwise state.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate">EsmFoldTriangleMultiplicativeUpdate</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.tri_mul_in">tri_mul_in</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A module that performs triangular multiplicative update on the pairwise state.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleMultiplicativeUpdate">EsmFoldTriangleMultiplicativeUpdate</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.tri_att_start">tri_att_start</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A module that performs triangular attention on the pairwise state starting from a specific position.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention">EsmFoldTriangleAttention</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.tri_att_end">tri_att_end</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A module that performs triangular attention on the pairwise state ending at a specific position.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangleAttention">EsmFoldTriangleAttention</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.mlp_seq">mlp_seq</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A multilayer perceptron module for the sequence state.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP">EsmFoldResidueMLP</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.mlp_pair">mlp_pair</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A multilayer perceptron module for the pairwise state.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldResidueMLP">EsmFoldResidueMLP</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.drop">drop</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A dropout module.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Dropout">Dropout</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.row_drop">row_drop</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A dropout module that applies dropout on rows of the pairwise state.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout">EsmFoldDropout</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.col_drop">col_drop</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A dropout module that applies dropout on columns of the pairwise state.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldDropout">EsmFoldDropout</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.forward" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Process the sequence and pairwise states.</p>
<p>Args:</p>
<ul>
<li>sequence_state (torch.Tensor): Input sequence state tensor of shape
(batch_size, sequence_length, sequence_state_dim).</li>
<li>pairwise_state (torch.Tensor): Input pairwise state tensor of shape
(batch_size, sequence_length, sequence_length, pairwise_state_dim).</li>
<li>mask (torch.Tensor, optional): Boolean tensor of valid positions, with shape
(batch_size, sequence_length). Defaults to None.</li>
<li>chunk_size (int, optional): The size of the attention chunks. Defaults to None.</li>
</ul>
<p>Returns:</p>
<ul>
<li>sequence_state (torch.Tensor): Processed sequence state tensor of shape
(batch_size, sequence_length, sequence_state_dim).</li>
<li>pairwise_state (torch.Tensor): Processed pairwise state tensor of shape
(batch_size, sequence_length, sequence_length, pairwise_state_dim).</li>
</ul>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldTriangularSelfAttentionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a block of Triangular Self-Attention for the EsmFold model.</span>
<span class="sd">    It is used to process sequence and pairwise states in the EsmFold model.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        layernorm_1 (nn.LayerNorm): A layer normalization module for the sequence state dimension.</span>
<span class="sd">        sequence_to_pair (EsmFoldSequenceToPair): A module that converts the sequence state to pairwise state.</span>
<span class="sd">        pair_to_sequence (EsmFoldPairToSequence): A module that converts the pairwise state to sequence state.</span>
<span class="sd">        seq_attention (EsmFoldSelfAttention): A self-attention module for the sequence state.</span>
<span class="sd">        tri_mul_out (EsmFoldTriangleMultiplicativeUpdate):</span>
<span class="sd">            A module that performs triangular multiplicative update on the pairwise state.</span>
<span class="sd">        tri_mul_in (EsmFoldTriangleMultiplicativeUpdate):</span>
<span class="sd">            A module that performs triangular multiplicative update on the pairwise state.</span>
<span class="sd">        tri_att_start (EsmFoldTriangleAttention):</span>
<span class="sd">            A module that performs triangular attention on the pairwise state starting from a specific position.</span>
<span class="sd">        tri_att_end (EsmFoldTriangleAttention):</span>
<span class="sd">            A module that performs triangular attention on the pairwise state ending at a specific position.</span>
<span class="sd">        mlp_seq (EsmFoldResidueMLP): A multilayer perceptron module for the sequence state.</span>
<span class="sd">        mlp_pair (EsmFoldResidueMLP): A multilayer perceptron module for the pairwise state.</span>
<span class="sd">        drop (nn.Dropout): A dropout module.</span>
<span class="sd">        row_drop (EsmFoldDropout): A dropout module that applies dropout on rows of the pairwise state.</span>
<span class="sd">        col_drop (EsmFoldDropout): A dropout module that applies dropout on columns of the pairwise state.</span>

<span class="sd">    Methods:</span>
<span class="sd">        forward(sequence_state, pairwise_state, mask=None, chunk_size=None, **__kwargs):</span>
<span class="sd">            Process the sequence and pairwise states.</span>

<span class="sd">            Args:</span>

<span class="sd">            - sequence_state (torch.Tensor): Input sequence state tensor of shape</span>
<span class="sd">            (batch_size, sequence_length, sequence_state_dim).</span>
<span class="sd">            - pairwise_state (torch.Tensor): Input pairwise state tensor of shape</span>
<span class="sd">            (batch_size, sequence_length, sequence_length, pairwise_state_dim).</span>
<span class="sd">            - mask (torch.Tensor, optional): Boolean tensor of valid positions, with shape</span>
<span class="sd">            (batch_size, sequence_length). Defaults to None.</span>
<span class="sd">            - chunk_size (int, optional): The size of the attention chunks. Defaults to None.</span>

<span class="sd">            Returns:</span>

<span class="sd">            - sequence_state (torch.Tensor): Processed sequence state tensor of shape</span>
<span class="sd">            (batch_size, sequence_length, sequence_state_dim).</span>
<span class="sd">            - pairwise_state (torch.Tensor): Processed pairwise state tensor of shape</span>
<span class="sd">            (batch_size, sequence_length, sequence_length, pairwise_state_dim).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method initializes an instance of the EsmFoldTriangularSelfAttentionBlock class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the EsmFoldTriangularSelfAttentionBlock class.</span>
<span class="sd">            config: The configuration object containing parameters for the attention block.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: If the method is not implemented for any reason.</span>
<span class="sd">            ValueError: If the provided configuration object is invalid or missing required parameters.</span>
<span class="sd">            TypeError: If the provided configuration object is of incorrect type.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="n">sequence_state_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_state_dim</span>
        <span class="n">pairwise_state_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span>
        <span class="n">sequence_num_heads</span> <span class="o">=</span> <span class="n">sequence_state_dim</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_head_width</span>
        <span class="n">pairwise_num_heads</span> <span class="o">=</span> <span class="n">pairwise_state_dim</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_head_width</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">sequence_state_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sequence_to_pair</span> <span class="o">=</span> <span class="n">EsmFoldSequenceToPair</span><span class="p">(</span><span class="n">sequence_state_dim</span><span class="p">,</span> <span class="n">pairwise_state_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pairwise_state_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pair_to_sequence</span> <span class="o">=</span> <span class="n">EsmFoldPairToSequence</span><span class="p">(</span><span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">sequence_num_heads</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">seq_attention</span> <span class="o">=</span> <span class="n">EsmFoldSelfAttention</span><span class="p">(</span>
            <span class="n">sequence_state_dim</span><span class="p">,</span> <span class="n">sequence_num_heads</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_head_width</span><span class="p">,</span> <span class="n">gated</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tri_mul_out</span> <span class="o">=</span> <span class="n">EsmFoldTriangleMultiplicativeUpdate</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">_outgoing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tri_mul_in</span> <span class="o">=</span> <span class="n">EsmFoldTriangleMultiplicativeUpdate</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">_outgoing</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tri_att_start</span> <span class="o">=</span> <span class="n">EsmFoldTriangleAttention</span><span class="p">(</span>
            <span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_head_width</span><span class="p">,</span> <span class="n">pairwise_num_heads</span><span class="p">,</span> <span class="n">inf</span><span class="o">=</span><span class="mf">1e9</span><span class="p">,</span> <span class="n">starting</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tri_att_end</span> <span class="o">=</span> <span class="n">EsmFoldTriangleAttention</span><span class="p">(</span>
            <span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_head_width</span><span class="p">,</span> <span class="n">pairwise_num_heads</span><span class="p">,</span> <span class="n">inf</span><span class="o">=</span><span class="mf">1e9</span><span class="p">,</span> <span class="n">starting</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_seq</span> <span class="o">=</span> <span class="n">EsmFoldResidueMLP</span><span class="p">(</span><span class="n">sequence_state_dim</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">sequence_state_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_pair</span> <span class="o">=</span> <span class="n">EsmFoldResidueMLP</span><span class="p">(</span><span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">row_drop</span> <span class="o">=</span> <span class="n">EsmFoldDropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">col_drop</span> <span class="o">=</span> <span class="n">EsmFoldDropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence_state</span><span class="p">,</span> <span class="n">pairwise_state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">__kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">          sequence_state: B x L x sequence_state_dim pairwise_state: B x L x L x pairwise_state_dim mask: B x L boolean</span>
<span class="sd">          tensor of valid positions</span>

<span class="sd">        Output:</span>
<span class="sd">          sequence_state: B x L x sequence_state_dim pairwise_state: B x L x L x pairwise_state_dim</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`sequence_state` should be a 3d-tensor, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sequence_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> dims.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`pairwise_state` should be a 4d-tensor, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> dims.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`mask` should be a 2d-tensor, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> dims.&quot;</span><span class="p">)</span>

        <span class="n">batch_dim</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">,</span> <span class="n">sequence_state_dim</span> <span class="o">=</span> <span class="n">sequence_state</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">pairwise_state_dim</span> <span class="o">=</span> <span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">sequence_state_dim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_state_dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`sequence_state` last dimension should be equal to `self.sequence_state_dim`. Got &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">sequence_state_dim</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_state_dim</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">pairwise_state_dim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`pairwise_state` last dimension should be equal to `self.pairwise_state_dim`. Got &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">batch_dim</span> <span class="o">!=</span> <span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`sequence_state` and `pairwise_state` have inconsistent batch size: </span><span class="si">{</span><span class="n">batch_dim</span><span class="si">}</span><span class="s2"> != &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">seq_dim</span> <span class="o">!=</span> <span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">or</span> <span class="n">seq_dim</span> <span class="o">!=</span> <span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`sequence_state` and `pairwise_state` have inconsistent sequence length: </span><span class="si">{</span><span class="n">seq_dim</span><span class="si">}</span><span class="s2"> != &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> or </span><span class="si">{</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Update sequence state</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_to_sequence</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">)</span>

        <span class="c1"># Self attention with bias + mlp.</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_1</span><span class="p">(</span><span class="n">sequence_state</span><span class="p">)</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_attention</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">sequence_state</span> <span class="o">=</span> <span class="n">sequence_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">sequence_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_seq</span><span class="p">(</span><span class="n">sequence_state</span><span class="p">)</span>

        <span class="c1"># Update pairwise state</span>
        <span class="n">pairwise_state</span> <span class="o">=</span> <span class="n">pairwise_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_to_pair</span><span class="p">(</span><span class="n">sequence_state</span><span class="p">)</span>

        <span class="c1"># Axial attention with triangular bias.</span>
        <span class="n">tri_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">pairwise_state</span> <span class="o">=</span> <span class="n">pairwise_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tri_mul_out</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">tri_mask</span><span class="p">))</span>
        <span class="n">pairwise_state</span> <span class="o">=</span> <span class="n">pairwise_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tri_mul_in</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">tri_mask</span><span class="p">))</span>
        <span class="n">pairwise_state</span> <span class="o">=</span> <span class="n">pairwise_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_drop</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tri_att_start</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">tri_mask</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">pairwise_state</span> <span class="o">=</span> <span class="n">pairwise_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_drop</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tri_att_end</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">tri_mask</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># MLP over pairs.</span>
        <span class="n">pairwise_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_pair</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">sequence_state</span><span class="p">,</span> <span class="n">pairwise_state</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldTriangularSelfAttentionBlock</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method initializes an instance of the EsmFoldTriangularSelfAttentionBlock class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmFoldTriangularSelfAttentionBlock class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing parameters for the attention block.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>NotImplementedError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the method is not implemented for any reason.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the provided configuration object is invalid or missing required parameters.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the provided configuration object is of incorrect type.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method initializes an instance of the EsmFoldTriangularSelfAttentionBlock class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the EsmFoldTriangularSelfAttentionBlock class.</span>
<span class="sd">        config: The configuration object containing parameters for the attention block.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        NotImplementedError: If the method is not implemented for any reason.</span>
<span class="sd">        ValueError: If the provided configuration object is invalid or missing required parameters.</span>
<span class="sd">        TypeError: If the provided configuration object is of incorrect type.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="n">sequence_state_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_state_dim</span>
    <span class="n">pairwise_state_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span>
    <span class="n">sequence_num_heads</span> <span class="o">=</span> <span class="n">sequence_state_dim</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_head_width</span>
    <span class="n">pairwise_num_heads</span> <span class="o">=</span> <span class="n">pairwise_state_dim</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_head_width</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">sequence_state_dim</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">sequence_to_pair</span> <span class="o">=</span> <span class="n">EsmFoldSequenceToPair</span><span class="p">(</span><span class="n">sequence_state_dim</span><span class="p">,</span> <span class="n">pairwise_state_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pairwise_state_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pair_to_sequence</span> <span class="o">=</span> <span class="n">EsmFoldPairToSequence</span><span class="p">(</span><span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">sequence_num_heads</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">seq_attention</span> <span class="o">=</span> <span class="n">EsmFoldSelfAttention</span><span class="p">(</span>
        <span class="n">sequence_state_dim</span><span class="p">,</span> <span class="n">sequence_num_heads</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_head_width</span><span class="p">,</span> <span class="n">gated</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tri_mul_out</span> <span class="o">=</span> <span class="n">EsmFoldTriangleMultiplicativeUpdate</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">_outgoing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tri_mul_in</span> <span class="o">=</span> <span class="n">EsmFoldTriangleMultiplicativeUpdate</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">_outgoing</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">tri_att_start</span> <span class="o">=</span> <span class="n">EsmFoldTriangleAttention</span><span class="p">(</span>
        <span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_head_width</span><span class="p">,</span> <span class="n">pairwise_num_heads</span><span class="p">,</span> <span class="n">inf</span><span class="o">=</span><span class="mf">1e9</span><span class="p">,</span> <span class="n">starting</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tri_att_end</span> <span class="o">=</span> <span class="n">EsmFoldTriangleAttention</span><span class="p">(</span>
        <span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_head_width</span><span class="p">,</span> <span class="n">pairwise_num_heads</span><span class="p">,</span> <span class="n">inf</span><span class="o">=</span><span class="mf">1e9</span><span class="p">,</span> <span class="n">starting</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_seq</span> <span class="o">=</span> <span class="n">EsmFoldResidueMLP</span><span class="p">(</span><span class="n">sequence_state_dim</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">sequence_state_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_pair</span> <span class="o">=</span> <span class="n">EsmFoldResidueMLP</span><span class="p">(</span><span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">pairwise_state_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">row_drop</span> <span class="o">=</span> <span class="n">EsmFoldDropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">col_drop</span> <span class="o">=</span> <span class="n">EsmFoldDropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldTriangularSelfAttentionBlock</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">sequence_state</span><span class="p">,</span> <span class="n">pairwise_state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">__kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldTriangularSelfAttentionBlock.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<details class="inputs" open>
  <summary>Inputs</summary>
  <p>tensor of valid positions</p>
</details>

<details class="output" open>
  <summary>Output</summary>
  
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence_state</span><span class="p">,</span> <span class="n">pairwise_state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">__kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inputs:</span>
<span class="sd">      sequence_state: B x L x sequence_state_dim pairwise_state: B x L x L x pairwise_state_dim mask: B x L boolean</span>
<span class="sd">      tensor of valid positions</span>

<span class="sd">    Output:</span>
<span class="sd">      sequence_state: B x L x sequence_state_dim pairwise_state: B x L x L x pairwise_state_dim</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`sequence_state` should be a 3d-tensor, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sequence_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> dims.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`pairwise_state` should be a 4d-tensor, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> dims.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`mask` should be a 2d-tensor, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> dims.&quot;</span><span class="p">)</span>

    <span class="n">batch_dim</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">,</span> <span class="n">sequence_state_dim</span> <span class="o">=</span> <span class="n">sequence_state</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">pairwise_state_dim</span> <span class="o">=</span> <span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">sequence_state_dim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_state_dim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;`sequence_state` last dimension should be equal to `self.sequence_state_dim`. Got &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">sequence_state_dim</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sequence_state_dim</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">pairwise_state_dim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;`pairwise_state` last dimension should be equal to `self.pairwise_state_dim`. Got &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">batch_dim</span> <span class="o">!=</span> <span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;`sequence_state` and `pairwise_state` have inconsistent batch size: </span><span class="si">{</span><span class="n">batch_dim</span><span class="si">}</span><span class="s2"> != &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">seq_dim</span> <span class="o">!=</span> <span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">or</span> <span class="n">seq_dim</span> <span class="o">!=</span> <span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;`sequence_state` and `pairwise_state` have inconsistent sequence length: </span><span class="si">{</span><span class="n">seq_dim</span><span class="si">}</span><span class="s2"> != &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> or </span><span class="si">{</span><span class="n">pairwise_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Update sequence state</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_to_sequence</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">)</span>

    <span class="c1"># Self attention with bias + mlp.</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_1</span><span class="p">(</span><span class="n">sequence_state</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_attention</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
    <span class="n">sequence_state</span> <span class="o">=</span> <span class="n">sequence_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">sequence_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_seq</span><span class="p">(</span><span class="n">sequence_state</span><span class="p">)</span>

    <span class="c1"># Update pairwise state</span>
    <span class="n">pairwise_state</span> <span class="o">=</span> <span class="n">pairwise_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_to_pair</span><span class="p">(</span><span class="n">sequence_state</span><span class="p">)</span>

    <span class="c1"># Axial attention with triangular bias.</span>
    <span class="n">tri_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">pairwise_state</span> <span class="o">=</span> <span class="n">pairwise_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tri_mul_out</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">tri_mask</span><span class="p">))</span>
    <span class="n">pairwise_state</span> <span class="o">=</span> <span class="n">pairwise_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tri_mul_in</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">tri_mask</span><span class="p">))</span>
    <span class="n">pairwise_state</span> <span class="o">=</span> <span class="n">pairwise_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_drop</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tri_att_start</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">tri_mask</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pairwise_state</span> <span class="o">=</span> <span class="n">pairwise_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_drop</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tri_att_end</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">tri_mask</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># MLP over pairs.</span>
    <span class="n">pairwise_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_pair</span><span class="p">(</span><span class="n">pairwise_state</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">sequence_state</span><span class="p">,</span> <span class="n">pairwise_state</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>EsmFoldingTrunk is a neural network cell that represents the trunk of the ESM-Fold model.
It inherits from the nn.Module class and contains methods for initializing and forwarding the model, as well as a
static method for computing distograms.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.config">config</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A configuration object specifying the dimensions and parameters for the ESM-Fold model.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.__init__" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the EsmFoldingTrunk instance with the provided configuration.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.set_chunk_size" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.set_chunk_size">set_chunk_size</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Sets the chunk size for processing sequences and pair features.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.forward" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the ESM-Fold model using the provided input tensors and parameters, and returns the</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.distogram" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.distogram">distogram</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>A static method that computes distograms based on the input coordinates and bin parameters.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <p>This class assumes the presence of the required modules and dependencies for the ESM-Fold model.</p>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2696</span>
<span class="normal">2697</span>
<span class="normal">2698</span>
<span class="normal">2699</span>
<span class="normal">2700</span>
<span class="normal">2701</span>
<span class="normal">2702</span>
<span class="normal">2703</span>
<span class="normal">2704</span>
<span class="normal">2705</span>
<span class="normal">2706</span>
<span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span>
<span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span>
<span class="normal">2756</span>
<span class="normal">2757</span>
<span class="normal">2758</span>
<span class="normal">2759</span>
<span class="normal">2760</span>
<span class="normal">2761</span>
<span class="normal">2762</span>
<span class="normal">2763</span>
<span class="normal">2764</span>
<span class="normal">2765</span>
<span class="normal">2766</span>
<span class="normal">2767</span>
<span class="normal">2768</span>
<span class="normal">2769</span>
<span class="normal">2770</span>
<span class="normal">2771</span>
<span class="normal">2772</span>
<span class="normal">2773</span>
<span class="normal">2774</span>
<span class="normal">2775</span>
<span class="normal">2776</span>
<span class="normal">2777</span>
<span class="normal">2778</span>
<span class="normal">2779</span>
<span class="normal">2780</span>
<span class="normal">2781</span>
<span class="normal">2782</span>
<span class="normal">2783</span>
<span class="normal">2784</span>
<span class="normal">2785</span>
<span class="normal">2786</span>
<span class="normal">2787</span>
<span class="normal">2788</span>
<span class="normal">2789</span>
<span class="normal">2790</span>
<span class="normal">2791</span>
<span class="normal">2792</span>
<span class="normal">2793</span>
<span class="normal">2794</span>
<span class="normal">2795</span>
<span class="normal">2796</span>
<span class="normal">2797</span>
<span class="normal">2798</span>
<span class="normal">2799</span>
<span class="normal">2800</span>
<span class="normal">2801</span>
<span class="normal">2802</span>
<span class="normal">2803</span>
<span class="normal">2804</span>
<span class="normal">2805</span>
<span class="normal">2806</span>
<span class="normal">2807</span>
<span class="normal">2808</span>
<span class="normal">2809</span>
<span class="normal">2810</span>
<span class="normal">2811</span>
<span class="normal">2812</span>
<span class="normal">2813</span>
<span class="normal">2814</span>
<span class="normal">2815</span>
<span class="normal">2816</span>
<span class="normal">2817</span>
<span class="normal">2818</span>
<span class="normal">2819</span>
<span class="normal">2820</span>
<span class="normal">2821</span>
<span class="normal">2822</span>
<span class="normal">2823</span>
<span class="normal">2824</span>
<span class="normal">2825</span>
<span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span>
<span class="normal">2842</span>
<span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span>
<span class="normal">2847</span>
<span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmFoldingTrunk</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    EsmFoldingTrunk is a neural network cell that represents the trunk of the ESM-Fold model.</span>
<span class="sd">    It inherits from the nn.Module class and contains methods for initializing and forwarding the model, as well as a</span>
<span class="sd">    static method for computing distograms.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        config: A configuration object specifying the dimensions and parameters for the ESM-Fold model.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the EsmFoldingTrunk instance with the provided configuration.</span>

<span class="sd">        set_chunk_size: Sets the chunk size for processing sequences and pair features.</span>

<span class="sd">        forward: Constructs the ESM-Fold model using the provided input tensors and parameters, and returns the</span>
<span class="sd">        predicted structure wrapped in a Coordinates object.</span>

<span class="sd">        distogram(coords, min_bin, max_bin, num_bins):</span>
<span class="sd">            A static method that computes distograms based on the input coordinates and bin parameters.</span>

<span class="sd">    Note:</span>
<span class="sd">        This class assumes the presence of the required modules and dependencies for the ESM-Fold model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes an instance of the EsmFoldingTrunk class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config:</span>
<span class="sd">                An object containing the configuration parameters for the EsmFoldingTrunk.</span>

<span class="sd">                - sequence_state_dim: An integer representing the dimension of the sequence state.</span>
<span class="sd">                - pairwise_state_dim: An integer representing the dimension of the pairwise state.</span>
<span class="sd">                - num_blocks: An integer specifying the number of blocks.</span>
<span class="sd">                - structure_module: An object containing the configuration parameters for the structure module.</span>

<span class="sd">                    - sequence_dim: An integer representing the dimension of the sequence.</span>
<span class="sd">                    - pairwise_dim: An integer representing the dimension of the pairwise.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="n">c_s</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_state_dim</span>
        <span class="n">c_z</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_positional_embedding</span> <span class="o">=</span> <span class="n">EsmFoldRelativePosition</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">EsmFoldTriangularSelfAttentionBlock</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_blocks</span><span class="p">)])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">recycle_bins</span> <span class="o">=</span> <span class="mi">15</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recycle_s_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_s</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recycle_z_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_z</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recycle_disto</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">recycle_bins</span><span class="p">,</span> <span class="n">c_z</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recycle_disto</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span> <span class="o">=</span> <span class="n">EsmFoldStructureModule</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">structure_module</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trunk2sm_s</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">structure_module</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trunk2sm_z</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_z</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">structure_module</span><span class="o">.</span><span class="n">pairwise_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">chunk_size</span>

    <span class="k">def</span> <span class="nf">set_chunk_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the chunk size for the EsmFoldingTrunk.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the EsmFoldingTrunk class.</span>
<span class="sd">            chunk_size (int): The size of the chunk to be set. It should be a positive integer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># This parameter means the axial attention will be computed</span>
        <span class="c1"># in a chunked manner. This should make the memory used more or less O(L) instead of O(L^2).</span>
        <span class="c1"># It&#39;s equivalent to running a for loop over chunks of the dimension we&#39;re iterative over,</span>
        <span class="c1"># where the chunk_size is the size of the chunks, so 128 would mean to parse 128-length chunks.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size</span> <span class="o">=</span> <span class="n">chunk_size</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_feats</span><span class="p">,</span> <span class="n">pair_feats</span><span class="p">,</span> <span class="n">true_aa</span><span class="p">,</span> <span class="n">residx</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">no_recycles</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            seq_feats: B x L x C tensor of sequence features pair_feats: B x L x L x C tensor of pair features residx: B</span>
<span class="sd">            x L long tensor giving the position in the sequence mask: B x L boolean tensor indicating valid residues</span>

<span class="sd">        Output:</span>
<span class="sd">            predicted_structure: B x L x (num_atoms_per_residue * 3) tensor wrapped in a Coordinates object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">s_s_0</span> <span class="o">=</span> <span class="n">seq_feats</span>
        <span class="n">s_z_0</span> <span class="o">=</span> <span class="n">pair_feats</span>

        <span class="k">if</span> <span class="n">no_recycles</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">no_recycles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_recycles</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">no_recycles</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of recycles must not be negative.&quot;</span><span class="p">)</span>
            <span class="n">no_recycles</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># First &#39;recycle&#39; is just the standard forward pass through the model.</span>

        <span class="k">def</span> <span class="nf">trunk_iter</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">residx</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_positional_embedding</span><span class="p">(</span><span class="n">residx</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
                <span class="n">s</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">residue_index</span><span class="o">=</span><span class="n">residx</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">chunk_size</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">z</span>

        <span class="n">s_s</span> <span class="o">=</span> <span class="n">s_s_0</span>
        <span class="n">s_z</span> <span class="o">=</span> <span class="n">s_z_0</span>
        <span class="n">recycle_s</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">s_s</span><span class="p">)</span>
        <span class="n">recycle_z</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">s_z</span><span class="p">)</span>
        <span class="n">recycle_bins</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="o">*</span><span class="n">s_z</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">no_recycles</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">ContextManagers</span><span class="p">([]):</span>
                <span class="c1"># === Recycling ===</span>
                <span class="n">recycle_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">recycle_s_norm</span><span class="p">(</span><span class="n">recycle_s</span><span class="p">)</span>
                <span class="n">recycle_z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">recycle_z_norm</span><span class="p">(</span><span class="n">recycle_z</span><span class="p">)</span>
                <span class="n">recycle_z</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">recycle_disto</span><span class="p">(</span><span class="n">recycle_bins</span><span class="p">)</span>

                <span class="n">s_s</span><span class="p">,</span> <span class="n">s_z</span> <span class="o">=</span> <span class="n">trunk_iter</span><span class="p">(</span><span class="n">s_s_0</span> <span class="o">+</span> <span class="n">recycle_s</span><span class="p">,</span> <span class="n">s_z_0</span> <span class="o">+</span> <span class="n">recycle_z</span><span class="p">,</span> <span class="n">residx</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

                <span class="c1"># === Structure module ===</span>
                <span class="n">structure</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span><span class="p">(</span>
                    <span class="p">{</span><span class="s2">&quot;single&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk2sm_s</span><span class="p">(</span><span class="n">s_s</span><span class="p">),</span> <span class="s2">&quot;pair&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk2sm_z</span><span class="p">(</span><span class="n">s_z</span><span class="p">)},</span>
                    <span class="n">true_aa</span><span class="p">,</span>
                    <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
                <span class="p">)</span>

                <span class="n">recycle_s</span> <span class="o">=</span> <span class="n">s_s</span>
                <span class="n">recycle_z</span> <span class="o">=</span> <span class="n">s_z</span>
                <span class="c1"># Distogram needs the N, CA, C coordinates, and bin constants same as alphafold.</span>
                <span class="n">recycle_bins</span> <span class="o">=</span> <span class="n">EsmFoldingTrunk</span><span class="o">.</span><span class="n">distogram</span><span class="p">(</span>
                    <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;positions&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][:,</span> <span class="p">:,</span> <span class="p">:</span><span class="mi">3</span><span class="p">],</span>
                    <span class="mf">3.375</span><span class="p">,</span>
                    <span class="mf">21.375</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">recycle_bins</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;s_s&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">s_s</span>
        <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;s_z&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">s_z</span>

        <span class="k">return</span> <span class="n">structure</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">distogram</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">min_bin</span><span class="p">,</span> <span class="n">max_bin</span><span class="p">,</span> <span class="n">num_bins</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to calculate the distance histogram based on the provided coordinates.</span>

<span class="sd">        Args:</span>
<span class="sd">            coords (Tensor): A tensor containing the coordinates of atoms in the structure.</span>
<span class="sd">                Expected shape should be (N, 3, L), where N is the number of atoms, 3 represents x, y, z coordinates,</span>
<span class="sd">                and L is the length of the structure.</span>
<span class="sd">            min_bin (int): The minimum distance value for binning the distances.</span>
<span class="sd">            max_bin (int): The maximum distance value for binning the distances.</span>
<span class="sd">            num_bins (int): The number of bins to divide the distance range into.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: The method calculates the distance histogram and returns the histogram bins.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the input coordinates tensor is not in the expected shape or if any of the distance</span>
<span class="sd">                parameters (min_bin, max_bin, num_bins) are invalid.</span>
<span class="sd">            RuntimeError: If there is an issue with the calculation process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Coords are [... L x 3 x 3], where it&#39;s [N, CA, C] x 3 coordinates.</span>
        <span class="n">boundaries</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
            <span class="n">min_bin</span><span class="p">,</span>
            <span class="n">max_bin</span><span class="p">,</span>
            <span class="n">num_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">boundaries</span> <span class="o">=</span> <span class="n">boundaries</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">N</span><span class="p">,</span> <span class="n">CA</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">coords</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">)]</span>
        <span class="c1"># Infer CB coordinates.</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">CA</span> <span class="o">-</span> <span class="n">N</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">C</span> <span class="o">-</span> <span class="n">CA</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">CB</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.58273431</span> <span class="o">*</span> <span class="n">a</span> <span class="o">+</span> <span class="mf">0.56802827</span> <span class="o">*</span> <span class="n">b</span> <span class="o">-</span> <span class="mf">0.54067466</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">CA</span>
        <span class="n">dists</span> <span class="o">=</span> <span class="p">(</span><span class="n">CB</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">CB</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">bins</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dists</span> <span class="o">&gt;</span> <span class="n">boundaries</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [..., L, L]</span>
        <span class="k">return</span> <span class="n">bins</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldingTrunk</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmFoldingTrunk class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An object containing the configuration parameters for the EsmFoldingTrunk.</p>
<ul>
<li>sequence_state_dim: An integer representing the dimension of the sequence state.</li>
<li>pairwise_state_dim: An integer representing the dimension of the pairwise state.</li>
<li>num_blocks: An integer specifying the number of blocks.</li>
<li>
<p>structure_module: An object containing the configuration parameters for the structure module.</p>
<ul>
<li>sequence_dim: An integer representing the dimension of the sequence.</li>
<li>pairwise_dim: An integer representing the dimension of the pairwise.</li>
</ul>
</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span>
<span class="normal">2756</span>
<span class="normal">2757</span>
<span class="normal">2758</span>
<span class="normal">2759</span>
<span class="normal">2760</span>
<span class="normal">2761</span>
<span class="normal">2762</span>
<span class="normal">2763</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Initializes an instance of the EsmFoldingTrunk class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config:</span>
<span class="sd">            An object containing the configuration parameters for the EsmFoldingTrunk.</span>

<span class="sd">            - sequence_state_dim: An integer representing the dimension of the sequence state.</span>
<span class="sd">            - pairwise_state_dim: An integer representing the dimension of the pairwise state.</span>
<span class="sd">            - num_blocks: An integer specifying the number of blocks.</span>
<span class="sd">            - structure_module: An object containing the configuration parameters for the structure module.</span>

<span class="sd">                - sequence_dim: An integer representing the dimension of the sequence.</span>
<span class="sd">                - pairwise_dim: An integer representing the dimension of the pairwise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="n">c_s</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">sequence_state_dim</span>
    <span class="n">c_z</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pairwise_state_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_positional_embedding</span> <span class="o">=</span> <span class="n">EsmFoldRelativePosition</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">EsmFoldTriangularSelfAttentionBlock</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_blocks</span><span class="p">)])</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">recycle_bins</span> <span class="o">=</span> <span class="mi">15</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">recycle_s_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_s</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">recycle_z_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_z</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">recycle_disto</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">recycle_bins</span><span class="p">,</span> <span class="n">c_z</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">recycle_disto</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span> <span class="o">=</span> <span class="n">EsmFoldStructureModule</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">structure_module</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trunk2sm_s</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">structure_module</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trunk2sm_z</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_z</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">structure_module</span><span class="o">.</span><span class="n">pairwise_dim</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">chunk_size</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.distogram" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldingTrunk</span><span class="o">.</span><span class="n">distogram</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">min_bin</span><span class="p">,</span> <span class="n">max_bin</span><span class="p">,</span> <span class="n">num_bins</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.distogram" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Method to calculate the distance histogram based on the provided coordinates.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>coords</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tensor containing the coordinates of atoms in the structure.
Expected shape should be (N, 3, L), where N is the number of atoms, 3 represents x, y, z coordinates,
and L is the length of the structure.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_bin</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The minimum distance value for binning the distances.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_bin</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum distance value for binning the distances.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_bins</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of bins to divide the distance range into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The method calculates the distance histogram and returns the histogram bins.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input coordinates tensor is not in the expected shape or if any of the distance
parameters (min_bin, max_bin, num_bins) are invalid.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If there is an issue with the calculation process.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">distogram</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">min_bin</span><span class="p">,</span> <span class="n">max_bin</span><span class="p">,</span> <span class="n">num_bins</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method to calculate the distance histogram based on the provided coordinates.</span>

<span class="sd">    Args:</span>
<span class="sd">        coords (Tensor): A tensor containing the coordinates of atoms in the structure.</span>
<span class="sd">            Expected shape should be (N, 3, L), where N is the number of atoms, 3 represents x, y, z coordinates,</span>
<span class="sd">            and L is the length of the structure.</span>
<span class="sd">        min_bin (int): The minimum distance value for binning the distances.</span>
<span class="sd">        max_bin (int): The maximum distance value for binning the distances.</span>
<span class="sd">        num_bins (int): The number of bins to divide the distance range into.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: The method calculates the distance histogram and returns the histogram bins.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the input coordinates tensor is not in the expected shape or if any of the distance</span>
<span class="sd">            parameters (min_bin, max_bin, num_bins) are invalid.</span>
<span class="sd">        RuntimeError: If there is an issue with the calculation process.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Coords are [... L x 3 x 3], where it&#39;s [N, CA, C] x 3 coordinates.</span>
    <span class="n">boundaries</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
        <span class="n">min_bin</span><span class="p">,</span>
        <span class="n">max_bin</span><span class="p">,</span>
        <span class="n">num_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">boundaries</span> <span class="o">=</span> <span class="n">boundaries</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">CA</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">coords</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">)]</span>
    <span class="c1"># Infer CB coordinates.</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">CA</span> <span class="o">-</span> <span class="n">N</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">C</span> <span class="o">-</span> <span class="n">CA</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">CB</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.58273431</span> <span class="o">*</span> <span class="n">a</span> <span class="o">+</span> <span class="mf">0.56802827</span> <span class="o">*</span> <span class="n">b</span> <span class="o">-</span> <span class="mf">0.54067466</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">CA</span>
    <span class="n">dists</span> <span class="o">=</span> <span class="p">(</span><span class="n">CB</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">CB</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dists</span> <span class="o">&gt;</span> <span class="n">boundaries</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [..., L, L]</span>
    <span class="k">return</span> <span class="n">bins</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldingTrunk</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">seq_feats</span><span class="p">,</span> <span class="n">pair_feats</span><span class="p">,</span> <span class="n">true_aa</span><span class="p">,</span> <span class="n">residx</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">no_recycles</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<details class="inputs" open>
  <summary>Inputs</summary>
  <p>x L long tensor giving the position in the sequence mask: B x L boolean tensor indicating valid residues</p>
</details>

<details class="output" open>
  <summary>Output</summary>
  
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2785</span>
<span class="normal">2786</span>
<span class="normal">2787</span>
<span class="normal">2788</span>
<span class="normal">2789</span>
<span class="normal">2790</span>
<span class="normal">2791</span>
<span class="normal">2792</span>
<span class="normal">2793</span>
<span class="normal">2794</span>
<span class="normal">2795</span>
<span class="normal">2796</span>
<span class="normal">2797</span>
<span class="normal">2798</span>
<span class="normal">2799</span>
<span class="normal">2800</span>
<span class="normal">2801</span>
<span class="normal">2802</span>
<span class="normal">2803</span>
<span class="normal">2804</span>
<span class="normal">2805</span>
<span class="normal">2806</span>
<span class="normal">2807</span>
<span class="normal">2808</span>
<span class="normal">2809</span>
<span class="normal">2810</span>
<span class="normal">2811</span>
<span class="normal">2812</span>
<span class="normal">2813</span>
<span class="normal">2814</span>
<span class="normal">2815</span>
<span class="normal">2816</span>
<span class="normal">2817</span>
<span class="normal">2818</span>
<span class="normal">2819</span>
<span class="normal">2820</span>
<span class="normal">2821</span>
<span class="normal">2822</span>
<span class="normal">2823</span>
<span class="normal">2824</span>
<span class="normal">2825</span>
<span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span>
<span class="normal">2842</span>
<span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_feats</span><span class="p">,</span> <span class="n">pair_feats</span><span class="p">,</span> <span class="n">true_aa</span><span class="p">,</span> <span class="n">residx</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">no_recycles</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inputs:</span>
<span class="sd">        seq_feats: B x L x C tensor of sequence features pair_feats: B x L x L x C tensor of pair features residx: B</span>
<span class="sd">        x L long tensor giving the position in the sequence mask: B x L boolean tensor indicating valid residues</span>

<span class="sd">    Output:</span>
<span class="sd">        predicted_structure: B x L x (num_atoms_per_residue * 3) tensor wrapped in a Coordinates object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s_s_0</span> <span class="o">=</span> <span class="n">seq_feats</span>
    <span class="n">s_z_0</span> <span class="o">=</span> <span class="n">pair_feats</span>

    <span class="k">if</span> <span class="n">no_recycles</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">no_recycles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_recycles</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">no_recycles</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of recycles must not be negative.&quot;</span><span class="p">)</span>
        <span class="n">no_recycles</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># First &#39;recycle&#39; is just the standard forward pass through the model.</span>

    <span class="k">def</span> <span class="nf">trunk_iter</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">residx</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_positional_embedding</span><span class="p">(</span><span class="n">residx</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">residue_index</span><span class="o">=</span><span class="n">residx</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">chunk_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">z</span>

    <span class="n">s_s</span> <span class="o">=</span> <span class="n">s_s_0</span>
    <span class="n">s_z</span> <span class="o">=</span> <span class="n">s_z_0</span>
    <span class="n">recycle_s</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">s_s</span><span class="p">)</span>
    <span class="n">recycle_z</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">s_z</span><span class="p">)</span>
    <span class="n">recycle_bins</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="o">*</span><span class="n">s_z</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">no_recycles</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">ContextManagers</span><span class="p">([]):</span>
            <span class="c1"># === Recycling ===</span>
            <span class="n">recycle_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">recycle_s_norm</span><span class="p">(</span><span class="n">recycle_s</span><span class="p">)</span>
            <span class="n">recycle_z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">recycle_z_norm</span><span class="p">(</span><span class="n">recycle_z</span><span class="p">)</span>
            <span class="n">recycle_z</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">recycle_disto</span><span class="p">(</span><span class="n">recycle_bins</span><span class="p">)</span>

            <span class="n">s_s</span><span class="p">,</span> <span class="n">s_z</span> <span class="o">=</span> <span class="n">trunk_iter</span><span class="p">(</span><span class="n">s_s_0</span> <span class="o">+</span> <span class="n">recycle_s</span><span class="p">,</span> <span class="n">s_z_0</span> <span class="o">+</span> <span class="n">recycle_z</span><span class="p">,</span> <span class="n">residx</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

            <span class="c1"># === Structure module ===</span>
            <span class="n">structure</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure_module</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;single&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk2sm_s</span><span class="p">(</span><span class="n">s_s</span><span class="p">),</span> <span class="s2">&quot;pair&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk2sm_z</span><span class="p">(</span><span class="n">s_z</span><span class="p">)},</span>
                <span class="n">true_aa</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
            <span class="p">)</span>

            <span class="n">recycle_s</span> <span class="o">=</span> <span class="n">s_s</span>
            <span class="n">recycle_z</span> <span class="o">=</span> <span class="n">s_z</span>
            <span class="c1"># Distogram needs the N, CA, C coordinates, and bin constants same as alphafold.</span>
            <span class="n">recycle_bins</span> <span class="o">=</span> <span class="n">EsmFoldingTrunk</span><span class="o">.</span><span class="n">distogram</span><span class="p">(</span>
                <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;positions&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][:,</span> <span class="p">:,</span> <span class="p">:</span><span class="mi">3</span><span class="p">],</span>
                <span class="mf">3.375</span><span class="p">,</span>
                <span class="mf">21.375</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">recycle_bins</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;s_s&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">s_s</span>
    <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;s_z&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">s_z</span>

    <span class="k">return</span> <span class="n">structure</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.set_chunk_size" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmFoldingTrunk</span><span class="o">.</span><span class="n">set_chunk_size</span><span class="p">(</span><span class="n">chunk_size</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmFoldingTrunk.set_chunk_size" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Sets the chunk size for the EsmFoldingTrunk.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmFoldingTrunk class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>chunk_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the chunk to be set. It should be a positive integer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2765</span>
<span class="normal">2766</span>
<span class="normal">2767</span>
<span class="normal">2768</span>
<span class="normal">2769</span>
<span class="normal">2770</span>
<span class="normal">2771</span>
<span class="normal">2772</span>
<span class="normal">2773</span>
<span class="normal">2774</span>
<span class="normal">2775</span>
<span class="normal">2776</span>
<span class="normal">2777</span>
<span class="normal">2778</span>
<span class="normal">2779</span>
<span class="normal">2780</span>
<span class="normal">2781</span>
<span class="normal">2782</span>
<span class="normal">2783</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_chunk_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the chunk size for the EsmFoldingTrunk.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the EsmFoldingTrunk class.</span>
<span class="sd">        chunk_size (int): The size of the chunk to be set. It should be a positive integer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This parameter means the axial attention will be computed</span>
    <span class="c1"># in a chunked manner. This should make the memory used more or less O(L) instead of O(L^2).</span>
    <span class="c1"># It&#39;s equivalent to running a for loop over chunks of the dimension we&#39;re iterative over,</span>
    <span class="c1"># where the chunk_size is the size of the chunks, so 128 would mean to parse 128-length chunks.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size</span> <span class="o">=</span> <span class="n">chunk_size</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding</code>


<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel" href="#mindnlp.transformers.models.esm.modeling_esm.EsmPreTrainedModel">EsmPreTrainedModel</a></code></p>


        <p>EsmForProteinFolding is a class that represents a model for protein folding using the ESM
(Evolutionary Scale Modeling) approach.
It inherits from EsmPreTrainedModel and implements methods for protein structure prediction and inference.</p>
<p>The class includes methods for initializing the model, converting input sequences to protein structures,
and generating Protein Data Bank (PDB) files from model outputs. It also provides functionality for
language model representations, masking input sequences, and inferring protein structures from input sequences.</p>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">EsmForProteinFolding</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">EsmForProteinFolding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/esmfold_v1&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/esmfold_v1&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;MLKNVQVQLV&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># A tiny random peptide</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">folded_positions</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">positions</span>
</code></pre></div>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2887</span>
<span class="normal">2888</span>
<span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span>
<span class="normal">2967</span>
<span class="normal">2968</span>
<span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span>
<span class="normal">2976</span>
<span class="normal">2977</span>
<span class="normal">2978</span>
<span class="normal">2979</span>
<span class="normal">2980</span>
<span class="normal">2981</span>
<span class="normal">2982</span>
<span class="normal">2983</span>
<span class="normal">2984</span>
<span class="normal">2985</span>
<span class="normal">2986</span>
<span class="normal">2987</span>
<span class="normal">2988</span>
<span class="normal">2989</span>
<span class="normal">2990</span>
<span class="normal">2991</span>
<span class="normal">2992</span>
<span class="normal">2993</span>
<span class="normal">2994</span>
<span class="normal">2995</span>
<span class="normal">2996</span>
<span class="normal">2997</span>
<span class="normal">2998</span>
<span class="normal">2999</span>
<span class="normal">3000</span>
<span class="normal">3001</span>
<span class="normal">3002</span>
<span class="normal">3003</span>
<span class="normal">3004</span>
<span class="normal">3005</span>
<span class="normal">3006</span>
<span class="normal">3007</span>
<span class="normal">3008</span>
<span class="normal">3009</span>
<span class="normal">3010</span>
<span class="normal">3011</span>
<span class="normal">3012</span>
<span class="normal">3013</span>
<span class="normal">3014</span>
<span class="normal">3015</span>
<span class="normal">3016</span>
<span class="normal">3017</span>
<span class="normal">3018</span>
<span class="normal">3019</span>
<span class="normal">3020</span>
<span class="normal">3021</span>
<span class="normal">3022</span>
<span class="normal">3023</span>
<span class="normal">3024</span>
<span class="normal">3025</span>
<span class="normal">3026</span>
<span class="normal">3027</span>
<span class="normal">3028</span>
<span class="normal">3029</span>
<span class="normal">3030</span>
<span class="normal">3031</span>
<span class="normal">3032</span>
<span class="normal">3033</span>
<span class="normal">3034</span>
<span class="normal">3035</span>
<span class="normal">3036</span>
<span class="normal">3037</span>
<span class="normal">3038</span>
<span class="normal">3039</span>
<span class="normal">3040</span>
<span class="normal">3041</span>
<span class="normal">3042</span>
<span class="normal">3043</span>
<span class="normal">3044</span>
<span class="normal">3045</span>
<span class="normal">3046</span>
<span class="normal">3047</span>
<span class="normal">3048</span>
<span class="normal">3049</span>
<span class="normal">3050</span>
<span class="normal">3051</span>
<span class="normal">3052</span>
<span class="normal">3053</span>
<span class="normal">3054</span>
<span class="normal">3055</span>
<span class="normal">3056</span>
<span class="normal">3057</span>
<span class="normal">3058</span>
<span class="normal">3059</span>
<span class="normal">3060</span>
<span class="normal">3061</span>
<span class="normal">3062</span>
<span class="normal">3063</span>
<span class="normal">3064</span>
<span class="normal">3065</span>
<span class="normal">3066</span>
<span class="normal">3067</span>
<span class="normal">3068</span>
<span class="normal">3069</span>
<span class="normal">3070</span>
<span class="normal">3071</span>
<span class="normal">3072</span>
<span class="normal">3073</span>
<span class="normal">3074</span>
<span class="normal">3075</span>
<span class="normal">3076</span>
<span class="normal">3077</span>
<span class="normal">3078</span>
<span class="normal">3079</span>
<span class="normal">3080</span>
<span class="normal">3081</span>
<span class="normal">3082</span>
<span class="normal">3083</span>
<span class="normal">3084</span>
<span class="normal">3085</span>
<span class="normal">3086</span>
<span class="normal">3087</span>
<span class="normal">3088</span>
<span class="normal">3089</span>
<span class="normal">3090</span>
<span class="normal">3091</span>
<span class="normal">3092</span>
<span class="normal">3093</span>
<span class="normal">3094</span>
<span class="normal">3095</span>
<span class="normal">3096</span>
<span class="normal">3097</span>
<span class="normal">3098</span>
<span class="normal">3099</span>
<span class="normal">3100</span>
<span class="normal">3101</span>
<span class="normal">3102</span>
<span class="normal">3103</span>
<span class="normal">3104</span>
<span class="normal">3105</span>
<span class="normal">3106</span>
<span class="normal">3107</span>
<span class="normal">3108</span>
<span class="normal">3109</span>
<span class="normal">3110</span>
<span class="normal">3111</span>
<span class="normal">3112</span>
<span class="normal">3113</span>
<span class="normal">3114</span>
<span class="normal">3115</span>
<span class="normal">3116</span>
<span class="normal">3117</span>
<span class="normal">3118</span>
<span class="normal">3119</span>
<span class="normal">3120</span>
<span class="normal">3121</span>
<span class="normal">3122</span>
<span class="normal">3123</span>
<span class="normal">3124</span>
<span class="normal">3125</span>
<span class="normal">3126</span>
<span class="normal">3127</span>
<span class="normal">3128</span>
<span class="normal">3129</span>
<span class="normal">3130</span>
<span class="normal">3131</span>
<span class="normal">3132</span>
<span class="normal">3133</span>
<span class="normal">3134</span>
<span class="normal">3135</span>
<span class="normal">3136</span>
<span class="normal">3137</span>
<span class="normal">3138</span>
<span class="normal">3139</span>
<span class="normal">3140</span>
<span class="normal">3141</span>
<span class="normal">3142</span>
<span class="normal">3143</span>
<span class="normal">3144</span>
<span class="normal">3145</span>
<span class="normal">3146</span>
<span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span>
<span class="normal">3157</span>
<span class="normal">3158</span>
<span class="normal">3159</span>
<span class="normal">3160</span>
<span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span>
<span class="normal">3192</span>
<span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span>
<span class="normal">3199</span>
<span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span>
<span class="normal">3204</span>
<span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span>
<span class="normal">3225</span>
<span class="normal">3226</span>
<span class="normal">3227</span>
<span class="normal">3228</span>
<span class="normal">3229</span>
<span class="normal">3230</span>
<span class="normal">3231</span>
<span class="normal">3232</span>
<span class="normal">3233</span>
<span class="normal">3234</span>
<span class="normal">3235</span>
<span class="normal">3236</span>
<span class="normal">3237</span>
<span class="normal">3238</span>
<span class="normal">3239</span>
<span class="normal">3240</span>
<span class="normal">3241</span>
<span class="normal">3242</span>
<span class="normal">3243</span>
<span class="normal">3244</span>
<span class="normal">3245</span>
<span class="normal">3246</span>
<span class="normal">3247</span>
<span class="normal">3248</span>
<span class="normal">3249</span>
<span class="normal">3250</span>
<span class="normal">3251</span>
<span class="normal">3252</span>
<span class="normal">3253</span>
<span class="normal">3254</span>
<span class="normal">3255</span>
<span class="normal">3256</span>
<span class="normal">3257</span>
<span class="normal">3258</span>
<span class="normal">3259</span>
<span class="normal">3260</span>
<span class="normal">3261</span>
<span class="normal">3262</span>
<span class="normal">3263</span>
<span class="normal">3264</span>
<span class="normal">3265</span>
<span class="normal">3266</span>
<span class="normal">3267</span>
<span class="normal">3268</span>
<span class="normal">3269</span>
<span class="normal">3270</span>
<span class="normal">3271</span>
<span class="normal">3272</span>
<span class="normal">3273</span>
<span class="normal">3274</span>
<span class="normal">3275</span>
<span class="normal">3276</span>
<span class="normal">3277</span>
<span class="normal">3278</span>
<span class="normal">3279</span>
<span class="normal">3280</span>
<span class="normal">3281</span>
<span class="normal">3282</span>
<span class="normal">3283</span>
<span class="normal">3284</span>
<span class="normal">3285</span>
<span class="normal">3286</span>
<span class="normal">3287</span>
<span class="normal">3288</span>
<span class="normal">3289</span>
<span class="normal">3290</span>
<span class="normal">3291</span>
<span class="normal">3292</span>
<span class="normal">3293</span>
<span class="normal">3294</span>
<span class="normal">3295</span>
<span class="normal">3296</span>
<span class="normal">3297</span>
<span class="normal">3298</span>
<span class="normal">3299</span>
<span class="normal">3300</span>
<span class="normal">3301</span>
<span class="normal">3302</span>
<span class="normal">3303</span>
<span class="normal">3304</span>
<span class="normal">3305</span>
<span class="normal">3306</span>
<span class="normal">3307</span>
<span class="normal">3308</span>
<span class="normal">3309</span>
<span class="normal">3310</span>
<span class="normal">3311</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmForProteinFolding</span><span class="p">(</span><span class="n">EsmPreTrainedModel</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    EsmForProteinFolding is a class that represents a model for protein folding using the ESM</span>
<span class="sd">    (Evolutionary Scale Modeling) approach.</span>
<span class="sd">    It inherits from EsmPreTrainedModel and implements methods for protein structure prediction and inference.</span>

<span class="sd">    The class includes methods for initializing the model, converting input sequences to protein structures,</span>
<span class="sd">    and generating Protein Data Bank (PDB) files from model outputs. It also provides functionality for</span>
<span class="sd">    language model representations, masking input sequences, and inferring protein structures from input sequences.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoTokenizer, EsmForProteinFolding</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; model = EsmForProteinFolding.from_pretrained(&quot;facebook/esmfold_v1&quot;)</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/esmfold_v1&quot;)</span>
<span class="sd">        &gt;&gt;&gt; inputs = tokenizer([&quot;MLKNVQVQLV&quot;], return_tensors=&quot;ms&quot;, add_special_tokens=False)  # A tiny random peptide</span>
<span class="sd">        &gt;&gt;&gt; outputs = model(**inputs)</span>
<span class="sd">        &gt;&gt;&gt; folded_positions = outputs.positions</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;EsmFoldStructureModule&quot;</span><span class="p">,</span> <span class="s2">&quot;EsmFoldTriangularSelfAttentionBlock&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes an instance of the EsmForProteinFolding class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config: An object containing configuration settings for the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">distogram_bins</span> <span class="o">=</span> <span class="mi">64</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">esm</span> <span class="o">=</span> <span class="n">EsmModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">fp16_esm</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">esm_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">esm_attns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">esm_layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">af2_to_esm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_af2_to_esm_from_vocab_list</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">esm_s_combine</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">esm_layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,)))</span>

        <span class="n">trunk_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">trunk</span>
        <span class="n">c_s</span> <span class="o">=</span> <span class="n">trunk_config</span><span class="o">.</span><span class="n">sequence_state_dim</span>
        <span class="n">c_z</span> <span class="o">=</span> <span class="n">trunk_config</span><span class="o">.</span><span class="n">pairwise_state_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">esm_s_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">esm_feats</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">esm_feats</span><span class="p">,</span> <span class="n">c_s</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_s</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># 0 is padding, N is unknown residues, N + 1 is mask.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_tokens_embed</span> <span class="o">=</span> <span class="n">residue_constants</span><span class="o">.</span><span class="n">restype_num</span> <span class="o">+</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unk_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_tokens_embed</span> <span class="o">-</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_tokens_embed</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_cls_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;&lt;cls&gt;&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_mask_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;&lt;mask&gt;&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_eos_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;&lt;eos&gt;&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_padding_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">embed_aa</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_tokens_embed</span><span class="p">,</span> <span class="n">c_s</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span> <span class="o">=</span> <span class="n">EsmFoldingTrunk</span><span class="p">(</span><span class="n">trunk_config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">distogram_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">distogram_bins</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ptm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">distogram_bins</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_tokens_embed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lddt_bins</span> <span class="o">=</span> <span class="mi">50</span>
        <span class="n">structure_module_config</span> <span class="o">=</span> <span class="n">trunk_config</span><span class="o">.</span><span class="n">structure_module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lddt_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">structure_module_config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">structure_module_config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">lddt_head_hid_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">lddt_head_hid_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">lddt_head_hid_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">lddt_head_hid_dim</span><span class="p">,</span> <span class="mi">37</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lddt_bins</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_af2_to_esm_from_vocab_list</span><span class="p">(</span><span class="n">vocab_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a vocabulary list to a mindspore Tensor, specifically for the ESM (Evolutionary Scale Modeling)</span>
<span class="sd">        implementation, in the context of protein folding.</span>

<span class="sd">        Args:</span>
<span class="sd">            vocab_list (List[str]): A list of strings representing the vocabulary.</span>
<span class="sd">                Each string corresponds to a specific residue or token.</span>

<span class="sd">        Returns:</span>
<span class="sd">            mindspore.Tensor: The resulting Tensor representing the reordered vocabulary list.</span>
<span class="sd">                The Tensor contains the indices of the vocabulary list elements, with the first element being the index</span>
<span class="sd">                of &#39;&lt;pad&gt;&#39; and the following elements being the indices of the residues from the &#39;restypes_with_x&#39; list.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        Note:</span>
<span class="sd">            - The &#39;&lt;pad&gt;&#39; element is a special token used for padding sequences.</span>
<span class="sd">            - &#39;residue_constants.restypes_with_x&#39; is a predefined list of residue types with an additional &#39;x&#39; type.</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; vocab_list = [&#39;&lt;pad&gt;&#39;, &#39;A&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;]</span>
<span class="sd">            &gt;&gt;&gt; EsmForProteinFolding._af2_to_esm_from_vocab_list(vocab_list)</span>
<span class="sd">            Tensor(shape=[8], dtype=Int32, value=</span>
<span class="sd">            [0, 1, 2, 3, 4, 5, 6])</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Remember that t is shifted from residue_constants by 1 (0 is padding).</span>
        <span class="n">esm_reorder</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">vocab_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">residue_constants</span><span class="o">.</span><span class="n">restypes_with_x</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">esm_reorder</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">masking_pattern</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_recycles</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EsmForProteinFoldingOutput</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            EsmForProteinFoldingOutput</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; from transformers import AutoTokenizer, EsmForProteinFolding</span>
<span class="sd">            ...</span>
<span class="sd">            &gt;&gt;&gt; model = EsmForProteinFolding.from_pretrained(&quot;facebook/esmfold_v1&quot;)</span>
<span class="sd">            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/esmfold_v1&quot;)</span>
<span class="sd">            &gt;&gt;&gt; inputs = tokenizer([&quot;MLKNVQVQLV&quot;], return_tensors=&quot;ms&quot;, add_special_tokens=False)  # A tiny random peptide</span>
<span class="sd">            &gt;&gt;&gt; outputs = model(**inputs)</span>
<span class="sd">            &gt;&gt;&gt; folded_positions = outputs.positions</span>
<span class="sd">            ```</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span>

        <span class="n">aa</span> <span class="o">=</span> <span class="n">input_ids</span>  <span class="c1"># B x L</span>
        <span class="n">B</span> <span class="o">=</span> <span class="n">aa</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">aa</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">aa</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">L</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="c1"># === ESM ===</span>
        <span class="n">esmaa</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">af2_idx_to_esm_idx</span><span class="p">(</span><span class="n">aa</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">masking_pattern</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">masked_aa</span><span class="p">,</span> <span class="n">esmaa</span><span class="p">,</span> <span class="n">mlm_targets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert_mask</span><span class="p">(</span><span class="n">aa</span><span class="p">,</span> <span class="n">esmaa</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">masking_pattern</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">masked_aa</span> <span class="o">=</span> <span class="n">aa</span>
            <span class="n">mlm_targets</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># We get sequence and pair representations from whatever version of ESM /</span>
        <span class="c1"># configuration we are using. The sequence representation esm_s is always</span>
        <span class="c1"># present. The pair embedding esm_z may be present depending on the</span>
        <span class="c1"># configuration of the model. If esm_z is not used by the model then it</span>
        <span class="c1"># is returned as None here.</span>
        <span class="n">esm_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_language_model_representations</span><span class="p">(</span><span class="n">esmaa</span><span class="p">)</span>

        <span class="c1"># Convert esm_s and esm_z, if present, to the precision used by the trunk and</span>
        <span class="c1"># the structure module. These tensors may be a lower precision if, for example,</span>
        <span class="c1"># we&#39;re running the language model in fp16 precision.</span>
        <span class="n">esm_s</span> <span class="o">=</span> <span class="n">esm_s</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">esm_s_combine</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">esm_ablate_sequence</span><span class="p">:</span>
            <span class="n">esm_s</span> <span class="o">=</span> <span class="n">esm_s</span> <span class="o">*</span> <span class="mi">0</span>

        <span class="c1"># === preprocessing ===</span>
        <span class="n">esm_s</span> <span class="o">=</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">esm_s_combine</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">@</span> <span class="n">esm_s</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">s_s_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_s_mlp</span><span class="p">(</span><span class="n">esm_s</span><span class="p">)</span>

        <span class="n">s_z_0</span> <span class="o">=</span> <span class="n">s_s_0</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">trunk</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">embed_aa</span><span class="p">:</span>
            <span class="n">s_s_0</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">masked_aa</span><span class="p">)</span>

        <span class="n">structure</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span><span class="p">(</span><span class="n">s_s_0</span><span class="p">,</span> <span class="n">s_z_0</span><span class="p">,</span> <span class="n">aa</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">no_recycles</span><span class="o">=</span><span class="n">num_recycles</span><span class="p">)</span>
        <span class="c1"># Documenting what we expect:</span>
        <span class="n">structure</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">v</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">structure</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">k</span>
            <span class="ow">in</span> <span class="p">[</span>
                <span class="s2">&quot;s_z&quot;</span><span class="p">,</span>
                <span class="s2">&quot;s_s&quot;</span><span class="p">,</span>
                <span class="s2">&quot;frames&quot;</span><span class="p">,</span>
                <span class="s2">&quot;sidechain_frames&quot;</span><span class="p">,</span>
                <span class="s2">&quot;unnormalized_angles&quot;</span><span class="p">,</span>
                <span class="s2">&quot;angles&quot;</span><span class="p">,</span>
                <span class="s2">&quot;positions&quot;</span><span class="p">,</span>
                <span class="s2">&quot;states&quot;</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">}</span>

        <span class="c1"># Add BERT mask for the loss to use, if available.</span>
        <span class="k">if</span> <span class="n">mlm_targets</span><span class="p">:</span>
            <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;mlm_targets&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mlm_targets</span>

        <span class="n">disto_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distogram_head</span><span class="p">(</span><span class="n">structure</span><span class="p">[</span><span class="s2">&quot;s_z&quot;</span><span class="p">])</span>
        <span class="n">disto_logits</span> <span class="o">=</span> <span class="p">(</span><span class="n">disto_logits</span> <span class="o">+</span> <span class="n">disto_logits</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;distogram_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">disto_logits</span>

        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">structure</span><span class="p">[</span><span class="s2">&quot;s_s&quot;</span><span class="p">])</span>
        <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;lm_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lm_logits</span>

        <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;aatype&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">aa</span>
        <span class="n">make_atom14_masks</span><span class="p">(</span><span class="n">structure</span><span class="p">)</span>
        <span class="c1"># Of course, this doesn&#39;t respect the true mask because it doesn&#39;t know about it...</span>
        <span class="c1"># We&#39;re not going to properly mask change of index tensors:</span>
        <span class="c1">#    &quot;residx_atom14_to_atom37&quot;,</span>
        <span class="c1">#    &quot;residx_atom37_to_atom14&quot;,</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;atom14_atom_exists&quot;</span><span class="p">,</span>
            <span class="s2">&quot;atom37_atom_exists&quot;</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="n">structure</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;residue_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">position_ids</span>

        <span class="n">lddt_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lddt_head</span><span class="p">(</span><span class="n">structure</span><span class="p">[</span><span class="s2">&quot;states&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">structure</span><span class="p">[</span><span class="s2">&quot;states&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lddt_bins</span><span class="p">)</span>
        <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;lddt_head&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lddt_head</span>
        <span class="n">plddt</span> <span class="o">=</span> <span class="n">categorical_lddt</span><span class="p">(</span><span class="n">lddt_head</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lddt_bins</span><span class="p">)</span>
        <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;plddt&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">plddt</span>

        <span class="n">ptm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptm_head</span><span class="p">(</span><span class="n">structure</span><span class="p">[</span><span class="s2">&quot;s_z&quot;</span><span class="p">])</span>
        <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;ptm_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ptm_logits</span>
        <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;ptm&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_tm</span><span class="p">(</span><span class="n">ptm_logits</span><span class="p">,</span> <span class="n">max_bin</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span> <span class="n">no_bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">distogram_bins</span><span class="p">)</span>
        <span class="n">structure</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">compute_predicted_aligned_error</span><span class="p">(</span><span class="n">ptm_logits</span><span class="p">,</span> <span class="n">max_bin</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span> <span class="n">no_bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">distogram_bins</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">EsmForProteinFoldingOutput</span><span class="p">(</span><span class="o">**</span><span class="n">structure</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">af2_idx_to_esm_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aa</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method &#39;af2_idx_to_esm_idx&#39; is defined in the class &#39;EsmForProteinFolding&#39; and is used to convert the</span>
<span class="sd">        input indices from one representation to another.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class. It is automatically passed as the first argument. Used to access the</span>
<span class="sd">                attributes and methods of the class.</span>
<span class="sd">            aa:</span>
<span class="sd">                A tensor representing the input indices.</span>

<span class="sd">                - Type: torch.Tensor.</span>
<span class="sd">                - Purpose: It is used to calculate the converted indices. Restrictions: Should be a tensor of indices.</span>
<span class="sd">            mask:</span>
<span class="sd">                A tensor representing the mask.</span>

<span class="sd">                - Type: torch.Tensor.</span>
<span class="sd">                - Purpose: It is used to mask the input indices. Restrictions: Should be a tensor of masks.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any value.</span>
<span class="sd">                The converted indices are updated in the instance attribute &#39;af2_to_esm&#39;.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">aa</span> <span class="o">=</span> <span class="p">(</span><span class="n">aa</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">af2_to_esm</span><span class="p">[</span><span class="n">aa</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">compute_language_model_representations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">esmaa</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39; </span>
<span class="sd">        The method &#39;compute_language_model_representations&#39; in the class &#39;EsmForProteinFolding&#39; computes the</span>
<span class="sd">        representations of the language model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            esmaa (mindspore.Tensor): A tensor representing the input data with shape (B, L), where B is the batch size</span>
<span class="sd">                and L is the sequence length.</span>

<span class="sd">        Returns:</span>
<span class="sd">            mindspore.Tensor: A tensor representing the language model representations.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="n">esmaa</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># B = batch size, L = sequence length.</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">bypass_lm</span><span class="p">:</span>
            <span class="n">esm_s</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_s_combine</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_feats</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">esm_s</span>

        <span class="n">bosi</span><span class="p">,</span> <span class="n">eosi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_cls_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_eos_idx</span>
        <span class="n">bos</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">esmaa</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">bosi</span>
        <span class="n">eos</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">esmaa</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_padding_idx</span>
        <span class="n">esmaa</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">bos</span><span class="p">,</span> <span class="n">esmaa</span><span class="p">,</span> <span class="n">eos</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Use the first padding index as eos during inference.</span>
        <span class="n">esmaa</span><span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">B</span><span class="p">),</span> <span class="p">(</span><span class="n">esmaa</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">eosi</span>

        <span class="c1"># _, esm_z, esm_s = self.esm(esmaa, return_pairs=self.config.esmfold_config.use_esm_attn_map)</span>
        <span class="c1"># Because we do not support use_esm_attn_map in the HF port as it is not used in any public models,</span>
        <span class="c1"># esm_z is always None</span>
        <span class="n">esm_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="p">(</span><span class="n">esmaa</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">esmaa</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">]</span>
        <span class="n">esm_s</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">esm_hidden_states</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">esm_s</span> <span class="o">=</span> <span class="n">esm_s</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># B, L, nLayers, C</span>

        <span class="k">return</span> <span class="n">esm_s</span>

    <span class="k">def</span> <span class="nf">bert_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aa</span><span class="p">,</span> <span class="n">esmaa</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">pattern</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method &#39;bert_mask&#39; in the class &#39;EsmForProteinFolding&#39; masks specific elements in the input arrays based on</span>
<span class="sd">        the provided pattern.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            aa (numpy array): The input array of amino acids.</span>
<span class="sd">            esmaa (numpy array): The input array of ESMs for amino acids.</span>
<span class="sd">            mask (int): The mask index to be applied to &#39;aa&#39; where pattern equals 1.</span>
<span class="sd">            pattern (numpy array): The pattern array used to determine which elements to mask.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any explicit value but modifies the input arrays in-place. It returns None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_aa</span> <span class="o">=</span> <span class="n">aa</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">aa</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">new_esmaa</span> <span class="o">=</span> <span class="n">esmaa</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">new_aa</span><span class="p">[</span><span class="n">pattern</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_idx</span>
        <span class="n">target</span><span class="p">[</span><span class="n">pattern</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">new_esmaa</span><span class="p">[</span><span class="n">pattern</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_mask_idx</span>
        <span class="k">return</span> <span class="n">new_aa</span><span class="p">,</span> <span class="n">new_esmaa</span><span class="p">,</span> <span class="n">target</span>

    <span class="k">def</span> <span class="nf">infer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs inference on protein folding using the ESM model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmForProteinFolding): An instance of the EsmForProteinFolding class.</span>
<span class="sd">            seqs (Union[str, List[str]]): The protein sequences to perform inference on.</span>
<span class="sd">                It can be a single sequence as a string or a list of multiple sequences.</span>
<span class="sd">                Each sequence should be a string.</span>
<span class="sd">            position_ids (Optional[Tensor]): The position IDs for the sequences.</span>
<span class="sd">                If None, default position IDs will be used. Default is None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">lst</span> <span class="o">=</span> <span class="p">[</span><span class="n">seqs</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lst</span> <span class="o">=</span> <span class="n">seqs</span>
        <span class="c1"># Returns the raw outputs of the model given an input sequence.</span>
        <span class="n">aatype</span> <span class="o">=</span> <span class="n">collate_dense_tensors</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span>
                    <span class="n">residue_constants</span><span class="o">.</span><span class="n">sequence_to_onehot</span><span class="p">(</span>
                        <span class="n">sequence</span><span class="o">=</span><span class="n">seq</span><span class="p">,</span>
                        <span class="n">mapping</span><span class="o">=</span><span class="n">residue_constants</span><span class="o">.</span><span class="n">restype_order_with_x</span><span class="p">,</span>
                        <span class="n">map_unknown_to_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">lst</span>
            <span class="p">]</span>
        <span class="p">)</span>  <span class="c1"># B=1 x L</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">collate_dense_tensors</span><span class="p">([</span><span class="n">aatype</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">))</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">lst</span><span class="p">])</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">aatype</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">lst</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">position_ids</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">aatype</span><span class="p">,</span>
            <span class="n">mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">output_to_pdb</span><span class="p">(</span><span class="n">output</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the pbd (file) string from the model given the model output.&quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">pdbs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">final_atom_positions</span> <span class="o">=</span> <span class="n">atom14_to_atom37</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;positions&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">output</span><span class="p">)</span>
        <span class="n">final_atom_mask</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">&quot;atom37_atom_exists&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;aatype&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">aa</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">&quot;aatype&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
            <span class="n">pred_pos</span> <span class="o">=</span> <span class="n">final_atom_positions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">final_atom_mask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">resid</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">&quot;residue_index&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">OFProtein</span><span class="p">(</span>
                <span class="n">aatype</span><span class="o">=</span><span class="n">aa</span><span class="p">,</span>
                <span class="n">atom_positions</span><span class="o">=</span><span class="n">pred_pos</span><span class="p">,</span>
                <span class="n">atom_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span>
                <span class="n">residue_index</span><span class="o">=</span><span class="n">resid</span><span class="p">,</span>
                <span class="n">b_factors</span><span class="o">=</span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;plddt&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="n">pdbs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">to_pdb</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">pdbs</span>

    <span class="k">def</span> <span class="nf">infer_pdb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seqs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the pdb (file) string from the model given an input sequence.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_to_pdb</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_pdbs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seqs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the pdb (file) string from the model given an input sequence.&quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_to_pdb</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmForProteinFolding</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmForProteinFolding class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An object containing configuration settings for the model.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span>
<span class="normal">2967</span>
<span class="normal">2968</span>
<span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initializes an instance of the EsmForProteinFolding class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config: An object containing configuration settings for the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">distogram_bins</span> <span class="o">=</span> <span class="mi">64</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">esm</span> <span class="o">=</span> <span class="n">EsmModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">fp16_esm</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">esm_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">esm_attns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">esm_layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">af2_to_esm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_af2_to_esm_from_vocab_list</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_list</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">esm_s_combine</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">esm_layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,)))</span>

    <span class="n">trunk_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">trunk</span>
    <span class="n">c_s</span> <span class="o">=</span> <span class="n">trunk_config</span><span class="o">.</span><span class="n">sequence_state_dim</span>
    <span class="n">c_z</span> <span class="o">=</span> <span class="n">trunk_config</span><span class="o">.</span><span class="n">pairwise_state_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">esm_s_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">esm_feats</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">esm_feats</span><span class="p">,</span> <span class="n">c_s</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_s</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="c1"># 0 is padding, N is unknown residues, N + 1 is mask.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_tokens_embed</span> <span class="o">=</span> <span class="n">residue_constants</span><span class="o">.</span><span class="n">restype_num</span> <span class="o">+</span> <span class="mi">3</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pad_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unk_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_tokens_embed</span> <span class="o">-</span> <span class="mi">2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_tokens_embed</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_cls_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;&lt;cls&gt;&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_mask_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;&lt;mask&gt;&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_eos_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;&lt;eos&gt;&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_padding_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">embed_aa</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_tokens_embed</span><span class="p">,</span> <span class="n">c_s</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span> <span class="o">=</span> <span class="n">EsmFoldingTrunk</span><span class="p">(</span><span class="n">trunk_config</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">distogram_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">distogram_bins</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ptm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">distogram_bins</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_tokens_embed</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lddt_bins</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">structure_module_config</span> <span class="o">=</span> <span class="n">trunk_config</span><span class="o">.</span><span class="n">structure_module</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lddt_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">structure_module_config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">structure_module_config</span><span class="o">.</span><span class="n">sequence_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">lddt_head_hid_dim</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">lddt_head_hid_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">lddt_head_hid_dim</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">lddt_head_hid_dim</span><span class="p">,</span> <span class="mi">37</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lddt_bins</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.af2_idx_to_esm_idx" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmForProteinFolding</span><span class="o">.</span><span class="n">af2_idx_to_esm_idx</span><span class="p">(</span><span class="n">aa</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.af2_idx_to_esm_idx" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method 'af2_idx_to_esm_idx' is defined in the class 'EsmForProteinFolding' and is used to convert the
input indices from one representation to another.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class. It is automatically passed as the first argument. Used to access the
attributes and methods of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>aa</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tensor representing the input indices.</p>
<ul>
<li>Type: torch.Tensor.</li>
<li>Purpose: It is used to calculate the converted indices. Restrictions: Should be a tensor of indices.</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tensor representing the mask.</p>
<ul>
<li>Type: torch.Tensor.</li>
<li>Purpose: It is used to mask the input indices. Restrictions: Should be a tensor of masks.</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method does not return any value.
The converted indices are updated in the instance attribute 'af2_to_esm'.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3132</span>
<span class="normal">3133</span>
<span class="normal">3134</span>
<span class="normal">3135</span>
<span class="normal">3136</span>
<span class="normal">3137</span>
<span class="normal">3138</span>
<span class="normal">3139</span>
<span class="normal">3140</span>
<span class="normal">3141</span>
<span class="normal">3142</span>
<span class="normal">3143</span>
<span class="normal">3144</span>
<span class="normal">3145</span>
<span class="normal">3146</span>
<span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span>
<span class="normal">3157</span>
<span class="normal">3158</span>
<span class="normal">3159</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">af2_idx_to_esm_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aa</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method &#39;af2_idx_to_esm_idx&#39; is defined in the class &#39;EsmForProteinFolding&#39; and is used to convert the</span>
<span class="sd">    input indices from one representation to another.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class. It is automatically passed as the first argument. Used to access the</span>
<span class="sd">            attributes and methods of the class.</span>
<span class="sd">        aa:</span>
<span class="sd">            A tensor representing the input indices.</span>

<span class="sd">            - Type: torch.Tensor.</span>
<span class="sd">            - Purpose: It is used to calculate the converted indices. Restrictions: Should be a tensor of indices.</span>
<span class="sd">        mask:</span>
<span class="sd">            A tensor representing the mask.</span>

<span class="sd">            - Type: torch.Tensor.</span>
<span class="sd">            - Purpose: It is used to mask the input indices. Restrictions: Should be a tensor of masks.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: This method does not return any value.</span>
<span class="sd">            The converted indices are updated in the instance attribute &#39;af2_to_esm&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">aa</span> <span class="o">=</span> <span class="p">(</span><span class="n">aa</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">af2_to_esm</span><span class="p">[</span><span class="n">aa</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.bert_mask" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmForProteinFolding</span><span class="o">.</span><span class="n">bert_mask</span><span class="p">(</span><span class="n">aa</span><span class="p">,</span> <span class="n">esmaa</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">pattern</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.bert_mask" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method 'bert_mask' in the class 'EsmForProteinFolding' masks specific elements in the input arrays based on
the provided pattern.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>aa</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input array of amino acids.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>numpy array</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>esmaa</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input array of ESMs for amino acids.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>numpy array</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The mask index to be applied to 'aa' where pattern equals 1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pattern</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The pattern array used to determine which elements to mask.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>numpy array</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method does not return any explicit value but modifies the input arrays in-place. It returns None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span>
<span class="normal">3204</span>
<span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">bert_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aa</span><span class="p">,</span> <span class="n">esmaa</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">pattern</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method &#39;bert_mask&#39; in the class &#39;EsmForProteinFolding&#39; masks specific elements in the input arrays based on</span>
<span class="sd">    the provided pattern.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        aa (numpy array): The input array of amino acids.</span>
<span class="sd">        esmaa (numpy array): The input array of ESMs for amino acids.</span>
<span class="sd">        mask (int): The mask index to be applied to &#39;aa&#39; where pattern equals 1.</span>
<span class="sd">        pattern (numpy array): The pattern array used to determine which elements to mask.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: This method does not return any explicit value but modifies the input arrays in-place. It returns None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">new_aa</span> <span class="o">=</span> <span class="n">aa</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">aa</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">new_esmaa</span> <span class="o">=</span> <span class="n">esmaa</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">new_aa</span><span class="p">[</span><span class="n">pattern</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_idx</span>
    <span class="n">target</span><span class="p">[</span><span class="n">pattern</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">new_esmaa</span><span class="p">[</span><span class="n">pattern</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_mask_idx</span>
    <span class="k">return</span> <span class="n">new_aa</span><span class="p">,</span> <span class="n">new_esmaa</span><span class="p">,</span> <span class="n">target</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.compute_language_model_representations" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmForProteinFolding</span><span class="o">.</span><span class="n">compute_language_model_representations</span><span class="p">(</span><span class="n">esmaa</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.compute_language_model_representations" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>The method 'compute_language_model_representations' in the class 'EsmForProteinFolding' computes the
representations of the language model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>esmaa</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tensor representing the input data with shape (B, L), where B is the batch size
and L is the sequence length.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindspore.Tensor">Tensor</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor: A tensor representing the language model representations.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span>
<span class="normal">3192</span>
<span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">compute_language_model_representations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">esmaa</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39; </span>
<span class="sd">    The method &#39;compute_language_model_representations&#39; in the class &#39;EsmForProteinFolding&#39; computes the</span>
<span class="sd">    representations of the language model.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        esmaa (mindspore.Tensor): A tensor representing the input data with shape (B, L), where B is the batch size</span>
<span class="sd">            and L is the sequence length.</span>

<span class="sd">    Returns:</span>
<span class="sd">        mindspore.Tensor: A tensor representing the language model representations.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="n">esmaa</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># B = batch size, L = sequence length.</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">bypass_lm</span><span class="p">:</span>
        <span class="n">esm_s</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_s_combine</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_feats</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">esm_s</span>

    <span class="n">bosi</span><span class="p">,</span> <span class="n">eosi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_cls_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_eos_idx</span>
    <span class="n">bos</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">esmaa</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">bosi</span>
    <span class="n">eos</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">esmaa</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_dict_padding_idx</span>
    <span class="n">esmaa</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">bos</span><span class="p">,</span> <span class="n">esmaa</span><span class="p">,</span> <span class="n">eos</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Use the first padding index as eos during inference.</span>
    <span class="n">esmaa</span><span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">B</span><span class="p">),</span> <span class="p">(</span><span class="n">esmaa</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">eosi</span>

    <span class="c1"># _, esm_z, esm_s = self.esm(esmaa, return_pairs=self.config.esmfold_config.use_esm_attn_map)</span>
    <span class="c1"># Because we do not support use_esm_attn_map in the HF port as it is not used in any public models,</span>
    <span class="c1"># esm_z is always None</span>
    <span class="n">esm_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm</span><span class="p">(</span><span class="n">esmaa</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">esmaa</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">]</span>
    <span class="n">esm_s</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">esm_hidden_states</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">esm_s</span> <span class="o">=</span> <span class="n">esm_s</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># B, L, nLayers, C</span>

    <span class="k">return</span> <span class="n">esm_s</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmForProteinFolding</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masking_pattern</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_recycles</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFoldingOutput" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFoldingOutput">EsmForProteinFoldingOutput</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>EsmForProteinFoldingOutput</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">EsmForProteinFolding</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">EsmForProteinFolding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/esmfold_v1&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/esmfold_v1&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;MLKNVQVQLV&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># A tiny random peptide</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">folded_positions</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">positions</span>
</code></pre></div>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3011</span>
<span class="normal">3012</span>
<span class="normal">3013</span>
<span class="normal">3014</span>
<span class="normal">3015</span>
<span class="normal">3016</span>
<span class="normal">3017</span>
<span class="normal">3018</span>
<span class="normal">3019</span>
<span class="normal">3020</span>
<span class="normal">3021</span>
<span class="normal">3022</span>
<span class="normal">3023</span>
<span class="normal">3024</span>
<span class="normal">3025</span>
<span class="normal">3026</span>
<span class="normal">3027</span>
<span class="normal">3028</span>
<span class="normal">3029</span>
<span class="normal">3030</span>
<span class="normal">3031</span>
<span class="normal">3032</span>
<span class="normal">3033</span>
<span class="normal">3034</span>
<span class="normal">3035</span>
<span class="normal">3036</span>
<span class="normal">3037</span>
<span class="normal">3038</span>
<span class="normal">3039</span>
<span class="normal">3040</span>
<span class="normal">3041</span>
<span class="normal">3042</span>
<span class="normal">3043</span>
<span class="normal">3044</span>
<span class="normal">3045</span>
<span class="normal">3046</span>
<span class="normal">3047</span>
<span class="normal">3048</span>
<span class="normal">3049</span>
<span class="normal">3050</span>
<span class="normal">3051</span>
<span class="normal">3052</span>
<span class="normal">3053</span>
<span class="normal">3054</span>
<span class="normal">3055</span>
<span class="normal">3056</span>
<span class="normal">3057</span>
<span class="normal">3058</span>
<span class="normal">3059</span>
<span class="normal">3060</span>
<span class="normal">3061</span>
<span class="normal">3062</span>
<span class="normal">3063</span>
<span class="normal">3064</span>
<span class="normal">3065</span>
<span class="normal">3066</span>
<span class="normal">3067</span>
<span class="normal">3068</span>
<span class="normal">3069</span>
<span class="normal">3070</span>
<span class="normal">3071</span>
<span class="normal">3072</span>
<span class="normal">3073</span>
<span class="normal">3074</span>
<span class="normal">3075</span>
<span class="normal">3076</span>
<span class="normal">3077</span>
<span class="normal">3078</span>
<span class="normal">3079</span>
<span class="normal">3080</span>
<span class="normal">3081</span>
<span class="normal">3082</span>
<span class="normal">3083</span>
<span class="normal">3084</span>
<span class="normal">3085</span>
<span class="normal">3086</span>
<span class="normal">3087</span>
<span class="normal">3088</span>
<span class="normal">3089</span>
<span class="normal">3090</span>
<span class="normal">3091</span>
<span class="normal">3092</span>
<span class="normal">3093</span>
<span class="normal">3094</span>
<span class="normal">3095</span>
<span class="normal">3096</span>
<span class="normal">3097</span>
<span class="normal">3098</span>
<span class="normal">3099</span>
<span class="normal">3100</span>
<span class="normal">3101</span>
<span class="normal">3102</span>
<span class="normal">3103</span>
<span class="normal">3104</span>
<span class="normal">3105</span>
<span class="normal">3106</span>
<span class="normal">3107</span>
<span class="normal">3108</span>
<span class="normal">3109</span>
<span class="normal">3110</span>
<span class="normal">3111</span>
<span class="normal">3112</span>
<span class="normal">3113</span>
<span class="normal">3114</span>
<span class="normal">3115</span>
<span class="normal">3116</span>
<span class="normal">3117</span>
<span class="normal">3118</span>
<span class="normal">3119</span>
<span class="normal">3120</span>
<span class="normal">3121</span>
<span class="normal">3122</span>
<span class="normal">3123</span>
<span class="normal">3124</span>
<span class="normal">3125</span>
<span class="normal">3126</span>
<span class="normal">3127</span>
<span class="normal">3128</span>
<span class="normal">3129</span>
<span class="normal">3130</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">masking_pattern</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_recycles</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EsmForProteinFoldingOutput</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">        EsmForProteinFoldingOutput</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoTokenizer, EsmForProteinFolding</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; model = EsmForProteinFolding.from_pretrained(&quot;facebook/esmfold_v1&quot;)</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/esmfold_v1&quot;)</span>
<span class="sd">        &gt;&gt;&gt; inputs = tokenizer([&quot;MLKNVQVQLV&quot;], return_tensors=&quot;ms&quot;, add_special_tokens=False)  # A tiny random peptide</span>
<span class="sd">        &gt;&gt;&gt; outputs = model(**inputs)</span>
<span class="sd">        &gt;&gt;&gt; folded_positions = outputs.positions</span>
<span class="sd">        ```</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span>

    <span class="n">aa</span> <span class="o">=</span> <span class="n">input_ids</span>  <span class="c1"># B x L</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">aa</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">aa</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">aa</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">L</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="c1"># === ESM ===</span>
    <span class="n">esmaa</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">af2_idx_to_esm_idx</span><span class="p">(</span><span class="n">aa</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">masking_pattern</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">masked_aa</span><span class="p">,</span> <span class="n">esmaa</span><span class="p">,</span> <span class="n">mlm_targets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert_mask</span><span class="p">(</span><span class="n">aa</span><span class="p">,</span> <span class="n">esmaa</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">masking_pattern</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">masked_aa</span> <span class="o">=</span> <span class="n">aa</span>
        <span class="n">mlm_targets</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># We get sequence and pair representations from whatever version of ESM /</span>
    <span class="c1"># configuration we are using. The sequence representation esm_s is always</span>
    <span class="c1"># present. The pair embedding esm_z may be present depending on the</span>
    <span class="c1"># configuration of the model. If esm_z is not used by the model then it</span>
    <span class="c1"># is returned as None here.</span>
    <span class="n">esm_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_language_model_representations</span><span class="p">(</span><span class="n">esmaa</span><span class="p">)</span>

    <span class="c1"># Convert esm_s and esm_z, if present, to the precision used by the trunk and</span>
    <span class="c1"># the structure module. These tensors may be a lower precision if, for example,</span>
    <span class="c1"># we&#39;re running the language model in fp16 precision.</span>
    <span class="n">esm_s</span> <span class="o">=</span> <span class="n">esm_s</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">esm_s_combine</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">esm_ablate_sequence</span><span class="p">:</span>
        <span class="n">esm_s</span> <span class="o">=</span> <span class="n">esm_s</span> <span class="o">*</span> <span class="mi">0</span>

    <span class="c1"># === preprocessing ===</span>
    <span class="n">esm_s</span> <span class="o">=</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">esm_s_combine</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">@</span> <span class="n">esm_s</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">s_s_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">esm_s_mlp</span><span class="p">(</span><span class="n">esm_s</span><span class="p">)</span>

    <span class="n">s_z_0</span> <span class="o">=</span> <span class="n">s_s_0</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">trunk</span><span class="o">.</span><span class="n">pairwise_state_dim</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">esmfold_config</span><span class="o">.</span><span class="n">embed_aa</span><span class="p">:</span>
        <span class="n">s_s_0</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">masked_aa</span><span class="p">)</span>

    <span class="n">structure</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span><span class="p">(</span><span class="n">s_s_0</span><span class="p">,</span> <span class="n">s_z_0</span><span class="p">,</span> <span class="n">aa</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">no_recycles</span><span class="o">=</span><span class="n">num_recycles</span><span class="p">)</span>
    <span class="c1"># Documenting what we expect:</span>
    <span class="n">structure</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">v</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">structure</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">k</span>
        <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;s_z&quot;</span><span class="p">,</span>
            <span class="s2">&quot;s_s&quot;</span><span class="p">,</span>
            <span class="s2">&quot;frames&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sidechain_frames&quot;</span><span class="p">,</span>
            <span class="s2">&quot;unnormalized_angles&quot;</span><span class="p">,</span>
            <span class="s2">&quot;angles&quot;</span><span class="p">,</span>
            <span class="s2">&quot;positions&quot;</span><span class="p">,</span>
            <span class="s2">&quot;states&quot;</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">}</span>

    <span class="c1"># Add BERT mask for the loss to use, if available.</span>
    <span class="k">if</span> <span class="n">mlm_targets</span><span class="p">:</span>
        <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;mlm_targets&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mlm_targets</span>

    <span class="n">disto_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distogram_head</span><span class="p">(</span><span class="n">structure</span><span class="p">[</span><span class="s2">&quot;s_z&quot;</span><span class="p">])</span>
    <span class="n">disto_logits</span> <span class="o">=</span> <span class="p">(</span><span class="n">disto_logits</span> <span class="o">+</span> <span class="n">disto_logits</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;distogram_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">disto_logits</span>

    <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">structure</span><span class="p">[</span><span class="s2">&quot;s_s&quot;</span><span class="p">])</span>
    <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;lm_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lm_logits</span>

    <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;aatype&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">aa</span>
    <span class="n">make_atom14_masks</span><span class="p">(</span><span class="n">structure</span><span class="p">)</span>
    <span class="c1"># Of course, this doesn&#39;t respect the true mask because it doesn&#39;t know about it...</span>
    <span class="c1"># We&#39;re not going to properly mask change of index tensors:</span>
    <span class="c1">#    &quot;residx_atom14_to_atom37&quot;,</span>
    <span class="c1">#    &quot;residx_atom37_to_atom14&quot;,</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="s2">&quot;atom14_atom_exists&quot;</span><span class="p">,</span>
        <span class="s2">&quot;atom37_atom_exists&quot;</span><span class="p">,</span>
    <span class="p">]:</span>
        <span class="n">structure</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;residue_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">position_ids</span>

    <span class="n">lddt_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lddt_head</span><span class="p">(</span><span class="n">structure</span><span class="p">[</span><span class="s2">&quot;states&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">structure</span><span class="p">[</span><span class="s2">&quot;states&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lddt_bins</span><span class="p">)</span>
    <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;lddt_head&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lddt_head</span>
    <span class="n">plddt</span> <span class="o">=</span> <span class="n">categorical_lddt</span><span class="p">(</span><span class="n">lddt_head</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lddt_bins</span><span class="p">)</span>
    <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;plddt&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">plddt</span>

    <span class="n">ptm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptm_head</span><span class="p">(</span><span class="n">structure</span><span class="p">[</span><span class="s2">&quot;s_z&quot;</span><span class="p">])</span>
    <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;ptm_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ptm_logits</span>
    <span class="n">structure</span><span class="p">[</span><span class="s2">&quot;ptm&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_tm</span><span class="p">(</span><span class="n">ptm_logits</span><span class="p">,</span> <span class="n">max_bin</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span> <span class="n">no_bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">distogram_bins</span><span class="p">)</span>
    <span class="n">structure</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">compute_predicted_aligned_error</span><span class="p">(</span><span class="n">ptm_logits</span><span class="p">,</span> <span class="n">max_bin</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span> <span class="n">no_bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">distogram_bins</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">EsmForProteinFoldingOutput</span><span class="p">(</span><span class="o">**</span><span class="n">structure</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmForProteinFolding</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Performs inference on protein folding using the ESM model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the EsmForProteinFolding class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding" href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding">EsmForProteinFolding</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>seqs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The protein sequences to perform inference on.
It can be a single sequence as a string or a list of multiple sequences.
Each sequence should be a string.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Union">Union</span>[str, <span title="typing.List">List</span>[str]]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The position IDs for the sequences.
If None, default position IDs will be used. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[Tensor]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3226</span>
<span class="normal">3227</span>
<span class="normal">3228</span>
<span class="normal">3229</span>
<span class="normal">3230</span>
<span class="normal">3231</span>
<span class="normal">3232</span>
<span class="normal">3233</span>
<span class="normal">3234</span>
<span class="normal">3235</span>
<span class="normal">3236</span>
<span class="normal">3237</span>
<span class="normal">3238</span>
<span class="normal">3239</span>
<span class="normal">3240</span>
<span class="normal">3241</span>
<span class="normal">3242</span>
<span class="normal">3243</span>
<span class="normal">3244</span>
<span class="normal">3245</span>
<span class="normal">3246</span>
<span class="normal">3247</span>
<span class="normal">3248</span>
<span class="normal">3249</span>
<span class="normal">3250</span>
<span class="normal">3251</span>
<span class="normal">3252</span>
<span class="normal">3253</span>
<span class="normal">3254</span>
<span class="normal">3255</span>
<span class="normal">3256</span>
<span class="normal">3257</span>
<span class="normal">3258</span>
<span class="normal">3259</span>
<span class="normal">3260</span>
<span class="normal">3261</span>
<span class="normal">3262</span>
<span class="normal">3263</span>
<span class="normal">3264</span>
<span class="normal">3265</span>
<span class="normal">3266</span>
<span class="normal">3267</span>
<span class="normal">3268</span>
<span class="normal">3269</span>
<span class="normal">3270</span>
<span class="normal">3271</span>
<span class="normal">3272</span>
<span class="normal">3273</span>
<span class="normal">3274</span>
<span class="normal">3275</span>
<span class="normal">3276</span>
<span class="normal">3277</span>
<span class="normal">3278</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">infer</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">seqs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs inference on protein folding using the ESM model.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmForProteinFolding): An instance of the EsmForProteinFolding class.</span>
<span class="sd">        seqs (Union[str, List[str]]): The protein sequences to perform inference on.</span>
<span class="sd">            It can be a single sequence as a string or a list of multiple sequences.</span>
<span class="sd">            Each sequence should be a string.</span>
<span class="sd">        position_ids (Optional[Tensor]): The position IDs for the sequences.</span>
<span class="sd">            If None, default position IDs will be used. Default is None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">lst</span> <span class="o">=</span> <span class="p">[</span><span class="n">seqs</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lst</span> <span class="o">=</span> <span class="n">seqs</span>
    <span class="c1"># Returns the raw outputs of the model given an input sequence.</span>
    <span class="n">aatype</span> <span class="o">=</span> <span class="n">collate_dense_tensors</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span>
                <span class="n">residue_constants</span><span class="o">.</span><span class="n">sequence_to_onehot</span><span class="p">(</span>
                    <span class="n">sequence</span><span class="o">=</span><span class="n">seq</span><span class="p">,</span>
                    <span class="n">mapping</span><span class="o">=</span><span class="n">residue_constants</span><span class="o">.</span><span class="n">restype_order_with_x</span><span class="p">,</span>
                    <span class="n">map_unknown_to_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">lst</span>
        <span class="p">]</span>
    <span class="p">)</span>  <span class="c1"># B=1 x L</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">collate_dense_tensors</span><span class="p">([</span><span class="n">aatype</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">))</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">lst</span><span class="p">])</span>
    <span class="n">position_ids</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">aatype</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">lst</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">position_ids</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
        <span class="n">aatype</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer_pdb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmForProteinFolding</span><span class="o">.</span><span class="n">infer_pdb</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer_pdb" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the pdb (file) string from the model given an input sequence.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3302</span>
<span class="normal">3303</span>
<span class="normal">3304</span>
<span class="normal">3305</span>
<span class="normal">3306</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">infer_pdb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seqs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the pdb (file) string from the model given an input sequence.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_to_pdb</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer_pdbs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmForProteinFolding</span><span class="o">.</span><span class="n">infer_pdbs</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.infer_pdbs" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the pdb (file) string from the model given an input sequence.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3308</span>
<span class="normal">3309</span>
<span class="normal">3310</span>
<span class="normal">3311</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">infer_pdbs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seqs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the pdb (file) string from the model given an input sequence.&quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_to_pdb</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.output_to_pdb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">EsmForProteinFolding</span><span class="o">.</span><span class="n">output_to_pdb</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFolding.output_to_pdb" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the pbd (file) string from the model given the model output.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3280</span>
<span class="normal">3281</span>
<span class="normal">3282</span>
<span class="normal">3283</span>
<span class="normal">3284</span>
<span class="normal">3285</span>
<span class="normal">3286</span>
<span class="normal">3287</span>
<span class="normal">3288</span>
<span class="normal">3289</span>
<span class="normal">3290</span>
<span class="normal">3291</span>
<span class="normal">3292</span>
<span class="normal">3293</span>
<span class="normal">3294</span>
<span class="normal">3295</span>
<span class="normal">3296</span>
<span class="normal">3297</span>
<span class="normal">3298</span>
<span class="normal">3299</span>
<span class="normal">3300</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">output_to_pdb</span><span class="p">(</span><span class="n">output</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the pbd (file) string from the model given the model output.&quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">pdbs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">final_atom_positions</span> <span class="o">=</span> <span class="n">atom14_to_atom37</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;positions&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">final_atom_mask</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">&quot;atom37_atom_exists&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;aatype&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">aa</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">&quot;aatype&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="n">pred_pos</span> <span class="o">=</span> <span class="n">final_atom_positions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">final_atom_mask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">resid</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">&quot;residue_index&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">OFProtein</span><span class="p">(</span>
            <span class="n">aatype</span><span class="o">=</span><span class="n">aa</span><span class="p">,</span>
            <span class="n">atom_positions</span><span class="o">=</span><span class="n">pred_pos</span><span class="p">,</span>
            <span class="n">atom_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span>
            <span class="n">residue_index</span><span class="o">=</span><span class="n">resid</span><span class="p">,</span>
            <span class="n">b_factors</span><span class="o">=</span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;plddt&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">pdbs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">to_pdb</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pdbs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFoldingOutput" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFoldingOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.EsmForProteinFoldingOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.transformers.modeling_outputs.ModelOutput">ModelOutput</span></code></p>


        <p>Output type of [<code>EsmForProteinFoldingOutput</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>frames</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Output frames.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sidechain_frames</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Output sidechain frames.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unnormalized_angles</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Predicted unnormalized backbone and side chain torsion angles.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>angles</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Predicted backbone and side chain torsion angles.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>positions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Predicted positions of the backbone and side chain atoms.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Hidden states from the protein folding trunk.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s_s</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Per-residue embeddings derived by concatenating the hidden states of each layer of the ESM-2 LM stem.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s_z</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pairwise residue embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>distogram_logits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input logits to the distogram used to compute residue distances.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lm_logits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Logits output by the ESM-2 protein language model stem.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>aatype</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input amino acids (AlphaFold2 indices).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>atom14_atom_exists</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether each atom exists in the atom14 representation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>residx_atom14_to_atom37</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mapping between atoms in the atom14 and atom37 representations.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>residx_atom37_to_atom14</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mapping between atoms in the atom37 and atom14 representations.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>atom37_atom_exists</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether each atom exists in the atom37 representation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>residue_index</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The index of each residue in the protein chain. Unless internal padding tokens are used, this will just be
a sequence of integers from 0 to <code>sequence_length</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lddt_head</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Raw outputs from the lddt head used to compute plddt.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>plddt</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Per-residue confidence scores. Regions of low confidence may indicate areas where the model's prediction is
uncertain, or where the protein structure is disordered.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ptm_logits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Raw logits used for computing ptm.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ptm</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>TM-score output representing the model's high-level confidence in the overall structure.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>aligned_confidence_probs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Per-residue confidence scores for the aligned structure.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>predicted_aligned_error</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Predicted error between the model's prediction and the ground truth.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_predicted_aligned_error</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Per-sample maximum predicted error.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">EsmForProteinFoldingOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Output type of [`EsmForProteinFoldingOutput`].</span>

<span class="sd">    Args:</span>
<span class="sd">        frames (`mindspore.Tensor`):</span>
<span class="sd">            Output frames.</span>
<span class="sd">        sidechain_frames (`mindspore.Tensor`):</span>
<span class="sd">            Output sidechain frames.</span>
<span class="sd">        unnormalized_angles (`mindspore.Tensor`):</span>
<span class="sd">            Predicted unnormalized backbone and side chain torsion angles.</span>
<span class="sd">        angles (`mindspore.Tensor`):</span>
<span class="sd">            Predicted backbone and side chain torsion angles.</span>
<span class="sd">        positions (`mindspore.Tensor`):</span>
<span class="sd">            Predicted positions of the backbone and side chain atoms.</span>
<span class="sd">        states (`mindspore.Tensor`):</span>
<span class="sd">            Hidden states from the protein folding trunk.</span>
<span class="sd">        s_s (`mindspore.Tensor`):</span>
<span class="sd">            Per-residue embeddings derived by concatenating the hidden states of each layer of the ESM-2 LM stem.</span>
<span class="sd">        s_z (`mindspore.Tensor`):</span>
<span class="sd">            Pairwise residue embeddings.</span>
<span class="sd">        distogram_logits (`mindspore.Tensor`):</span>
<span class="sd">            Input logits to the distogram used to compute residue distances.</span>
<span class="sd">        lm_logits (`mindspore.Tensor`):</span>
<span class="sd">            Logits output by the ESM-2 protein language model stem.</span>
<span class="sd">        aatype (`mindspore.Tensor`):</span>
<span class="sd">            Input amino acids (AlphaFold2 indices).</span>
<span class="sd">        atom14_atom_exists (`mindspore.Tensor`):</span>
<span class="sd">            Whether each atom exists in the atom14 representation.</span>
<span class="sd">        residx_atom14_to_atom37 (`mindspore.Tensor`):</span>
<span class="sd">            Mapping between atoms in the atom14 and atom37 representations.</span>
<span class="sd">        residx_atom37_to_atom14 (`mindspore.Tensor`):</span>
<span class="sd">            Mapping between atoms in the atom37 and atom14 representations.</span>
<span class="sd">        atom37_atom_exists (`mindspore.Tensor`):</span>
<span class="sd">            Whether each atom exists in the atom37 representation.</span>
<span class="sd">        residue_index (`mindspore.Tensor`):</span>
<span class="sd">            The index of each residue in the protein chain. Unless internal padding tokens are used, this will just be</span>
<span class="sd">            a sequence of integers from 0 to `sequence_length`.</span>
<span class="sd">        lddt_head (`mindspore.Tensor`):</span>
<span class="sd">            Raw outputs from the lddt head used to compute plddt.</span>
<span class="sd">        plddt (`mindspore.Tensor`):</span>
<span class="sd">            Per-residue confidence scores. Regions of low confidence may indicate areas where the model&#39;s prediction is</span>
<span class="sd">            uncertain, or where the protein structure is disordered.</span>
<span class="sd">        ptm_logits (`mindspore.Tensor`):</span>
<span class="sd">            Raw logits used for computing ptm.</span>
<span class="sd">        ptm (`mindspore.Tensor`):</span>
<span class="sd">            TM-score output representing the model&#39;s high-level confidence in the overall structure.</span>
<span class="sd">        aligned_confidence_probs (`mindspore.Tensor`):</span>
<span class="sd">            Per-residue confidence scores for the aligned structure.</span>
<span class="sd">        predicted_aligned_error (`mindspore.Tensor`):</span>
<span class="sd">            Predicted error between the model&#39;s prediction and the ground truth.</span>
<span class="sd">        max_predicted_aligned_error (`mindspore.Tensor`):</span>
<span class="sd">            Per-sample maximum predicted error.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">frames</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">sidechain_frames</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">unnormalized_angles</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">angles</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">positions</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">s_s</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">s_z</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">distogram_logits</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">lm_logits</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">aatype</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">atom14_atom_exists</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">residx_atom14_to_atom37</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">residx_atom37_to_atom14</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">atom37_atom_exists</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">residue_index</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">lddt_head</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">plddt</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">ptm_logits</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">ptm</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">aligned_confidence_probs</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">predicted_aligned_error</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">max_predicted_aligned_error</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.categorical_lddt" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">categorical_lddt</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.categorical_lddt" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This function calculates the average log-likelihood of a categorical distribution.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The logits representing the categorical distribution.
It should be a 2-dimensional array-like object with shape (n_samples, n_classes),
where n_samples is the number of samples and n_classes is the number of classes.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>array - like</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bins</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of bins used for discretizing the logits. Defaults to 50.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>50</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>float</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The average log-likelihood of the categorical distribution.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the logits parameter is not a 2-dimensional array-like object.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the bins parameter is not a positive integer.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">categorical_lddt</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function calculates the average log-likelihood of a categorical distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (array-like): The logits representing the categorical distribution.</span>
<span class="sd">            It should be a 2-dimensional array-like object with shape (n_samples, n_classes),</span>
<span class="sd">            where n_samples is the number of samples and n_classes is the number of classes.</span>
<span class="sd">        bins (int, optional): The number of bins used for discretizing the logits. Defaults to 50.</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: The average log-likelihood of the categorical distribution.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the logits parameter is not a 2-dimensional array-like object.</span>
<span class="sd">        ValueError: If the bins parameter is not a positive integer.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Logits are ..., 37, bins.</span>
    <span class="k">return</span> <span class="n">EsmCategoricalMixture</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.collate_dense_tensors" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">collate_dense_tensors</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">pad_v</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.collate_dense_tensors" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Takes a list of tensors with the following dimensions:</p>
<p><code>[(d_11, ..., d_1K),
 (d_21, ..., d_2K), ..., (d_N1, ..., d_NK)]</code></p>
<p>and stack + pads them into a single tensor of:</p>
<p><code>(N, max_i=1,N { d_i1 }, ..., max_i=1,N {diK})</code></p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">collate_dense_tensors</span><span class="p">(</span><span class="n">samples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">pad_v</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a list of tensors with the following dimensions:</span>

<span class="sd">    ```[(d_11, ..., d_1K),</span>
<span class="sd">     (d_21, ..., d_2K), ..., (d_N1, ..., d_NK)]```</span>

<span class="sd">    and stack + pads them into a single tensor of:</span>

<span class="sd">    ```(N, max_i=1,N { d_i1 }, ..., max_i=1,N {diK})```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">({</span><span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">})</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Samples has varying dimensions: </span><span class="si">{</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">samples</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">max_shape</span> <span class="o">=</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span> <span class="k">for</span> <span class="n">lst</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">])]</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="o">*</span><span class="n">max_shape</span><span class="p">),</span> <span class="n">pad_v</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
        <span class="n">result_i</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">result_i</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)]</span> <span class="o">=</span> <span class="n">t</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.dict_multimap" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">dict_multimap</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">dicts</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.dict_multimap" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This function takes two parameters: </p>
<ul>
<li>fn: A function that will be applied to the values of the dictionaries.</li>
<li>dicts: A list of dictionaries.</li>
</ul>
<p>The function returns a new dictionary with the same keys as the first dictionary in the input list,
where the values are the result of applying the given function to the corresponding values from all the
input dictionaries.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>KeyError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If a key is not found in the dictionaries.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the values are not suitable for the given function.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">dict_multimap</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">dicts</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function takes two parameters: </span>

<span class="sd">    - fn: A function that will be applied to the values of the dictionaries.</span>
<span class="sd">    - dicts: A list of dictionaries.</span>

<span class="sd">    The function returns a new dictionary with the same keys as the first dictionary in the input list,</span>
<span class="sd">    where the values are the result of applying the given function to the corresponding values from all the</span>
<span class="sd">    input dictionaries.</span>

<span class="sd">    Raises:</span>
<span class="sd">        KeyError: If a key is not found in the dictionaries.</span>
<span class="sd">        TypeError: If the values are not suitable for the given function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">first</span> <span class="o">=</span> <span class="n">dicts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">new_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">first</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">all_v</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dicts</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">new_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">dict_multimap</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">all_v</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">all_v</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">new_dict</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.flatten_final_dims" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">flatten_final_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">no_dims</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.flatten_final_dims" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Flatten the final dimensions of a tensor.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>t</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor to be flattened.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>no_dims</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of dimensions to be flattened.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor: A tensor with the specified number of final dimensions flattened.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input tensor does not have enough dimensions to flatten.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">flatten_final_dims</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">no_dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flatten the final dimensions of a tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        t (mindspore.Tensor): The input tensor to be flattened.</span>
<span class="sd">        no_dims (int): The number of dimensions to be flattened.</span>

<span class="sd">    Returns:</span>
<span class="sd">        mindspore.Tensor: A tensor with the specified number of final dimensions flattened.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the input tensor does not have enough dimensions to flatten.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="n">no_dims</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.get_axial_mask" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">get_axial_mask</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.get_axial_mask" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Helper to convert B x L mask of valid positions to axial mask used in row column attentions.</p>


<details class="input" open>
  <summary>Input</summary>
  
</details>

<details class="output" open>
  <summary>Output</summary>
  
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_axial_mask</span><span class="p">(</span><span class="n">mask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper to convert B x L mask of valid positions to axial mask used in row column attentions.</span>

<span class="sd">    Input:</span>
<span class="sd">        mask: B x L tensor of booleans</span>

<span class="sd">    Output:</span>
<span class="sd">        mask: B x L x L tensor of booleans</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`mask` should be a 2d-tensor, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> dims.&quot;</span><span class="p">)</span>
    <span class="n">batch_dim</span><span class="p">,</span> <span class="n">seq_dim</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="n">batch_dim</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_dim</span> <span class="o">*</span> <span class="n">seq_dim</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.ipa_point_weights_init_" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">ipa_point_weights_init_</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.ipa_point_weights_init_" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Initializes the IPA (International Phonetic Alphabet) point weights.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>weights</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of weights to be initialized.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>list</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">ipa_point_weights_init_</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the IPA (International Phonetic Alphabet) point weights.</span>

<span class="sd">    Args:</span>
<span class="sd">        weights (list): A list of weights to be initialized.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">softplus_inverse_1</span> <span class="o">=</span> <span class="mf">0.541324854612918</span>
    <span class="n">weights</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">softplus_inverse_1</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.permute_final_dims" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">permute_final_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">inds</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.permute_final_dims" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This function permutes the final dimensions of the input tensor based on the provided indices.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tensor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor to be permuted.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of integers representing the indices of the dimensions to be permuted. The dimensions are 0-indexed.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[int]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the indices provided in 'inds' are out of bounds or not in the correct format.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input tensor is not of type mindspore.Tensor.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">permute_final_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">inds</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function permutes the final dimensions of the input tensor based on the provided indices.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (mindspore.Tensor): The input tensor to be permuted.</span>
<span class="sd">        inds (List[int]):</span>
<span class="sd">            A list of integers representing the indices of the dimensions to be permuted. The dimensions are 0-indexed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the indices provided in &#39;inds&#39; are out of bounds or not in the correct format.</span>
<span class="sd">        TypeError: If the input tensor is not of type mindspore.Tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">zero_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">inds</span><span class="p">)</span>
    <span class="n">first_inds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">zero_index</span><span class="p">])))</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">first_inds</span> <span class="o">+</span> <span class="p">[</span><span class="n">zero_index</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inds</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.softmax_no_cast" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">softmax_no_cast</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.softmax_no_cast" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Softmax, but without automatic casting to fp32 when the input is of type bfloat16</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">softmax_no_cast</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax, but without automatic casting to fp32 when the input is of type bfloat16</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">s</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.modeling_esmfold.trunc_normal_init_" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">modeling_esmfold</span><span class="o">.</span><span class="n">trunc_normal_init_</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">fan</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.modeling_esmfold.trunc_normal_init_" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This function initializes weights with a truncated normal distribution.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>weights</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The weights to be initialized.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The scale factor for the standard deviation. Defaults to 1.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>fan</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Specifies the mode for computing the fan. Defaults to 'fan_in'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;fan_in&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the shape of the weights is not valid.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ImportError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If scipy is not available and it is required for the initialization.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\modeling_esmfold.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">trunc_normal_init_</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">fan</span><span class="o">=</span><span class="s2">&quot;fan_in&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function initializes weights with a truncated normal distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        weights (Tensor): The weights to be initialized.</span>
<span class="sd">        scale (float, optional): The scale factor for the standard deviation. Defaults to 1.0.</span>
<span class="sd">        fan (str, optional): Specifies the mode for computing the fan. Defaults to &#39;fan_in&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the shape of the weights is not valid.</span>
<span class="sd">        ImportError: If scipy is not available and it is required for the initialization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_scipy_available</span><span class="p">():</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;This init requires scipy, but scipy was not found, default to an approximation that might not be&quot;</span>
            <span class="s2">&quot; equivalent.&quot;</span>
        <span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">weights</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="n">std</span><span class="p">),</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">std</span><span class="p">))</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">truncnorm</span>

        <span class="n">std</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span> <span class="o">/</span> <span class="n">truncnorm</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">a</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">truncnorm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">a</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">weights</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
        <span class="n">weights</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.esm.tokenization_esm" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.tokenization_esm</code>


<a href="#mindnlp.transformers.models.esm.tokenization_esm" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>Tokenization classes for ESM.</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer" class="doc doc-heading">
            <code>mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer</code>


<a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.tokenization_utils.PreTrainedTokenizer" href="../../tokenization_utils/#mindnlp.transformers.tokenization_utils.PreTrainedTokenizer">PreTrainedTokenizer</a></code></p>


        <p>Constructs an ESM tokenizer.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\esm\tokenization_esm.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">EsmTokenizer</span><span class="p">(</span><span class="n">PreTrainedTokenizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs an ESM tokenizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">vocab_files_names</span> <span class="o">=</span> <span class="n">VOCAB_FILES_NAMES</span>
    <span class="n">pretrained_vocab_files_map</span> <span class="o">=</span> <span class="n">PRETRAINED_VOCAB_FILES_MAP</span>
    <span class="n">max_model_input_sizes</span> <span class="o">=</span> <span class="n">PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES</span>
    <span class="n">model_input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_file</span><span class="p">,</span>
        <span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span>
        <span class="n">cls_token</span><span class="o">=</span><span class="s2">&quot;&lt;cls&gt;&quot;</span><span class="p">,</span>
        <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span>
        <span class="n">mask_token</span><span class="o">=</span><span class="s2">&quot;&lt;mask&gt;&quot;</span><span class="p">,</span>
        <span class="n">eos_token</span><span class="o">=</span><span class="s2">&quot;&lt;eos&gt;&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the EsmTokenizer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class itself.</span>
<span class="sd">            vocab_file (str): The path to the vocabulary file.</span>
<span class="sd">            unk_token (str, optional): The token to represent unknown words. Defaults to &#39;&lt;unk&gt;&#39;.</span>
<span class="sd">            cls_token (str, optional): The token to represent the start of a sequence. Defaults to &#39;&lt;cls&gt;&#39;.</span>
<span class="sd">            pad_token (str, optional): The token to represent padding. Defaults to &#39;&lt;pad&gt;&#39;.</span>
<span class="sd">            mask_token (str, optional): The token to represent masked values. Defaults to &#39;&lt;mask&gt;&#39;.</span>
<span class="sd">            eos_token (str, optional): The token to represent the end of a sequence. Defaults to &#39;&lt;eos&gt;&#39;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_tokens</span> <span class="o">=</span> <span class="n">load_vocab_file</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_id_to_token</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_tokens</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_token_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">tok</span><span class="p">:</span> <span class="n">ind</span> <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="n">tok</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_tokens</span><span class="p">)}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">unk_token</span><span class="o">=</span><span class="n">unk_token</span><span class="p">,</span>
            <span class="n">cls_token</span><span class="o">=</span><span class="n">cls_token</span><span class="p">,</span>
            <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span>
            <span class="n">mask_token</span><span class="o">=</span><span class="n">mask_token</span><span class="p">,</span>
            <span class="n">eos_token</span><span class="o">=</span><span class="n">eos_token</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># TODO, all the tokens are added? But they are also part of the vocab... bit strange.</span>
        <span class="c1"># none of them are special, but they all need special splitting.</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">unique_no_split_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_trie</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unique_no_split_tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_convert_id_to_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts an index to a token using the mapping stored in the EsmTokenizer instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmTokenizer): The instance of the EsmTokenizer class.</span>
<span class="sd">                This parameter is used to access the mapping between indices and tokens.</span>
<span class="sd">            index (int): The index of the token to be converted.</span>
<span class="sd">                This parameter specifies the index of the token for which the conversion is needed.</span>
<span class="sd">                It must be an integer representing the position of the token in the mapping.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: The token corresponding to the provided index.</span>
<span class="sd">                Returns the token associated with the provided index in the mapping.</span>
<span class="sd">                If the index is not found in the mapping, the method returns the unknown token (unk_token).</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_id_to_token</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_convert_token_to_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a token to its corresponding ID using the provided token string.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmTokenizer): An instance of the EsmTokenizer class.</span>
<span class="sd">            token (str): The token to be converted to its corresponding ID.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: The ID corresponding to the input token. If the token is not found in the token-to-ID mapping, </span>
<span class="sd">                 the ID corresponding to the unknown token (unk_token) is returned as a fallback.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method _tokenize in the EsmTokenizer class tokenizes the input text.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmTokenizer): An instance of the EsmTokenizer class.</span>
<span class="sd">            text (str): The input text to be tokenized.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to retrieve the vocabulary from the EsmTokenizer instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmTokenizer):</span>
<span class="sd">                The EsmTokenizer instance itself.</span>

<span class="sd">                - Type: EsmTokenizer object</span>
<span class="sd">                - Purpose: Represents the current instance of the EsmTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict:</span>
<span class="sd">                A dictionary containing the combined vocabulary.</span>

<span class="sd">                - Type: dict</span>
<span class="sd">                - Purpose: Represents the vocabulary with the base vocabulary and any added tokens.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">base_vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_to_id</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">base_vocab</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">base_vocab</span>

    <span class="k">def</span> <span class="nf">token_to_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to retrieve the ID corresponding to a given token from the EsmTokenizer instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmTokenizer): The EsmTokenizer instance on which the method is called.</span>
<span class="sd">            token (str): The input token for which the corresponding ID needs to be retrieved. It should be a string.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: Returns the ID corresponding to the input token from the EsmTokenizer instance.</span>
<span class="sd">                If the token is not found in the internal token-to-ID mapping,</span>
<span class="sd">                the method returns the ID associated with the unknown token (unk_token) if defined.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">id_to_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieve the token associated with the provided index from the EsmTokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmTokenizer): The instance of the EsmTokenizer class.</span>
<span class="sd">            index (int): The index of the token to retrieve.</span>
<span class="sd">                Must be a non-negative integer corresponding to a valid token index.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: The token associated with the provided index.</span>
<span class="sd">                If the index is not found in the mapping, the unknown token (unk_token) is returned.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_id_to_token</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_inputs_with_special_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method builds inputs with special tokens for the EsmTokenizer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the EsmTokenizer class.</span>
<span class="sd">            token_ids_0 (List[int]): List of token IDs for the first sequence.</span>
<span class="sd">            token_ids_1 (Optional[List[int]]): List of token IDs for the second sequence, if present. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List[int]: A list of token IDs representing the input sequences with special tokens added.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: Raised if token_ids_1 is not None and self.eos_token_id is None,</span>
<span class="sd">                indicating that multiple sequences cannot be tokenized when the EOS token is not set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token_id</span><span class="p">]</span>
        <span class="n">sep</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>  <span class="c1"># No sep token in ESM vocabulary</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span>
            <span class="k">return</span> <span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot tokenize multiple sequences when EOS token is not set!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span> <span class="o">+</span> <span class="n">token_ids_1</span> <span class="o">+</span> <span class="n">sep</span>  <span class="c1"># Multiple inputs always have an EOS token</span>

    <span class="k">def</span> <span class="nf">get_special_tokens_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding</span>
<span class="sd">        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (`List[int]`):</span>
<span class="sd">                List of ids of the first sequence.</span>
<span class="sd">            token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">                List of ids of the second sequence.</span>
<span class="sd">            already_has_special_tokens (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not the token list is already formatted with special tokens for the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">already_has_special_tokens</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;You should not supply a second sequence if the provided sequence of &quot;</span>
                    <span class="s2">&quot;ids is already formatted with special tokens for the model.&quot;</span>
                <span class="p">)</span>

            <span class="k">return</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_ids</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">token_ids_0</span><span class="p">]</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">))</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_1</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">mask</span>

    <span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the vocabulary to a text file.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmTokenizer): The instance of the EsmTokenizer class.</span>
<span class="sd">            save_directory (str): The directory path where the vocabulary file will be saved.</span>
<span class="sd">            filename_prefix (str): A prefix to be added to the vocabulary file name. If None, no prefix is added.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            FileNotFoundError: If the specified save_directory does not exist.</span>
<span class="sd">            PermissionError: If the method does not have permission to write to the save_directory.</span>
<span class="sd">            OSError: If an error occurs while opening or writing to the vocabulary file.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;vocab.txt&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_tokens</span><span class="p">))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">vocab_file</span><span class="p">,)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vocab_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method, vocab_size, in the class EsmTokenizer calculates the size of the vocabulary based on the</span>
<span class="sd">        number of unique tokens present.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (EsmTokenizer): The instance of the EsmTokenizer class.</span>
<span class="sd">                This parameter represents the current instance of the EsmTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: The method returns an integer value representing the size of the vocabulary, which is determined</span>
<span class="sd">                by the number of unique tokens present in the instance.</span>

<span class="sd">        Raises:</span>
<span class="sd">            No specific exceptions are raised by this method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_tokens</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.vocab_size" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">tokenization_esm</span><span class="o">.</span><span class="n">EsmTokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.vocab_size" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method, vocab_size, in the class EsmTokenizer calculates the size of the vocabulary based on the
number of unique tokens present.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmTokenizer class.
This parameter represents the current instance of the EsmTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer" href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer">EsmTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>int</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The method returns an integer value representing the size of the vocabulary, which is determined
by the number of unique tokens present in the instance.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>
    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">tokenization_esm</span><span class="o">.</span><span class="n">EsmTokenizer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">unk_token</span><span class="o">=</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">,</span> <span class="n">cls_token</span><span class="o">=</span><span class="s1">&#39;&lt;cls&gt;&#39;</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s1">&#39;&lt;pad&gt;&#39;</span><span class="p">,</span> <span class="n">mask_token</span><span class="o">=</span><span class="s1">&#39;&lt;mask&gt;&#39;</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="s1">&#39;&lt;eos&gt;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the EsmTokenizer class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class itself.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The path to the vocabulary file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unk_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to represent unknown words. Defaults to '<unk>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;unk&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cls_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to represent the start of a sequence. Defaults to '<cls>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;cls&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to represent padding. Defaults to '<pad>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;pad&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to represent masked values. Defaults to '<mask>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;mask&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to represent the end of a sequence. Defaults to '<eos>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;eos&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\tokenization_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_file</span><span class="p">,</span>
    <span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span>
    <span class="n">cls_token</span><span class="o">=</span><span class="s2">&quot;&lt;cls&gt;&quot;</span><span class="p">,</span>
    <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span>
    <span class="n">mask_token</span><span class="o">=</span><span class="s2">&quot;&lt;mask&gt;&quot;</span><span class="p">,</span>
    <span class="n">eos_token</span><span class="o">=</span><span class="s2">&quot;&lt;eos&gt;&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the EsmTokenizer class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class itself.</span>
<span class="sd">        vocab_file (str): The path to the vocabulary file.</span>
<span class="sd">        unk_token (str, optional): The token to represent unknown words. Defaults to &#39;&lt;unk&gt;&#39;.</span>
<span class="sd">        cls_token (str, optional): The token to represent the start of a sequence. Defaults to &#39;&lt;cls&gt;&#39;.</span>
<span class="sd">        pad_token (str, optional): The token to represent padding. Defaults to &#39;&lt;pad&gt;&#39;.</span>
<span class="sd">        mask_token (str, optional): The token to represent masked values. Defaults to &#39;&lt;mask&gt;&#39;.</span>
<span class="sd">        eos_token (str, optional): The token to represent the end of a sequence. Defaults to &#39;&lt;eos&gt;&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">all_tokens</span> <span class="o">=</span> <span class="n">load_vocab_file</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_id_to_token</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_tokens</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_token_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">tok</span><span class="p">:</span> <span class="n">ind</span> <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="n">tok</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_tokens</span><span class="p">)}</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">unk_token</span><span class="o">=</span><span class="n">unk_token</span><span class="p">,</span>
        <span class="n">cls_token</span><span class="o">=</span><span class="n">cls_token</span><span class="p">,</span>
        <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span>
        <span class="n">mask_token</span><span class="o">=</span><span class="n">mask_token</span><span class="p">,</span>
        <span class="n">eos_token</span><span class="o">=</span><span class="n">eos_token</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># TODO, all the tokens are added? But they are also part of the vocab... bit strange.</span>
    <span class="c1"># none of them are special, but they all need special splitting.</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">unique_no_split_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_tokens</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_update_trie</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unique_no_split_tokens</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.build_inputs_with_special_tokens" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">tokenization_esm</span><span class="o">.</span><span class="n">EsmTokenizer</span><span class="o">.</span><span class="n">build_inputs_with_special_tokens</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.build_inputs_with_special_tokens" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method builds inputs with special tokens for the EsmTokenizer class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmTokenizer class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_0</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of token IDs for the first sequence.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[int]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_1</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of token IDs for the second sequence, if present. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[int]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.List">List</span>[int]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>List[int]: A list of token IDs representing the input sequences with special tokens added.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>Raised if token_ids_1 is not None and self.eos_token_id is None,
indicating that multiple sequences cannot be tokenized when the EOS token is not set.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\tokenization_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">build_inputs_with_special_tokens</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method builds inputs with special tokens for the EsmTokenizer class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the EsmTokenizer class.</span>
<span class="sd">        token_ids_0 (List[int]): List of token IDs for the first sequence.</span>
<span class="sd">        token_ids_1 (Optional[List[int]]): List of token IDs for the second sequence, if present. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: A list of token IDs representing the input sequences with special tokens added.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: Raised if token_ids_1 is not None and self.eos_token_id is None,</span>
<span class="sd">            indicating that multiple sequences cannot be tokenized when the EOS token is not set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token_id</span><span class="p">]</span>
    <span class="n">sep</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>  <span class="c1"># No sep token in ESM vocabulary</span>
    <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span>
        <span class="k">return</span> <span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot tokenize multiple sequences when EOS token is not set!&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span> <span class="o">+</span> <span class="n">token_ids_1</span> <span class="o">+</span> <span class="n">sep</span>  <span class="c1"># Multiple inputs always have an EOS token</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.get_special_tokens_mask" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">tokenization_esm</span><span class="o">.</span><span class="n">EsmTokenizer</span><span class="o">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.get_special_tokens_mask" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> or <code>encode_plus</code> methods.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_0</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of ids of the first sequence.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_1</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of ids of the second sequence.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>already_has_special_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not the token list is already formatted with special tokens for the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.List">List</span>[int]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\tokenization_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_special_tokens_mask</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding</span>
<span class="sd">    special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.</span>

<span class="sd">    Args:</span>
<span class="sd">        token_ids_0 (`List[int]`):</span>
<span class="sd">            List of ids of the first sequence.</span>
<span class="sd">        token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">            List of ids of the second sequence.</span>
<span class="sd">        already_has_special_tokens (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not the token list is already formatted with special tokens for the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">already_has_special_tokens</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You should not supply a second sequence if the provided sequence of &quot;</span>
                <span class="s2">&quot;ids is already formatted with special tokens for the model.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_ids</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">token_ids_0</span><span class="p">]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">))</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_1</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">mask</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.get_vocab" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">tokenization_esm</span><span class="o">.</span><span class="n">EsmTokenizer</span><span class="o">.</span><span class="n">get_vocab</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.get_vocab" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Method to retrieve the vocabulary from the EsmTokenizer instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The EsmTokenizer instance itself.</p>
<ul>
<li>Type: EsmTokenizer object</li>
<li>Purpose: Represents the current instance of the EsmTokenizer class.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer" href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer">EsmTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>dict</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A dictionary containing the combined vocabulary.</p>
<ul>
<li>Type: dict</li>
<li>Purpose: Represents the vocabulary with the base vocabulary and any added tokens.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\tokenization_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method to retrieve the vocabulary from the EsmTokenizer instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmTokenizer):</span>
<span class="sd">            The EsmTokenizer instance itself.</span>

<span class="sd">            - Type: EsmTokenizer object</span>
<span class="sd">            - Purpose: Represents the current instance of the EsmTokenizer class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict:</span>
<span class="sd">            A dictionary containing the combined vocabulary.</span>

<span class="sd">            - Type: dict</span>
<span class="sd">            - Purpose: Represents the vocabulary with the base vocabulary and any added tokens.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">base_vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_to_id</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">base_vocab</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">base_vocab</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.id_to_token" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">tokenization_esm</span><span class="o">.</span><span class="n">EsmTokenizer</span><span class="o">.</span><span class="n">id_to_token</span><span class="p">(</span><span class="n">index</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.id_to_token" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Retrieve the token associated with the provided index from the EsmTokenizer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer" href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer">EsmTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>index</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The index of the token to retrieve.
Must be a non-negative integer corresponding to a valid token index.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>str</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The token associated with the provided index.
If the index is not found in the mapping, the unknown token (unk_token) is returned.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\tokenization_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">id_to_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieve the token associated with the provided index from the EsmTokenizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmTokenizer): The instance of the EsmTokenizer class.</span>
<span class="sd">        index (int): The index of the token to retrieve.</span>
<span class="sd">            Must be a non-negative integer corresponding to a valid token index.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: The token associated with the provided index.</span>
<span class="sd">            If the index is not found in the mapping, the unknown token (unk_token) is returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_id_to_token</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.save_vocabulary" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">tokenization_esm</span><span class="o">.</span><span class="n">EsmTokenizer</span><span class="o">.</span><span class="n">save_vocabulary</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.save_vocabulary" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Save the vocabulary to a text file.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the EsmTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer" href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer">EsmTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The directory path where the vocabulary file will be saved.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>filename_prefix</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A prefix to be added to the vocabulary file name. If None, no prefix is added.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>FileNotFoundError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the specified save_directory does not exist.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>PermissionError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the method does not have permission to write to the save_directory.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>OSError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If an error occurs while opening or writing to the vocabulary file.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\tokenization_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the vocabulary to a text file.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmTokenizer): The instance of the EsmTokenizer class.</span>
<span class="sd">        save_directory (str): The directory path where the vocabulary file will be saved.</span>
<span class="sd">        filename_prefix (str): A prefix to be added to the vocabulary file name. If None, no prefix is added.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        FileNotFoundError: If the specified save_directory does not exist.</span>
<span class="sd">        PermissionError: If the method does not have permission to write to the save_directory.</span>
<span class="sd">        OSError: If an error occurs while opening or writing to the vocabulary file.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;vocab.txt&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_tokens</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">vocab_file</span><span class="p">,)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.token_to_id" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">tokenization_esm</span><span class="o">.</span><span class="n">EsmTokenizer</span><span class="o">.</span><span class="n">token_to_id</span><span class="p">(</span><span class="n">token</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer.token_to_id" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Method to retrieve the ID corresponding to a given token from the EsmTokenizer instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The EsmTokenizer instance on which the method is called.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer" href="#mindnlp.transformers.models.esm.tokenization_esm.EsmTokenizer">EsmTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input token for which the corresponding ID needs to be retrieved. It should be a string.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>int</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Returns the ID corresponding to the input token from the EsmTokenizer instance.
If the token is not found in the internal token-to-ID mapping,
the method returns the ID associated with the unknown token (unk_token) if defined.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\tokenization_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">token_to_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method to retrieve the ID corresponding to a given token from the EsmTokenizer instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (EsmTokenizer): The EsmTokenizer instance on which the method is called.</span>
<span class="sd">        token (str): The input token for which the corresponding ID needs to be retrieved. It should be a string.</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Returns the ID corresponding to the input token from the EsmTokenizer instance.</span>
<span class="sd">            If the token is not found in the internal token-to-ID mapping,</span>
<span class="sd">            the method returns the ID associated with the unknown token (unk_token) if defined.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.esm.tokenization_esm.load_vocab_file" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">esm</span><span class="o">.</span><span class="n">tokenization_esm</span><span class="o">.</span><span class="n">load_vocab_file</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.esm.tokenization_esm.load_vocab_file" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Loads a vocabulary file and returns a list of stripped lines.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The path of the vocabulary file to be loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>list</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A list of strings representing each line in the vocabulary file,
with leading and trailing whitespaces removed.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>FileNotFoundError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the specified vocabulary file does not exist.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>PermissionError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If there is a permission issue with accessing the vocabulary file.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>OSError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If there is an error while reading the vocabulary file.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\esm\tokenization_esm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_vocab_file</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Loads a vocabulary file and returns a list of stripped lines.</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_file (str): The path of the vocabulary file to be loaded.</span>

<span class="sd">    Returns:</span>
<span class="sd">        list: A list of strings representing each line in the vocabulary file,</span>
<span class="sd">            with leading and trailing whitespaces removed.</span>

<span class="sd">    Raises:</span>
<span class="sd">        FileNotFoundError: If the specified vocabulary file does not exist.</span>
<span class="sd">        PermissionError: If there is a permission issue with accessing the vocabulary file.</span>
<span class="sd">        OSError: If there is an error while reading the vocabulary file.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../ernie_m/" class="md-footer__link md-footer__link--prev" aria-label="Previous: ernie_m">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                ernie_m
              </div>
            </div>
          </a>
        
        
          
          <a href="../falcon/" class="md-footer__link md-footer__link--next" aria-label="Next: falcon">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                falcon
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 MindSpore Lab and CQU NLP Team.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:lvyufeng@cqu.edu.cn" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindnlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/lu-yu-feng-46-1" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>