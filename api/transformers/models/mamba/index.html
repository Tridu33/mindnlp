
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../luke/">
      
      
        <link rel="next" href="../marian/">
      
      
      <link rel="icon" href="../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>mamba - MindNLP Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindnlp.transformers.models.mamba.modeling_mamba" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="MindNLP Docs" class="md-header__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindNLP Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              mamba
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="./" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../../../zh/api/transformers/models/mamba/" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../tutorials/quick_start/" class="md-tabs__link">
          
  
    
  
  Tutorials

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../supported_models/" class="md-tabs__link">
        
  
    
  
  Supported Models

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../contribute/" class="md-tabs__link">
        
  
    
  
  How-To Contribute

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../accelerate/" class="md-tabs__link">
          
  
    
  
  API Reference

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../notes/changelog/" class="md-tabs__link">
          
  
    
  
  Notes

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="MindNLP Docs" class="md-nav__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    MindNLP Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/data_preprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocess
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_mirror/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Mirror
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../supported_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Supported Models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How-To Contribute
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../accelerate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/load_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    load_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/BaseMapFunction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BaseMapFunction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transforms
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engine
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Engine
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_1" >
        
          
          <label class="md-nav__link" for="__nav_5_4_1" id="__nav_5_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    train_args
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_1">
            <span class="md-nav__icon md-icon"></span>
            train_args
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seq2seq
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_2" >
        
          
          <label class="md-nav__link" for="__nav_5_4_2" id="__nav_5_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_2">
            <span class="md-nav__icon md-icon"></span>
            trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/default_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    default_func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../peft/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_2" >
        
          
          <label class="md-nav__link" for="__nav_5_7_2" id="__nav_5_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tuners
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_2">
            <span class="md-nav__icon md-icon"></span>
            tuners
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adalora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AdaLoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adaption_prompt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adaption_Prompt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/ia3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lokr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoKr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/prompt_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt tuning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_3" >
        
          
          <label class="md-nav__link" for="__nav_5_7_3" id="__nav_5_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/utils/merge_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    merge_utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/peft_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    peft_model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sentence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_2" >
        
          
          <label class="md-nav__link" for="__nav_5_9_2" id="__nav_5_9_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_2">
            <span class="md-nav__icon md-icon"></span>
            generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/generation/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/logits_process/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/stopping_criteria/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stopping_criteria
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/streamers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    streamers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_3" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9_3" id="__nav_5_9_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9_3">
            <span class="md-nav__icon md-icon"></span>
            models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../albert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    albert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    align
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../altclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    altclip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../audio_spectrogram_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    audio_spectrogram_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../auto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    auto
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    autoformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../baichuan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    baichuan
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../barthez/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    barthez
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bartpho/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bartpho
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../beit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bertweet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bertweet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bge_m3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bge_m3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../big_bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    big_bird
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bigbird_pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bigbird_pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../biogpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    biogpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot_small/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot_small
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip_2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bloom
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bridgetower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bros/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bros
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../byt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    byt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../camembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    camembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../canine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    canine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../codegen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    codegen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cogvlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cogvlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cohere/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convnext/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convnext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpmant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpmbee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ctrl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctrl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cvt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cvt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data2vec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data2vec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../decision_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decision_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distilbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    distilbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../electra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    electra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../encodec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encodec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie_m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie_m
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../esm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    esm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    flava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../funnel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    funnel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gemma
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_bigcode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_bigcode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_pangu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_pangu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gptj/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gptj
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../graphormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    graphormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../groupvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    groupvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hubert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hubert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../imagegpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    imagegpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../internlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    internlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jetmoe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jetmoe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlmv2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlmv2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../led/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    led
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava_next/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava_next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../luke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    luke
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    mamba
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    mamba
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_mamba
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_mamba">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaCausalLMOutput" class="md-nav__link">
    <span class="md-ellipsis">
      MambaCausalLMOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaForCausalLM" class="md-nav__link">
    <span class="md-ellipsis">
      MambaForCausalLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MambaForCausalLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaForCausalLM.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaMixer" class="md-nav__link">
    <span class="md-ellipsis">
      MambaMixer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaOutput" class="md-nav__link">
    <span class="md-ellipsis">
      MambaOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      MambaPreTrainedModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaRMSNorm" class="md-nav__link">
    <span class="md-ellipsis">
      MambaRMSNorm
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MambaRMSNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaRMSNorm.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.configuration_mamba" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_mamba
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_mamba">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.configuration_mamba.MambaConfig" class="md-nav__link">
    <span class="md-ellipsis">
      MambaConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MambaConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.configuration_mamba.MambaConfig.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_graph_mamba
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_graph_mamba">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaCache
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaCache">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaForCausalLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaForCausalLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.get_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_output_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.prepare_inputs_for_generation" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_inputs_for_generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.set_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_output_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaMixer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaMixer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaPreTrainedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaPreTrainedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaRMSNorm
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaRMSNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MambaDense" class="md-nav__link">
    <span class="md-ellipsis">
      MambaDense
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MambaDense">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MambaDense.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../marian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    marian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../maskformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maskformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mbart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mbart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minicpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minicpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minigpt4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minigpt4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mixtral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilevit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilevit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../moss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    moss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen_melody/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen_melody
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mvp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mvp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nezha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nezha
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nystromformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nystromformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../olmo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    olmo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../openelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    openelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    opt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../owlvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    owlvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../poolformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    poolformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pop2piano/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pop2piano
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2_moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    resnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roc_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roc_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rwkv
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seamless_m4t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seamless_m4t_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../segformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    segformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seggpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seggpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_encoder_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_encoder_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_to_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_to_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../squeezebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    squeezebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stablelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stablelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    starcoder2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../swiftformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    swiftformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../switch_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    switch_transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../t5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    t5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../table_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    table_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../timesformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    timesformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tinybert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tinybert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../van/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    van
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vipllava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vipllava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vision_text_dual_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision_text_dual_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../visual_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visual_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_conformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_conformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_with_lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_with_lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wavlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    whisper
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../x_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    x_clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta_xl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta_xl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlnet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_4" >
        
          
          <label class="md-nav__link" for="__nav_5_9_4" id="__nav_5_9_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_4">
            <span class="md-nav__icon md-icon"></span>
            pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/pipeline/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/automatic_speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    automatic_speech_recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/document_question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    document_question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/fill_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fill_mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text2text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text2text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/zero_shot_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_shot_classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../configuration_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    configuration_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modeling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_fast/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_fast
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TRL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Notes
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Change Log
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code of Conduct
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_mamba
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_mamba">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaCausalLMOutput" class="md-nav__link">
    <span class="md-ellipsis">
      MambaCausalLMOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaForCausalLM" class="md-nav__link">
    <span class="md-ellipsis">
      MambaForCausalLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MambaForCausalLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaForCausalLM.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaMixer" class="md-nav__link">
    <span class="md-ellipsis">
      MambaMixer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaOutput" class="md-nav__link">
    <span class="md-ellipsis">
      MambaOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      MambaPreTrainedModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaRMSNorm" class="md-nav__link">
    <span class="md-ellipsis">
      MambaRMSNorm
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MambaRMSNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaRMSNorm.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.configuration_mamba" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_mamba
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_mamba">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.configuration_mamba.MambaConfig" class="md-nav__link">
    <span class="md-ellipsis">
      MambaConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MambaConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.configuration_mamba.MambaConfig.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_graph_mamba
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_graph_mamba">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaCache
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaCache">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaForCausalLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaForCausalLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.get_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_output_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.prepare_inputs_for_generation" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_inputs_for_generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.set_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_output_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaMixer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaMixer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaPreTrainedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaPreTrainedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm" class="md-nav__link">
    <span class="md-ellipsis">
      MSMambaRMSNorm
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSMambaRMSNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MambaDense" class="md-nav__link">
    <span class="md-ellipsis">
      MambaDense
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MambaDense">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MambaDense.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindnlp/edit/master/docs/en/api/transformers/models/mamba.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindnlp/raw/master/docs/en/api/transformers/models/mamba.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>mamba</h1>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.mamba.modeling_mamba" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_mamba</code>


<a href="#mindnlp.transformers.models.mamba.modeling_mamba" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>MindSpore MAMBA model.</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_mamba.MambaCausalLMOutput" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_mamba.MambaCausalLMOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaCausalLMOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.utils.ModelOutput">ModelOutput</span></code></p>


        <p>Base class for causal language model (or autoregressive) outputs.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>loss</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Language modeling loss (for next-token prediction).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_params</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The state of the model at the last time step. Can be used in a forward method with the next <code>input_ids</code> to
avoid providing the old <code>input_ids</code>.</p>
<p>Includes both the State space model state matrices after the selective scan, and the Convolutional states</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`MambaCache`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">MambaCausalLMOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for causal language model (or autoregressive) outputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        loss (`mindspore.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided):</span>
<span class="sd">            Language modeling loss (for next-token prediction).</span>
<span class="sd">        logits (`mindspore.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`):</span>
<span class="sd">            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</span>
<span class="sd">        cache_params (`MambaCache`):</span>
<span class="sd">            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to</span>
<span class="sd">            avoid providing the old `input_ids`.</span>

<span class="sd">            Includes both the State space model state matrices after the selective scan, and the Convolutional states</span>
<span class="sd">        hidden_states (`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for the output of the embeddings, if the model has an embedding layer, +</span>
<span class="sd">            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">loss</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">cache_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MambaCache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_mamba.MambaForCausalLM" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_mamba.MambaForCausalLM</code>


<a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaForCausalLM" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_mamba.MambaPreTrainedModel" href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaPreTrainedModel">MambaPreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MambaForCausalLM</span><span class="p">(</span><span class="n">MambaPreTrainedModel</span><span class="p">):</span>
    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;lm_head.weight&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">MambaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_model_kwargs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">ModelOutput</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">num_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;cache_params&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;use_cache&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="ow">and</span> <span class="s2">&quot;cache_position&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span>
            <span class="ow">and</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_position&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_position&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_position&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="n">num_new_tokens</span>

        <span class="k">if</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">model_kwargs</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MambaCache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="c1"># `cache_position` should have been initialized in `generate`</span>
            <span class="k">if</span> <span class="n">cache_position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`cache_position` should not be None as it should have been initialized in &quot;</span>
                    <span class="s2">&quot;`model.generate`, you are responsible for passing in a valid `cache_position` if &quot;</span>
                    <span class="s2">&quot;you are calling `prepare_inputs_for_generation` directly with `use_cache=True`&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">cache_position</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">attention_mask</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># we initialize the `cache_position` to full size of `conv_states` at prefill stage</span>
                <span class="c1"># considering padding will be applied when input length is shorter, and truncation</span>
                <span class="c1"># will be applied when it is longer, so it will be equivalent to always have it match</span>
                <span class="c1"># the length of `cache_params.conv_states`, which is `config.conv_kernel`</span>
                <span class="n">cache_position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">:</span> <span class="n">inputs_embeds</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">}</span>

        <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;cache_params&quot;</span><span class="p">:</span> <span class="n">cache_params</span><span class="p">,</span>
                <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
                <span class="s2">&quot;cache_position&quot;</span><span class="p">:</span> <span class="n">cache_position</span><span class="p">,</span>
                <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">model_inputs</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MambaCache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>  <span class="c1"># for now we need this for generation</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">MambaCausalLMOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set</span>
<span class="sd">            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`</span>
<span class="sd">            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">mamba_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">cache_params</span><span class="o">=</span><span class="n">cache_params</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">cache_position</span><span class="o">=</span><span class="n">cache_position</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Shift so that tokens &lt; n predict n</span>
            <span class="n">shift_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">shift_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
            <span class="c1"># Flatten the tokens</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">shift_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shift_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">MambaCausalLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">cache_params</span><span class="o">=</span><span class="n">mamba_outputs</span><span class="o">.</span><span class="n">cache_params</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">mamba_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_mamba.MambaForCausalLM.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_mamba</span><span class="o">.</span><span class="n">MambaForCausalLM</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cache_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaForCausalLM.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>labels (<code>mindspore.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
    Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
    <code>labels = input_ids</code> Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code> All labels set to <code>-100</code>
    are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code></p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cache_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MambaCache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cache_position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>  <span class="c1"># for now we need this for generation</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">MambaCausalLMOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">        Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set</span>
<span class="sd">        `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`</span>
<span class="sd">        are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="n">mamba_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">cache_params</span><span class="o">=</span><span class="n">cache_params</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
        <span class="n">cache_position</span><span class="o">=</span><span class="n">cache_position</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Shift so that tokens &lt; n predict n</span>
        <span class="n">shift_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">shift_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="c1"># Flatten the tokens</span>
        <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">shift_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shift_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

    <span class="k">return</span> <span class="n">MambaCausalLMOutput</span><span class="p">(</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
        <span class="n">cache_params</span><span class="o">=</span><span class="n">mamba_outputs</span><span class="o">.</span><span class="n">cache_params</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">mamba_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_mamba.MambaMixer" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_mamba.MambaMixer</code>


<a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaMixer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Compute ∆, A, B, C, and D the state space parameters and compute the <code>contextualized_states</code>.
A, D are input independent (see Mamba paper [1] Section 3.5.2 "Interpretation of A" for why A isn't selective)
∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,
and is why Mamba is called <strong>selective</strong> state spaces)</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MambaMixer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute ∆, A, B, C, and D the state space parameters and compute the `contextualized_states`.</span>
<span class="sd">    A, D are input independent (see Mamba paper [1] Section 3.5.2 &quot;Interpretation of A&quot; for why A isn&#39;t selective)</span>
<span class="sd">    ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,</span>
<span class="sd">    and is why Mamba is called **selective** state spaces)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MambaConfig</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_kernel_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_conv_bias</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_conv_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_conv_bias</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">]</span>

        <span class="c1"># projection of the input hidden states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_bias</span><span class="p">)</span>
        <span class="c1"># selective projection used to make dt, B and C input dependant</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># time step projection (discretization)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dt_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># S4D real initialization. These are not discretized!</span>
        <span class="c1"># The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">A_log</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_bias</span>

    <span class="c1"># fmt: off</span>
    <span class="k">def</span> <span class="nf">slow_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MambaCache</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cache_position</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">input_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_states</span><span class="o">.</span><span class="n">dtype</span>
        <span class="c1"># 1. Gated MLP&#39;s linear projection</span>
        <span class="n">projected_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_proj</span><span class="p">(</span><span class="n">input_states</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>                   <span class="c1"># [batch, 2 * intermediate_size, seq_len]</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">gate</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">projected_states</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 2. Convolution sequence transformation</span>
        <span class="k">if</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ssm_state</span> <span class="o">=</span> <span class="n">cache_params</span><span class="o">.</span><span class="n">ssm_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="c1"># use `cache_position.shape[0]` to check whether we are in prefill</span>
            <span class="c1"># stage, it&#39;s equivalent to check `cache_position[0] == 0`, which</span>
            <span class="c1"># breaks dynamo fullgraph constraints</span>
            <span class="k">if</span> <span class="n">cache_position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_kernel_size</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_kernel_size</span> <span class="o">-</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">conv_state</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                        <span class="n">hidden_states</span><span class="p">,</span>
                        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_kernel_size</span> <span class="o">-</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">conv_state</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_kernel_size</span><span class="p">:]</span>

                <span class="n">cache_params</span><span class="o">.</span><span class="n">update_conv_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">conv_state</span><span class="p">,</span> <span class="n">cache_position</span><span class="p">)</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">])</span>     <span class="c1"># [batch, intermediate_size, seq_len]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">conv_state</span> <span class="o">=</span> <span class="n">cache_params</span><span class="o">.</span><span class="n">update_conv_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">cache_position</span><span class="p">)</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">conv_state</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_conv_bias</span><span class="p">:</span>
                    <span class="n">hidden_states</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="o">.</span><span class="n">bias</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>         <span class="c1"># [batch, intermediate_size, 1] : decoding</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ssm_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
            <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">])</span>         <span class="c1"># [batch, intermediate_size, seq_len]</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 3. State Space Model sequence transformation</span>
        <span class="c1"># 3.a. Selection:  [batch, seq_len, self.time_step_rank + self.ssm_state_size * 2]</span>
        <span class="n">ssm_parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_proj</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">time_step</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">ssm_parameters</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">discrete_time_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt_proj</span><span class="p">(</span><span class="n">time_step</span><span class="p">)</span>                                    <span class="c1"># [batch, seq_len, intermediate_size]</span>
        <span class="n">discrete_time_step</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">discrete_time_step</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># [batch, intermediate_size, seq_len]</span>

        <span class="c1"># 3.b. Discretization: B and C to [batch, seq_len, intermediate_size, ssm_state_size] (SRAM)</span>
        <span class="n">A</span> <span class="o">=</span> <span class="o">-</span><span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A_log</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>                                              <span class="c1"># [intermediate_size, ssm_state_size]</span>
        <span class="n">discrete_A</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">discrete_time_step</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span> <span class="c1"># [batch, intermediate_size, seq_len, ssm_state_size]</span>
        <span class="n">discrete_B</span> <span class="o">=</span> <span class="n">discrete_time_step</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>       <span class="c1"># [batch, intermediate_size, seq_len, ssm_state_size]</span>
        <span class="n">deltaB_u</span> <span class="o">=</span> <span class="n">discrete_B</span> <span class="o">*</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>


        <span class="n">scan_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="n">ssm_state</span> <span class="o">=</span> <span class="n">discrete_A</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">ssm_state</span> <span class="o">+</span> <span class="n">deltaB_u</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span>      <span class="c1"># [batch, intermediade_size, ssm_state]</span>
            <span class="n">scan_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">ssm_state</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># [batch, intermediade_size, 1]</span>
            <span class="n">scan_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scan_output</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">scan_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">scan_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                                <span class="c1"># [batch, seq_len, intermediade_size]</span>
        <span class="n">scan_output</span> <span class="o">=</span> <span class="n">scan_output</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span>
        <span class="n">scan_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">scan_output</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">gate</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache_params</span><span class="o">.</span><span class="n">ssm_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">ssm_state</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cache_params</span><span class="o">.</span><span class="n">ssm_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># 4. Final linear projection</span>
        <span class="n">contextualized_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">scan_output</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># [batch, seq_len, hidden_size]</span>
        <span class="k">return</span> <span class="n">contextualized_states</span>
    <span class="c1"># fmt: on</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">cache_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MambaCache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">slow_forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="p">,</span> <span class="n">cache_position</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_mamba.MambaOutput" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_mamba.MambaOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.utils.ModelOutput">ModelOutput</span></code></p>


        <p>Class for the MAMBA model outputs.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>last_hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Sequence of hidden-states at the output of the last layer of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_params</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The state of the model at the last time step. Can be used in a forward method with the next <code>input_ids</code> to
avoid providing the old <code>input_ids</code>.</p>
<p>Includes both the State space model state matrices after the selective scan, and the Convolutional states</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`MambaCache`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">MambaOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for the MAMBA model outputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        last_hidden_state (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):</span>
<span class="sd">            Sequence of hidden-states at the output of the last layer of the model.</span>
<span class="sd">        cache_params (`MambaCache`):</span>
<span class="sd">            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to</span>
<span class="sd">            avoid providing the old `input_ids`.</span>

<span class="sd">            Includes both the State space model state matrices after the selective scan, and the Convolutional states</span>
<span class="sd">        hidden_states (`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for the output of the embeddings, if the model has an embedding layer, +</span>
<span class="sd">            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">last_hidden_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">cache_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MambaCache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_mamba.MambaPreTrainedModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_mamba.MambaPreTrainedModel</code>


<a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaPreTrainedModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.modeling_utils.PreTrainedModel" href="../../modeling_utils/#mindnlp.transformers.modeling_utils.PreTrainedModel">PreTrainedModel</a></code></p>


        <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
models.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MambaPreTrainedModel</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained</span>
<span class="sd">    models.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">config_class</span> <span class="o">=</span> <span class="n">MambaConfig</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;backbone&quot;</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;MambaBlock&quot;</span><span class="p">,</span> <span class="s2">&quot;MambaMixer&quot;</span><span class="p">]</span>
    <span class="n">supports_gradient_checkpointing</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">_is_stateful</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the weights.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">MambaMixer</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">A_log</span><span class="o">.</span><span class="n">_no_weight_decay</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">module</span><span class="o">.</span><span class="n">D</span><span class="o">.</span><span class="n">_no_weight_decay</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">dt_init_std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_rank</span><span class="o">**-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_scale</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_init_scheme</span> <span class="o">==</span> <span class="s2">&quot;constant&quot;</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">dt_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">dt_init_std</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_init_scheme</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">dt_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="n">dt_init_std</span><span class="p">,</span> <span class="n">dt_init_std</span><span class="p">)</span>

            <span class="n">dt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">)</span>
                <span class="o">*</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_max</span><span class="p">)</span> <span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_min</span><span class="p">))</span>
                <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_min</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_floor</span><span class="p">)</span>
            <span class="c1"># # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759</span>
            <span class="n">inv_dt</span> <span class="o">=</span> <span class="n">dt</span> <span class="o">+</span> <span class="n">ops</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="o">-</span><span class="n">ops</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="o">-</span><span class="n">dt</span><span class="p">))</span>
            <span class="k">with</span> <span class="n">no_grad</span><span class="p">():</span>
                <span class="n">module</span><span class="o">.</span><span class="n">dt_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">assign_value</span><span class="p">(</span><span class="n">inv_dt</span><span class="p">)</span>
            <span class="n">module</span><span class="o">.</span><span class="n">dt_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">_no_reinit</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="s2">&quot;_no_reinit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rescale_prenorm_residual</span><span class="p">:</span>
            <span class="c1"># Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:</span>
            <span class="c1">#   &gt; A modified initialization which accounts for the accumulation on the residual path with model depth. Scale</span>
            <span class="c1">#   &gt; the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.</span>
            <span class="c1">#   &gt;   -- GPT-2 :: https://openai.com/blog/better-language-models/</span>
            <span class="c1">#</span>
            <span class="c1"># Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;out_proj.weight&quot;</span><span class="p">]:</span>
                    <span class="c1"># Special Scaled Initialization --&gt; There are 2 Layer Norms per Transformer Block</span>
                    <span class="c1"># Following Pytorch init, except scale by 1/sqrt(2 * n_layer)</span>
                    <span class="c1"># We need to reinit p since this code could be called multiple times</span>
                    <span class="c1"># Having just p *= scale would repeatedly scale it down</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
                    <span class="k">with</span> <span class="n">no_grad</span><span class="p">():</span>
                        <span class="n">p</span> <span class="o">/=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_mamba.MambaRMSNorm" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_mamba.MambaRMSNorm</code>


<a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaRMSNorm" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MambaRMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        MambaRMSNorm is equivalent to T5LayerNorm and LlamaRMSNorm</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">variance</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, eps=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_mamba.MambaRMSNorm.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_mamba</span><span class="o">.</span><span class="n">MambaRMSNorm</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_mamba.MambaRMSNorm.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>MambaRMSNorm is equivalent to T5LayerNorm and LlamaRMSNorm</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MambaRMSNorm is equivalent to T5LayerNorm and LlamaRMSNorm</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span> <span class="o">=</span> <span class="n">eps</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.mamba.configuration_mamba" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.configuration_mamba</code>


<a href="#mindnlp.transformers.models.mamba.configuration_mamba" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>MAMBA configuration</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.configuration_mamba.MambaConfig" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.configuration_mamba.MambaConfig</code>


<a href="#mindnlp.transformers.models.mamba.configuration_mamba.MambaConfig" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.configuration_utils.PretrainedConfig" href="../../configuration_utils/#mindnlp.transformers.configuration_utils.PretrainedConfig">PretrainedConfig</a></code></p>


        <p>This is the configuration class to store the configuration of a [<code>MambaModel</code>]. It is used to instantiate a MAMBA
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the MAMBA
<a href="https://hf-mirror.com/state-spaces/mamba-2.8b">state-spaces/mamba-2.8b</a> architecture.</p>
<p>Configuration objects inherit from [<code>PretrainedConfig</code>] and can be used to control the model outputs. Read the
documentation from [<code>PretrainedConfig</code>] for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vocabulary size of the MAMBA model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling [<code>MambaModel</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 50280</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>50280</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimensionality of the embeddings and hidden states.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 768</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>768</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>state_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>shape of the state space latents.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_hidden_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 32</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>32</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_norm_epsilon</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The epsilon to use in the layer normalization layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 1e-05</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Padding token id.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the beginning of sentence token in the vocabulary.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the end of sentence token in the vocabulary.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>expand</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Expanding factor used to determine the intermediate size.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 2</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>conv_kernel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Size of the convolution kernel.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 4</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to use bias in ["in_proj", "out_proj"] of the mixer block</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_conv_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to use bias in the convolution layer of the mixer block.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_act</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The non-linear activation function (function or string) in the decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;silu&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;silu&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>initializer_range</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>residual_in_fp32</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not residuals should be in <code>float32</code>. If set to <code>False</code> residuals will keep the same <code>dtype</code>
as the rest of the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_rank</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Rank of the the discretization projection matrix. <code>"auto"</code> means that it will default to
<code>math.ceil(self.hidden_size / 16)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[int,str]`, *optional*, defaults to `&#34;auto&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;auto&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Scale used used to scale <code>dt_proj.bias</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_min</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Minimum <code>time_step</code> used to bound <code>dt_proj.bias</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.001</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.001</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_max</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Maximum <code>time_step</code> used to bound <code>dt_proj.bias</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_init_scheme</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Init scheme used for <code>dt_proj.weight</code>. Should be one of <code>["random","uniform"]</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to `&#34;random&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;random&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_floor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Minimum clamping value of the <code>dt_proj.bias</code> layer initialization.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.0001</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0001</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rescale_prenorm_residual</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to rescale <code>out_proj</code> weights when initializing.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not the cache should be used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">MambaConfig</span><span class="p">,</span> <span class="n">MambaModel</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a Mamba configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">MambaConfig</span><span class="p">()</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a model (with random weights) from the configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">MambaModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Accessing the model configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</code></pre></div>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\configuration_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MambaConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the configuration class to store the configuration of a [`MambaModel`]. It is used to instantiate a MAMBA</span>
<span class="sd">    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the</span>
<span class="sd">    defaults will yield a similar configuration to that of the MAMBA</span>
<span class="sd">    [state-spaces/mamba-2.8b](https://hf-mirror.com/state-spaces/mamba-2.8b) architecture.</span>

<span class="sd">    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the</span>
<span class="sd">    documentation from [`PretrainedConfig`] for more information.</span>


<span class="sd">    Args:</span>
<span class="sd">        vocab_size (`int`, *optional*, defaults to 50280):</span>
<span class="sd">            Vocabulary size of the MAMBA model. Defines the number of different tokens that can be represented by the</span>
<span class="sd">            `inputs_ids` passed when calling [`MambaModel`].</span>
<span class="sd">        hidden_size (`int`, *optional*, defaults to 768):</span>
<span class="sd">            Dimensionality of the embeddings and hidden states.</span>
<span class="sd">        state_size (`int`, *optional*, defaults to 16): shape of the state space latents.</span>
<span class="sd">        num_hidden_layers (`int`, *optional*, defaults to 32):</span>
<span class="sd">            Number of hidden layers in the model.</span>
<span class="sd">        layer_norm_epsilon (`float`, *optional*, defaults to 1e-05):</span>
<span class="sd">            The epsilon to use in the layer normalization layers.</span>
<span class="sd">        pad_token_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            Padding token id.</span>
<span class="sd">        bos_token_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the beginning of sentence token in the vocabulary.</span>
<span class="sd">        eos_token_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the end of sentence token in the vocabulary.</span>
<span class="sd">        expand (`int`, *optional*, defaults to 2): Expanding factor used to determine the intermediate size.</span>
<span class="sd">        conv_kernel (`int`, *optional*, defaults to 4): Size of the convolution kernel.</span>
<span class="sd">        use_bias (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to use bias in [&quot;in_proj&quot;, &quot;out_proj&quot;] of the mixer block</span>
<span class="sd">        use_conv_bias (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether or not to use bias in the convolution layer of the mixer block.</span>
<span class="sd">        hidden_act (`str`, *optional*, defaults to `&quot;silu&quot;`):</span>
<span class="sd">            The non-linear activation function (function or string) in the decoder.</span>
<span class="sd">        initializer_range (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</span>
<span class="sd">        residual_in_fp32 (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether or not residuals should be in `float32`. If set to `False` residuals will keep the same `dtype`</span>
<span class="sd">            as the rest of the model</span>
<span class="sd">        time_step_rank (`Union[int,str]`, *optional*, defaults to `&quot;auto&quot;`):</span>
<span class="sd">            Rank of the the discretization projection matrix. `&quot;auto&quot;` means that it will default to</span>
<span class="sd">            `math.ceil(self.hidden_size / 16)`</span>
<span class="sd">        time_step_scale (`float`, *optional*, defaults to 1.0):</span>
<span class="sd">            Scale used used to scale `dt_proj.bias`.</span>
<span class="sd">        time_step_min (`float`, *optional*, defaults to 0.001):</span>
<span class="sd">            Minimum `time_step` used to bound `dt_proj.bias`.</span>
<span class="sd">        time_step_max (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            Maximum `time_step` used to bound `dt_proj.bias`.</span>
<span class="sd">        time_step_init_scheme (`float`, *optional*, defaults to `&quot;random&quot;`):</span>
<span class="sd">            Init scheme used for `dt_proj.weight`. Should be one of `[&quot;random&quot;,&quot;uniform&quot;]`</span>
<span class="sd">        time_step_floor (`float`, *optional*, defaults to 0.0001):</span>
<span class="sd">            Minimum clamping value of the `dt_proj.bias` layer initialization.</span>
<span class="sd">        rescale_prenorm_residual (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to rescale `out_proj` weights when initializing.</span>
<span class="sd">        use_cache (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether or not the cache should be used.</span>


<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import MambaConfig, MambaModel</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a Mamba configuration</span>
<span class="sd">        &gt;&gt;&gt; configuration = MambaConfig()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a model (with random weights) from the configuration</span>
<span class="sd">        &gt;&gt;&gt; model = MambaModel(configuration)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Accessing the model configuration</span>
<span class="sd">        &gt;&gt;&gt; configuration = model.config</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;mamba&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="mi">50280</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
        <span class="n">state_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">expand</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">conv_kernel</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">use_conv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">hidden_act</span><span class="o">=</span><span class="s2">&quot;silu&quot;</span><span class="p">,</span>
        <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">residual_in_fp32</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">time_step_rank</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">time_step_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">time_step_min</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
        <span class="n">time_step_max</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">time_step_init_scheme</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">,</span>
        <span class="n">time_step_floor</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">rescale_prenorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes a new instance of the MambaConfig class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MambaConfig): The current instance of the MambaConfig class.</span>
<span class="sd">            vocab_size (int, optional): The size of the vocabulary. Defaults to 50280.</span>
<span class="sd">            hidden_size (int, optional): The size of the hidden state. Defaults to 768.</span>
<span class="sd">            state_size (int, optional): The size of the state. Defaults to 16.</span>
<span class="sd">            num_hidden_layers (int, optional): The number of hidden layers. Defaults to 32.</span>
<span class="sd">            layer_norm_epsilon (float, optional): The epsilon value for layer normalization. Defaults to 1e-05.</span>
<span class="sd">            pad_token_id (int, optional): The token ID for padding. Defaults to 0.</span>
<span class="sd">            bos_token_id (int, optional): The token ID for the beginning of sequence. Defaults to 0.</span>
<span class="sd">            eos_token_id (int, optional): The token ID for the end of sequence. Defaults to 0.</span>
<span class="sd">            expand (int, optional): The expansion factor. Defaults to 2.</span>
<span class="sd">            conv_kernel (int, optional): The kernel size for convolution. Defaults to 4.</span>
<span class="sd">            use_bias (bool, optional): Whether to use bias. Defaults to False.</span>
<span class="sd">            use_conv_bias (bool, optional): Whether to use bias in convolution. Defaults to True.</span>
<span class="sd">            hidden_act (str, optional): The activation function for hidden layers. Defaults to &#39;silu&#39;.</span>
<span class="sd">            initializer_range (float, optional): The range for weight initialization. Defaults to 0.1.</span>
<span class="sd">            residual_in_fp32 (bool, optional): Whether to keep residual in FP32. Defaults to True.</span>
<span class="sd">            time_step_rank (str or int, optional): The rank or &#39;auto&#39; for time step. Defaults to &#39;auto&#39;.</span>
<span class="sd">            time_step_scale (float, optional): The scale factor for time step. Defaults to 1.0.</span>
<span class="sd">            time_step_min (float, optional): The minimum value for time step. Defaults to 0.001.</span>
<span class="sd">            time_step_max (float, optional): The maximum value for time step. Defaults to 0.1.</span>
<span class="sd">            time_step_init_scheme (str, optional): The initialization scheme for time step. Defaults to &#39;random&#39;.</span>
<span class="sd">            time_step_floor (float, optional): The floor value for time step. Defaults to 0.0001.</span>
<span class="sd">            rescale_prenorm_residual (bool, optional): Whether to rescale pre-norm residuals. Defaults to False.</span>
<span class="sd">            use_cache (bool, optional): Whether to use cache. Defaults to True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_epsilon</span> <span class="o">=</span> <span class="n">layer_norm_epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_kernel</span> <span class="o">=</span> <span class="n">conv_kernel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand</span> <span class="o">=</span> <span class="n">expand</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">expand</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_conv_bias</span> <span class="o">=</span> <span class="n">use_conv_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_act</span> <span class="o">=</span> <span class="n">hidden_act</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">/</span> <span class="mi">16</span><span class="p">)</span> <span class="k">if</span> <span class="n">time_step_rank</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span> <span class="k">else</span> <span class="n">time_step_rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_step_scale</span> <span class="o">=</span> <span class="n">time_step_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_step_min</span> <span class="o">=</span> <span class="n">time_step_min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_step_max</span> <span class="o">=</span> <span class="n">time_step_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_step_init_scheme</span> <span class="o">=</span> <span class="n">time_step_init_scheme</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_step_floor</span> <span class="o">=</span> <span class="n">time_step_floor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rescale_prenorm_residual</span> <span class="o">=</span> <span class="n">rescale_prenorm_residual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span> <span class="o">=</span> <span class="n">residual_in_fp32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.configuration_mamba.MambaConfig.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">configuration_mamba</span><span class="o">.</span><span class="n">MambaConfig</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">50280</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">state_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">conv_kernel</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_conv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;silu&#39;</span><span class="p">,</span> <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">residual_in_fp32</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">time_step_rank</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">time_step_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">time_step_min</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">time_step_max</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">time_step_init_scheme</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">time_step_floor</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">rescale_prenorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.configuration_mamba.MambaConfig.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a new instance of the MambaConfig class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The current instance of the MambaConfig class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.configuration_mamba.MambaConfig" href="#mindnlp.transformers.models.mamba.configuration_mamba.MambaConfig">MambaConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the vocabulary. Defaults to 50280.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>50280</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the hidden state. Defaults to 768.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>768</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>state_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the state. Defaults to 16.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_hidden_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of hidden layers. Defaults to 32.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>32</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_norm_epsilon</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The epsilon value for layer normalization. Defaults to 1e-05.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for padding. Defaults to 0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for the beginning of sequence. Defaults to 0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for the end of sequence. Defaults to 0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>expand</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The expansion factor. Defaults to 2.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>conv_kernel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size for convolution. Defaults to 4.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to use bias. Defaults to False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_conv_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to use bias in convolution. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_act</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The activation function for hidden layers. Defaults to 'silu'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;silu&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>initializer_range</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The range for weight initialization. Defaults to 0.1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>residual_in_fp32</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to keep residual in FP32. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_rank</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The rank or 'auto' for time step. Defaults to 'auto'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str or int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;auto&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The scale factor for time step. Defaults to 1.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_min</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The minimum value for time step. Defaults to 0.001.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.001</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_max</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum value for time step. Defaults to 0.1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_init_scheme</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The initialization scheme for time step. Defaults to 'random'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;random&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_step_floor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The floor value for time step. Defaults to 0.0001.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0001</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rescale_prenorm_residual</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to rescale pre-norm residuals. Defaults to False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to use cache. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\configuration_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">50280</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
    <span class="n">state_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">bos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">eos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">expand</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">conv_kernel</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_conv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">hidden_act</span><span class="o">=</span><span class="s2">&quot;silu&quot;</span><span class="p">,</span>
    <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">residual_in_fp32</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">time_step_rank</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">time_step_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">time_step_min</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">time_step_max</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">time_step_init_scheme</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">,</span>
    <span class="n">time_step_floor</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">rescale_prenorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Initializes a new instance of the MambaConfig class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MambaConfig): The current instance of the MambaConfig class.</span>
<span class="sd">        vocab_size (int, optional): The size of the vocabulary. Defaults to 50280.</span>
<span class="sd">        hidden_size (int, optional): The size of the hidden state. Defaults to 768.</span>
<span class="sd">        state_size (int, optional): The size of the state. Defaults to 16.</span>
<span class="sd">        num_hidden_layers (int, optional): The number of hidden layers. Defaults to 32.</span>
<span class="sd">        layer_norm_epsilon (float, optional): The epsilon value for layer normalization. Defaults to 1e-05.</span>
<span class="sd">        pad_token_id (int, optional): The token ID for padding. Defaults to 0.</span>
<span class="sd">        bos_token_id (int, optional): The token ID for the beginning of sequence. Defaults to 0.</span>
<span class="sd">        eos_token_id (int, optional): The token ID for the end of sequence. Defaults to 0.</span>
<span class="sd">        expand (int, optional): The expansion factor. Defaults to 2.</span>
<span class="sd">        conv_kernel (int, optional): The kernel size for convolution. Defaults to 4.</span>
<span class="sd">        use_bias (bool, optional): Whether to use bias. Defaults to False.</span>
<span class="sd">        use_conv_bias (bool, optional): Whether to use bias in convolution. Defaults to True.</span>
<span class="sd">        hidden_act (str, optional): The activation function for hidden layers. Defaults to &#39;silu&#39;.</span>
<span class="sd">        initializer_range (float, optional): The range for weight initialization. Defaults to 0.1.</span>
<span class="sd">        residual_in_fp32 (bool, optional): Whether to keep residual in FP32. Defaults to True.</span>
<span class="sd">        time_step_rank (str or int, optional): The rank or &#39;auto&#39; for time step. Defaults to &#39;auto&#39;.</span>
<span class="sd">        time_step_scale (float, optional): The scale factor for time step. Defaults to 1.0.</span>
<span class="sd">        time_step_min (float, optional): The minimum value for time step. Defaults to 0.001.</span>
<span class="sd">        time_step_max (float, optional): The maximum value for time step. Defaults to 0.1.</span>
<span class="sd">        time_step_init_scheme (str, optional): The initialization scheme for time step. Defaults to &#39;random&#39;.</span>
<span class="sd">        time_step_floor (float, optional): The floor value for time step. Defaults to 0.0001.</span>
<span class="sd">        rescale_prenorm_residual (bool, optional): Whether to rescale pre-norm residuals. Defaults to False.</span>
<span class="sd">        use_cache (bool, optional): Whether to use cache. Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_epsilon</span> <span class="o">=</span> <span class="n">layer_norm_epsilon</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv_kernel</span> <span class="o">=</span> <span class="n">conv_kernel</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expand</span> <span class="o">=</span> <span class="n">expand</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">expand</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_conv_bias</span> <span class="o">=</span> <span class="n">use_conv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_act</span> <span class="o">=</span> <span class="n">hidden_act</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">/</span> <span class="mi">16</span><span class="p">)</span> <span class="k">if</span> <span class="n">time_step_rank</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span> <span class="k">else</span> <span class="n">time_step_rank</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">time_step_scale</span> <span class="o">=</span> <span class="n">time_step_scale</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">time_step_min</span> <span class="o">=</span> <span class="n">time_step_min</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">time_step_max</span> <span class="o">=</span> <span class="n">time_step_max</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">time_step_init_scheme</span> <span class="o">=</span> <span class="n">time_step_init_scheme</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">time_step_floor</span> <span class="o">=</span> <span class="n">time_step_floor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rescale_prenorm_residual</span> <span class="o">=</span> <span class="n">rescale_prenorm_residual</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span> <span class="o">=</span> <span class="n">residual_in_fp32</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.mamba.modeling_graph_mamba" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_graph_mamba</code>


<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>MindSpore MAMBA model.</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock</code>


<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>The MSMambaBlock class represents a block for the MSMamba model. It inherits from the nn.Module class and is designed
to handle the configuration and processing of hidden states for the MSMamba model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.config">config</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An object containing configuration settings for the block.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.layer_idx">layer_idx</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An integer representing the index of the layer.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.residual_in_fp32">residual_in_fp32</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A boolean indicating whether residual values are in 32-bit floating point format.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.norm">norm</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the MSMambaRMSNorm class for performing layer normalization.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.mixer">mixer</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the MSMambaMixer class for mixing hidden states based on the configuration and layer index.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.forward" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Processes the input hidden states using the configured normalization and mixing operations,
and returns the processed hidden states.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <p>This class is part of the MSMamba model and is specifically designed for handling the processing of
hidden states within the model architecture.</p>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MSMambaBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The MSMambaBlock class represents a block for the MSMamba model. It inherits from the nn.Module class and is designed</span>
<span class="sd">    to handle the configuration and processing of hidden states for the MSMamba model.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        config: An object containing configuration settings for the block.</span>
<span class="sd">        layer_idx: An integer representing the index of the layer.</span>
<span class="sd">        residual_in_fp32: A boolean indicating whether residual values are in 32-bit floating point format.</span>
<span class="sd">        norm: An instance of the MSMambaRMSNorm class for performing layer normalization.</span>
<span class="sd">        mixer: An instance of the MSMambaMixer class for mixing hidden states based on the configuration and layer index.</span>

<span class="sd">    Methods:</span>
<span class="sd">        forward: Processes the input hidden states using the configured normalization and mixing operations,</span>
<span class="sd">            and returns the processed hidden states.</span>

<span class="sd">    Note:</span>
<span class="sd">        This class is part of the MSMamba model and is specifically designed for handling the processing of</span>
<span class="sd">        hidden states within the model architecture.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a new instance of the MSMambaBlock class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MSMambaBlock): The instance of the MSMambaBlock class.</span>
<span class="sd">            config (object): The configuration object containing various settings.</span>
<span class="sd">            layer_idx (int): The index of the layer in the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">residual_in_fp32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">MSMambaRMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_epsilon</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mixer</span> <span class="o">=</span> <span class="n">MSMambaMixer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the MSMambaBlock.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MSMambaBlock): An instance of the MSMambaBlock class.</span>
<span class="sd">            hidden_states (Tensor): The input hidden states to the block.</span>
<span class="sd">            cache_params (Optional[Dict]): A dictionary containing cache parameters (default: None).</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixer</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="n">cache_params</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaBlock</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a new instance of the MSMambaBlock class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the MSMambaBlock class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock">MSMambaBlock</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing various settings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_idx</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The index of the layer in the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a new instance of the MSMambaBlock class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MSMambaBlock): The instance of the MSMambaBlock class.</span>
<span class="sd">        config (object): The configuration object containing various settings.</span>
<span class="sd">        layer_idx (int): The index of the layer in the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">residual_in_fp32</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">MSMambaRMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_epsilon</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mixer</span> <span class="o">=</span> <span class="n">MSMambaMixer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaBlock</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the MSMambaBlock.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the MSMambaBlock class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaBlock">MSMambaBlock</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input hidden states to the block.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_params</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary containing cache parameters (default: None).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the MSMambaBlock.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MSMambaBlock): An instance of the MSMambaBlock class.</span>
<span class="sd">        hidden_states (Tensor): The input hidden states to the block.</span>
<span class="sd">        cache_params (Optional[Dict]): A dictionary containing cache parameters (default: None).</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixer</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="n">cache_params</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
    <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache</code>


<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>The <code>MSMambaCache</code> class represents a cache for storing intermediate states and parameters used in the
MSMamba algorithm. It is designed to be used in conjunction with the <code>MSMambaModel</code> class.</p>
<p>This class provides functionality for initializing the cache and storing intermediate states and parameters.
The cache is used to store the convolutional states (<code>conv_states</code>) and the state-space model
states (<code>ssm_states</code>) for each hidden layer in the MSMamba algorithm.
The cache is initialized with zero tensors of appropriate shapes.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache.`seqlen_offset`">`seqlen_offset`</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A parameter representing the sequence length offset.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache.`dtype`">`dtype`</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The data type of the cache tensors (default: mindspore.float16).</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache.`conv_states`">`conv_states`</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A parameter storing the convolutional states for each hidden layer.
It is a tensor of shape (num_hidden_layers, batch_size, intermediate_size, conv_kernel_size).</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache.`ssm_states`">`ssm_states`</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A parameter storing the state-space model states for each hidden layer.
It is a tensor of shape (num_hidden_layers, batch_size, intermediate_size, ssm_state_size).</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <p>This class inherits from [Parent Class Name].</p>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@mindspore</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span> <span class="nc">MSMambaCache</span><span class="p">:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The `MSMambaCache` class represents a cache for storing intermediate states and parameters used in the</span>
<span class="sd">    MSMamba algorithm. It is designed to be used in conjunction with the `MSMambaModel` class.</span>

<span class="sd">    This class provides functionality for initializing the cache and storing intermediate states and parameters.</span>
<span class="sd">    The cache is used to store the convolutional states (`conv_states`) and the state-space model</span>
<span class="sd">    states (`ssm_states`) for each hidden layer in the MSMamba algorithm.</span>
<span class="sd">    The cache is initialized with zero tensors of appropriate shapes.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        `seqlen_offset`: A parameter representing the sequence length offset.</span>
<span class="sd">        `dtype`: The data type of the cache tensors (default: mindspore.float16).</span>
<span class="sd">        `conv_states`: A parameter storing the convolutional states for each hidden layer.</span>
<span class="sd">            It is a tensor of shape (num_hidden_layers, batch_size, intermediate_size, conv_kernel_size).</span>
<span class="sd">        `ssm_states`: A parameter storing the state-space model states for each hidden layer.</span>
<span class="sd">            It is a tensor of shape (num_hidden_layers, batch_size, intermediate_size, ssm_state_size).</span>

<span class="sd">    Note:</span>
<span class="sd">        This class inherits from [Parent Class Name].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method initializes an instance of the MSMambaCache class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (object): The instance of the class.</span>
<span class="sd">            config (object): The configuration object containing parameters for the cache.</span>
<span class="sd">            batch_size (int): The size of the batch for processing.</span>
<span class="sd">            dtype (object, optional): The data type for the cache, defaults to mindspore.float16.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the batch_size is not a positive integer.</span>
<span class="sd">            TypeError: If the dtype is not a valid data type.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seqlen_offset</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span>
        <span class="n">ssm_state_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">state_size</span>
        <span class="n">conv_kernel_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv_states</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="n">conv_kernel_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv_states&#39;</span><span class="p">)</span>
        <span class="c1"># {</span>
        <span class="c1">#     i: ops.zeros(batch_size, intermediate_size, conv_kernel_size, dtype=dtype)</span>
        <span class="c1">#     for i in range(config.num_hidden_layers)</span>
        <span class="c1"># }</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ssm_states</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="n">ssm_state_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ssm_states&#39;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaCache</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaCache.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method initializes an instance of the MSMambaCache class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing parameters for the cache.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>batch_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the batch for processing.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dtype</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The data type for the cache, defaults to mindspore.float16.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code><span title="mindspore.float16">float16</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the batch_size is not a positive integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the dtype is not a valid data type.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method initializes an instance of the MSMambaCache class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (object): The instance of the class.</span>
<span class="sd">        config (object): The configuration object containing parameters for the cache.</span>
<span class="sd">        batch_size (int): The size of the batch for processing.</span>
<span class="sd">        dtype (object, optional): The data type for the cache, defaults to mindspore.float16.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the batch_size is not a positive integer.</span>
<span class="sd">        TypeError: If the dtype is not a valid data type.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">seqlen_offset</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
    <span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span>
    <span class="n">ssm_state_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">state_size</span>
    <span class="n">conv_kernel_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">conv_states</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="n">conv_kernel_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv_states&#39;</span><span class="p">)</span>
    <span class="c1"># {</span>
    <span class="c1">#     i: ops.zeros(batch_size, intermediate_size, conv_kernel_size, dtype=dtype)</span>
    <span class="c1">#     for i in range(config.num_hidden_layers)</span>
    <span class="c1"># }</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ssm_states</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="n">ssm_state_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ssm_states&#39;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM</code>


<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel">MSMambaPreTrainedModel</a></code></p>


        <p>MSMambaForCausalLM is a class that represents a Mamba model for Causal Language Modeling.
It inherits from MSMambaPreTrainedModel and includes methods for setting and getting input and output embeddings,
as well as preparing inputs for generation and forwarding the model for training and evaluation.</p>
<p>The class includes the following methods:</p>
<ul>
<li><strong>init</strong>: Initializes the model with a given configuration.</li>
<li>get_output_embeddings: Retrieves the output embeddings of the model.</li>
<li>set_output_embeddings: Sets new output embeddings for the model.</li>
<li>get_input_embeddings: Retrieves the input embeddings of the model.</li>
<li>set_input_embeddings: Sets new input embeddings for the model.</li>
<li>_update_model_kwargs_for_generation: Updates model keyword arguments for generation.</li>
<li>prepare_inputs_for_generation: Prepares inputs for generation based on the given parameters.</li>
<li>forward: Constructs the model for training and evaluation, including handling labels for
language modeling and computing loss.</li>
</ul>
<p>When utilizing the MSMambaForCausalLM class, users can easily manage input and output embeddings,
prepare inputs for generating text, and forward the model for training and evaluation purposes.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MSMambaForCausalLM</span><span class="p">(</span><span class="n">MSMambaPreTrainedModel</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MSMambaForCausalLM is a class that represents a Mamba model for Causal Language Modeling.</span>
<span class="sd">    It inherits from MSMambaPreTrainedModel and includes methods for setting and getting input and output embeddings,</span>
<span class="sd">    as well as preparing inputs for generation and forwarding the model for training and evaluation.</span>

<span class="sd">    The class includes the following methods:</span>

<span class="sd">    - __init__: Initializes the model with a given configuration.</span>
<span class="sd">    - get_output_embeddings: Retrieves the output embeddings of the model.</span>
<span class="sd">    - set_output_embeddings: Sets new output embeddings for the model.</span>
<span class="sd">    - get_input_embeddings: Retrieves the input embeddings of the model.</span>
<span class="sd">    - set_input_embeddings: Sets new input embeddings for the model.</span>
<span class="sd">    - _update_model_kwargs_for_generation: Updates model keyword arguments for generation.</span>
<span class="sd">    - prepare_inputs_for_generation: Prepares inputs for generation based on the given parameters.</span>
<span class="sd">    - forward: Constructs the model for training and evaluation, including handling labels for</span>
<span class="sd">    language modeling and computing loss.</span>

<span class="sd">    When utilizing the MSMambaForCausalLM class, users can easily manage input and output embeddings,</span>
<span class="sd">    prepare inputs for generating text, and forward the model for training and evaluation purposes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;lm_head.weight&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of MSMambaForCausalLM.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (object): The instance of the class.</span>
<span class="sd">            config (object): An object containing configuration parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">MSMambaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">MambaDense</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to retrieve the output embeddings from the MSMambaForCausalLM model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the MSMambaForCausalLM class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the output embeddings of the MSMambaForCausalLM model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MSMambaForCausalLM): The MSMambaForCausalLM object.</span>
<span class="sd">            new_embeddings (Tensor): The new embeddings to be set as the output embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        This method allows for setting the output embeddings of the MSMambaForCausalLM model.</span>
<span class="sd">        The output embeddings are used in the generation of predictions by the language model head. By setting new</span>
<span class="sd">        embeddings, you can modify the characteristics of the generated predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieve the input embeddings from the MSMambaForCausalLM model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MSMambaForCausalLM): An instance of the MSMambaForCausalLM class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the input embeddings for the MSMambaForCausalLM model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MSMambaForCausalLM): The instance of the MSMambaForCausalLM class.</span>
<span class="sd">            new_embeddings (Tensor): The new input embeddings to be set for the model.</span>
<span class="sd">                Should be a tensor of shape (vocab_size, embedding_dim).</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: The method sets the input embeddings for the model and does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the new_embeddings tensor does not have the correct shape (vocab_size, embedding_dim).</span>
<span class="sd">            TypeError: If the new_embeddings parameter is not a tensor.</span>
<span class="sd">            RuntimeError: If the operation to set the input embeddings fails for any reason.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_model_kwargs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">ModelOutput</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates model keyword arguments for generation.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MSMambaForCausalLM): The instance of MSMambaForCausalLM class.</span>
<span class="sd">            outputs (ModelOutput): The output results from the model.</span>
<span class="sd">            model_kwargs (Dict[str, Any]): The dictionary containing model keyword arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]:</span>
<span class="sd">                Updated model keyword arguments after incorporating cache_params from outputs.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;cache_params&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model_kwargs</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare inputs for generation.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MSMambaForCausalLM): The instance of the MSMambaForCausalLM class.</span>
<span class="sd">            input_ids (Tensor): The input tensor containing tokenized input sequence.</span>
<span class="sd">            cache_params (dict, optional): Parameters for caching intermediate computations.</span>
<span class="sd">            inputs_embeds (Tensor, optional): The embedded input tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: The model inputs containing either &#39;inputs_embeds&#39; or &#39;input_ids&#39; based on the availability of</span>
<span class="sd">                &#39;inputs_embeds&#39; and &#39;cache_params&#39;.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># only last token for inputs_ids if the state is passed along.</span>
        <span class="k">if</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">:</span> <span class="n">inputs_embeds</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">}</span>

        <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;cache_params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache_params</span>
        <span class="k">return</span> <span class="n">model_inputs</span>

    <span class="nd">@mindspore</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>  <span class="c1"># for now we need this for generation</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set</span>
<span class="sd">                `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`</span>
<span class="sd">                are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">mamba_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">cache_params</span><span class="o">=</span><span class="n">cache_params</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="s1">&#39;last_hidden_state&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># move labels to correct device to enable model parallelism</span>
            <span class="c1"># Shift so that tokens &lt; n predict n</span>
            <span class="n">shift_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">shift_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
            <span class="c1"># Flatten the tokens</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">shift_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shift_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
            <span class="s1">&#39;logits&#39;</span><span class="p">:</span> <span class="n">logits</span><span class="p">,</span>
            <span class="s1">&#39;cache_params&#39;</span><span class="p">:</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="s1">&#39;cache_params&#39;</span><span class="p">],</span>
            <span class="s1">&#39;hidden_states&#39;</span><span class="p">:</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="s1">&#39;hidden_states&#39;</span><span class="p">],</span>
        <span class="p">}</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaForCausalLM</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of MSMambaForCausalLM.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An object containing configuration parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of MSMambaForCausalLM.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (object): The instance of the class.</span>
<span class="sd">        config (object): An object containing configuration parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">MSMambaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">MambaDense</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Initialize weights and apply final processing</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaForCausalLM</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code>labels = input_ids</code> Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code> All labels set to <code>-100</code>
are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@mindspore</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cache_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>  <span class="c1"># for now we need this for generation</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set</span>
<span class="sd">            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`</span>
<span class="sd">            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="n">mamba_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">cache_params</span><span class="o">=</span><span class="n">cache_params</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="s1">&#39;last_hidden_state&#39;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># move labels to correct device to enable model parallelism</span>
        <span class="c1"># Shift so that tokens &lt; n predict n</span>
        <span class="n">shift_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">shift_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="c1"># Flatten the tokens</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">shift_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shift_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
        <span class="s1">&#39;logits&#39;</span><span class="p">:</span> <span class="n">logits</span><span class="p">,</span>
        <span class="s1">&#39;cache_params&#39;</span><span class="p">:</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="s1">&#39;cache_params&#39;</span><span class="p">],</span>
        <span class="s1">&#39;hidden_states&#39;</span><span class="p">:</span> <span class="n">mamba_outputs</span><span class="p">[</span><span class="s1">&#39;hidden_states&#39;</span><span class="p">],</span>
    <span class="p">}</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.get_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaForCausalLM</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.get_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Retrieve the input embeddings from the MSMambaForCausalLM model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the MSMambaForCausalLM class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM">MSMambaForCausalLM</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieve the input embeddings from the MSMambaForCausalLM model.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MSMambaForCausalLM): An instance of the MSMambaForCausalLM class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.get_output_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaForCausalLM</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.get_output_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Method to retrieve the output embeddings from the MSMambaForCausalLM model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the MSMambaForCausalLM class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method to retrieve the output embeddings from the MSMambaForCausalLM model.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the MSMambaForCausalLM class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.prepare_inputs_for_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaForCausalLM</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.prepare_inputs_for_generation" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Prepare inputs for generation.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the MSMambaForCausalLM class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM">MSMambaForCausalLM</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor containing tokenized input sequence.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_params</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Parameters for caching intermediate computations.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>dict</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inputs_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The embedded input tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>dict</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The model inputs containing either 'inputs_embeds' or 'input_ids' based on the availability of
'inputs_embeds' and 'cache_params'.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prepare inputs for generation.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MSMambaForCausalLM): The instance of the MSMambaForCausalLM class.</span>
<span class="sd">        input_ids (Tensor): The input tensor containing tokenized input sequence.</span>
<span class="sd">        cache_params (dict, optional): Parameters for caching intermediate computations.</span>
<span class="sd">        inputs_embeds (Tensor, optional): The embedded input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: The model inputs containing either &#39;inputs_embeds&#39; or &#39;input_ids&#39; based on the availability of</span>
<span class="sd">            &#39;inputs_embeds&#39; and &#39;cache_params&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># only last token for inputs_ids if the state is passed along.</span>
    <span class="k">if</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">:</span> <span class="n">inputs_embeds</span><span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">}</span>

    <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;cache_params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache_params</span>
    <span class="k">return</span> <span class="n">model_inputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.set_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaForCausalLM</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.set_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Sets the input embeddings for the MSMambaForCausalLM model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the MSMambaForCausalLM class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM">MSMambaForCausalLM</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>new_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The new input embeddings to be set for the model.
Should be a tensor of shape (vocab_size, embedding_dim).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The method sets the input embeddings for the model and does not return any value.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the new_embeddings tensor does not have the correct shape (vocab_size, embedding_dim).</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the new_embeddings parameter is not a tensor.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the operation to set the input embeddings fails for any reason.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the input embeddings for the MSMambaForCausalLM model.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MSMambaForCausalLM): The instance of the MSMambaForCausalLM class.</span>
<span class="sd">        new_embeddings (Tensor): The new input embeddings to be set for the model.</span>
<span class="sd">            Should be a tensor of shape (vocab_size, embedding_dim).</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: The method sets the input embeddings for the model and does not return any value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the new_embeddings tensor does not have the correct shape (vocab_size, embedding_dim).</span>
<span class="sd">        TypeError: If the new_embeddings parameter is not a tensor.</span>
<span class="sd">        RuntimeError: If the operation to set the input embeddings fails for any reason.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.set_output_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaForCausalLM</span><span class="o">.</span><span class="n">set_output_embeddings</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM.set_output_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Sets the output embeddings of the MSMambaForCausalLM model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The MSMambaForCausalLM object.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaForCausalLM">MSMambaForCausalLM</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>new_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The new embeddings to be set as the output embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <p>This method allows for setting the output embeddings of the MSMambaForCausalLM model.
The output embeddings are used in the generation of predictions by the language model head. By setting new
embeddings, you can modify the characteristics of the generated predictions.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the output embeddings of the MSMambaForCausalLM model.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MSMambaForCausalLM): The MSMambaForCausalLM object.</span>
<span class="sd">        new_embeddings (Tensor): The new embeddings to be set as the output embeddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>

<span class="sd">    This method allows for setting the output embeddings of the MSMambaForCausalLM model.</span>
<span class="sd">    The output embeddings are used in the generation of predictions by the language model head. By setting new</span>
<span class="sd">    embeddings, you can modify the characteristics of the generated predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer</code>


<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Compute ∆, A, B, C, and D the state space parameters and compute the <code>contextualized_states</code>.
A, D are input independent (see MSMamba paper [1] Section 3.5.2 "Interpretation of A" for why A isn't selective)
∆, B, C are input-dependent (this is a key difference between MSMamba and the linear time invariant S4,
and is why MSMamba is called <strong>selective</strong> state spaces)</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MSMambaMixer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute ∆, A, B, C, and D the state space parameters and compute the `contextualized_states`.</span>
<span class="sd">    A, D are input independent (see MSMamba paper [1] Section 3.5.2 &quot;Interpretation of A&quot; for why A isn&#39;t selective)</span>
<span class="sd">    ∆, B, C are input-dependent (this is a key difference between MSMamba and the linear time invariant S4,</span>
<span class="sd">    and is why MSMamba is called **selective** state spaces)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the MSMambaMixer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config:</span>
<span class="sd">                An object containing configuration parameters for the mixer.</span>

<span class="sd">                - hidden_size (int): Size of the hidden layer.</span>
<span class="sd">                - state_size (int): Size of the state.</span>
<span class="sd">                - conv_kernel (int): Kernel size for convolution.</span>
<span class="sd">                - intermediate_size (int): Size of the intermediate layer.</span>
<span class="sd">                - time_step_rank (int): Rank of the time step.</span>
<span class="sd">                - use_conv_bias (bool): Indicates whether to use bias in convolution.</span>
<span class="sd">                - hidden_act (str): Activation function for the hidden layer.</span>
<span class="sd">                - use_bias (bool): Indicates whether to use bias in the dense layers.</span>
<span class="sd">            layer_idx: Index of the current layer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_kernel_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">time_step_rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_conv_bias</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_conv_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_conv_bias</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">]</span>

        <span class="c1"># projection of the input hidden states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_proj</span> <span class="o">=</span> <span class="n">MambaDense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_bias</span><span class="p">)</span>
        <span class="c1"># selective projection used to make dt, B and C input dependant</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_proj</span> <span class="o">=</span> <span class="n">MambaDense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># time step projection (discretization)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dt_proj</span> <span class="o">=</span> <span class="n">MambaDense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># S4D real initialization. These are not discretized!</span>
        <span class="c1"># The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">A_log</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">MambaDense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_bias</span>

    <span class="c1"># fmt: off</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs contextualized states using the MSMambaMixer algorithm.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MSMambaMixer): An instance of the MSMambaMixer class.</span>
<span class="sd">            input_states (ndarray): The input states of shape (batch_size, seq_len, _).</span>
<span class="sd">            cache_params (Optional[CacheParams]): The cache parameters. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">input_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_states</span><span class="o">.</span><span class="n">dtype</span>
        <span class="c1"># 1. Gated MLP&#39;s linear projection</span>
        <span class="n">projected_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_proj</span><span class="p">(</span><span class="n">input_states</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>                   <span class="c1"># [batch, 2 * intermediate_size, seq_len]</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">gate</span> <span class="o">=</span> <span class="n">projected_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 2. Convolution sequence transformation</span>
        <span class="k">if</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ssm_state</span> <span class="o">=</span> <span class="n">cache_params</span><span class="o">.</span><span class="n">ssm_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">cache_params</span><span class="o">.</span><span class="n">seqlen_offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">conv_state</span> <span class="o">=</span> <span class="n">cache_params</span><span class="o">.</span><span class="n">conv_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span>                   <span class="c1"># [batch, intermediate_size, conv_kernel_size]</span>
                <span class="n">conv_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">conv_state</span><span class="p">,</span> <span class="n">shifts</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dims</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">conv_state</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
                <span class="n">cache_params</span><span class="o">.</span><span class="n">conv_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">conv_state</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">conv_state</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_conv_bias</span><span class="p">:</span>
                    <span class="n">hidden_states</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="o">.</span><span class="n">bias</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>         <span class="c1"># [batch, intermediate_size, 1] : decoding</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">conv_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_kernel_size</span> <span class="o">-</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">cache_params</span><span class="o">.</span><span class="n">conv_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">conv_state</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">])</span>     <span class="c1"># [batch, intermediate_size, seq_len]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ssm_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
            <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">])</span>         <span class="c1"># [batch, intermediate_size, seq_len]</span>

        <span class="c1"># 3. State Space Model sequence transformation</span>
        <span class="c1"># 3.a. Selection:  [batch, seq_len, self.time_step_rank + self.ssm_state_size * 2]</span>
        <span class="n">ssm_parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">time_step</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">ssm_parameters</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">discrete_time_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt_proj</span><span class="p">(</span><span class="n">time_step</span><span class="p">)</span>                                    <span class="c1"># [batch, seq_len, intermediate_size]</span>
        <span class="n">discrete_time_step</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">discrete_time_step</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># [batch, intermediate_size, seq_len]</span>

        <span class="c1"># 3.b. Discretization: B and C to [batch, seq_len, intermediate_size, ssm_state_size] (SRAM)</span>
        <span class="n">A</span> <span class="o">=</span> <span class="o">-</span><span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A_log</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>                                              <span class="c1"># [intermediate_size, ssm_state_size]</span>
        <span class="n">discrete_A</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">discrete_time_step</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span> <span class="c1"># [batch, intermediate_size, seq_len, ssm_state_size]</span>
        <span class="n">discrete_B</span> <span class="o">=</span> <span class="n">discrete_time_step</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>       <span class="c1"># [batch, intermediade_size, seq_len, ssm_state_size]</span>
        <span class="n">deltaB_u</span> <span class="o">=</span> <span class="n">discrete_B</span> <span class="o">*</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="c1"># 3.c perform the recurrence y ← SSM(A, B, C)(x)</span>
        <span class="n">scan_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="n">ssm_state</span> <span class="o">=</span> <span class="n">discrete_A</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">ssm_state</span> <span class="o">+</span> <span class="n">deltaB_u</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span>      <span class="c1"># [batch, intermediade_size, ssm_state]</span>
            <span class="n">scan_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">ssm_state</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># [batch, intermediade_size, 1]</span>
            <span class="n">scan_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scan_output</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">scan_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">scan_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                                <span class="c1"># [batch, seq_len, intermediade_size]</span>
        <span class="n">scan_output</span> <span class="o">=</span> <span class="n">scan_output</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span>
        <span class="n">scan_output</span> <span class="o">=</span> <span class="n">scan_output</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">gate</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache_params</span><span class="o">.</span><span class="n">ssm_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">ssm_state</span>

        <span class="c1"># 4. Final linear projection</span>
        <span class="n">contextualized_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">scan_output</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>             <span class="c1"># [batch, seq_len, hidden_size]</span>
        <span class="k">return</span> <span class="n">contextualized_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaMixer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the MSMambaMixer class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An object containing configuration parameters for the mixer.</p>
<ul>
<li>hidden_size (int): Size of the hidden layer.</li>
<li>state_size (int): Size of the state.</li>
<li>conv_kernel (int): Kernel size for convolution.</li>
<li>intermediate_size (int): Size of the intermediate layer.</li>
<li>time_step_rank (int): Rank of the time step.</li>
<li>use_conv_bias (bool): Indicates whether to use bias in convolution.</li>
<li>hidden_act (str): Activation function for the hidden layer.</li>
<li>use_bias (bool): Indicates whether to use bias in the dense layers.</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_idx</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Index of the current layer.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the MSMambaMixer class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config:</span>
<span class="sd">            An object containing configuration parameters for the mixer.</span>

<span class="sd">            - hidden_size (int): Size of the hidden layer.</span>
<span class="sd">            - state_size (int): Size of the state.</span>
<span class="sd">            - conv_kernel (int): Kernel size for convolution.</span>
<span class="sd">            - intermediate_size (int): Size of the intermediate layer.</span>
<span class="sd">            - time_step_rank (int): Rank of the time step.</span>
<span class="sd">            - use_conv_bias (bool): Indicates whether to use bias in convolution.</span>
<span class="sd">            - hidden_act (str): Activation function for the hidden layer.</span>
<span class="sd">            - use_bias (bool): Indicates whether to use bias in the dense layers.</span>
<span class="sd">        layer_idx: Index of the current layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">state_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv_kernel_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">time_step_rank</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_conv_bias</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_conv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_conv_bias</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">conv_kernel</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">]</span>

    <span class="c1"># projection of the input hidden states</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_proj</span> <span class="o">=</span> <span class="n">MambaDense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="c1"># selective projection used to make dt, B and C input dependant</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x_proj</span> <span class="o">=</span> <span class="n">MambaDense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># time step projection (discretization)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dt_proj</span> <span class="o">=</span> <span class="n">MambaDense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># S4D real initialization. These are not discretized!</span>
    <span class="c1"># The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">A_log</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">MambaDense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_bias</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaMixer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs contextualized states using the MSMambaMixer algorithm.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the MSMambaMixer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaMixer">MSMambaMixer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input states of shape (batch_size, seq_len, _).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>ndarray</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_params</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The cache parameters. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[CacheParams]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs contextualized states using the MSMambaMixer algorithm.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MSMambaMixer): An instance of the MSMambaMixer class.</span>
<span class="sd">        input_states (ndarray): The input states of shape (batch_size, seq_len, _).</span>
<span class="sd">        cache_params (Optional[CacheParams]): The cache parameters. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">input_states</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_states</span><span class="o">.</span><span class="n">dtype</span>
    <span class="c1"># 1. Gated MLP&#39;s linear projection</span>
    <span class="n">projected_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_proj</span><span class="p">(</span><span class="n">input_states</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>                   <span class="c1"># [batch, 2 * intermediate_size, seq_len]</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">gate</span> <span class="o">=</span> <span class="n">projected_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 2. Convolution sequence transformation</span>
    <span class="k">if</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ssm_state</span> <span class="o">=</span> <span class="n">cache_params</span><span class="o">.</span><span class="n">ssm_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">cache_params</span><span class="o">.</span><span class="n">seqlen_offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">conv_state</span> <span class="o">=</span> <span class="n">cache_params</span><span class="o">.</span><span class="n">conv_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span>                   <span class="c1"># [batch, intermediate_size, conv_kernel_size]</span>
            <span class="n">conv_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">conv_state</span><span class="p">,</span> <span class="n">shifts</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dims</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">conv_state</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">cache_params</span><span class="o">.</span><span class="n">conv_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">conv_state</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">conv_state</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_conv_bias</span><span class="p">:</span>
                <span class="n">hidden_states</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="o">.</span><span class="n">bias</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>         <span class="c1"># [batch, intermediate_size, 1] : decoding</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">conv_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_kernel_size</span> <span class="o">-</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">cache_params</span><span class="o">.</span><span class="n">conv_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">conv_state</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">])</span>     <span class="c1"># [batch, intermediate_size, seq_len]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ssm_state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">])</span>         <span class="c1"># [batch, intermediate_size, seq_len]</span>

    <span class="c1"># 3. State Space Model sequence transformation</span>
    <span class="c1"># 3.a. Selection:  [batch, seq_len, self.time_step_rank + self.ssm_state_size * 2]</span>
    <span class="n">ssm_parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">time_step</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
        <span class="n">ssm_parameters</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">time_step_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssm_state_size</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">discrete_time_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt_proj</span><span class="p">(</span><span class="n">time_step</span><span class="p">)</span>                                    <span class="c1"># [batch, seq_len, intermediate_size]</span>
    <span class="n">discrete_time_step</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">discrete_time_step</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># [batch, intermediate_size, seq_len]</span>

    <span class="c1"># 3.b. Discretization: B and C to [batch, seq_len, intermediate_size, ssm_state_size] (SRAM)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="o">-</span><span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A_log</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>                                              <span class="c1"># [intermediate_size, ssm_state_size]</span>
    <span class="n">discrete_A</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">discrete_time_step</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span> <span class="c1"># [batch, intermediate_size, seq_len, ssm_state_size]</span>
    <span class="n">discrete_B</span> <span class="o">=</span> <span class="n">discrete_time_step</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>       <span class="c1"># [batch, intermediade_size, seq_len, ssm_state_size]</span>
    <span class="n">deltaB_u</span> <span class="o">=</span> <span class="n">discrete_B</span> <span class="o">*</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="c1"># 3.c perform the recurrence y ← SSM(A, B, C)(x)</span>
    <span class="n">scan_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
        <span class="n">ssm_state</span> <span class="o">=</span> <span class="n">discrete_A</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">ssm_state</span> <span class="o">+</span> <span class="n">deltaB_u</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span>      <span class="c1"># [batch, intermediade_size, ssm_state]</span>
        <span class="n">scan_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">ssm_state</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">C</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># [batch, intermediade_size, 1]</span>
        <span class="n">scan_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scan_output</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">scan_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">scan_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                                <span class="c1"># [batch, seq_len, intermediade_size]</span>
    <span class="n">scan_output</span> <span class="o">=</span> <span class="n">scan_output</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span>
    <span class="n">scan_output</span> <span class="o">=</span> <span class="n">scan_output</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">gate</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">cache_params</span><span class="o">.</span><span class="n">ssm_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">ssm_state</span>

    <span class="c1"># 4. Final linear projection</span>
    <span class="n">contextualized_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">scan_output</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>             <span class="c1"># [batch, seq_len, hidden_size]</span>
    <span class="k">return</span> <span class="n">contextualized_states</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel</code>


<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel">MSMambaPreTrainedModel</a></code></p>


        <p>MSMambaModel represents a model for MSMamba that inherits from MSMambaPreTrainedModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.embeddings">embeddings</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An embedding layer for the model's vocabulary.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Embedding">Embedding</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.layers">layers</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A list of MSMambaBlock layers for the model.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.ModuleList">ModuleList</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.gradient_checkpointing">gradient_checkpointing</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Indicates if gradient checkpointing is enabled.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.norm_f">norm_f</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Normalization function for the model's hidden states.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm">MSMambaRMSNorm</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.__init__" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the MSMambaModel with the given configuration.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.get_input_embeddings" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.get_input_embeddings">get_input_embeddings</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Retrieves the input embeddings for the model.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.set_input_embeddings" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.set_input_embeddings">set_input_embeddings</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Sets new input embeddings for the model.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.forward" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the model based on the input and configuration parameters.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MSMambaModel</span><span class="p">(</span><span class="n">MSMambaPreTrainedModel</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MSMambaModel represents a model for MSMamba that inherits from MSMambaPreTrainedModel.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        embeddings (nn.Embedding): An embedding layer for the model&#39;s vocabulary.</span>
<span class="sd">        layers (nn.ModuleList): A list of MSMambaBlock layers for the model.</span>
<span class="sd">        gradient_checkpointing (bool): Indicates if gradient checkpointing is enabled.</span>
<span class="sd">        norm_f (MSMambaRMSNorm): Normalization function for the model&#39;s hidden states.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the MSMambaModel with the given configuration.</span>
<span class="sd">        get_input_embeddings: Retrieves the input embeddings for the model.</span>
<span class="sd">        set_input_embeddings: Sets new input embeddings for the model.</span>
<span class="sd">        forward: Constructs the model based on the input and configuration parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of MSMambaModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (object): The instance of MSMambaModel.</span>
<span class="sd">            config (object): The configuration object containing parameters for the model.</span>
<span class="sd">                Must include the following attributes:</span>

<span class="sd">                - vocab_size (int): The size of the vocabulary.</span>
<span class="sd">                - hidden_size (int): The size of the hidden layers.</span>
<span class="sd">                - num_hidden_layers (int): The number of hidden layers.</span>
<span class="sd">                - layer_norm_epsilon (float): The epsilon value for layer normalization.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">MSMambaBlock</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_f</span> <span class="o">=</span> <span class="n">MSMambaRMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_epsilon</span><span class="p">)</span>
        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieve the input embeddings for the MSMambaModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the MSMambaModel class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The embeddings associated with the input.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the input embeddings for the MSMambaModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MSMambaModel): The instance of the MSMambaModel class.</span>
<span class="sd">            new_embeddings (object): The new input embeddings to be set for the MSMambaModel.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>  <span class="c1"># `attention_mask` is passed by the tokenizer and we don&#39;t want it</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Construct the MSMambaModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MSMambaModel): The instance of the MSMambaModel.</span>
<span class="sd">            input_ids (Optional[mindspore.Tensor]): The input tensor containing the indices of tokens in the</span>
<span class="sd">                input sequence. Default is None.</span>
<span class="sd">            inputs_embeds (Optional[mindspore.Tensor]): The input tensor for the embeddings. Default is None.</span>
<span class="sd">            cache_params (Optional[List[mindspore.Tensor]]): The optional cache parameters for the model. Default is None.</span>
<span class="sd">            use_cache (Optional[bool]): Flag to use cache. Default is None.</span>
<span class="sd">            output_hidden_states (Optional[bool]): Flag to output hidden states. Default is None.</span>
<span class="sd">            return_dict (Optional[bool]): Flag to return a dictionary. Default is None.</span>
<span class="sd">            **kwargs: Additional keyword arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[Tuple, Dict]:</span>
<span class="sd">                Depending on the value of &#39;return_dict&#39;, it returns either a tuple or a dictionary.</span>

<span class="sd">                - If &#39;return_dict&#39; is False, returns a tuple containing &#39;hidden_states&#39;, &#39;cache_params&#39;,</span>
<span class="sd">                and &#39;all_hidden_states&#39; if not None.</span>
<span class="sd">                - If &#39;return_dict&#39; is True, returns a dictionary with keys &#39;last_hidden_state&#39;, &#39;cache_params&#39;</span>
<span class="sd">                (if &#39;use_cache&#39; is True), and &#39;hidden_states&#39;.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the input_ids and inputs_embeds are both None.</span>
<span class="sd">            RuntimeError: If an error occurs during the forwardion process.</span>
<span class="sd">            TypeError: If the input_ids or inputs_embeds are not of type mindspore.Tensor.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="c1"># if (input_ids is None) ^ (inputs_embeds is not None):  # ^ is python for xor</span>
        <span class="c1">#     raise ValueError(</span>
        <span class="c1">#         &quot;You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one&quot;</span>
        <span class="c1">#     )</span>

        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">cache_params</span> <span class="o">=</span> <span class="n">MSMambaCache</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span>
            <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">mixer_block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mixer_block</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="n">cache_params</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">cache_params</span><span class="o">.</span><span class="n">seqlen_offset</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">.</span><span class="n">seqlen_offset</span> <span class="o">+</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_f</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;last_hidden_state&#39;</span><span class="p">:</span> <span class="n">hidden_states</span><span class="p">,</span>
            <span class="s1">&#39;cache_params&#39;</span><span class="p">:</span> <span class="n">cache_params</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;hidden_states&#39;</span><span class="p">:</span> <span class="n">all_hidden_states</span><span class="p">,</span>
        <span class="p">}</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaModel</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of MSMambaModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of MSMambaModel.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing parameters for the model.
Must include the following attributes:</p>
<ul>
<li>vocab_size (int): The size of the vocabulary.</li>
<li>hidden_size (int): The size of the hidden layers.</li>
<li>num_hidden_layers (int): The number of hidden layers.</li>
<li>layer_norm_epsilon (float): The epsilon value for layer normalization.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of MSMambaModel.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (object): The instance of MSMambaModel.</span>
<span class="sd">        config (object): The configuration object containing parameters for the model.</span>
<span class="sd">            Must include the following attributes:</span>

<span class="sd">            - vocab_size (int): The size of the vocabulary.</span>
<span class="sd">            - hidden_size (int): The size of the hidden layers.</span>
<span class="sd">            - num_hidden_layers (int): The number of hidden layers.</span>
<span class="sd">            - layer_norm_epsilon (float): The epsilon value for layer normalization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">MSMambaBlock</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)])</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">norm_f</span> <span class="o">=</span> <span class="n">MSMambaRMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_epsilon</span><span class="p">)</span>
    <span class="c1"># Initialize weights and apply final processing</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaModel</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Construct the MSMambaModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the MSMambaModel.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel">MSMambaModel</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor containing the indices of tokens in the
input sequence. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inputs_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor for the embeddings. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_params</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional cache parameters for the model. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="mindspore.Tensor">Tensor</span>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag to use cache. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag to output hidden states. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag to return a dictionary. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional keyword arguments.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="typing.Tuple">Tuple</span>, <span title="typing.Dict">Dict</span>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Union[Tuple, Dict]:
Depending on the value of 'return_dict', it returns either a tuple or a dictionary.</p>
<ul>
<li>If 'return_dict' is False, returns a tuple containing 'hidden_states', 'cache_params',
and 'all_hidden_states' if not None.</li>
<li>If 'return_dict' is True, returns a dictionary with keys 'last_hidden_state', 'cache_params'
(if 'use_cache' is True), and 'hidden_states'.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input_ids and inputs_embeds are both None.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If an error occurs during the forwardion process.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input_ids or inputs_embeds are not of type mindspore.Tensor.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cache_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>  <span class="c1"># `attention_mask` is passed by the tokenizer and we don&#39;t want it</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Construct the MSMambaModel.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MSMambaModel): The instance of the MSMambaModel.</span>
<span class="sd">        input_ids (Optional[mindspore.Tensor]): The input tensor containing the indices of tokens in the</span>
<span class="sd">            input sequence. Default is None.</span>
<span class="sd">        inputs_embeds (Optional[mindspore.Tensor]): The input tensor for the embeddings. Default is None.</span>
<span class="sd">        cache_params (Optional[List[mindspore.Tensor]]): The optional cache parameters for the model. Default is None.</span>
<span class="sd">        use_cache (Optional[bool]): Flag to use cache. Default is None.</span>
<span class="sd">        output_hidden_states (Optional[bool]): Flag to output hidden states. Default is None.</span>
<span class="sd">        return_dict (Optional[bool]): Flag to return a dictionary. Default is None.</span>
<span class="sd">        **kwargs: Additional keyword arguments.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Union[Tuple, Dict]:</span>
<span class="sd">            Depending on the value of &#39;return_dict&#39;, it returns either a tuple or a dictionary.</span>

<span class="sd">            - If &#39;return_dict&#39; is False, returns a tuple containing &#39;hidden_states&#39;, &#39;cache_params&#39;,</span>
<span class="sd">            and &#39;all_hidden_states&#39; if not None.</span>
<span class="sd">            - If &#39;return_dict&#39; is True, returns a dictionary with keys &#39;last_hidden_state&#39;, &#39;cache_params&#39;</span>
<span class="sd">            (if &#39;use_cache&#39; is True), and &#39;hidden_states&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the input_ids and inputs_embeds are both None.</span>
<span class="sd">        RuntimeError: If an error occurs during the forwardion process.</span>
<span class="sd">        TypeError: If the input_ids or inputs_embeds are not of type mindspore.Tensor.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="c1"># if (input_ids is None) ^ (inputs_embeds is not None):  # ^ is python for xor</span>
    <span class="c1">#     raise ValueError(</span>
    <span class="c1">#         &quot;You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one&quot;</span>
    <span class="c1">#     )</span>

    <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="n">use_cache</span><span class="p">:</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">cache_params</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">use_cache</span><span class="p">:</span>
        <span class="n">cache_params</span> <span class="o">=</span> <span class="n">MSMambaCache</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span>
    <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">mixer_block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mixer_block</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">=</span><span class="n">cache_params</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

    <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">cache_params</span><span class="o">.</span><span class="n">seqlen_offset</span><span class="p">,</span> <span class="n">cache_params</span><span class="o">.</span><span class="n">seqlen_offset</span> <span class="o">+</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_f</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">cache_params</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;last_hidden_state&#39;</span><span class="p">:</span> <span class="n">hidden_states</span><span class="p">,</span>
        <span class="s1">&#39;cache_params&#39;</span><span class="p">:</span> <span class="n">cache_params</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;hidden_states&#39;</span><span class="p">:</span> <span class="n">all_hidden_states</span><span class="p">,</span>
    <span class="p">}</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.get_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaModel</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.get_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Retrieve the input embeddings for the MSMambaModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the MSMambaModel class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The embeddings associated with the input.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieve the input embeddings for the MSMambaModel.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the MSMambaModel class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The embeddings associated with the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.set_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaModel</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel.set_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set the input embeddings for the MSMambaModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the MSMambaModel class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaModel">MSMambaModel</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>new_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The new input embeddings to be set for the MSMambaModel.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set the input embeddings for the MSMambaModel.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MSMambaModel): The instance of the MSMambaModel class.</span>
<span class="sd">        new_embeddings (object): The new input embeddings to be set for the MSMambaModel.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">new_embeddings</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel</code>


<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.modeling_utils.PreTrainedModel" href="../../modeling_utils/#mindnlp.transformers.modeling_utils.PreTrainedModel">PreTrainedModel</a></code></p>


        <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
models.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MSMambaPreTrainedModel</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained</span>
<span class="sd">    models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">MambaConfig</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;backbone&quot;</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;MSMambaBlock&quot;</span><span class="p">]</span>
    <span class="n">supports_gradient_checkpointing</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the weights.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">MSMambaMixer</span><span class="p">):</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">A_log</span><span class="o">.</span><span class="n">_no_weight_decay</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">D</span><span class="o">.</span><span class="n">_no_weight_decay</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">dt_init_std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_rank</span><span class="o">**-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_scale</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_init_scheme</span> <span class="o">==</span> <span class="s2">&quot;constant&quot;</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">dt_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">dt_init_std</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_init_scheme</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">dt_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">Uniform</span><span class="p">(</span><span class="n">dt_init_std</span><span class="p">),</span> <span class="n">cell</span><span class="o">.</span><span class="n">dt_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">dt_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

            <span class="n">dt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">)</span>
                <span class="o">*</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_max</span><span class="p">)</span> <span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_min</span><span class="p">))</span>
                <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_min</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">time_step_floor</span><span class="p">)</span>
            <span class="c1"># # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759</span>
            <span class="n">inv_dt</span> <span class="o">=</span> <span class="n">dt</span> <span class="o">+</span> <span class="n">ops</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="o">-</span><span class="n">ops</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="o">-</span><span class="n">dt</span><span class="p">))</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">dt_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">inv_dt</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">dt_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">_no_reinit</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">MambaDense</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="s2">&quot;_no_reinit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                    <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">),</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rescale_prenorm_residual</span><span class="p">:</span>
            <span class="c1"># Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:</span>
            <span class="c1">#   &gt; A modified initialization which accounts for the accumulation on the residual path with model depth. Scale</span>
            <span class="c1">#   &gt; the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.</span>
            <span class="c1">#   &gt;   -- GPT-2 :: https://openai.com/blog/better-language-models/</span>
            <span class="c1">#</span>
            <span class="c1"># Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">cell</span><span class="o">.</span><span class="n">parameters_and_names</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;out_proj.weight&quot;</span><span class="p">]:</span>
                    <span class="c1"># Special Scaled Initialization --&gt; There are 2 Layer Norms per Transformer Block</span>
                    <span class="c1"># Following MindSpore init, except scale by 1/sqrt(2 * n_layer)</span>
                    <span class="c1"># We need to reinit p since this code could be called multiple times</span>
                    <span class="c1"># Having just p *= scale would repeatedly scale it down</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">HeUniform</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">)),</span> <span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method __call__ is defined within the class MSMambaPreTrainedModel and is used to handle the call operation</span>
<span class="sd">        when an instance of the class is called as a function.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the MSMambaPreTrainedModel class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Conditional returns:</span>

<span class="sd">                - If the outputs from the super().__call__(*args, **kwargs) are of type dict, the method returns</span>
<span class="sd">                an instance of ADDict(outputs).</span>
<span class="sd">                - Otherwise, it returns the outputs as is.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">ADDict</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaPreTrainedModel</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaPreTrainedModel.__call__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method <strong>call</strong> is defined within the class MSMambaPreTrainedModel and is used to handle the call operation
when an instance of the class is called as a function.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the MSMambaPreTrainedModel class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Conditional returns:</p>
<ul>
<li>If the outputs from the super().<strong>call</strong>(*args, **kwargs) are of type dict, the method returns
an instance of ADDict(outputs).</li>
<li>Otherwise, it returns the outputs as is.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method __call__ is defined within the class MSMambaPreTrainedModel and is used to handle the call operation</span>
<span class="sd">    when an instance of the class is called as a function.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the MSMambaPreTrainedModel class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Conditional returns:</span>

<span class="sd">            - If the outputs from the super().__call__(*args, **kwargs) are of type dict, the method returns</span>
<span class="sd">            an instance of ADDict(outputs).</span>
<span class="sd">            - Otherwise, it returns the outputs as is.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ADDict</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm</code>


<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>MSMambaRMSNorm is a class that represents a modified version of the T5LayerNorm, called LlamaRMSNorm.
It is designed to normalize the hidden states of a neural network layer.</p>
<p>This class inherits from nn.Module and provides functionality to normalize the hidden states using a modified
RMS normalization technique.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.weight">weight</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A parameter tensor that stores the weight values for the normalization.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Parameter">Parameter</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.variance_epsilon">variance_epsilon</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A small value added to the variance to avoid division by zero.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.__init__" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes an instance of MSMambaRMSNorm.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.forward" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Normalizes the input hidden states using the RMS normalization technique.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>The input hidden states are expected to be of shape (batch_size, sequence_length, hidden_size).</li>
<li>The normalization is performed along the last dimension (hidden_size).</li>
</ul>
</details>

<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">random_normal</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">MSMambaRMSNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">normalized_states</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</code></pre></div>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MSMambaRMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MSMambaRMSNorm is a class that represents a modified version of the T5LayerNorm, called LlamaRMSNorm.</span>
<span class="sd">    It is designed to normalize the hidden states of a neural network layer.</span>

<span class="sd">    This class inherits from nn.Module and provides functionality to normalize the hidden states using a modified</span>
<span class="sd">    RMS normalization technique.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Parameter): A parameter tensor that stores the weight values for the normalization.</span>
<span class="sd">        variance_epsilon (float): A small value added to the variance to avoid division by zero.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes an instance of MSMambaRMSNorm.</span>
<span class="sd">        forward: Normalizes the input hidden states using the RMS normalization technique.</span>

<span class="sd">    Note:</span>
<span class="sd">        - The input hidden states are expected to be of shape (batch_size, sequence_length, hidden_size).</span>
<span class="sd">        - The normalization is performed along the last dimension (hidden_size).</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; hidden_states = ops.random_normal((batch_size, sequence_length, hidden_size))</span>
<span class="sd">        &gt;&gt;&gt; norm_layer = MSMambaRMSNorm(hidden_size)</span>
<span class="sd">        &gt;&gt;&gt; normalized_states = norm_layer.forward(hidden_states)</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        LlamaRMSNorm is equivalent to T5LayerNorm</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Constructs an instance of MSMambaRMSNorm.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MSMambaRMSNorm): The instance of the MSMambaRMSNorm class.</span>
<span class="sd">            hidden_states (Tensor): The input tensor containing the hidden states.</span>
<span class="sd">                It should be of type tensor and have a shape (batch_size, sequence_length, hidden_size).</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: The method modifies the hidden_states tensor in-place.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If the hidden_states parameter is not of type tensor.</span>
<span class="sd">            ValueError: If the hidden_states tensor does not have the expected shape.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">variance</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaRMSNorm</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>LlamaRMSNorm is equivalent to T5LayerNorm</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LlamaRMSNorm is equivalent to T5LayerNorm</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span> <span class="o">=</span> <span class="n">eps</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MSMambaRMSNorm</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs an instance of MSMambaRMSNorm.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the MSMambaRMSNorm class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MSMambaRMSNorm">MSMambaRMSNorm</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor containing the hidden states.
It should be of type tensor and have a shape (batch_size, sequence_length, hidden_size).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The method modifies the hidden_states tensor in-place.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the hidden_states parameter is not of type tensor.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the hidden_states tensor does not have the expected shape.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Constructs an instance of MSMambaRMSNorm.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MSMambaRMSNorm): The instance of the MSMambaRMSNorm class.</span>
<span class="sd">        hidden_states (Tensor): The input tensor containing the hidden states.</span>
<span class="sd">            It should be of type tensor and have a shape (batch_size, sequence_length, hidden_size).</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: The method modifies the hidden_states tensor in-place.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the hidden_states parameter is not of type tensor.</span>
<span class="sd">        ValueError: If the hidden_states tensor does not have the expected shape.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MambaDense" class="doc doc-heading">
            <code>mindnlp.transformers.models.mamba.modeling_graph_mamba.MambaDense</code>


<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MambaDense" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Linear">Linear</span></code></p>


        <p>MambaDense represents a dense layer in a neural network.
It performs matrix multiplication with optional bias addition and reshaping of input data.
This class inherits from nn.Linear.</p>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">out_shape</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">out_shape</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MambaDense</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MambaDense represents a dense layer in a neural network.</span>
<span class="sd">    It performs matrix multiplication with optional bias addition and reshaping of input data.</span>
<span class="sd">    This class inherits from nn.Linear.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; def forward(self, x):</span>
<span class="sd">        &gt;&gt;&gt;     x_shape = x.shape</span>
<span class="sd">        &gt;&gt;&gt;     if len(x_shape) != 2:</span>
<span class="sd">        &gt;&gt;&gt;         x = x.reshape(-1, x.shape[-1])</span>
<span class="sd">        &gt;&gt;&gt;     x = ops.matmul(x, self.weight.T)</span>
<span class="sd">        &gt;&gt;&gt;     if self.bias:</span>
<span class="sd">        &gt;&gt;&gt;         x = ops.add(x, self.bias)</span>
<span class="sd">        &gt;&gt;&gt;     if len(x_shape) != 2:</span>
<span class="sd">        &gt;&gt;&gt;         out_shape = x_shape[:-1] + (x.shape[-1], )</span>
<span class="sd">        &gt;&gt;&gt;         x = x.reshape(out_shape)</span>
<span class="sd">        &gt;&gt;&gt;     return x</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the output of the MambaDense layer by performing matrix multiplication with weights and</span>
<span class="sd">        adding bias if applicable.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (MambaDense): The instance of the MambaDense class.</span>
<span class="sd">            x (ndarray): Input data for the layer. Should be a 2D numpy array, but will reshape to 2D if necessary.</span>

<span class="sd">        Returns:</span>
<span class="sd">            ndarray: The output of the MambaDense layer after matrix multiplication with weights and addition of</span>
<span class="sd">                bias if specified.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the input data x is not a 2D numpy array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">out_shape</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">out_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.mamba.modeling_graph_mamba.MambaDense.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mamba</span><span class="o">.</span><span class="n">modeling_graph_mamba</span><span class="o">.</span><span class="n">MambaDense</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MambaDense.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the output of the MambaDense layer by performing matrix multiplication with weights and
adding bias if applicable.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the MambaDense class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.mamba.modeling_graph_mamba.MambaDense" href="#mindnlp.transformers.models.mamba.modeling_graph_mamba.MambaDense">MambaDense</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input data for the layer. Should be a 2D numpy array, but will reshape to 2D if necessary.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>ndarray</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>ndarray</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The output of the MambaDense layer after matrix multiplication with weights and addition of
bias if specified.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input data x is not a 2D numpy array.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\mamba\modeling_graph_mamba.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the output of the MambaDense layer by performing matrix multiplication with weights and</span>
<span class="sd">    adding bias if applicable.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (MambaDense): The instance of the MambaDense class.</span>
<span class="sd">        x (ndarray): Input data for the layer. Should be a 2D numpy array, but will reshape to 2D if necessary.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ndarray: The output of the MambaDense layer after matrix multiplication with weights and addition of</span>
<span class="sd">            bias if specified.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the input data x is not a 2D numpy array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">out_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../luke/" class="md-footer__link md-footer__link--prev" aria-label="Previous: luke">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                luke
              </div>
            </div>
          </a>
        
        
          
          <a href="../marian/" class="md-footer__link md-footer__link--next" aria-label="Next: marian">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                marian
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 MindSpore Lab and CQU NLP Team.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:lvyufeng@cqu.edu.cn" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindnlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/lu-yu-feng-46-1" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>