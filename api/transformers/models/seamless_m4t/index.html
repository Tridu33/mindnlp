
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../sam/">
      
      
        <link rel="next" href="../seamless_m4t_v2/">
      
      
      <link rel="icon" href="../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>seamless_m4t - MindNLP Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="MindNLP Docs" class="md-header__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindNLP Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              seamless_m4t
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="./" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../../../zh/api/transformers/models/seamless_m4t/" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../tutorials/quick_start/" class="md-tabs__link">
          
  
    
  
  Tutorials

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../supported_models/" class="md-tabs__link">
        
  
    
  
  Supported Models

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../contribute/" class="md-tabs__link">
        
  
    
  
  How-To Contribute

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../accelerate/" class="md-tabs__link">
          
  
    
  
  API Reference

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../notes/changelog/" class="md-tabs__link">
          
  
    
  
  Notes

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="MindNLP Docs" class="md-nav__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    MindNLP Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/data_preprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocess
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_mirror/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Mirror
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../supported_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Supported Models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How-To Contribute
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../accelerate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/load_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    load_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/BaseMapFunction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BaseMapFunction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transforms
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engine
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Engine
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_1" >
        
          
          <label class="md-nav__link" for="__nav_5_4_1" id="__nav_5_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    train_args
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_1">
            <span class="md-nav__icon md-icon"></span>
            train_args
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seq2seq
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_2" >
        
          
          <label class="md-nav__link" for="__nav_5_4_2" id="__nav_5_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_2">
            <span class="md-nav__icon md-icon"></span>
            trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/default_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    default_func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../peft/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_2" >
        
          
          <label class="md-nav__link" for="__nav_5_7_2" id="__nav_5_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tuners
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_2">
            <span class="md-nav__icon md-icon"></span>
            tuners
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adalora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AdaLoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adaption_prompt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adaption_Prompt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/ia3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lokr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoKr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/prompt_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt tuning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_3" >
        
          
          <label class="md-nav__link" for="__nav_5_7_3" id="__nav_5_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/utils/merge_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    merge_utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/peft_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    peft_model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sentence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_2" >
        
          
          <label class="md-nav__link" for="__nav_5_9_2" id="__nav_5_9_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_2">
            <span class="md-nav__icon md-icon"></span>
            generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/generation/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/logits_process/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/stopping_criteria/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stopping_criteria
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/streamers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    streamers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_3" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9_3" id="__nav_5_9_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9_3">
            <span class="md-nav__icon md-icon"></span>
            models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../albert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    albert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    align
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../altclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    altclip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../audio_spectrogram_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    audio_spectrogram_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../auto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    auto
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    autoformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../baichuan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    baichuan
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../barthez/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    barthez
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bartpho/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bartpho
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../beit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bertweet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bertweet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bge_m3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bge_m3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../big_bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    big_bird
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bigbird_pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bigbird_pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../biogpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    biogpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot_small/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot_small
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip_2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bloom
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bridgetower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bros/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bros
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../byt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    byt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../camembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    camembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../canine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    canine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../codegen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    codegen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cogvlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cogvlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cohere/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convnext/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convnext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpmant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpmbee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ctrl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctrl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cvt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cvt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data2vec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data2vec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../decision_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decision_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distilbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    distilbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../electra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    electra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../encodec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encodec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie_m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie_m
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../esm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    esm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    flava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../funnel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    funnel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gemma
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_bigcode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_bigcode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_pangu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_pangu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gptj/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gptj
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../graphormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    graphormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../groupvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    groupvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hubert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hubert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../imagegpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    imagegpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../internlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    internlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jetmoe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jetmoe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlmv2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlmv2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../led/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    led
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava_next/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava_next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../luke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    luke
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../marian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    marian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../maskformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maskformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mbart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mbart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minicpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minicpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minigpt4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minigpt4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mixtral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilevit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilevit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../moss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    moss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen_melody/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen_melody
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mvp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mvp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nezha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nezha
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nystromformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nystromformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../olmo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    olmo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../openelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    openelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    opt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../owlvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    owlvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../poolformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    poolformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pop2piano/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pop2piano
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2_moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    resnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roc_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roc_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rwkv
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_seamless_m4t
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_seamless_m4t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TAttention" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TCodeHifiGan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TCodeHifiGan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerConvolutionModule" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConformerConvolutionModule
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerEncoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConformerEncoderLayer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerRelPositionalEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConformerRelPositionalEmbedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerRotaryPositionalEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConformerRotaryPositionalEmbedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerSelfAttention" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConformerSelfAttention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TDecoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TDecoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TDecoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TDecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoderLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TEncoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TEncoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TEncoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoderLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TForSpeechToSpeech
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TForSpeechToSpeech">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TForSpeechToText
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TForSpeechToText">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TForTextToSpeech
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TForTextToSpeech">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToText" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TForTextToText
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TForTextToText">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToText.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TGenerationOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4THifiGan" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4THifiGan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4THifiGan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4THifiGan.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TPreTrainedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TPreTrainedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel.compute_last_hidden_states_per_sample" class="md-nav__link">
    <span class="md-ellipsis">
      compute_last_hidden_states_per_sample
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TScaledWordEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TScaledWordEmbedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TSinusoidalPositionalEmbedding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TSinusoidalPositionalEmbedding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds" class="md-nav__link">
    <span class="md-ellipsis">
      create_position_ids_from_inputs_embeds
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding.get_embedding" class="md-nav__link">
    <span class="md-ellipsis">
      get_embedding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.create_position_ids_from_input_ids" class="md-nav__link">
    <span class="md-ellipsis">
      create_position_ids_from_input_ids
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.format_speech_generation_kwargs" class="md-nav__link">
    <span class="md-ellipsis">
      format_speech_generation_kwargs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.shift_tokens_right" class="md-nav__link">
    <span class="md-ellipsis">
      shift_tokens_right
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_seamless_m4t
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_seamless_m4t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t.SeamlessM4TConfig" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t.SeamlessM4TConfig.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization_seamless_m4t
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tokenization_seamless_m4t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TTokenizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.src_lang" class="md-nav__link">
    <span class="md-ellipsis">
      src_lang
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.tgt_lang" class="md-nav__link">
    <span class="md-ellipsis">
      tgt_lang
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.unk_token_length" class="md-nav__link">
    <span class="md-ellipsis">
      unk_token_length
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.vocab_size" class="md-nav__link">
    <span class="md-ellipsis">
      vocab_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__getstate__" class="md-nav__link">
    <span class="md-ellipsis">
      __getstate__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__setstate__" class="md-nav__link">
    <span class="md-ellipsis">
      __setstate__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.build_inputs_with_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      build_inputs_with_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.convert_tokens_to_string" class="md-nav__link">
    <span class="md-ellipsis">
      convert_tokens_to_string
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.create_token_type_ids_from_sequences" class="md-nav__link">
    <span class="md-ellipsis">
      create_token_type_ids_from_sequences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_special_tokens_mask" class="md-nav__link">
    <span class="md-ellipsis">
      get_special_tokens_mask
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_spm_processor" class="md-nav__link">
    <span class="md-ellipsis">
      get_spm_processor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_vocab" class="md-nav__link">
    <span class="md-ellipsis">
      get_vocab
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.prepare_seq2seq_batch" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_seq2seq_batch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.save_vocabulary" class="md-nav__link">
    <span class="md-ellipsis">
      save_vocabulary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.set_src_lang_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      set_src_lang_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.set_tgt_lang_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      set_tgt_lang_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.tokenize" class="md-nav__link">
    <span class="md-ellipsis">
      tokenize
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization_seamless_m4t_fast
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tokenization_seamless_m4t_fast">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TTokenizerFast
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TTokenizerFast">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.can_save_slow_tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      can_save_slow_tokenizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.src_lang" class="md-nav__link">
    <span class="md-ellipsis">
      src_lang
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.tgt_lang" class="md-nav__link">
    <span class="md-ellipsis">
      tgt_lang
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.build_inputs_with_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      build_inputs_with_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.create_token_type_ids_from_sequences" class="md-nav__link">
    <span class="md-ellipsis">
      create_token_type_ids_from_sequences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.prepare_seq2seq_batch" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_seq2seq_batch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.save_vocabulary" class="md-nav__link">
    <span class="md-ellipsis">
      save_vocabulary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.set_src_lang_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      set_src_lang_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.set_tgt_lang_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      set_tgt_lang_special_tokens
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t" class="md-nav__link">
    <span class="md-ellipsis">
      feature_extraction_seamless_m4t
    </span>
  </a>
  
    <nav class="md-nav" aria-label="feature_extraction_seamless_m4t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TFeatureExtractor
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TFeatureExtractor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.zero_mean_unit_var_norm" class="md-nav__link">
    <span class="md-ellipsis">
      zero_mean_unit_var_norm
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t" class="md-nav__link">
    <span class="md-ellipsis">
      processing_seamless_m4t
    </span>
  </a>
  
    <nav class="md-nav" aria-label="processing_seamless_m4t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TProcessor
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TProcessor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.model_input_names" class="md-nav__link">
    <span class="md-ellipsis">
      model_input_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.batch_decode" class="md-nav__link">
    <span class="md-ellipsis">
      batch_decode
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.decode" class="md-nav__link">
    <span class="md-ellipsis">
      decode
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seamless_m4t_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../segformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    segformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seggpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seggpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_encoder_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_encoder_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_to_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_to_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../squeezebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    squeezebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stablelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stablelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    starcoder2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../swiftformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    swiftformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../switch_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    switch_transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../t5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    t5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../table_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    table_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../timesformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    timesformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tinybert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tinybert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../van/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    van
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vipllava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vipllava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vision_text_dual_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision_text_dual_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../visual_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visual_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_conformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_conformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_with_lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_with_lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wavlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    whisper
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../x_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    x_clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta_xl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta_xl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlnet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_4" >
        
          
          <label class="md-nav__link" for="__nav_5_9_4" id="__nav_5_9_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_4">
            <span class="md-nav__icon md-icon"></span>
            pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/pipeline/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/automatic_speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    automatic_speech_recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/document_question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    document_question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/fill_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fill_mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text2text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text2text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/zero_shot_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_shot_classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../configuration_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    configuration_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modeling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_fast/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_fast
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TRL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Notes
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Change Log
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code of Conduct
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_seamless_m4t
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_seamless_m4t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TAttention" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TCodeHifiGan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TCodeHifiGan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerConvolutionModule" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConformerConvolutionModule
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerEncoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConformerEncoderLayer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerRelPositionalEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConformerRelPositionalEmbedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerRotaryPositionalEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConformerRotaryPositionalEmbedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerSelfAttention" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConformerSelfAttention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TDecoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TDecoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TDecoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TDecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoderLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TEncoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TEncoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TEncoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoderLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TForSpeechToSpeech
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TForSpeechToSpeech">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TForSpeechToText
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TForSpeechToText">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TForTextToSpeech
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TForTextToSpeech">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToText" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TForTextToText
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TForTextToText">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToText.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TGenerationOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4THifiGan" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4THifiGan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4THifiGan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4THifiGan.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TPreTrainedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TPreTrainedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel.compute_last_hidden_states_per_sample" class="md-nav__link">
    <span class="md-ellipsis">
      compute_last_hidden_states_per_sample
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TScaledWordEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TScaledWordEmbedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TSinusoidalPositionalEmbedding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TSinusoidalPositionalEmbedding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds" class="md-nav__link">
    <span class="md-ellipsis">
      create_position_ids_from_inputs_embeds
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding.get_embedding" class="md-nav__link">
    <span class="md-ellipsis">
      get_embedding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.create_position_ids_from_input_ids" class="md-nav__link">
    <span class="md-ellipsis">
      create_position_ids_from_input_ids
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.format_speech_generation_kwargs" class="md-nav__link">
    <span class="md-ellipsis">
      format_speech_generation_kwargs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.shift_tokens_right" class="md-nav__link">
    <span class="md-ellipsis">
      shift_tokens_right
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_seamless_m4t
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_seamless_m4t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t.SeamlessM4TConfig" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t.SeamlessM4TConfig.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization_seamless_m4t
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tokenization_seamless_m4t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TTokenizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.src_lang" class="md-nav__link">
    <span class="md-ellipsis">
      src_lang
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.tgt_lang" class="md-nav__link">
    <span class="md-ellipsis">
      tgt_lang
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.unk_token_length" class="md-nav__link">
    <span class="md-ellipsis">
      unk_token_length
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.vocab_size" class="md-nav__link">
    <span class="md-ellipsis">
      vocab_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__getstate__" class="md-nav__link">
    <span class="md-ellipsis">
      __getstate__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__setstate__" class="md-nav__link">
    <span class="md-ellipsis">
      __setstate__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.build_inputs_with_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      build_inputs_with_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.convert_tokens_to_string" class="md-nav__link">
    <span class="md-ellipsis">
      convert_tokens_to_string
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.create_token_type_ids_from_sequences" class="md-nav__link">
    <span class="md-ellipsis">
      create_token_type_ids_from_sequences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_special_tokens_mask" class="md-nav__link">
    <span class="md-ellipsis">
      get_special_tokens_mask
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_spm_processor" class="md-nav__link">
    <span class="md-ellipsis">
      get_spm_processor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_vocab" class="md-nav__link">
    <span class="md-ellipsis">
      get_vocab
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.prepare_seq2seq_batch" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_seq2seq_batch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.save_vocabulary" class="md-nav__link">
    <span class="md-ellipsis">
      save_vocabulary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.set_src_lang_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      set_src_lang_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.set_tgt_lang_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      set_tgt_lang_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.tokenize" class="md-nav__link">
    <span class="md-ellipsis">
      tokenize
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization_seamless_m4t_fast
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tokenization_seamless_m4t_fast">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TTokenizerFast
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TTokenizerFast">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.can_save_slow_tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      can_save_slow_tokenizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.src_lang" class="md-nav__link">
    <span class="md-ellipsis">
      src_lang
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.tgt_lang" class="md-nav__link">
    <span class="md-ellipsis">
      tgt_lang
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.build_inputs_with_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      build_inputs_with_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.create_token_type_ids_from_sequences" class="md-nav__link">
    <span class="md-ellipsis">
      create_token_type_ids_from_sequences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.prepare_seq2seq_batch" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_seq2seq_batch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.save_vocabulary" class="md-nav__link">
    <span class="md-ellipsis">
      save_vocabulary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.set_src_lang_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      set_src_lang_special_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.set_tgt_lang_special_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      set_tgt_lang_special_tokens
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t" class="md-nav__link">
    <span class="md-ellipsis">
      feature_extraction_seamless_m4t
    </span>
  </a>
  
    <nav class="md-nav" aria-label="feature_extraction_seamless_m4t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TFeatureExtractor
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TFeatureExtractor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.zero_mean_unit_var_norm" class="md-nav__link">
    <span class="md-ellipsis">
      zero_mean_unit_var_norm
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t" class="md-nav__link">
    <span class="md-ellipsis">
      processing_seamless_m4t
    </span>
  </a>
  
    <nav class="md-nav" aria-label="processing_seamless_m4t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4TProcessor
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4TProcessor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.model_input_names" class="md-nav__link">
    <span class="md-ellipsis">
      model_input_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.batch_decode" class="md-nav__link">
    <span class="md-ellipsis">
      batch_decode
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.decode" class="md-nav__link">
    <span class="md-ellipsis">
      decode
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindnlp/edit/master/docs/en/api/transformers/models/seamless_m4t.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindnlp/raw/master/docs/en/api/transformers/models/seamless_m4t.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>seamless_m4t</h1>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>MindSpore SeamlessM4T model.</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TAttention" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TAttention</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TAttention" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Multi-headed attention from 'Attention Is All You Need' paper</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multi-headed attention from &#39;Attention Is All You Need&#39; paper&quot;&quot;&quot;</span>

    <span class="c1"># Copied from transformers.models.bart.modeling_bart.BartAttention.__init__ with Bart-&gt;SeamlessM4T</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">is_decoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SeamlessM4TConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;embed_dim must be divisible by num_heads (got `embed_dim`: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; and `num_heads`: </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="o">**-</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span> <span class="o">=</span> <span class="n">is_decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_causal</span> <span class="o">=</span> <span class="n">is_causal</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bsz</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Input shape: Batch x Time x Channel&quot;&quot;&quot;</span>

        <span class="c1"># if encoder_hidden_states are provided this layer is used as a cross-attention layer</span>
        <span class="c1"># for the decoder</span>
        <span class="n">is_cross_attention</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="n">bsz</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># get query proj</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span>
        <span class="c1"># get key, value proj</span>
        <span class="c1"># `past_key_value[0].shape[2] == encoder_hidden_states.shape[1]`</span>
        <span class="c1"># is checking that the `sequence_length` of the `past_key_value` is the same as</span>
        <span class="c1"># the provided `encoder_hidden_states` to support prefix tuning</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">is_cross_attention</span>
            <span class="ow">and</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="c1"># reuse k,v, cross_attentions</span>
            <span class="n">key_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">value_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">is_cross_attention</span><span class="p">:</span>
            <span class="c1"># cross_attentions</span>
            <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
            <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># reuse k, v, self_attention</span>
            <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
            <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
            <span class="n">key_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">key_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">value_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">value_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># self_attention</span>
            <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
            <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
            <span class="c1"># if cross_attention save Tuple(mindspore.Tensor, mindspore.Tensor) of all cross attention key/value_states.</span>
            <span class="c1"># Further calls to cross_attention layer can then reuse all cross-attention</span>
            <span class="c1"># key/value_states (first &quot;if&quot; case)</span>
            <span class="c1"># if uni-directional self-attention (decoder) save Tuple(mindspore.Tensor, mindspore.Tensor) of</span>
            <span class="c1"># all previous decoder key/value_states. Further calls to uni-directional self-attention</span>
            <span class="c1"># can concat previous decoder key/value_states to current projected key/value_states (third &quot;elif&quot; case)</span>
            <span class="c1"># if encoder bi-directional self-attention `past_key_value` is always `None`</span>
            <span class="n">past_key_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>

        <span class="n">proj_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>

        <span class="n">src_len</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Attention weights should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span><span class="w"> </span><span class="n">tgt_len</span><span class="p">,</span><span class="w"> </span><span class="n">src_len</span><span class="p">)</span><span class="si">}</span><span class="s2">, but is&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">attn_weights</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Attention mask should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">tgt_len</span><span class="p">,</span><span class="w"> </span><span class="n">src_len</span><span class="p">)</span><span class="si">}</span><span class="s2">, but is </span><span class="si">{</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span> <span class="o">+</span> <span class="n">attention_mask</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="c1"># this operation is a bit awkward, but it&#39;s required to</span>
            <span class="c1"># make sure that attn_weights keeps its gradient.</span>
            <span class="c1"># In order to do so, attn_weights have to be reshaped</span>
            <span class="c1"># twice and have to be reused in the following</span>
            <span class="n">attn_weights_reshaped</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights_reshaped</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_weights_reshaped</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">attn_probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attn_probs</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`attn_output` should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span><span class="w"> </span><span class="n">tgt_len</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="si">}</span><span class="s2">, but is&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">attn_output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be</span>
        <span class="c1"># partitioned across GPUs when using tensor-parallelism.</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights_reshaped</span><span class="p">,</span> <span class="n">past_key_value</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TAttention</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TAttention.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Input shape: Batch x Time x Channel</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Input shape: Batch x Time x Channel&quot;&quot;&quot;</span>

    <span class="c1"># if encoder_hidden_states are provided this layer is used as a cross-attention layer</span>
    <span class="c1"># for the decoder</span>
    <span class="n">is_cross_attention</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="n">bsz</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># get query proj</span>
    <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span>
    <span class="c1"># get key, value proj</span>
    <span class="c1"># `past_key_value[0].shape[2] == encoder_hidden_states.shape[1]`</span>
    <span class="c1"># is checking that the `sequence_length` of the `past_key_value` is the same as</span>
    <span class="c1"># the provided `encoder_hidden_states` to support prefix tuning</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">is_cross_attention</span>
        <span class="ow">and</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="c1"># reuse k,v, cross_attentions</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">is_cross_attention</span><span class="p">:</span>
        <span class="c1"># cross_attentions</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># reuse k, v, self_attention</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">key_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">value_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># self_attention</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
        <span class="c1"># if cross_attention save Tuple(mindspore.Tensor, mindspore.Tensor) of all cross attention key/value_states.</span>
        <span class="c1"># Further calls to cross_attention layer can then reuse all cross-attention</span>
        <span class="c1"># key/value_states (first &quot;if&quot; case)</span>
        <span class="c1"># if uni-directional self-attention (decoder) save Tuple(mindspore.Tensor, mindspore.Tensor) of</span>
        <span class="c1"># all previous decoder key/value_states. Further calls to uni-directional self-attention</span>
        <span class="c1"># can concat previous decoder key/value_states to current projected key/value_states (third &quot;elif&quot; case)</span>
        <span class="c1"># if encoder bi-directional self-attention `past_key_value` is always `None`</span>
        <span class="n">past_key_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>

    <span class="n">proj_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>
    <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>
    <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>

    <span class="n">src_len</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Attention weights should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span><span class="w"> </span><span class="n">tgt_len</span><span class="p">,</span><span class="w"> </span><span class="n">src_len</span><span class="p">)</span><span class="si">}</span><span class="s2">, but is&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">attn_weights</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Attention mask should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">tgt_len</span><span class="p">,</span><span class="w"> </span><span class="n">src_len</span><span class="p">)</span><span class="si">}</span><span class="s2">, but is </span><span class="si">{</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span> <span class="o">+</span> <span class="n">attention_mask</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>

    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="c1"># this operation is a bit awkward, but it&#39;s required to</span>
        <span class="c1"># make sure that attn_weights keeps its gradient.</span>
        <span class="c1"># In order to do so, attn_weights have to be reshaped</span>
        <span class="c1"># twice and have to be reused in the following</span>
        <span class="n">attn_weights_reshaped</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights_reshaped</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">attn_weights_reshaped</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">attn_probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attn_probs</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;`attn_output` should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span><span class="w"> </span><span class="n">tgt_len</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="si">}</span><span class="s2">, but is&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">attn_output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be</span>
    <span class="c1"># partitioned across GPUs when using tensor-parallelism.</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights_reshaped</span><span class="p">,</span> <span class="n">past_key_value</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.modeling_utils.PreTrainedModel" href="../../modeling_utils/#mindnlp.transformers.modeling_utils.PreTrainedModel">PreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span>
<span class="normal">2401</span>
<span class="normal">2402</span>
<span class="normal">2403</span>
<span class="normal">2404</span>
<span class="normal">2405</span>
<span class="normal">2406</span>
<span class="normal">2407</span>
<span class="normal">2408</span>
<span class="normal">2409</span>
<span class="normal">2410</span>
<span class="normal">2411</span>
<span class="normal">2412</span>
<span class="normal">2413</span>
<span class="normal">2414</span>
<span class="normal">2415</span>
<span class="normal">2416</span>
<span class="normal">2417</span>
<span class="normal">2418</span>
<span class="normal">2419</span>
<span class="normal">2420</span>
<span class="normal">2421</span>
<span class="normal">2422</span>
<span class="normal">2423</span>
<span class="normal">2424</span>
<span class="normal">2425</span>
<span class="normal">2426</span>
<span class="normal">2427</span>
<span class="normal">2428</span>
<span class="normal">2429</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TCodeHifiGan</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">SeamlessM4TConfig</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_embeds&quot;</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dur_predictor</span> <span class="o">=</span> <span class="n">SeamlessM4TVariancePredictor</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">unit_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">unit_hifi_gan_vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">unit_embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speaker_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_num_spkrs</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">spkr_embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">language_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_num_langs</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">lang_embed_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span> <span class="o">=</span> <span class="n">SeamlessM4THifiGan</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_dur_output_lengths</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the output length after the duration layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">unit_lengths</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_ids</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># take care of edge cases where no padding or too many padding</span>
        <span class="n">unit_lengths</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">unit_lengths</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dur_out</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">cumulative_dur_out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dur_out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">unit_lengths</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">cumulative_dur_out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">unit_lengths</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">unit_lengths</span>

    <span class="k">def</span> <span class="nf">_get_output_hifigan_lengths</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the output length of the hifigan convolutional layers</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">_conv_out_length</span><span class="p">(</span><span class="n">input_length</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
            <span class="c1"># 1D convolutional layer output length formula taken</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">input_length</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="s2">&quot;floor&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">)</span>

        <span class="k">def</span> <span class="nf">_transpose_conv_out_length</span><span class="p">(</span><span class="n">input_length</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">input_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">+</span> <span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># conv_pre</span>
        <span class="n">input_lengths</span> <span class="o">=</span> <span class="n">_conv_out_length</span><span class="p">(</span><span class="n">input_lengths</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="c1"># upsampler</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">upsample_rate</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_rates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_kernel_sizes</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">input_lengths</span> <span class="o">=</span> <span class="n">_transpose_conv_out_length</span><span class="p">(</span>
                <span class="n">input_lengths</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">upsample_rate</span><span class="p">,</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="n">upsample_rate</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="p">)</span>

        <span class="c1"># resblock</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_rates</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">resblock_kernel_sizes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">resblock_dilation_sizes</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">dil</span> <span class="ow">in</span> <span class="n">dilation</span><span class="p">:</span>
                    <span class="n">input_lengths</span> <span class="o">=</span> <span class="n">_conv_out_length</span><span class="p">(</span>
                        <span class="n">input_lengths</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dil</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dil</span>
                    <span class="p">)</span>

                <span class="k">for</span> <span class="n">dil</span> <span class="ow">in</span> <span class="n">dilation</span><span class="p">:</span>
                    <span class="n">input_lengths</span> <span class="o">=</span> <span class="n">_conv_out_length</span><span class="p">(</span><span class="n">input_lengths</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># conv_post</span>
        <span class="n">input_lengths</span> <span class="o">=</span> <span class="n">_conv_out_length</span><span class="p">(</span><span class="n">input_lengths</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">input_lengths</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">spkr_id</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lang_id</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">                Indices can be obtained using [`SeamlessM4TTextToUnitForConditionalGeneration`]. [What are input</span>
<span class="sd">                IDs?](../glossary#input-ids)</span>
<span class="sd">            spkr_id (`int`, *optional*):</span>
<span class="sd">                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language id to use as target language for translation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unit_embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">spkr</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">speaker_embedding</span><span class="p">(</span><span class="n">spkr_id</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">lang</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">language_embedding</span><span class="p">(</span><span class="n">lang_id</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">log_dur_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dur_predictor</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">dur_out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_dur_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># B x C x T</span>
        <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if batched sample, need to interleave per sample, and pad -&gt; loss of parallelism</span>
            <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;`self.training=True` and you use batching. You lose parallelism during the hifigan</span>
<span class="sd">                               forward pass because the samples are interleaved.&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">duration</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">duration</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">)</span>
            <span class="p">]</span>

            <span class="c1"># hidden_states = nn.utils.rnn.pad_sequence(hidden_states, batch_first=True).transpose(1, 2)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">spkr</span> <span class="o">=</span> <span class="n">spkr</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">lang</span> <span class="o">=</span> <span class="n">lang</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">lang</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">spkr</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">unit_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dur_output_lengths</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">)</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_output_hifigan_lengths</span><span class="p">(</span><span class="n">unit_lengths</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">lengths</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the weights.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose1d</span><span class="p">)):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">module</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">apply_weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">conv_pre</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">upsampler</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">resblocks</span><span class="p">:</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">apply_weight_norm</span><span class="p">()</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">conv_post</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">remove_weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">conv_pre</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">upsampler</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">resblocks</span><span class="p">:</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">()</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">conv_post</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TCodeHifiGan</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">spkr_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<code>SeamlessM4TTextToUnitForConditionalGeneration</code>]. <a href="../glossary#input-ids">What are input
IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>spkr_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the speaker used for speech synthesis. Must be lower than <code>config.vocoder_num_spkrs</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language id to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span>
<span class="normal">2401</span>
<span class="normal">2402</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">spkr_id</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lang_id</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">            Indices can be obtained using [`SeamlessM4TTextToUnitForConditionalGeneration`]. [What are input</span>
<span class="sd">            IDs?](../glossary#input-ids)</span>
<span class="sd">        spkr_id (`int`, *optional*):</span>
<span class="sd">            The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language id to use as target language for translation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unit_embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">spkr</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">speaker_embedding</span><span class="p">(</span><span class="n">spkr_id</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">lang</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">language_embedding</span><span class="p">(</span><span class="n">lang_id</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">log_dur_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dur_predictor</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">dur_out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_dur_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># B x C x T</span>
    <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># if batched sample, need to interleave per sample, and pad -&gt; loss of parallelism</span>
        <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;`self.training=True` and you use batching. You lose parallelism during the hifigan</span>
<span class="sd">                           forward pass because the samples are interleaved.&quot;&quot;&quot;</span>
            <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">duration</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">duration</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># hidden_states = nn.utils.rnn.pad_sequence(hidden_states, batch_first=True).transpose(1, 2)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">spkr</span> <span class="o">=</span> <span class="n">spkr</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">lang</span> <span class="o">=</span> <span class="n">lang</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">lang</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">spkr</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">unit_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dur_output_lengths</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_output_hifigan_lengths</span><span class="p">(</span><span class="n">unit_lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">lengths</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerConvolutionModule" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerConvolutionModule</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerConvolutionModule" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Convolution block used in the conformer block</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TConformerConvolutionModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convolution block used in the conformer block&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">conv_depthwise_kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`config.conv_depthwise_kernel_size` should be a odd number for &#39;SAME&#39; padding&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">glu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GLU</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">conv_depthwise_kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_hidden_act</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># Ensure that we do not leak padded positions in depthwise convolution.</span>
        <span class="c1"># Put 0 where necessary</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>

        <span class="c1"># exchange the temporal dimension and the feature dimension</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># GLU mechanism</span>
        <span class="c1"># =&gt; (batch, 2*channel, dim)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv1</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># =&gt; (batch, channel, dim)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># 1D Depthwise Conv</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv2</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerEncoderLayer" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerEncoderLayer</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerEncoderLayer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Conformer block based on https://arxiv.org/abs/2005.08100.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TConformerEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Conformer block based on https://arxiv.org/abs/2005.08100.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_dropout</span>

        <span class="c1"># Feed-forward 1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn1_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn1</span> <span class="o">=</span> <span class="n">SeamlessM4TConformerFeedForward</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Self-Attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">SeamlessM4TConformerSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Conformer Convolution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="o">=</span> <span class="n">SeamlessM4TConformerConvolutionModule</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Feed-forward 2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn2_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn2</span> <span class="o">=</span> <span class="n">SeamlessM4TConformerFeedForward</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">relative_position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">conv_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># 1. Feed-Forward 1 layer</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn1_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn1</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">residual</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="c1"># 2. Self-Attention layer</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weigts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">relative_position_embeddings</span><span class="o">=</span><span class="n">relative_position_embeddings</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="c1"># 3. Convolutional Layer</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">conv_attention_mask</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="c1"># 4. Feed-Forward 2 Layer</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn2_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn2</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">residual</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weigts</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerRelPositionalEmbedding" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerRelPositionalEmbedding</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerRelPositionalEmbedding" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Relative positional encoding module.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TConformerRelPositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Relative positional encoding module.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_source_positions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extend_pe</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">extend_pe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Reset the positional encodings</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># self.pe contains both positive and negative parts</span>
            <span class="c1"># the length of self.pe is 2 * input_len - 1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="k">return</span>
        <span class="c1"># Suppose `i` is the position of query vector and `j` is the</span>
        <span class="c1"># position of key vector. We use positive relative positions when keys</span>
        <span class="c1"># are to the left (i&gt;j) and negative relative positions otherwise (i&lt;j).</span>
        <span class="n">pe_positive</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">pe_negative</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">pe_positive</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe_positive</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe_negative</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe_negative</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>

        <span class="c1"># Reverse the order of positive indices and concat both positive and</span>
        <span class="c1"># negative indices. This is used to support the shifting trick</span>
        <span class="c1"># as in https://arxiv.org/abs/1901.02860</span>
        <span class="n">pe_positive</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">pe_positive</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">pe_negative</span> <span class="o">=</span> <span class="n">pe_negative</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pe_positive</span><span class="p">,</span> <span class="n">pe_negative</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extend_pe</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">end_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">relative_position_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">relative_position_embeddings</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerRotaryPositionalEmbedding" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerRotaryPositionalEmbedding</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerRotaryPositionalEmbedding" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Rotary positional embedding
Reference : https://blog.eleuther.ai/rotary-embeddings/ Paper: https://arxiv.org/pdf/2104.09864.pdf</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TConformerRotaryPositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Rotary positional embedding</span>
<span class="sd">    Reference : https://blog.eleuther.ai/rotary-embeddings/ Paper: https://arxiv.org/pdf/2104.09864.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_attention_heads</span>
        <span class="n">base</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rotary_embedding_base</span>

        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">base</span> <span class="o">**</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;inv_freq&quot;</span><span class="p">,</span> <span class="n">inv_freq</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cached_sequence_length</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cached_rotary_positional_embedding</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">sequence_length</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">cached_sequence_length</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cached_rotary_positional_embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cached_rotary_positional_embedding</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cached_sequence_length</span> <span class="o">=</span> <span class="n">sequence_length</span>
        <span class="c1"># Embeddings are computed in the dtype of the inv_freq constant</span>
        <span class="n">time_stamps</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;i,j-&gt;ij&quot;</span><span class="p">,</span> <span class="n">time_stamps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">freqs</span><span class="p">,</span> <span class="n">freqs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">cos_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">cos</span><span class="p">()[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">sin_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">sin</span><span class="p">()[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="c1"># Computed embeddings are cast to the dtype of the hidden state inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cached_rotary_positional_embedding</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">cos_embeddings</span><span class="p">,</span> <span class="n">sin_embeddings</span><span class="p">])</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cached_rotary_positional_embedding</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerSelfAttention" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerSelfAttention</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerSelfAttention" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Construct a SeamlessM4TConformerSelfAttention object.
Can be enhanced with rotary or relative position embeddings.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TConformerSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a SeamlessM4TConformerSelfAttention object.</span>
<span class="sd">    Can be enhanced with rotary or relative position embeddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">use_position_embeddings</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="k">if</span> <span class="n">use_position_embeddings</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_dropout</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="o">==</span> <span class="s2">&quot;relative&quot;</span><span class="p">:</span>
            <span class="c1"># linear transformation for positional encoding</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">linear_pos</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="c1"># these two learnable bias are used in matrix c and matrix d</span>
            <span class="c1"># as described in https://arxiv.org/abs/1901.02860 Section 3.3</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_u</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">))</span>

    <span class="c1"># Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention.forward</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">relative_position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="c1"># self-attention mechanism</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># make sure query/key states can be != value states</span>
        <span class="n">query_key_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="o">==</span> <span class="s2">&quot;rotary&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">relative_position_embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`relative_position_embeddings` has to be defined when `self.position_embeddings_type == &#39;rotary&#39;&quot;</span>
                <span class="p">)</span>
            <span class="n">query_key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_rotary_embedding</span><span class="p">(</span><span class="n">query_key_states</span><span class="p">,</span> <span class="n">relative_position_embeddings</span><span class="p">)</span>

        <span class="c1"># project query_key_states and value_states</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">query_key_states</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">query_key_states</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">value_states</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>

        <span class="c1"># =&gt; (batch, head, time1, d_k)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="o">==</span> <span class="s2">&quot;relative&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">relative_position_embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`relative_position_embeddings` has to be defined when `self.position_embeddings_type ==&quot;</span>
                    <span class="s2">&quot; &#39;relative&#39;&quot;</span>
                <span class="p">)</span>
            <span class="c1"># apply relative_position_embeddings to qk scores</span>
            <span class="c1"># as proposed in Transformer_XL: https://arxiv.org/abs/1901.02860</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_relative_embeddings</span><span class="p">(</span>
                <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span> <span class="n">relative_position_embeddings</span><span class="o">=</span><span class="n">relative_position_embeddings</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>

        <span class="c1"># apply attention_mask if necessary</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">attention_mask</span>

        <span class="c1"># =&gt; (batch, head, time1, time2)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>

        <span class="c1"># =&gt; (batch, head, time1, d_k)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="c1"># =&gt; (batch, time1, hidden_size)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">probs</span>

    <span class="c1"># Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention._apply_rotary_embedding</span>
    <span class="k">def</span> <span class="nf">_apply_rotary_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">relative_position_embeddings</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>

        <span class="n">cos</span> <span class="o">=</span> <span class="n">relative_position_embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="n">sequence_length</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
        <span class="n">sin</span> <span class="o">=</span> <span class="n">relative_position_embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="n">sequence_length</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

        <span class="c1"># rotate hidden_states with rotary embeddings</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">rotated_states_begin</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>
        <span class="n">rotated_states_end</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:]</span>
        <span class="n">rotated_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="o">-</span><span class="n">rotated_states_end</span><span class="p">,</span> <span class="n">rotated_states_begin</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">rotated_states_begin</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span> <span class="o">*</span> <span class="n">cos</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">rotated_states</span> <span class="o">*</span> <span class="n">sin</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>

    <span class="c1"># Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention._apply_relative_embeddings</span>
    <span class="k">def</span> <span class="nf">_apply_relative_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">relative_position_embeddings</span><span class="p">):</span>
        <span class="c1"># 1. project positional embeddings</span>
        <span class="c1"># =&gt; (batch, head, 2*time1-1, d_k)</span>
        <span class="n">proj_relative_position_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_pos</span><span class="p">(</span><span class="n">relative_position_embeddings</span><span class="p">)</span>
        <span class="n">proj_relative_position_embeddings</span> <span class="o">=</span> <span class="n">proj_relative_position_embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">relative_position_embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span>
        <span class="p">)</span>
        <span class="n">proj_relative_position_embeddings</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">proj_relative_position_embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">proj_relative_position_embeddings</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">proj_relative_position_embeddings</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="c1"># 2. Add bias to query</span>
        <span class="c1"># =&gt; (batch, head, time1, d_k)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">q_with_bias_u</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="n">query</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_u</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">q_with_bias_v</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="n">query</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_bias_v</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># 3. attention score: first compute matrix a and matrix c</span>
        <span class="c1"># as described in https://arxiv.org/abs/1901.02860 Section 3.3</span>
        <span class="c1"># =&gt; (batch, head, time1, time2)</span>
        <span class="n">scores_ac</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_with_bias_u</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># 4. then compute matrix b and matrix d</span>
        <span class="c1"># =&gt; (batch, head, time1, 2*time1-1)</span>
        <span class="n">scores_bd</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_with_bias_v</span><span class="p">,</span> <span class="n">proj_relative_position_embeddings</span><span class="p">)</span>

        <span class="c1"># 5. shift matrix b and matrix d</span>
        <span class="n">zero_pad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="o">*</span><span class="n">scores_bd</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">scores_bd</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">scores_bd_padded</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">zero_pad</span><span class="p">,</span> <span class="n">scores_bd</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores_bd_padded_shape</span> <span class="o">=</span> <span class="n">scores_bd</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">scores_bd</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">scores_bd</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">scores_bd_padded</span> <span class="o">=</span> <span class="n">scores_bd_padded</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">scores_bd_padded_shape</span><span class="p">)</span>
        <span class="n">scores_bd</span> <span class="o">=</span> <span class="n">scores_bd_padded</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">scores_bd</span><span class="p">)</span>
        <span class="n">scores_bd</span> <span class="o">=</span> <span class="n">scores_bd</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">scores_bd</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

        <span class="c1"># 6. sum matrices</span>
        <span class="c1"># =&gt; (batch, head, time1, time2)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores_ac</span> <span class="o">+</span> <span class="n">scores_bd</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoder" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoder</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel">SeamlessM4TPreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TDecoder</span><span class="p">(</span><span class="n">SeamlessM4TPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4TConfig</span><span class="p">,</span>
        <span class="n">embed_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">decoder_layerdrop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_target_positions</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>
        <span class="n">embed_scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_embedding</span> <span class="k">else</span> <span class="mf">1.0</span>

        <span class="k">if</span> <span class="n">embed_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if embed_tokens defined, use its shape instead</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">SeamlessM4TScaledWordEmbedding</span><span class="p">(</span>
                <span class="n">embed_tokens</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embed_tokens</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">embed_scale</span><span class="o">=</span><span class="n">embed_scale</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">embed_tokens</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">SeamlessM4TScaledWordEmbedding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">embed_scale</span><span class="o">=</span><span class="n">embed_scale</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span> <span class="o">=</span> <span class="n">SeamlessM4TSinusoidalPositionalEmbedding</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_target_positions</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">SeamlessM4TDecoderLayer</span><span class="p">(</span>
                    <span class="n">config</span><span class="p">,</span>
                    <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
                    <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_ffn_dim</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutputWithPastAndCrossAttentions</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you</span>
<span class="sd">                provide it.</span>

<span class="sd">                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">                [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class="sd">                - 1 for tokens that are **not masked**,</span>
<span class="sd">                - 0 for tokens that are **masked**.</span>

<span class="sd">                [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">            encoder_hidden_states (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention</span>
<span class="sd">                of the decoder.</span>
<span class="sd">            encoder_attention_mask (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):</span>
<span class="sd">                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values</span>
<span class="sd">                selected in `[0, 1]`:</span>

<span class="sd">                - 1 for tokens that are **not masked**,</span>
<span class="sd">                - 0 for tokens that are **masked**.</span>

<span class="sd">                [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">            past_key_values (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">                Tuple of `tuple(mindspore.Tensor)` of length `config.n_layers`, with each tuple having 2 tensors of</span>
<span class="sd">                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of</span>
<span class="sd">                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.</span>

<span class="sd">                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the</span>
<span class="sd">                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.</span>

<span class="sd">                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those</span>
<span class="sd">                that don&#39;t have their past key value states given to this model) of shape `(batch_size, 1)` instead of</span>
<span class="sd">                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.</span>
<span class="sd">            inputs_embeds (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.</span>
<span class="sd">                This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class="sd">                than the model&#39;s internal embedding lookup matrix.</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">                returned tensors for more detail.</span>
<span class="sd">            output_hidden_states (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class="sd">                for more detail.</span>
<span class="sd">            return_dict (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="c1"># retrieve input_ids and inputs_embeds</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">input_ids</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either decoder_input_ids or decoder_inputs_embeds&quot;</span><span class="p">)</span>

        <span class="c1"># past_key_values_length</span>
        <span class="n">past_key_values_length</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_causal_attention_mask</span><span class="p">(</span>
            <span class="n">attention_mask</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span>
        <span class="p">)</span>

        <span class="c1"># expand encoder attention mask</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">encoder_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_attention_mask</span><span class="p">(</span>
                <span class="n">encoder_attention_mask</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">tgt_len</span><span class="o">=</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># embed positions</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="o">=</span><span class="n">past_key_values_length</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">positions</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                    <span class="s2">&quot;`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...&quot;</span>
                <span class="p">)</span>
                <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># decoder layers</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">output_attentions</span> <span class="ow">and</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">next_decoder_cache</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">decoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">([])</span>
                <span class="k">if</span> <span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">:</span>
                    <span class="k">continue</span>

            <span class="n">past_key_value</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
                    <span class="n">decoder_layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">encoder_attention_mask</span><span class="p">,</span>
                    <span class="kc">None</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="p">,</span>
                    <span class="n">use_cache</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">decoder_layer</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                    <span class="n">past_key_value</span><span class="o">=</span><span class="n">past_key_value</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                    <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">next_decoder_cache</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],)</span>

                <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">all_cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">3</span><span class="p">],)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># add hidden states from the last decoder layer</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="n">next_cache</span> <span class="o">=</span> <span class="n">next_decoder_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
                <span class="n">v</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">next_cache</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attns</span><span class="p">,</span> <span class="n">all_cross_attentions</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">BaseModelOutputWithPastAndCrossAttentions</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">next_cache</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attns</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">all_cross_attentions</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TDecoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoder.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
provide it.</p>
<p>Indices can be obtained using [<code>AutoTokenizer</code>]. See [<code>PreTrainedTokenizer.encode</code>] and
[<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
of the decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values
selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>tuple(mindspore.Tensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
cross-attention blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those
that don't have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of
all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inputs_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors
than the model's internal embedding lookup matrix.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return a [<code>~utils.ModelOutput</code>] instead of a plain tuple.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutputWithPastAndCrossAttentions</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you</span>
<span class="sd">            provide it.</span>

<span class="sd">            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">            [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class="sd">            - 1 for tokens that are **not masked**,</span>
<span class="sd">            - 0 for tokens that are **masked**.</span>

<span class="sd">            [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">        encoder_hidden_states (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention</span>
<span class="sd">            of the decoder.</span>
<span class="sd">        encoder_attention_mask (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):</span>
<span class="sd">            Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values</span>
<span class="sd">            selected in `[0, 1]`:</span>

<span class="sd">            - 1 for tokens that are **not masked**,</span>
<span class="sd">            - 0 for tokens that are **masked**.</span>

<span class="sd">            [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">        past_key_values (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">            Tuple of `tuple(mindspore.Tensor)` of length `config.n_layers`, with each tuple having 2 tensors of</span>
<span class="sd">            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of</span>
<span class="sd">            shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.</span>

<span class="sd">            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the</span>
<span class="sd">            cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.</span>

<span class="sd">            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those</span>
<span class="sd">            that don&#39;t have their past key value states given to this model) of shape `(batch_size, 1)` instead of</span>
<span class="sd">            all `decoder_input_ids` of shape `(batch_size, sequence_length)`.</span>
<span class="sd">        inputs_embeds (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.</span>
<span class="sd">            This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class="sd">            than the model&#39;s internal embedding lookup matrix.</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">            returned tensors for more detail.</span>
<span class="sd">        output_hidden_states (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class="sd">            for more detail.</span>
<span class="sd">        return_dict (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="c1"># retrieve input_ids and inputs_embeds</span>
    <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">input_ids</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either decoder_input_ids or decoder_inputs_embeds&quot;</span><span class="p">)</span>

    <span class="c1"># past_key_values_length</span>
    <span class="n">past_key_values_length</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_causal_attention_mask</span><span class="p">(</span>
        <span class="n">attention_mask</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span>
    <span class="p">)</span>

    <span class="c1"># expand encoder attention mask</span>
    <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">encoder_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>
        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_attention_mask</span><span class="p">(</span>
            <span class="n">encoder_attention_mask</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">tgt_len</span><span class="o">=</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="c1"># embed positions</span>
    <span class="n">positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="o">=</span><span class="n">past_key_values_length</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">positions</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                <span class="s2">&quot;`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...&quot;</span>
            <span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># decoder layers</span>
    <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">output_attentions</span> <span class="ow">and</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">next_decoder_cache</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">decoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
        <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">([])</span>
            <span class="k">if</span> <span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">:</span>
                <span class="k">continue</span>

        <span class="n">past_key_value</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
                <span class="n">decoder_layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="kc">None</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">decoder_layer</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="n">past_key_value</span><span class="o">=</span><span class="n">past_key_value</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">next_decoder_cache</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],)</span>

            <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">all_cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">3</span><span class="p">],)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="c1"># add hidden states from the last decoder layer</span>
    <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
        <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

    <span class="n">next_cache</span> <span class="o">=</span> <span class="n">next_decoder_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">v</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">next_cache</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attns</span><span class="p">,</span> <span class="n">all_cross_attentions</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">BaseModelOutputWithPastAndCrossAttentions</span><span class="p">(</span>
        <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">next_cache</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
        <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attns</span><span class="p">,</span>
        <span class="n">cross_attentions</span><span class="o">=</span><span class="n">all_cross_attentions</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoderLayer" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoderLayer</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoderLayer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TDecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4TConfig</span><span class="p">,</span> <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">decoder_ffn_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">decoder_ffn_dim</span> <span class="k">if</span> <span class="n">decoder_ffn_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">decoder_ffn_dim</span>
        <span class="n">decoder_attention_heads</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">decoder_attention_heads</span> <span class="k">if</span> <span class="n">decoder_attention_heads</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">decoder_attention_heads</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">SeamlessM4TAttention</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
            <span class="n">is_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">activation_function</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span> <span class="o">=</span> <span class="n">SeamlessM4TAttention</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">decoder_attention_heads</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span> <span class="n">is_decoder</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">SeamlessM4TFeedForwardNetwork</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="o">=</span><span class="n">decoder_ffn_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">activation_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor`):</span>
<span class="sd">                input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">            attention_mask (`mindspore.Tensor`):</span>
<span class="sd">                attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very</span>
<span class="sd">                large negative values.</span>
<span class="sd">            encoder_hidden_states (`mindspore.Tensor`):</span>
<span class="sd">                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">            encoder_attention_mask (`mindspore.Tensor`):</span>
<span class="sd">                encoder attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by</span>
<span class="sd">                very large negative values.</span>
<span class="sd">            past_key_value (`Tuple(mindspore.Tensor)`):</span>
<span class="sd">                cached past key and value projection states</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">                returned tensors for more detail.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># Self Attention</span>
        <span class="c1"># decoder uni-directional self-attention cached key/values tuple is at positions 1,2</span>
        <span class="n">self_attn_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="c1"># add present self-attn cache to positions 1,2 of present_key_value tuple</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">past_key_value</span><span class="o">=</span><span class="n">self_attn_past_key_value</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="c1"># Cross-Attention Block</span>
        <span class="n">cross_attn_present_key_value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">cross_attn_weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

            <span class="c1"># cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple</span>
            <span class="n">cross_attn_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span> <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">cross_attn_weights</span><span class="p">,</span> <span class="n">cross_attn_present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">past_key_value</span><span class="o">=</span><span class="n">cross_attn_past_key_value</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

            <span class="c1"># add cross-attn to positions 3,4 of present_key_value tuple</span>
            <span class="n">present_key_value</span> <span class="o">+=</span> <span class="n">cross_attn_present_key_value</span>

        <span class="c1"># Fully Connected</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_value</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">cross_attn_weights</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoderLayer.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TDecoderLayer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoderLayer.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>input to the layer of shape <code>(batch, seq_len, embed_dim)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>attention mask of size <code>(batch, 1, tgt_len, src_len)</code> where padding elements are indicated by very
large negative values.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>cross attention input to the layer of shape <code>(batch, seq_len, embed_dim)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>encoder attention mask of size <code>(batch, 1, tgt_len, src_len)</code> where padding elements are indicated by
very large negative values.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>cached past key and value projection states</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple(mindspore.Tensor)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor`):</span>
<span class="sd">            input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">        attention_mask (`mindspore.Tensor`):</span>
<span class="sd">            attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very</span>
<span class="sd">            large negative values.</span>
<span class="sd">        encoder_hidden_states (`mindspore.Tensor`):</span>
<span class="sd">            cross attention input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">        encoder_attention_mask (`mindspore.Tensor`):</span>
<span class="sd">            encoder attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by</span>
<span class="sd">            very large negative values.</span>
<span class="sd">        past_key_value (`Tuple(mindspore.Tensor)`):</span>
<span class="sd">            cached past key and value projection states</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">            returned tensors for more detail.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="c1"># Self Attention</span>
    <span class="c1"># decoder uni-directional self-attention cached key/values tuple is at positions 1,2</span>
    <span class="n">self_attn_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="c1"># add present self-attn cache to positions 1,2 of present_key_value tuple</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="o">=</span><span class="n">self_attn_past_key_value</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="c1"># Cross-Attention Block</span>
    <span class="n">cross_attn_present_key_value</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">cross_attn_weights</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple</span>
        <span class="n">cross_attn_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span> <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">cross_attn_weights</span><span class="p">,</span> <span class="n">cross_attn_present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">past_key_value</span><span class="o">=</span><span class="n">cross_attn_past_key_value</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="c1"># add cross-attn to positions 3,4 of present_key_value tuple</span>
        <span class="n">present_key_value</span> <span class="o">+=</span> <span class="n">cross_attn_present_key_value</span>

    <span class="c1"># Fully Connected</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_value</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">cross_attn_weights</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoder" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoder</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel">SeamlessM4TPreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TEncoder</span><span class="p">(</span><span class="n">SeamlessM4TPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4TConfig</span><span class="p">,</span>
        <span class="n">embed_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_t2u_encoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">encoder_layerdrop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span> <span class="o">=</span> <span class="n">is_t2u_encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_source_positions</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span><span class="p">:</span>
            <span class="n">embed_scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_embedding</span> <span class="k">else</span> <span class="mf">1.0</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">SeamlessM4TScaledWordEmbedding</span><span class="p">(</span>
                <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">embed_scale</span><span class="o">=</span><span class="n">embed_scale</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">embed_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">embed_tokens</span><span class="o">.</span><span class="n">weight</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span> <span class="o">=</span> <span class="n">SeamlessM4TSinusoidalPositionalEmbedding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_source_positions</span><span class="p">,</span>
                <span class="n">embed_dim</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">SeamlessM4TEncoderLayer</span><span class="p">(</span>
                    <span class="n">config</span><span class="p">,</span>
                    <span class="n">encoder_attention_heads</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">encoder_attention_heads</span><span class="p">,</span>
                    <span class="n">encoder_ffn_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">encoder_ffn_dim</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you</span>
<span class="sd">                provide it.</span>

<span class="sd">                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">                [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class="sd">                - 1 for tokens that are **not masked**,</span>
<span class="sd">                - 0 for tokens that are **masked**.</span>

<span class="sd">                [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">            inputs_embeds (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.</span>
<span class="sd">                This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class="sd">                than the model&#39;s internal embedding lookup matrix.</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">                returned tensors for more detail.</span>
<span class="sd">            output_hidden_states (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class="sd">                for more detail.</span>
<span class="sd">            return_dict (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You cannot pass input_ids to the encoder of the text_to_units model. Pass inputs_embeds instead.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># retrieve input_ids and inputs_embeds</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">input_ids</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either input_ids or inputs_embeds&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span><span class="p">:</span>
            <span class="n">embed_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">embed_pos</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># expand attention_mask</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">encoder_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">encoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">encoder_states</span> <span class="o">=</span> <span class="n">encoder_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
            <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
            <span class="n">to_drop</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">([])</span>
                <span class="k">if</span> <span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">:</span>  <span class="c1"># skip the layer</span>
                    <span class="n">to_drop</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="k">if</span> <span class="n">to_drop</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                    <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
                        <span class="n">encoder_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span>
                        <span class="n">hidden_states</span><span class="p">,</span>
                        <span class="n">attention_mask</span><span class="p">,</span>
                        <span class="n">output_attentions</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">encoder_layer</span><span class="p">(</span>
                        <span class="n">hidden_states</span><span class="p">,</span>
                        <span class="n">attention_mask</span><span class="p">,</span>
                        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_attentions</span> <span class="o">=</span> <span class="n">all_attentions</span> <span class="o">+</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">encoder_states</span> <span class="o">=</span> <span class="n">encoder_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_states</span><span class="p">,</span> <span class="n">all_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_states</span><span class="p">,</span> <span class="n">attentions</span><span class="o">=</span><span class="n">all_attentions</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TEncoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoder.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
provide it.</p>
<p>Indices can be obtained using [<code>AutoTokenizer</code>]. See [<code>PreTrainedTokenizer.encode</code>] and
[<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inputs_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors
than the model's internal embedding lookup matrix.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return a [<code>~utils.ModelOutput</code>] instead of a plain tuple.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you</span>
<span class="sd">            provide it.</span>

<span class="sd">            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">            [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class="sd">            - 1 for tokens that are **not masked**,</span>
<span class="sd">            - 0 for tokens that are **masked**.</span>

<span class="sd">            [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">        inputs_embeds (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.</span>
<span class="sd">            This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class="sd">            than the model&#39;s internal embedding lookup matrix.</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">            returned tensors for more detail.</span>
<span class="sd">        output_hidden_states (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class="sd">            for more detail.</span>
<span class="sd">        return_dict (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You cannot pass input_ids to the encoder of the text_to_units model. Pass inputs_embeds instead.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># retrieve input_ids and inputs_embeds</span>
    <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">input_ids</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either input_ids or inputs_embeds&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span><span class="p">:</span>
        <span class="n">embed_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">embed_pos</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

    <span class="c1"># expand attention_mask</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">encoder_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">encoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">encoder_states</span> <span class="o">=</span> <span class="n">encoder_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
        <span class="n">to_drop</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">([])</span>
            <span class="k">if</span> <span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">:</span>  <span class="c1"># skip the layer</span>
                <span class="n">to_drop</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">to_drop</span><span class="p">:</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
                    <span class="n">encoder_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">encoder_layer</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">all_attentions</span> <span class="o">=</span> <span class="n">all_attentions</span> <span class="o">+</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
        <span class="n">encoder_states</span> <span class="o">=</span> <span class="n">encoder_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_states</span><span class="p">,</span> <span class="n">all_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
        <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_states</span><span class="p">,</span> <span class="n">attentions</span><span class="o">=</span><span class="n">all_attentions</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoderLayer" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoderLayer</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoderLayer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4TConfig</span><span class="p">,</span> <span class="n">encoder_ffn_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_attention_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">encoder_ffn_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">encoder_ffn_dim</span> <span class="k">if</span> <span class="n">encoder_ffn_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_ffn_dim</span>
        <span class="n">encoder_attention_heads</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">encoder_attention_heads</span> <span class="k">if</span> <span class="n">encoder_attention_heads</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_attention_heads</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">SeamlessM4TAttention</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">encoder_attention_heads</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">SeamlessM4TFeedForwardNetwork</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="o">=</span><span class="n">encoder_ffn_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">activation_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor`):</span>
<span class="sd">                input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">            attention_mask (`mindspore.Tensor`):</span>
<span class="sd">                attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very</span>
<span class="sd">                large negative values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">attn_weights</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoderLayer.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TEncoderLayer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoderLayer.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>input to the layer of shape <code>(batch, seq_len, embed_dim)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>attention mask of size <code>(batch, 1, tgt_len, src_len)</code> where padding elements are indicated by very
large negative values.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor`):</span>
<span class="sd">            input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">        attention_mask (`mindspore.Tensor`):</span>
<span class="sd">            attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very</span>
<span class="sd">            large negative values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">attn_weights</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel">SeamlessM4TPreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3342</span>
<span class="normal">3343</span>
<span class="normal">3344</span>
<span class="normal">3345</span>
<span class="normal">3346</span>
<span class="normal">3347</span>
<span class="normal">3348</span>
<span class="normal">3349</span>
<span class="normal">3350</span>
<span class="normal">3351</span>
<span class="normal">3352</span>
<span class="normal">3353</span>
<span class="normal">3354</span>
<span class="normal">3355</span>
<span class="normal">3356</span>
<span class="normal">3357</span>
<span class="normal">3358</span>
<span class="normal">3359</span>
<span class="normal">3360</span>
<span class="normal">3361</span>
<span class="normal">3362</span>
<span class="normal">3363</span>
<span class="normal">3364</span>
<span class="normal">3365</span>
<span class="normal">3366</span>
<span class="normal">3367</span>
<span class="normal">3368</span>
<span class="normal">3369</span>
<span class="normal">3370</span>
<span class="normal">3371</span>
<span class="normal">3372</span>
<span class="normal">3373</span>
<span class="normal">3374</span>
<span class="normal">3375</span>
<span class="normal">3376</span>
<span class="normal">3377</span>
<span class="normal">3378</span>
<span class="normal">3379</span>
<span class="normal">3380</span>
<span class="normal">3381</span>
<span class="normal">3382</span>
<span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span>
<span class="normal">3414</span>
<span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span>
<span class="normal">3444</span>
<span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span>
<span class="normal">3476</span>
<span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span>
<span class="normal">3508</span>
<span class="normal">3509</span>
<span class="normal">3510</span>
<span class="normal">3511</span>
<span class="normal">3512</span>
<span class="normal">3513</span>
<span class="normal">3514</span>
<span class="normal">3515</span>
<span class="normal">3516</span>
<span class="normal">3517</span>
<span class="normal">3518</span>
<span class="normal">3519</span>
<span class="normal">3520</span>
<span class="normal">3521</span>
<span class="normal">3522</span>
<span class="normal">3523</span>
<span class="normal">3524</span>
<span class="normal">3525</span>
<span class="normal">3526</span>
<span class="normal">3527</span>
<span class="normal">3528</span>
<span class="normal">3529</span>
<span class="normal">3530</span>
<span class="normal">3531</span>
<span class="normal">3532</span>
<span class="normal">3533</span>
<span class="normal">3534</span>
<span class="normal">3535</span>
<span class="normal">3536</span>
<span class="normal">3537</span>
<span class="normal">3538</span>
<span class="normal">3539</span>
<span class="normal">3540</span>
<span class="normal">3541</span>
<span class="normal">3542</span>
<span class="normal">3543</span>
<span class="normal">3544</span>
<span class="normal">3545</span>
<span class="normal">3546</span>
<span class="normal">3547</span>
<span class="normal">3548</span>
<span class="normal">3549</span>
<span class="normal">3550</span>
<span class="normal">3551</span>
<span class="normal">3552</span>
<span class="normal">3553</span>
<span class="normal">3554</span>
<span class="normal">3555</span>
<span class="normal">3556</span>
<span class="normal">3557</span>
<span class="normal">3558</span>
<span class="normal">3559</span>
<span class="normal">3560</span>
<span class="normal">3561</span>
<span class="normal">3562</span>
<span class="normal">3563</span>
<span class="normal">3564</span>
<span class="normal">3565</span>
<span class="normal">3566</span>
<span class="normal">3567</span>
<span class="normal">3568</span>
<span class="normal">3569</span>
<span class="normal">3570</span>
<span class="normal">3571</span>
<span class="normal">3572</span>
<span class="normal">3573</span>
<span class="normal">3574</span>
<span class="normal">3575</span>
<span class="normal">3576</span>
<span class="normal">3577</span>
<span class="normal">3578</span>
<span class="normal">3579</span>
<span class="normal">3580</span>
<span class="normal">3581</span>
<span class="normal">3582</span>
<span class="normal">3583</span>
<span class="normal">3584</span>
<span class="normal">3585</span>
<span class="normal">3586</span>
<span class="normal">3587</span>
<span class="normal">3588</span>
<span class="normal">3589</span>
<span class="normal">3590</span>
<span class="normal">3591</span>
<span class="normal">3592</span>
<span class="normal">3593</span>
<span class="normal">3594</span>
<span class="normal">3595</span>
<span class="normal">3596</span>
<span class="normal">3597</span>
<span class="normal">3598</span>
<span class="normal">3599</span>
<span class="normal">3600</span>
<span class="normal">3601</span>
<span class="normal">3602</span>
<span class="normal">3603</span>
<span class="normal">3604</span>
<span class="normal">3605</span>
<span class="normal">3606</span>
<span class="normal">3607</span>
<span class="normal">3608</span>
<span class="normal">3609</span>
<span class="normal">3610</span>
<span class="normal">3611</span>
<span class="normal">3612</span>
<span class="normal">3613</span>
<span class="normal">3614</span>
<span class="normal">3615</span>
<span class="normal">3616</span>
<span class="normal">3617</span>
<span class="normal">3618</span>
<span class="normal">3619</span>
<span class="normal">3620</span>
<span class="normal">3621</span>
<span class="normal">3622</span>
<span class="normal">3623</span>
<span class="normal">3624</span>
<span class="normal">3625</span>
<span class="normal">3626</span>
<span class="normal">3627</span>
<span class="normal">3628</span>
<span class="normal">3629</span>
<span class="normal">3630</span>
<span class="normal">3631</span>
<span class="normal">3632</span>
<span class="normal">3633</span>
<span class="normal">3634</span>
<span class="normal">3635</span>
<span class="normal">3636</span>
<span class="normal">3637</span>
<span class="normal">3638</span>
<span class="normal">3639</span>
<span class="normal">3640</span>
<span class="normal">3641</span>
<span class="normal">3642</span>
<span class="normal">3643</span>
<span class="normal">3644</span>
<span class="normal">3645</span>
<span class="normal">3646</span>
<span class="normal">3647</span>
<span class="normal">3648</span>
<span class="normal">3649</span>
<span class="normal">3650</span>
<span class="normal">3651</span>
<span class="normal">3652</span>
<span class="normal">3653</span>
<span class="normal">3654</span>
<span class="normal">3655</span>
<span class="normal">3656</span>
<span class="normal">3657</span>
<span class="normal">3658</span>
<span class="normal">3659</span>
<span class="normal">3660</span>
<span class="normal">3661</span>
<span class="normal">3662</span>
<span class="normal">3663</span>
<span class="normal">3664</span>
<span class="normal">3665</span>
<span class="normal">3666</span>
<span class="normal">3667</span>
<span class="normal">3668</span>
<span class="normal">3669</span>
<span class="normal">3670</span>
<span class="normal">3671</span>
<span class="normal">3672</span>
<span class="normal">3673</span>
<span class="normal">3674</span>
<span class="normal">3675</span>
<span class="normal">3676</span>
<span class="normal">3677</span>
<span class="normal">3678</span>
<span class="normal">3679</span>
<span class="normal">3680</span>
<span class="normal">3681</span>
<span class="normal">3682</span>
<span class="normal">3683</span>
<span class="normal">3684</span>
<span class="normal">3685</span>
<span class="normal">3686</span>
<span class="normal">3687</span>
<span class="normal">3688</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TForSpeechToSpeech</span><span class="p">(</span><span class="n">SeamlessM4TPreTrainedModel</span><span class="p">):</span>
    <span class="n">_keys_to_ignore_on_load_missing</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;text_encoder&quot;</span><span class="p">]</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_features&quot;</span>

    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_decoder.embed_tokens.weight&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4TSpeechEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span> <span class="o">=</span> <span class="n">SeamlessM4TDecoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span> <span class="o">=</span> <span class="n">SeamlessM4TTextToUnitForConditionalGeneration</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span> <span class="o">=</span> <span class="n">SeamlessM4TCodeHifiGan</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span>

    <span class="k">def</span> <span class="nf">get_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_features</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Seq2SeqLMOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The `use_cache` argument is changed to `False` since `labels` is provided.&quot;</span><span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span>
                    <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
                <span class="p">)</span>

        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if encoder_outputs is not None, it&#39;s probably used within a .generate method so no need to warn</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This is the same forward method as `SeamlessM4TForSpeechToText`. It doesn&#39;t use `self.t2u_model`.&quot;</span>
                <span class="s2">&quot;If you want to generate speech, use the `generate` method.&quot;</span>
            <span class="p">)</span>

            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span>
                <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True</span>
        <span class="k">elif</span> <span class="n">return_dict</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">):</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
                <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
            <span class="p">)</span>

        <span class="c1"># decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">decoder_inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">decoder_outputs</span> <span class="o">+</span> <span class="n">encoder_outputs</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">spkr_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates translated audio waveforms.</span>

<span class="sd">        &lt;Tip&gt;</span>

<span class="sd">        This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">        arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">        that will be passed to one of them.</span>

<span class="sd">        For example, calling `.generate(input_features, num_beams=4, speech_do_sample=True)` will successively perform</span>
<span class="sd">        beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">        For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">        guide](./generation_strategies).</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`):</span>
<span class="sd">                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">                [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>
<span class="sd">            return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">                If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">                to get translated text alongside the audio.</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language to use as target language for translation.</span>
<span class="sd">            spkr_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>

<span class="sd">            kwargs (*optional*):</span>
<span class="sd">                Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">                arguments are of two types:</span>

<span class="sd">                    - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                    except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                    - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                    text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                    This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                    other.</span>


<span class="sd">        Returns:</span>
<span class="sd">            `Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`:</span>
<span class="sd">            - If `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].</span>
<span class="sd">            - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,</span>
<span class="sd">              sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># also accept __xxx__</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]:</span>
                <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                        to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                    Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4T supports</span>
<span class="s2">                    more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                    <span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
        <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
        <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

        <span class="c1"># first generation</span>
        <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
        <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

        <span class="c1"># prepare second generation</span>
        <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

        <span class="c1"># get last_hidden_state from encoder</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># input modality = speech so new attention mask for the decoder</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
            <span class="p">)</span>

        <span class="c1"># take care of num_return_sequences</span>
        <span class="c1"># take most probable hidden states per batch of return_sequences</span>
        <span class="c1"># (batch_size*num_return_sequences, ...) -&gt; (batch_size,...)</span>
        <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">idx_most_probable_sequences_per_batch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">+</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
            <span class="p">)</span>
            <span class="n">sequences</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="n">idx_most_probable_sequences_per_batch</span><span class="p">]</span>

        <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
        <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

        <span class="c1"># Compute new attention mask</span>
        <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
        <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

        <span class="c1"># Compute t2u decoder_input_ids</span>
        <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
        <span class="n">t2u_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">t2u_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">,</span> <span class="n">t2u_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_decoder_input_ids</span>

        <span class="c1"># second generation</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">)</span>
        <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># get rid of t2u_decoder_input_ids</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="p">[:,</span> <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:]</span>
        <span class="c1"># replace eos per pad</span>
        <span class="n">unit_ids</span><span class="p">[</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span>
        <span class="c1"># offset of control symbols</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
        <span class="p">)</span>

        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">spkr_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">spkr_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">spkr_id</span><span class="o">=</span><span class="n">spkr_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">(</span>
                <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
                <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
                <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
                <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="c1"># cached cross_attention states don&#39;t have to be reordered -&gt; they are always the same</span>
            <span class="n">reordered_past</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">past_state</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">past_state</span> <span class="ow">in</span> <span class="n">layer_past</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">reordered_past</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># cut decoder_input_ids if past is used</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># encoder_outputs is defined. input_ids not needed</span>
            <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">:</span> <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
        <span class="p">}</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TForSpeechToSpeech</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_intermediate_token_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">spkr_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates translated audio waveforms.</p>
<p><Tip></p>
<p>This method successively calls the <code>.generate</code> function of two different sub-models. You can specify keyword
arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments
that will be passed to one of them.</p>
<p>For example, calling <code>.generate(input_features, num_beams=4, speech_do_sample=True)</code> will successively perform
beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</p>
<p>For an overview of generation strategies and code examples, check out the <a href="./generation_strategies">following
guide</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_features</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input audio features. This should be returnes by the [<code>SeamlessM4TFeatureExtractor</code>] class or the
[<code>SeamlessM4TProcessor</code>] class. See [<code>SeamlessM4TFeatureExtractor.__call__</code>] for details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_intermediate_token_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If <code>True</code>, also returns the intermediate generated text and unit tokens. Set to <code>True</code> if you also want
to get translated text alongside the audio.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>spkr_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the speaker used for speech synthesis. Must be lower than <code>config.vocoder_num_spkrs</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Remaining dictionary of keyword arguments that will be passed to [<code>GenerationMixin.generate</code>]. Keyword
arguments are of two types:</p>
<div class="highlight"><pre><span></span><code>- Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,
except for `decoder_input_ids` which will only be passed through the text components.
- With a *text_* or *speech_* prefix, they will be input for the `generate` method of the
text model and speech model respectively. It has the priority over the keywords without a prefix.

This means you can, for example, specify a generation strategy for one generation but not for the
other.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput">SeamlessM4TGenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]</code>:</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput">SeamlessM4TGenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If <code>return_intermediate_token_ids</code>, returns [<code>SeamlessM4TGenerationOutput</code>].</li>
</ul>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput">SeamlessM4TGenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If not <code>return_intermediate_token_ids</code>, returns a tuple composed of waveforms of shape <code>(batch_size,
sequence_length)</code>and and <code>waveform_lengths</code> which gives the length of each sample.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span>
<span class="normal">3508</span>
<span class="normal">3509</span>
<span class="normal">3510</span>
<span class="normal">3511</span>
<span class="normal">3512</span>
<span class="normal">3513</span>
<span class="normal">3514</span>
<span class="normal">3515</span>
<span class="normal">3516</span>
<span class="normal">3517</span>
<span class="normal">3518</span>
<span class="normal">3519</span>
<span class="normal">3520</span>
<span class="normal">3521</span>
<span class="normal">3522</span>
<span class="normal">3523</span>
<span class="normal">3524</span>
<span class="normal">3525</span>
<span class="normal">3526</span>
<span class="normal">3527</span>
<span class="normal">3528</span>
<span class="normal">3529</span>
<span class="normal">3530</span>
<span class="normal">3531</span>
<span class="normal">3532</span>
<span class="normal">3533</span>
<span class="normal">3534</span>
<span class="normal">3535</span>
<span class="normal">3536</span>
<span class="normal">3537</span>
<span class="normal">3538</span>
<span class="normal">3539</span>
<span class="normal">3540</span>
<span class="normal">3541</span>
<span class="normal">3542</span>
<span class="normal">3543</span>
<span class="normal">3544</span>
<span class="normal">3545</span>
<span class="normal">3546</span>
<span class="normal">3547</span>
<span class="normal">3548</span>
<span class="normal">3549</span>
<span class="normal">3550</span>
<span class="normal">3551</span>
<span class="normal">3552</span>
<span class="normal">3553</span>
<span class="normal">3554</span>
<span class="normal">3555</span>
<span class="normal">3556</span>
<span class="normal">3557</span>
<span class="normal">3558</span>
<span class="normal">3559</span>
<span class="normal">3560</span>
<span class="normal">3561</span>
<span class="normal">3562</span>
<span class="normal">3563</span>
<span class="normal">3564</span>
<span class="normal">3565</span>
<span class="normal">3566</span>
<span class="normal">3567</span>
<span class="normal">3568</span>
<span class="normal">3569</span>
<span class="normal">3570</span>
<span class="normal">3571</span>
<span class="normal">3572</span>
<span class="normal">3573</span>
<span class="normal">3574</span>
<span class="normal">3575</span>
<span class="normal">3576</span>
<span class="normal">3577</span>
<span class="normal">3578</span>
<span class="normal">3579</span>
<span class="normal">3580</span>
<span class="normal">3581</span>
<span class="normal">3582</span>
<span class="normal">3583</span>
<span class="normal">3584</span>
<span class="normal">3585</span>
<span class="normal">3586</span>
<span class="normal">3587</span>
<span class="normal">3588</span>
<span class="normal">3589</span>
<span class="normal">3590</span>
<span class="normal">3591</span>
<span class="normal">3592</span>
<span class="normal">3593</span>
<span class="normal">3594</span>
<span class="normal">3595</span>
<span class="normal">3596</span>
<span class="normal">3597</span>
<span class="normal">3598</span>
<span class="normal">3599</span>
<span class="normal">3600</span>
<span class="normal">3601</span>
<span class="normal">3602</span>
<span class="normal">3603</span>
<span class="normal">3604</span>
<span class="normal">3605</span>
<span class="normal">3606</span>
<span class="normal">3607</span>
<span class="normal">3608</span>
<span class="normal">3609</span>
<span class="normal">3610</span>
<span class="normal">3611</span>
<span class="normal">3612</span>
<span class="normal">3613</span>
<span class="normal">3614</span>
<span class="normal">3615</span>
<span class="normal">3616</span>
<span class="normal">3617</span>
<span class="normal">3618</span>
<span class="normal">3619</span>
<span class="normal">3620</span>
<span class="normal">3621</span>
<span class="normal">3622</span>
<span class="normal">3623</span>
<span class="normal">3624</span>
<span class="normal">3625</span>
<span class="normal">3626</span>
<span class="normal">3627</span>
<span class="normal">3628</span>
<span class="normal">3629</span>
<span class="normal">3630</span>
<span class="normal">3631</span>
<span class="normal">3632</span>
<span class="normal">3633</span>
<span class="normal">3634</span>
<span class="normal">3635</span>
<span class="normal">3636</span>
<span class="normal">3637</span>
<span class="normal">3638</span>
<span class="normal">3639</span>
<span class="normal">3640</span>
<span class="normal">3641</span>
<span class="normal">3642</span>
<span class="normal">3643</span>
<span class="normal">3644</span>
<span class="normal">3645</span>
<span class="normal">3646</span>
<span class="normal">3647</span>
<span class="normal">3648</span>
<span class="normal">3649</span>
<span class="normal">3650</span>
<span class="normal">3651</span>
<span class="normal">3652</span>
<span class="normal">3653</span>
<span class="normal">3654</span>
<span class="normal">3655</span>
<span class="normal">3656</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">spkr_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates translated audio waveforms.</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">    arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">    that will be passed to one of them.</span>

<span class="sd">    For example, calling `.generate(input_features, num_beams=4, speech_do_sample=True)` will successively perform</span>
<span class="sd">    beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">    For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">    guide](./generation_strategies).</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`):</span>
<span class="sd">            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">            [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>
<span class="sd">        return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">            If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">            to get translated text alongside the audio.</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        spkr_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>

<span class="sd">        kwargs (*optional*):</span>
<span class="sd">            Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">            arguments are of two types:</span>

<span class="sd">                - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                other.</span>


<span class="sd">    Returns:</span>
<span class="sd">        `Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`:</span>
<span class="sd">        - If `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].</span>
<span class="sd">        - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,</span>
<span class="sd">          sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># also accept __xxx__</span>
        <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]:</span>
            <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                    to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4T supports</span>
<span class="s2">                more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                <span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
    <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
    <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

    <span class="c1"># first generation</span>
    <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

    <span class="c1"># prepare second generation</span>
    <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="c1"># get last_hidden_state from encoder</span>
    <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># input modality = speech so new attention mask for the decoder</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
        <span class="p">)</span>

    <span class="c1"># take care of num_return_sequences</span>
    <span class="c1"># take most probable hidden states per batch of return_sequences</span>
    <span class="c1"># (batch_size*num_return_sequences, ...) -&gt; (batch_size,...)</span>
    <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">idx_most_probable_sequences_per_batch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">+</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
        <span class="p">)</span>
        <span class="n">sequences</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="n">idx_most_probable_sequences_per_batch</span><span class="p">]</span>

    <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
    <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

    <span class="c1"># Compute new attention mask</span>
    <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
    <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

    <span class="c1"># Compute t2u decoder_input_ids</span>
    <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
    <span class="n">t2u_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">t2u_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">,</span> <span class="n">t2u_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_decoder_input_ids</span>

    <span class="c1"># second generation</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">)</span>
    <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># get rid of t2u_decoder_input_ids</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="p">[:,</span> <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:]</span>
    <span class="c1"># replace eos per pad</span>
    <span class="n">unit_ids</span><span class="p">[</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span>
    <span class="c1"># offset of control symbols</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
    <span class="p">)</span>

    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">spkr_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">spkr_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">spkr_id</span><span class="o">=</span><span class="n">spkr_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">(</span>
            <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
            <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
            <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
            <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel">SeamlessM4TPreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span>
<span class="normal">2756</span>
<span class="normal">2757</span>
<span class="normal">2758</span>
<span class="normal">2759</span>
<span class="normal">2760</span>
<span class="normal">2761</span>
<span class="normal">2762</span>
<span class="normal">2763</span>
<span class="normal">2764</span>
<span class="normal">2765</span>
<span class="normal">2766</span>
<span class="normal">2767</span>
<span class="normal">2768</span>
<span class="normal">2769</span>
<span class="normal">2770</span>
<span class="normal">2771</span>
<span class="normal">2772</span>
<span class="normal">2773</span>
<span class="normal">2774</span>
<span class="normal">2775</span>
<span class="normal">2776</span>
<span class="normal">2777</span>
<span class="normal">2778</span>
<span class="normal">2779</span>
<span class="normal">2780</span>
<span class="normal">2781</span>
<span class="normal">2782</span>
<span class="normal">2783</span>
<span class="normal">2784</span>
<span class="normal">2785</span>
<span class="normal">2786</span>
<span class="normal">2787</span>
<span class="normal">2788</span>
<span class="normal">2789</span>
<span class="normal">2790</span>
<span class="normal">2791</span>
<span class="normal">2792</span>
<span class="normal">2793</span>
<span class="normal">2794</span>
<span class="normal">2795</span>
<span class="normal">2796</span>
<span class="normal">2797</span>
<span class="normal">2798</span>
<span class="normal">2799</span>
<span class="normal">2800</span>
<span class="normal">2801</span>
<span class="normal">2802</span>
<span class="normal">2803</span>
<span class="normal">2804</span>
<span class="normal">2805</span>
<span class="normal">2806</span>
<span class="normal">2807</span>
<span class="normal">2808</span>
<span class="normal">2809</span>
<span class="normal">2810</span>
<span class="normal">2811</span>
<span class="normal">2812</span>
<span class="normal">2813</span>
<span class="normal">2814</span>
<span class="normal">2815</span>
<span class="normal">2816</span>
<span class="normal">2817</span>
<span class="normal">2818</span>
<span class="normal">2819</span>
<span class="normal">2820</span>
<span class="normal">2821</span>
<span class="normal">2822</span>
<span class="normal">2823</span>
<span class="normal">2824</span>
<span class="normal">2825</span>
<span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span>
<span class="normal">2842</span>
<span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span>
<span class="normal">2847</span>
<span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span>
<span class="normal">2885</span>
<span class="normal">2886</span>
<span class="normal">2887</span>
<span class="normal">2888</span>
<span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span>
<span class="normal">2967</span>
<span class="normal">2968</span>
<span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span>
<span class="normal">2976</span>
<span class="normal">2977</span>
<span class="normal">2978</span>
<span class="normal">2979</span>
<span class="normal">2980</span>
<span class="normal">2981</span>
<span class="normal">2982</span>
<span class="normal">2983</span>
<span class="normal">2984</span>
<span class="normal">2985</span>
<span class="normal">2986</span>
<span class="normal">2987</span>
<span class="normal">2988</span>
<span class="normal">2989</span>
<span class="normal">2990</span>
<span class="normal">2991</span>
<span class="normal">2992</span>
<span class="normal">2993</span>
<span class="normal">2994</span>
<span class="normal">2995</span>
<span class="normal">2996</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TForSpeechToText</span><span class="p">(</span><span class="n">SeamlessM4TPreTrainedModel</span><span class="p">):</span>
    <span class="n">_keys_to_ignore_on_load_missing</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;text_decoder&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_model&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder&quot;</span><span class="p">]</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_features&quot;</span>

    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_decoder.embed_tokens.weight&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4TConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4TSpeechEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span> <span class="o">=</span> <span class="n">SeamlessM4TDecoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span>

    <span class="k">def</span> <span class="nf">get_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_features</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Seq2SeqLMOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The `use_cache` argument is changed to `False` since `labels` is provided.&quot;</span><span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span>
                    <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
                <span class="p">)</span>

        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span>
                <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True</span>
        <span class="k">elif</span> <span class="n">return_dict</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">):</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
                <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
            <span class="p">)</span>

        <span class="c1"># decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">decoder_inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">decoder_outputs</span> <span class="o">+</span> <span class="n">encoder_outputs</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the</span>
<span class="sd">        model&#39;s default generation configuration. You can override any `generation_config` by passing the corresponding</span>
<span class="sd">        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.</span>

<span class="sd">        For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">        guide](./generation_strategies).</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`):</span>
<span class="sd">                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">                [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>

<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language to use as target language for translation.</span>
<span class="sd">            generation_config (`~generation.GenerationConfig`, *optional*):</span>
<span class="sd">                The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">                passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">                `generation_config` is not provided, the default will be used, which had the following loading</span>
<span class="sd">                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">                default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`, *optional*):</span>
<span class="sd">                Custom logits processors that complement the default logits processors built from arguments and</span>
<span class="sd">                generation config. If a logit processor is passed that is already created with the arguments or a</span>
<span class="sd">                generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`, *optional*):</span>
<span class="sd">                Custom stopping criteria that complement the default stopping criteria built from arguments and a</span>
<span class="sd">                generation config. If a stopping criteria is passed that is already created with the arguments or a</span>
<span class="sd">                generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">            prefix_allowed_tokens_fn (`Callable[[int, mindspore.Tensor], List[int]]`, *optional*):</span>
<span class="sd">                If provided, this function constraints the beam search to allowed tokens only at each step. If not</span>
<span class="sd">                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and</span>
<span class="sd">                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned</span>
<span class="sd">                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful</span>
<span class="sd">                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity</span>
<span class="sd">                Retrieval](https://arxiv.org/abs/2010.00904).</span>
<span class="sd">            synced_gpus (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">            kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">                Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be</span>
<span class="sd">                forwarded to the `forward` function of the model.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`~utils.ModelOutput`] or `mindspore.Tensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`</span>
<span class="sd">            or when `config.return_dict_in_generate=True`) or a `mindspore.Tensor`. The possible</span>
<span class="sd">            [`~utils.ModelOutput`] types are:</span>
<span class="sd">                - [`~generation.GenerateEncoderDecoderOutput`],</span>
<span class="sd">                - [`~generation.GenerateBeamEncoderDecoderOutput`]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
        <span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_embeds&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">input_features</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">inputs</span>
                <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">})[</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">):</span>
                <span class="c1"># also accept __xxx__</span>
                <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model. Please specify a `tgt_lang` in</span>
<span class="s2">                        </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="c1"># tgt_lang gets priority over decoder input ids</span>
                <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
                <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;This model generation config doesn&#39;t have a `text_decoder_lang_to_code_id` key which maps</span>
<span class="sd">                    the target language to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># only a warning, otherwise errors appear in the tests</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;You must either specify a `tgt_lang` or pass a correct `text_decoder_input_ids` to get</span>
<span class="sd">                a correct generation, otherwise the generation will probably make no sense.&quot;&quot;&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
            <span class="n">input_features</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="p">,</span>
            <span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="p">,</span>
            <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">text_decoder_input_ids</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># cut decoder_input_ids if past is used</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># encoder_outputs is defined. input_ids not needed</span>
            <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">:</span> <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="c1"># cached cross_attention states don&#39;t have to be reordered -&gt; they are always the same</span>
            <span class="n">reordered_past</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">past_state</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">past_state</span> <span class="ow">in</span> <span class="n">layer_past</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">reordered_past</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TForSpeechToText</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates sequences of token ids.</p>
<p><Tip warning={true}></p>
<p>Most generation-controlling parameters are set in <code>generation_config</code> which, if not passed, will be set to the
model's default generation configuration. You can override any <code>generation_config</code> by passing the corresponding
parameters to generate(), e.g. <code>.generate(inputs, num_beams=4, do_sample=True)</code>.</p>
<p>For an overview of generation strategies and code examples, check out the <a href="./generation_strategies">following
guide</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_features</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input audio features. This should be returnes by the [<code>SeamlessM4TFeatureExtractor</code>] class or the
[<code>SeamlessM4TProcessor</code>] class. See [<code>SeamlessM4TFeatureExtractor.__call__</code>] for details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>generation_config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generation configuration to be used as base parametrization for the generation call. <code>**kwargs</code>
passed to generate matching the attributes of <code>generation_config</code> will override them. If
<code>generation_config</code> is not provided, the default will be used, which had the following loading
priority: 1) from the <code>generation_config.json</code> model file, if it exists; 2) from the model
configuration. Please note that unspecified parameters will inherit [<code>~generation.GenerationConfig</code>]'s
default values, whose documentation should be checked to parameterize generation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`~generation.GenerationConfig`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logits_processor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom logits processors that complement the default logits processors built from arguments and
generation config. If a logit processor is passed that is already created with the arguments or a
generation config an error is thrown. This feature is intended for advanced users.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`LogitsProcessorList`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stopping_criteria</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom stopping criteria that complement the default stopping criteria built from arguments and a
generation config. If a stopping criteria is passed that is already created with the arguments or a
generation config an error is thrown. This feature is intended for advanced users.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`StoppingCriteriaList`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prefix_allowed_tokens_fn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments: the batch ID <code>batch_id</code> and
<code>input_ids</code>. It has to return a list with the allowed tokens for the next generation step conditioned
on the batch ID <code>batch_id</code> and the previously generated tokens <code>inputs_ids</code>. This argument is useful
for constrained generation conditioned on the prefix, as described in <a href="https://arxiv.org/abs/2010.00904">Autoregressive Entity
Retrieval</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable[[int, mindspore.Tensor], List[int]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>synced_gpus</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Ad hoc parametrization of <code>generate_config</code> and/or additional model-specific kwargs that will be
forwarded to the <code>forward</code> function of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Any]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p>[<code>~utils.ModelOutput</code>] or <code>mindspore.Tensor</code>: A [<code>~utils.ModelOutput</code>] (if <code>return_dict_in_generate=True</code>
or when <code>config.return_dict_in_generate=True</code>) or a <code>mindspore.Tensor</code>. The possible
[<code>~utils.ModelOutput</code>] types are:
    - [<code>~generation.GenerateEncoderDecoderOutput</code>],
    - [<code>~generation.GenerateBeamEncoderDecoderOutput</code>]</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span>
<span class="normal">2885</span>
<span class="normal">2886</span>
<span class="normal">2887</span>
<span class="normal">2888</span>
<span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates sequences of token ids.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the</span>
<span class="sd">    model&#39;s default generation configuration. You can override any `generation_config` by passing the corresponding</span>
<span class="sd">    parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.</span>

<span class="sd">    For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">    guide](./generation_strategies).</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`):</span>
<span class="sd">            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">            [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>

<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        generation_config (`~generation.GenerationConfig`, *optional*):</span>
<span class="sd">            The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">            passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">            `generation_config` is not provided, the default will be used, which had the following loading</span>
<span class="sd">            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">            default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">        logits_processor (`LogitsProcessorList`, *optional*):</span>
<span class="sd">            Custom logits processors that complement the default logits processors built from arguments and</span>
<span class="sd">            generation config. If a logit processor is passed that is already created with the arguments or a</span>
<span class="sd">            generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">        stopping_criteria (`StoppingCriteriaList`, *optional*):</span>
<span class="sd">            Custom stopping criteria that complement the default stopping criteria built from arguments and a</span>
<span class="sd">            generation config. If a stopping criteria is passed that is already created with the arguments or a</span>
<span class="sd">            generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">        prefix_allowed_tokens_fn (`Callable[[int, mindspore.Tensor], List[int]]`, *optional*):</span>
<span class="sd">            If provided, this function constraints the beam search to allowed tokens only at each step. If not</span>
<span class="sd">            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and</span>
<span class="sd">            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned</span>
<span class="sd">            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful</span>
<span class="sd">            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity</span>
<span class="sd">            Retrieval](https://arxiv.org/abs/2010.00904).</span>
<span class="sd">        synced_gpus (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">        kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">            Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be</span>
<span class="sd">            forwarded to the `forward` function of the model.</span>

<span class="sd">    Return:</span>
<span class="sd">        [`~utils.ModelOutput`] or `mindspore.Tensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`</span>
<span class="sd">        or when `config.return_dict_in_generate=True`) or a `mindspore.Tensor`. The possible</span>
<span class="sd">        [`~utils.ModelOutput`] types are:</span>
<span class="sd">            - [`~generation.GenerateEncoderDecoderOutput`],</span>
<span class="sd">            - [`~generation.GenerateBeamEncoderDecoderOutput`]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
    <span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_embeds&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">input_features</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">inputs</span>
            <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">})[</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">):</span>
            <span class="c1"># also accept __xxx__</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model. Please specify a `tgt_lang` in</span>
<span class="s2">                    </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="c1"># tgt_lang gets priority over decoder input ids</span>
            <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
            <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;This model generation config doesn&#39;t have a `text_decoder_lang_to_code_id` key which maps</span>
<span class="sd">                the target language to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># only a warning, otherwise errors appear in the tests</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;You must either specify a `tgt_lang` or pass a correct `text_decoder_input_ids` to get</span>
<span class="sd">            a correct generation, otherwise the generation will probably make no sense.&quot;&quot;&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">input_features</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">text_decoder_input_ids</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel">SeamlessM4TPreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2999</span>
<span class="normal">3000</span>
<span class="normal">3001</span>
<span class="normal">3002</span>
<span class="normal">3003</span>
<span class="normal">3004</span>
<span class="normal">3005</span>
<span class="normal">3006</span>
<span class="normal">3007</span>
<span class="normal">3008</span>
<span class="normal">3009</span>
<span class="normal">3010</span>
<span class="normal">3011</span>
<span class="normal">3012</span>
<span class="normal">3013</span>
<span class="normal">3014</span>
<span class="normal">3015</span>
<span class="normal">3016</span>
<span class="normal">3017</span>
<span class="normal">3018</span>
<span class="normal">3019</span>
<span class="normal">3020</span>
<span class="normal">3021</span>
<span class="normal">3022</span>
<span class="normal">3023</span>
<span class="normal">3024</span>
<span class="normal">3025</span>
<span class="normal">3026</span>
<span class="normal">3027</span>
<span class="normal">3028</span>
<span class="normal">3029</span>
<span class="normal">3030</span>
<span class="normal">3031</span>
<span class="normal">3032</span>
<span class="normal">3033</span>
<span class="normal">3034</span>
<span class="normal">3035</span>
<span class="normal">3036</span>
<span class="normal">3037</span>
<span class="normal">3038</span>
<span class="normal">3039</span>
<span class="normal">3040</span>
<span class="normal">3041</span>
<span class="normal">3042</span>
<span class="normal">3043</span>
<span class="normal">3044</span>
<span class="normal">3045</span>
<span class="normal">3046</span>
<span class="normal">3047</span>
<span class="normal">3048</span>
<span class="normal">3049</span>
<span class="normal">3050</span>
<span class="normal">3051</span>
<span class="normal">3052</span>
<span class="normal">3053</span>
<span class="normal">3054</span>
<span class="normal">3055</span>
<span class="normal">3056</span>
<span class="normal">3057</span>
<span class="normal">3058</span>
<span class="normal">3059</span>
<span class="normal">3060</span>
<span class="normal">3061</span>
<span class="normal">3062</span>
<span class="normal">3063</span>
<span class="normal">3064</span>
<span class="normal">3065</span>
<span class="normal">3066</span>
<span class="normal">3067</span>
<span class="normal">3068</span>
<span class="normal">3069</span>
<span class="normal">3070</span>
<span class="normal">3071</span>
<span class="normal">3072</span>
<span class="normal">3073</span>
<span class="normal">3074</span>
<span class="normal">3075</span>
<span class="normal">3076</span>
<span class="normal">3077</span>
<span class="normal">3078</span>
<span class="normal">3079</span>
<span class="normal">3080</span>
<span class="normal">3081</span>
<span class="normal">3082</span>
<span class="normal">3083</span>
<span class="normal">3084</span>
<span class="normal">3085</span>
<span class="normal">3086</span>
<span class="normal">3087</span>
<span class="normal">3088</span>
<span class="normal">3089</span>
<span class="normal">3090</span>
<span class="normal">3091</span>
<span class="normal">3092</span>
<span class="normal">3093</span>
<span class="normal">3094</span>
<span class="normal">3095</span>
<span class="normal">3096</span>
<span class="normal">3097</span>
<span class="normal">3098</span>
<span class="normal">3099</span>
<span class="normal">3100</span>
<span class="normal">3101</span>
<span class="normal">3102</span>
<span class="normal">3103</span>
<span class="normal">3104</span>
<span class="normal">3105</span>
<span class="normal">3106</span>
<span class="normal">3107</span>
<span class="normal">3108</span>
<span class="normal">3109</span>
<span class="normal">3110</span>
<span class="normal">3111</span>
<span class="normal">3112</span>
<span class="normal">3113</span>
<span class="normal">3114</span>
<span class="normal">3115</span>
<span class="normal">3116</span>
<span class="normal">3117</span>
<span class="normal">3118</span>
<span class="normal">3119</span>
<span class="normal">3120</span>
<span class="normal">3121</span>
<span class="normal">3122</span>
<span class="normal">3123</span>
<span class="normal">3124</span>
<span class="normal">3125</span>
<span class="normal">3126</span>
<span class="normal">3127</span>
<span class="normal">3128</span>
<span class="normal">3129</span>
<span class="normal">3130</span>
<span class="normal">3131</span>
<span class="normal">3132</span>
<span class="normal">3133</span>
<span class="normal">3134</span>
<span class="normal">3135</span>
<span class="normal">3136</span>
<span class="normal">3137</span>
<span class="normal">3138</span>
<span class="normal">3139</span>
<span class="normal">3140</span>
<span class="normal">3141</span>
<span class="normal">3142</span>
<span class="normal">3143</span>
<span class="normal">3144</span>
<span class="normal">3145</span>
<span class="normal">3146</span>
<span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span>
<span class="normal">3157</span>
<span class="normal">3158</span>
<span class="normal">3159</span>
<span class="normal">3160</span>
<span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span>
<span class="normal">3192</span>
<span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span>
<span class="normal">3199</span>
<span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span>
<span class="normal">3204</span>
<span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span>
<span class="normal">3225</span>
<span class="normal">3226</span>
<span class="normal">3227</span>
<span class="normal">3228</span>
<span class="normal">3229</span>
<span class="normal">3230</span>
<span class="normal">3231</span>
<span class="normal">3232</span>
<span class="normal">3233</span>
<span class="normal">3234</span>
<span class="normal">3235</span>
<span class="normal">3236</span>
<span class="normal">3237</span>
<span class="normal">3238</span>
<span class="normal">3239</span>
<span class="normal">3240</span>
<span class="normal">3241</span>
<span class="normal">3242</span>
<span class="normal">3243</span>
<span class="normal">3244</span>
<span class="normal">3245</span>
<span class="normal">3246</span>
<span class="normal">3247</span>
<span class="normal">3248</span>
<span class="normal">3249</span>
<span class="normal">3250</span>
<span class="normal">3251</span>
<span class="normal">3252</span>
<span class="normal">3253</span>
<span class="normal">3254</span>
<span class="normal">3255</span>
<span class="normal">3256</span>
<span class="normal">3257</span>
<span class="normal">3258</span>
<span class="normal">3259</span>
<span class="normal">3260</span>
<span class="normal">3261</span>
<span class="normal">3262</span>
<span class="normal">3263</span>
<span class="normal">3264</span>
<span class="normal">3265</span>
<span class="normal">3266</span>
<span class="normal">3267</span>
<span class="normal">3268</span>
<span class="normal">3269</span>
<span class="normal">3270</span>
<span class="normal">3271</span>
<span class="normal">3272</span>
<span class="normal">3273</span>
<span class="normal">3274</span>
<span class="normal">3275</span>
<span class="normal">3276</span>
<span class="normal">3277</span>
<span class="normal">3278</span>
<span class="normal">3279</span>
<span class="normal">3280</span>
<span class="normal">3281</span>
<span class="normal">3282</span>
<span class="normal">3283</span>
<span class="normal">3284</span>
<span class="normal">3285</span>
<span class="normal">3286</span>
<span class="normal">3287</span>
<span class="normal">3288</span>
<span class="normal">3289</span>
<span class="normal">3290</span>
<span class="normal">3291</span>
<span class="normal">3292</span>
<span class="normal">3293</span>
<span class="normal">3294</span>
<span class="normal">3295</span>
<span class="normal">3296</span>
<span class="normal">3297</span>
<span class="normal">3298</span>
<span class="normal">3299</span>
<span class="normal">3300</span>
<span class="normal">3301</span>
<span class="normal">3302</span>
<span class="normal">3303</span>
<span class="normal">3304</span>
<span class="normal">3305</span>
<span class="normal">3306</span>
<span class="normal">3307</span>
<span class="normal">3308</span>
<span class="normal">3309</span>
<span class="normal">3310</span>
<span class="normal">3311</span>
<span class="normal">3312</span>
<span class="normal">3313</span>
<span class="normal">3314</span>
<span class="normal">3315</span>
<span class="normal">3316</span>
<span class="normal">3317</span>
<span class="normal">3318</span>
<span class="normal">3319</span>
<span class="normal">3320</span>
<span class="normal">3321</span>
<span class="normal">3322</span>
<span class="normal">3323</span>
<span class="normal">3324</span>
<span class="normal">3325</span>
<span class="normal">3326</span>
<span class="normal">3327</span>
<span class="normal">3328</span>
<span class="normal">3329</span>
<span class="normal">3330</span>
<span class="normal">3331</span>
<span class="normal">3332</span>
<span class="normal">3333</span>
<span class="normal">3334</span>
<span class="normal">3335</span>
<span class="normal">3336</span>
<span class="normal">3337</span>
<span class="normal">3338</span>
<span class="normal">3339</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TForTextToSpeech</span><span class="p">(</span><span class="n">SeamlessM4TPreTrainedModel</span><span class="p">):</span>
    <span class="n">_keys_to_ignore_on_load_missing</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;speech_encoder&quot;</span><span class="p">]</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_ids&quot;</span>

    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_encoder.embed_tokens.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_decoder.embed_tokens.weight&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4TConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4TEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span> <span class="o">=</span> <span class="n">SeamlessM4TDecoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span> <span class="o">=</span> <span class="n">SeamlessM4TTextToUnitForConditionalGeneration</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span> <span class="o">=</span> <span class="n">SeamlessM4TCodeHifiGan</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span>

    <span class="k">def</span> <span class="nf">get_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Seq2SeqLMOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The `use_cache` argument is changed to `False` since `labels` is provided.&quot;</span><span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span>
                    <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
                <span class="p">)</span>

        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if encoder_outputs is not None, it&#39;s probably used within a .generate method so no need to warn</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This is the same forward method as `SeamlessM4TForTextToText`.&quot;</span>
                <span class="s2">&quot;It doesn&#39;t use the text-to-unit model `SeamlessM4TTextToUnitForConditionalGeneration`.&quot;</span>
                <span class="s2">&quot;If you want to generate speech, use the `.generate` method.&quot;</span>
            <span class="p">)</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True</span>
        <span class="k">elif</span> <span class="n">return_dict</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">):</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
                <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>

        <span class="c1"># decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">decoder_inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">decoder_outputs</span> <span class="o">+</span> <span class="n">encoder_outputs</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">spkr_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates translated audio waveforms.</span>

<span class="sd">        &lt;Tip&gt;</span>

<span class="sd">        This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">        arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">        that will be passed to one of them.</span>

<span class="sd">        For example, calling `.generate(input_ids, num_beams=4, speech_do_sample=True)` will successively perform</span>
<span class="sd">        beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">        For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">        guide](./generation_strategies).</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">                Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">                [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">                If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">                to get translated text alongside the audio.</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language to use as target language for translation.</span>
<span class="sd">            spkr_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">            kwargs (*optional*):</span>
<span class="sd">                Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">                arguments are of two types:</span>

<span class="sd">                    - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                    except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                    - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                    text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                    This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                    other.</span>


<span class="sd">        Returns:</span>
<span class="sd">            `Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`:</span>
<span class="sd">            - If `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].</span>
<span class="sd">            - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,</span>
<span class="sd">              sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># also accept __xxx__</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]:</span>
                <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                        to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                    Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4T supports</span>
<span class="s2">                    more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                    <span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>

        <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
        <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

        <span class="c1"># first generation</span>
        <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
        <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

        <span class="c1"># prepare second generation</span>
        <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">encoder_hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># take care of num_return_sequences</span>
        <span class="c1"># take most probable hidden states per batch of return_sequences</span>
        <span class="c1"># (batch_size*num_return_sequences, ...) -&gt; (batch_size,...)</span>
        <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">idx_most_probable_sequences_per_batch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">+</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
            <span class="p">)</span>
            <span class="n">sequences</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="n">idx_most_probable_sequences_per_batch</span><span class="p">]</span>

        <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
        <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

        <span class="c1"># Compute new attention mask</span>
        <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
        <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

        <span class="c1"># Compute t2u decoder_input_ids</span>
        <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
        <span class="n">t2u_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">t2u_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">,</span> <span class="n">t2u_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_decoder_input_ids</span>
        <span class="c1"># second generation</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">)</span>
        <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># get rid of t2u_decoder_input_ids</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="p">[:,</span> <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:]</span>
        <span class="c1"># replace eos per pad</span>
        <span class="n">unit_ids</span><span class="p">[</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span>
        <span class="c1"># offset of control symbols</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
        <span class="p">)</span>

        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">spkr_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">spkr_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">spkr_id</span><span class="o">=</span><span class="n">spkr_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">(</span>
                <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
                <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
                <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
                <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># cut decoder_input_ids if past is used</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># encoder_outputs is defined. input_ids not needed</span>
            <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">:</span> <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="c1"># cached cross_attention states don&#39;t have to be reordered -&gt; they are always the same</span>
            <span class="n">reordered_past</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">past_state</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">past_state</span> <span class="ow">in</span> <span class="n">layer_past</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">reordered_past</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TForTextToSpeech</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_intermediate_token_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">spkr_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates translated audio waveforms.</p>
<p><Tip></p>
<p>This method successively calls the <code>.generate</code> function of two different sub-models. You can specify keyword
arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments
that will be passed to one of them.</p>
<p>For example, calling <code>.generate(input_ids, num_beams=4, speech_do_sample=True)</code> will successively perform
beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</p>
<p>For an overview of generation strategies and code examples, check out the <a href="./generation_strategies">following
guide</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<code>SeamlessM4TTokenizer</code>] or [<code>SeamlessM4TProcessor</code>]. See
[<code>PreTrainedTokenizer.encode</code>] and [<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_intermediate_token_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If <code>True</code>, also returns the intermediate generated text and unit tokens. Set to <code>True</code> if you also want
to get translated text alongside the audio.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>spkr_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the speaker used for speech synthesis. Must be lower than <code>config.vocoder_num_spkrs</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Remaining dictionary of keyword arguments that will be passed to [<code>GenerationMixin.generate</code>]. Keyword
arguments are of two types:</p>
<div class="highlight"><pre><span></span><code>- Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,
except for `decoder_input_ids` which will only be passed through the text components.
- With a *text_* or *speech_* prefix, they will be input for the `generate` method of the
text model and speech model respectively. It has the priority over the keywords without a prefix.

This means you can, for example, specify a generation strategy for one generation but not for the
other.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput">SeamlessM4TGenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]</code>:</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput">SeamlessM4TGenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If <code>return_intermediate_token_ids</code>, returns [<code>SeamlessM4TGenerationOutput</code>].</li>
</ul>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput">SeamlessM4TGenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If not <code>return_intermediate_token_ids</code>, returns a tuple composed of waveforms of shape <code>(batch_size,
sequence_length)</code>and and <code>waveform_lengths</code> which gives the length of each sample.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3145</span>
<span class="normal">3146</span>
<span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span>
<span class="normal">3157</span>
<span class="normal">3158</span>
<span class="normal">3159</span>
<span class="normal">3160</span>
<span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span>
<span class="normal">3192</span>
<span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span>
<span class="normal">3199</span>
<span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span>
<span class="normal">3204</span>
<span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span>
<span class="normal">3225</span>
<span class="normal">3226</span>
<span class="normal">3227</span>
<span class="normal">3228</span>
<span class="normal">3229</span>
<span class="normal">3230</span>
<span class="normal">3231</span>
<span class="normal">3232</span>
<span class="normal">3233</span>
<span class="normal">3234</span>
<span class="normal">3235</span>
<span class="normal">3236</span>
<span class="normal">3237</span>
<span class="normal">3238</span>
<span class="normal">3239</span>
<span class="normal">3240</span>
<span class="normal">3241</span>
<span class="normal">3242</span>
<span class="normal">3243</span>
<span class="normal">3244</span>
<span class="normal">3245</span>
<span class="normal">3246</span>
<span class="normal">3247</span>
<span class="normal">3248</span>
<span class="normal">3249</span>
<span class="normal">3250</span>
<span class="normal">3251</span>
<span class="normal">3252</span>
<span class="normal">3253</span>
<span class="normal">3254</span>
<span class="normal">3255</span>
<span class="normal">3256</span>
<span class="normal">3257</span>
<span class="normal">3258</span>
<span class="normal">3259</span>
<span class="normal">3260</span>
<span class="normal">3261</span>
<span class="normal">3262</span>
<span class="normal">3263</span>
<span class="normal">3264</span>
<span class="normal">3265</span>
<span class="normal">3266</span>
<span class="normal">3267</span>
<span class="normal">3268</span>
<span class="normal">3269</span>
<span class="normal">3270</span>
<span class="normal">3271</span>
<span class="normal">3272</span>
<span class="normal">3273</span>
<span class="normal">3274</span>
<span class="normal">3275</span>
<span class="normal">3276</span>
<span class="normal">3277</span>
<span class="normal">3278</span>
<span class="normal">3279</span>
<span class="normal">3280</span>
<span class="normal">3281</span>
<span class="normal">3282</span>
<span class="normal">3283</span>
<span class="normal">3284</span>
<span class="normal">3285</span>
<span class="normal">3286</span>
<span class="normal">3287</span>
<span class="normal">3288</span>
<span class="normal">3289</span>
<span class="normal">3290</span>
<span class="normal">3291</span>
<span class="normal">3292</span>
<span class="normal">3293</span>
<span class="normal">3294</span>
<span class="normal">3295</span>
<span class="normal">3296</span>
<span class="normal">3297</span>
<span class="normal">3298</span>
<span class="normal">3299</span>
<span class="normal">3300</span>
<span class="normal">3301</span>
<span class="normal">3302</span>
<span class="normal">3303</span>
<span class="normal">3304</span>
<span class="normal">3305</span>
<span class="normal">3306</span>
<span class="normal">3307</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">spkr_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates translated audio waveforms.</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">    arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">    that will be passed to one of them.</span>

<span class="sd">    For example, calling `.generate(input_ids, num_beams=4, speech_do_sample=True)` will successively perform</span>
<span class="sd">    beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">    For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">    guide](./generation_strategies).</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">            Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">            If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">            to get translated text alongside the audio.</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        spkr_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">        kwargs (*optional*):</span>
<span class="sd">            Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">            arguments are of two types:</span>

<span class="sd">                - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                other.</span>


<span class="sd">    Returns:</span>
<span class="sd">        `Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`:</span>
<span class="sd">        - If `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].</span>
<span class="sd">        - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,</span>
<span class="sd">          sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># also accept __xxx__</span>
        <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]:</span>
            <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                    to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4T supports</span>
<span class="s2">                more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                <span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>

    <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
    <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

    <span class="c1"># first generation</span>
    <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

    <span class="c1"># prepare second generation</span>
    <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">encoder_hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># take care of num_return_sequences</span>
    <span class="c1"># take most probable hidden states per batch of return_sequences</span>
    <span class="c1"># (batch_size*num_return_sequences, ...) -&gt; (batch_size,...)</span>
    <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">idx_most_probable_sequences_per_batch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">+</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
        <span class="p">)</span>
        <span class="n">sequences</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="n">idx_most_probable_sequences_per_batch</span><span class="p">]</span>

    <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
    <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

    <span class="c1"># Compute new attention mask</span>
    <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
    <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

    <span class="c1"># Compute t2u decoder_input_ids</span>
    <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
    <span class="n">t2u_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">t2u_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">,</span> <span class="n">t2u_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_decoder_input_ids</span>
    <span class="c1"># second generation</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">)</span>
    <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># get rid of t2u_decoder_input_ids</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="p">[:,</span> <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:]</span>
    <span class="c1"># replace eos per pad</span>
    <span class="n">unit_ids</span><span class="p">[</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span>
    <span class="c1"># offset of control symbols</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
    <span class="p">)</span>

    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">spkr_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">spkr_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">spkr_id</span><span class="o">=</span><span class="n">spkr_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">(</span>
            <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
            <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
            <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
            <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToText" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToText</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToText" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel">SeamlessM4TPreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2435</span>
<span class="normal">2436</span>
<span class="normal">2437</span>
<span class="normal">2438</span>
<span class="normal">2439</span>
<span class="normal">2440</span>
<span class="normal">2441</span>
<span class="normal">2442</span>
<span class="normal">2443</span>
<span class="normal">2444</span>
<span class="normal">2445</span>
<span class="normal">2446</span>
<span class="normal">2447</span>
<span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span>
<span class="normal">2485</span>
<span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span>
<span class="normal">2559</span>
<span class="normal">2560</span>
<span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span>
<span class="normal">2569</span>
<span class="normal">2570</span>
<span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span>
<span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span>
<span class="normal">2694</span>
<span class="normal">2695</span>
<span class="normal">2696</span>
<span class="normal">2697</span>
<span class="normal">2698</span>
<span class="normal">2699</span>
<span class="normal">2700</span>
<span class="normal">2701</span>
<span class="normal">2702</span>
<span class="normal">2703</span>
<span class="normal">2704</span>
<span class="normal">2705</span>
<span class="normal">2706</span>
<span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TForTextToText</span><span class="p">(</span><span class="n">SeamlessM4TPreTrainedModel</span><span class="p">):</span>
    <span class="n">_keys_to_ignore_on_load_missing</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;speech_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_model&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder&quot;</span><span class="p">]</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_ids&quot;</span>

    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_encoder.embed_tokens.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_decoder.embed_tokens.weight&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4TConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4TEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span> <span class="o">=</span> <span class="n">SeamlessM4TDecoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span>

    <span class="k">def</span> <span class="nf">get_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Seq2SeqLMOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The `use_cache` argument is changed to `False` since `labels` is provided.&quot;</span><span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span>
                    <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
                <span class="p">)</span>

        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True</span>
        <span class="k">elif</span> <span class="n">return_dict</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">):</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
                <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>

        <span class="c1"># decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">decoder_inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">decoder_outputs</span> <span class="o">+</span> <span class="n">encoder_outputs</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the</span>
<span class="sd">        model&#39;s default generation configuration. You can override any `generation_config` by passing the corresponding</span>
<span class="sd">        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.</span>

<span class="sd">        For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">        guide](./generation_strategies).</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of varying shape depending on the modality, *optional*):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">                Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">                [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language to use as target language for translation.</span>
<span class="sd">            generation_config (`~generation.GenerationConfig`, *optional*):</span>
<span class="sd">                The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">                passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">                `generation_config` is not provided, the default will be used, which had the following loading</span>
<span class="sd">                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">                default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`, *optional*):</span>
<span class="sd">                Custom logits processors that complement the default logits processors built from arguments and</span>
<span class="sd">                generation config. If a logit processor is passed that is already created with the arguments or a</span>
<span class="sd">                generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`, *optional*):</span>
<span class="sd">                Custom stopping criteria that complement the default stopping criteria built from arguments and a</span>
<span class="sd">                generation config. If a stopping criteria is passed that is already created with the arguments or a</span>
<span class="sd">                generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">            prefix_allowed_tokens_fn (`Callable[[int, mindspore.Tensor], List[int]]`, *optional*):</span>
<span class="sd">                If provided, this function constraints the beam search to allowed tokens only at each step. If not</span>
<span class="sd">                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and</span>
<span class="sd">                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned</span>
<span class="sd">                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful</span>
<span class="sd">                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity</span>
<span class="sd">                Retrieval](https://arxiv.org/abs/2010.00904).</span>
<span class="sd">            synced_gpus (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">            kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">                Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be</span>
<span class="sd">                forwarded to the `forward` function of the model.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`~utils.ModelOutput`] or `mindspore.Tensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`</span>
<span class="sd">            or when `config.return_dict_in_generate=True`) or a `mindspore.Tensor`. The possible</span>
<span class="sd">            [`~utils.ModelOutput`] types are:</span>
<span class="sd">                - [`~generation.GenerateEncoderDecoderOutput`],</span>
<span class="sd">                - [`~generation.GenerateBeamEncoderDecoderOutput`]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># prepare text_decoder_input_ids</span>
        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">):</span>
                <span class="c1"># also accept __xxx__</span>
                <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model. Please specify a `tgt_lang` in</span>
<span class="s2">                        </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="c1"># tgt_lang gets priority over decoder input ids</span>
                <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
                <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;This model generation config doesn&#39;t have a `text_decoder_lang_to_code_id` key which maps</span>
<span class="sd">                    the target language to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># only a warning, otherwise errors appear in the tests</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;You must either specify a `tgt_lang` or pass a correct `text_decoder_input_ids` to get</span>
<span class="sd">                a correct generation, otherwise the generation will probably make no sense.&quot;&quot;&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="p">,</span>
            <span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="p">,</span>
            <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">text_decoder_input_ids</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># cut decoder_input_ids if past is used</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># encoder_outputs is defined. input_ids not needed</span>
            <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">:</span> <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="c1"># cached cross_attention states don&#39;t have to be reordered -&gt; they are always the same</span>
            <span class="n">reordered_past</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">past_state</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">past_state</span> <span class="ow">in</span> <span class="n">layer_past</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">reordered_past</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToText.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TForTextToText</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToText.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates sequences of token ids.</p>
<p><Tip warning={true}></p>
<p>Most generation-controlling parameters are set in <code>generation_config</code> which, if not passed, will be set to the
model's default generation configuration. You can override any <code>generation_config</code> by passing the corresponding
parameters to generate(), e.g. <code>.generate(inputs, num_beams=4, do_sample=True)</code>.</p>
<p>For an overview of generation strategies and code examples, check out the <a href="./generation_strategies">following
guide</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<code>SeamlessM4TTokenizer</code>] or [<code>SeamlessM4TProcessor</code>]. See
[<code>PreTrainedTokenizer.encode</code>] and [<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of varying shape depending on the modality, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>generation_config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generation configuration to be used as base parametrization for the generation call. <code>**kwargs</code>
passed to generate matching the attributes of <code>generation_config</code> will override them. If
<code>generation_config</code> is not provided, the default will be used, which had the following loading
priority: 1) from the <code>generation_config.json</code> model file, if it exists; 2) from the model
configuration. Please note that unspecified parameters will inherit [<code>~generation.GenerationConfig</code>]'s
default values, whose documentation should be checked to parameterize generation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`~generation.GenerationConfig`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logits_processor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom logits processors that complement the default logits processors built from arguments and
generation config. If a logit processor is passed that is already created with the arguments or a
generation config an error is thrown. This feature is intended for advanced users.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`LogitsProcessorList`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stopping_criteria</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom stopping criteria that complement the default stopping criteria built from arguments and a
generation config. If a stopping criteria is passed that is already created with the arguments or a
generation config an error is thrown. This feature is intended for advanced users.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`StoppingCriteriaList`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prefix_allowed_tokens_fn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments: the batch ID <code>batch_id</code> and
<code>input_ids</code>. It has to return a list with the allowed tokens for the next generation step conditioned
on the batch ID <code>batch_id</code> and the previously generated tokens <code>inputs_ids</code>. This argument is useful
for constrained generation conditioned on the prefix, as described in <a href="https://arxiv.org/abs/2010.00904">Autoregressive Entity
Retrieval</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable[[int, mindspore.Tensor], List[int]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>synced_gpus</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Ad hoc parametrization of <code>generate_config</code> and/or additional model-specific kwargs that will be
forwarded to the <code>forward</code> function of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Any]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p>[<code>~utils.ModelOutput</code>] or <code>mindspore.Tensor</code>: A [<code>~utils.ModelOutput</code>] (if <code>return_dict_in_generate=True</code>
or when <code>config.return_dict_in_generate=True</code>) or a <code>mindspore.Tensor</code>. The possible
[<code>~utils.ModelOutput</code>] types are:
    - [<code>~generation.GenerateEncoderDecoderOutput</code>],
    - [<code>~generation.GenerateBeamEncoderDecoderOutput</code>]</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates sequences of token ids.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the</span>
<span class="sd">    model&#39;s default generation configuration. You can override any `generation_config` by passing the corresponding</span>
<span class="sd">    parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.</span>

<span class="sd">    For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">    guide](./generation_strategies).</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of varying shape depending on the modality, *optional*):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">            Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        generation_config (`~generation.GenerationConfig`, *optional*):</span>
<span class="sd">            The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">            passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">            `generation_config` is not provided, the default will be used, which had the following loading</span>
<span class="sd">            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">            default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">        logits_processor (`LogitsProcessorList`, *optional*):</span>
<span class="sd">            Custom logits processors that complement the default logits processors built from arguments and</span>
<span class="sd">            generation config. If a logit processor is passed that is already created with the arguments or a</span>
<span class="sd">            generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">        stopping_criteria (`StoppingCriteriaList`, *optional*):</span>
<span class="sd">            Custom stopping criteria that complement the default stopping criteria built from arguments and a</span>
<span class="sd">            generation config. If a stopping criteria is passed that is already created with the arguments or a</span>
<span class="sd">            generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">        prefix_allowed_tokens_fn (`Callable[[int, mindspore.Tensor], List[int]]`, *optional*):</span>
<span class="sd">            If provided, this function constraints the beam search to allowed tokens only at each step. If not</span>
<span class="sd">            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and</span>
<span class="sd">            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned</span>
<span class="sd">            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful</span>
<span class="sd">            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity</span>
<span class="sd">            Retrieval](https://arxiv.org/abs/2010.00904).</span>
<span class="sd">        synced_gpus (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">        kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">            Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be</span>
<span class="sd">            forwarded to the `forward` function of the model.</span>

<span class="sd">    Return:</span>
<span class="sd">        [`~utils.ModelOutput`] or `mindspore.Tensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`</span>
<span class="sd">        or when `config.return_dict_in_generate=True`) or a `mindspore.Tensor`. The possible</span>
<span class="sd">        [`~utils.ModelOutput`] types are:</span>
<span class="sd">            - [`~generation.GenerateEncoderDecoderOutput`],</span>
<span class="sd">            - [`~generation.GenerateBeamEncoderDecoderOutput`]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># prepare text_decoder_input_ids</span>
    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">):</span>
            <span class="c1"># also accept __xxx__</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model. Please specify a `tgt_lang` in</span>
<span class="s2">                    </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="c1"># tgt_lang gets priority over decoder input ids</span>
            <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
            <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;This model generation config doesn&#39;t have a `text_decoder_lang_to_code_id` key which maps</span>
<span class="sd">                the target language to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># only a warning, otherwise errors appear in the tests</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;You must either specify a `tgt_lang` or pass a correct `text_decoder_input_ids` to get</span>
<span class="sd">            a correct generation, otherwise the generation will probably make no sense.&quot;&quot;&quot;</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">text_decoder_input_ids</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.utils.ModelOutput">ModelOutput</span></code></p>


        <p>Class defining the generated outputs from [<code>SeamlessM4TModel</code>], [<code>SeamlessM4TForTextToText</code>],
[<code>SeamlessM4TForTextToSpeech</code>], [<code>SeamlessM4TForSpeechToSpeech</code>] and [<code>SeamlessM4TForTextToSpeech</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>waveform</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The final audio waveform predicted by the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>waveform_lengths</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The length in samples of each element in the <code>waveform</code> batch.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size,)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sequences</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generated translated sequences. This is the output of the text-to-text or the speech-to-text models.
The second dimension (sequence_length) is either equal to <code>max_length</code> or shorter if all batches finished
early due to the <code>eos_token_id</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unit_sequences</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generated translated unit sequences. This is the output of the text-to-units model. The second
dimension (unit_sequence_length) is either equal to <code>t2u_max_length</code> or shorter if all batches finished
early due to the <code>t2u_eos_token_id</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, unit_sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SeamlessM4TGenerationOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class defining the generated outputs from [`SeamlessM4TModel`], [`SeamlessM4TForTextToText`],</span>
<span class="sd">    [`SeamlessM4TForTextToSpeech`], [`SeamlessM4TForSpeechToSpeech`] and [`SeamlessM4TForTextToSpeech`].</span>

<span class="sd">    Args:</span>
<span class="sd">        waveform (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            The final audio waveform predicted by the model.</span>
<span class="sd">        waveform_lengths (`mindspore.Tensor` of shape `(batch_size,)`, *optional*):</span>
<span class="sd">            The length in samples of each element in the `waveform` batch.</span>
<span class="sd">        sequences (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            The generated translated sequences. This is the output of the text-to-text or the speech-to-text models.</span>
<span class="sd">            The second dimension (sequence_length) is either equal to `max_length` or shorter if all batches finished</span>
<span class="sd">            early due to the `eos_token_id`.</span>
<span class="sd">        unit_sequences (`mindspore.Tensor` of shape `(batch_size, unit_sequence_length)`, *optional*):</span>
<span class="sd">            The generated translated unit sequences. This is the output of the text-to-units model. The second</span>
<span class="sd">            dimension (unit_sequence_length) is either equal to `t2u_max_length` or shorter if all batches finished</span>
<span class="sd">            early due to the `t2u_eos_token_id`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">waveform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">waveform_lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">unit_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4THifiGan" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4THifiGan</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4THifiGan" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4THifiGan</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4TConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">model_in_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">unit_embed_dim</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">lang_embed_dim</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">spkr_embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu_slope</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">leaky_relu_slope</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">resblock_kernel_sizes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_upsamples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_rates</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_pre</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">model_in_dim</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">upsample_initial_channel</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">upsampler</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">upsample_rate</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_rates</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">upsample_kernel_sizes</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">upsampler</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose1d</span><span class="p">(</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">upsample_initial_channel</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span><span class="p">),</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">upsample_initial_channel</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span>
                    <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                    <span class="n">stride</span><span class="o">=</span><span class="n">upsample_rate</span><span class="p">,</span>
                    <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="n">upsample_rate</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">upsampler</span><span class="p">)):</span>
            <span class="n">channels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">upsample_initial_channel</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">resblock_kernel_sizes</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">resblock_dilation_sizes</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HifiGanResidualBlock</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">leaky_relu_slope</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv_post</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_embeds</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a log-mel spectrogram into a speech waveform. Passing a batch of log-mel spectrograms returns a batch</span>
<span class="sd">        of speech waveforms. Passing a single, un-batched log-mel spectrogram returns a single, un-batched speech</span>
<span class="sd">        waveform.</span>

<span class="sd">        Args:</span>
<span class="sd">            spectrogram (`mindspore.Tensor`):</span>
<span class="sd">                Tensor containing the log-mel spectrograms. Can be batched and of shape `(batch_size, sequence_length,</span>
<span class="sd">                model_in_dim)`, or un-batched and of shape `(sequence_length, model_in_dim)`. Note that `model_in_dim`</span>
<span class="sd">                is the sum of `config.unit_embed_dim`, `config.lang_embed_dim` and `config.spkr_embed_dim`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `mindspore.Tensor`: Tensor containing the speech waveform. If the input spectrogram is batched, will be of</span>
<span class="sd">            shape `(batch_size, num_frames,)`. If un-batched, will be of shape `(num_frames,)`.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_pre</span><span class="p">(</span><span class="n">input_embeds</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_upsamples</span><span class="p">):</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu_slope</span><span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsampler</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

            <span class="n">res_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span><span class="p">):</span>
                <span class="n">res_state</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span> <span class="o">+</span> <span class="n">j</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">res_state</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_post</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># remove seq-len dim since this collapses to 1</span>
        <span class="n">waveform</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">waveform</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4THifiGan.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4THifiGan</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_embeds</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4THifiGan.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Converts a log-mel spectrogram into a speech waveform. Passing a batch of log-mel spectrograms returns a batch
of speech waveforms. Passing a single, un-batched log-mel spectrogram returns a single, un-batched speech
waveform.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>spectrogram</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tensor containing the log-mel spectrograms. Can be batched and of shape <code>(batch_size, sequence_length,
model_in_dim)</code>, or un-batched and of shape <code>(sequence_length, model_in_dim)</code>. Note that <code>model_in_dim</code>
is the sum of <code>config.unit_embed_dim</code>, <code>config.lang_embed_dim</code> and <code>config.spkr_embed_dim</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindspore.Tensor">Tensor</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>mindspore.Tensor</code>: Tensor containing the speech waveform. If the input spectrogram is batched, will be of</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindspore.Tensor">Tensor</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>shape <code>(batch_size, num_frames,)</code>. If un-batched, will be of shape <code>(num_frames,)</code>.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_embeds</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a log-mel spectrogram into a speech waveform. Passing a batch of log-mel spectrograms returns a batch</span>
<span class="sd">    of speech waveforms. Passing a single, un-batched log-mel spectrogram returns a single, un-batched speech</span>
<span class="sd">    waveform.</span>

<span class="sd">    Args:</span>
<span class="sd">        spectrogram (`mindspore.Tensor`):</span>
<span class="sd">            Tensor containing the log-mel spectrograms. Can be batched and of shape `(batch_size, sequence_length,</span>
<span class="sd">            model_in_dim)`, or un-batched and of shape `(sequence_length, model_in_dim)`. Note that `model_in_dim`</span>
<span class="sd">            is the sum of `config.unit_embed_dim`, `config.lang_embed_dim` and `config.spkr_embed_dim`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `mindspore.Tensor`: Tensor containing the speech waveform. If the input spectrogram is batched, will be of</span>
<span class="sd">        shape `(batch_size, num_frames,)`. If un-batched, will be of shape `(num_frames,)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_pre</span><span class="p">(</span><span class="n">input_embeds</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_upsamples</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu_slope</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsampler</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">res_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span><span class="p">):</span>
            <span class="n">res_state</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span> <span class="o">+</span> <span class="n">j</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">res_state</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_post</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="c1"># remove seq-len dim since this collapses to 1</span>
    <span class="n">waveform</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">waveform</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel">SeamlessM4TPreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3691</span>
<span class="normal">3692</span>
<span class="normal">3693</span>
<span class="normal">3694</span>
<span class="normal">3695</span>
<span class="normal">3696</span>
<span class="normal">3697</span>
<span class="normal">3698</span>
<span class="normal">3699</span>
<span class="normal">3700</span>
<span class="normal">3701</span>
<span class="normal">3702</span>
<span class="normal">3703</span>
<span class="normal">3704</span>
<span class="normal">3705</span>
<span class="normal">3706</span>
<span class="normal">3707</span>
<span class="normal">3708</span>
<span class="normal">3709</span>
<span class="normal">3710</span>
<span class="normal">3711</span>
<span class="normal">3712</span>
<span class="normal">3713</span>
<span class="normal">3714</span>
<span class="normal">3715</span>
<span class="normal">3716</span>
<span class="normal">3717</span>
<span class="normal">3718</span>
<span class="normal">3719</span>
<span class="normal">3720</span>
<span class="normal">3721</span>
<span class="normal">3722</span>
<span class="normal">3723</span>
<span class="normal">3724</span>
<span class="normal">3725</span>
<span class="normal">3726</span>
<span class="normal">3727</span>
<span class="normal">3728</span>
<span class="normal">3729</span>
<span class="normal">3730</span>
<span class="normal">3731</span>
<span class="normal">3732</span>
<span class="normal">3733</span>
<span class="normal">3734</span>
<span class="normal">3735</span>
<span class="normal">3736</span>
<span class="normal">3737</span>
<span class="normal">3738</span>
<span class="normal">3739</span>
<span class="normal">3740</span>
<span class="normal">3741</span>
<span class="normal">3742</span>
<span class="normal">3743</span>
<span class="normal">3744</span>
<span class="normal">3745</span>
<span class="normal">3746</span>
<span class="normal">3747</span>
<span class="normal">3748</span>
<span class="normal">3749</span>
<span class="normal">3750</span>
<span class="normal">3751</span>
<span class="normal">3752</span>
<span class="normal">3753</span>
<span class="normal">3754</span>
<span class="normal">3755</span>
<span class="normal">3756</span>
<span class="normal">3757</span>
<span class="normal">3758</span>
<span class="normal">3759</span>
<span class="normal">3760</span>
<span class="normal">3761</span>
<span class="normal">3762</span>
<span class="normal">3763</span>
<span class="normal">3764</span>
<span class="normal">3765</span>
<span class="normal">3766</span>
<span class="normal">3767</span>
<span class="normal">3768</span>
<span class="normal">3769</span>
<span class="normal">3770</span>
<span class="normal">3771</span>
<span class="normal">3772</span>
<span class="normal">3773</span>
<span class="normal">3774</span>
<span class="normal">3775</span>
<span class="normal">3776</span>
<span class="normal">3777</span>
<span class="normal">3778</span>
<span class="normal">3779</span>
<span class="normal">3780</span>
<span class="normal">3781</span>
<span class="normal">3782</span>
<span class="normal">3783</span>
<span class="normal">3784</span>
<span class="normal">3785</span>
<span class="normal">3786</span>
<span class="normal">3787</span>
<span class="normal">3788</span>
<span class="normal">3789</span>
<span class="normal">3790</span>
<span class="normal">3791</span>
<span class="normal">3792</span>
<span class="normal">3793</span>
<span class="normal">3794</span>
<span class="normal">3795</span>
<span class="normal">3796</span>
<span class="normal">3797</span>
<span class="normal">3798</span>
<span class="normal">3799</span>
<span class="normal">3800</span>
<span class="normal">3801</span>
<span class="normal">3802</span>
<span class="normal">3803</span>
<span class="normal">3804</span>
<span class="normal">3805</span>
<span class="normal">3806</span>
<span class="normal">3807</span>
<span class="normal">3808</span>
<span class="normal">3809</span>
<span class="normal">3810</span>
<span class="normal">3811</span>
<span class="normal">3812</span>
<span class="normal">3813</span>
<span class="normal">3814</span>
<span class="normal">3815</span>
<span class="normal">3816</span>
<span class="normal">3817</span>
<span class="normal">3818</span>
<span class="normal">3819</span>
<span class="normal">3820</span>
<span class="normal">3821</span>
<span class="normal">3822</span>
<span class="normal">3823</span>
<span class="normal">3824</span>
<span class="normal">3825</span>
<span class="normal">3826</span>
<span class="normal">3827</span>
<span class="normal">3828</span>
<span class="normal">3829</span>
<span class="normal">3830</span>
<span class="normal">3831</span>
<span class="normal">3832</span>
<span class="normal">3833</span>
<span class="normal">3834</span>
<span class="normal">3835</span>
<span class="normal">3836</span>
<span class="normal">3837</span>
<span class="normal">3838</span>
<span class="normal">3839</span>
<span class="normal">3840</span>
<span class="normal">3841</span>
<span class="normal">3842</span>
<span class="normal">3843</span>
<span class="normal">3844</span>
<span class="normal">3845</span>
<span class="normal">3846</span>
<span class="normal">3847</span>
<span class="normal">3848</span>
<span class="normal">3849</span>
<span class="normal">3850</span>
<span class="normal">3851</span>
<span class="normal">3852</span>
<span class="normal">3853</span>
<span class="normal">3854</span>
<span class="normal">3855</span>
<span class="normal">3856</span>
<span class="normal">3857</span>
<span class="normal">3858</span>
<span class="normal">3859</span>
<span class="normal">3860</span>
<span class="normal">3861</span>
<span class="normal">3862</span>
<span class="normal">3863</span>
<span class="normal">3864</span>
<span class="normal">3865</span>
<span class="normal">3866</span>
<span class="normal">3867</span>
<span class="normal">3868</span>
<span class="normal">3869</span>
<span class="normal">3870</span>
<span class="normal">3871</span>
<span class="normal">3872</span>
<span class="normal">3873</span>
<span class="normal">3874</span>
<span class="normal">3875</span>
<span class="normal">3876</span>
<span class="normal">3877</span>
<span class="normal">3878</span>
<span class="normal">3879</span>
<span class="normal">3880</span>
<span class="normal">3881</span>
<span class="normal">3882</span>
<span class="normal">3883</span>
<span class="normal">3884</span>
<span class="normal">3885</span>
<span class="normal">3886</span>
<span class="normal">3887</span>
<span class="normal">3888</span>
<span class="normal">3889</span>
<span class="normal">3890</span>
<span class="normal">3891</span>
<span class="normal">3892</span>
<span class="normal">3893</span>
<span class="normal">3894</span>
<span class="normal">3895</span>
<span class="normal">3896</span>
<span class="normal">3897</span>
<span class="normal">3898</span>
<span class="normal">3899</span>
<span class="normal">3900</span>
<span class="normal">3901</span>
<span class="normal">3902</span>
<span class="normal">3903</span>
<span class="normal">3904</span>
<span class="normal">3905</span>
<span class="normal">3906</span>
<span class="normal">3907</span>
<span class="normal">3908</span>
<span class="normal">3909</span>
<span class="normal">3910</span>
<span class="normal">3911</span>
<span class="normal">3912</span>
<span class="normal">3913</span>
<span class="normal">3914</span>
<span class="normal">3915</span>
<span class="normal">3916</span>
<span class="normal">3917</span>
<span class="normal">3918</span>
<span class="normal">3919</span>
<span class="normal">3920</span>
<span class="normal">3921</span>
<span class="normal">3922</span>
<span class="normal">3923</span>
<span class="normal">3924</span>
<span class="normal">3925</span>
<span class="normal">3926</span>
<span class="normal">3927</span>
<span class="normal">3928</span>
<span class="normal">3929</span>
<span class="normal">3930</span>
<span class="normal">3931</span>
<span class="normal">3932</span>
<span class="normal">3933</span>
<span class="normal">3934</span>
<span class="normal">3935</span>
<span class="normal">3936</span>
<span class="normal">3937</span>
<span class="normal">3938</span>
<span class="normal">3939</span>
<span class="normal">3940</span>
<span class="normal">3941</span>
<span class="normal">3942</span>
<span class="normal">3943</span>
<span class="normal">3944</span>
<span class="normal">3945</span>
<span class="normal">3946</span>
<span class="normal">3947</span>
<span class="normal">3948</span>
<span class="normal">3949</span>
<span class="normal">3950</span>
<span class="normal">3951</span>
<span class="normal">3952</span>
<span class="normal">3953</span>
<span class="normal">3954</span>
<span class="normal">3955</span>
<span class="normal">3956</span>
<span class="normal">3957</span>
<span class="normal">3958</span>
<span class="normal">3959</span>
<span class="normal">3960</span>
<span class="normal">3961</span>
<span class="normal">3962</span>
<span class="normal">3963</span>
<span class="normal">3964</span>
<span class="normal">3965</span>
<span class="normal">3966</span>
<span class="normal">3967</span>
<span class="normal">3968</span>
<span class="normal">3969</span>
<span class="normal">3970</span>
<span class="normal">3971</span>
<span class="normal">3972</span>
<span class="normal">3973</span>
<span class="normal">3974</span>
<span class="normal">3975</span>
<span class="normal">3976</span>
<span class="normal">3977</span>
<span class="normal">3978</span>
<span class="normal">3979</span>
<span class="normal">3980</span>
<span class="normal">3981</span>
<span class="normal">3982</span>
<span class="normal">3983</span>
<span class="normal">3984</span>
<span class="normal">3985</span>
<span class="normal">3986</span>
<span class="normal">3987</span>
<span class="normal">3988</span>
<span class="normal">3989</span>
<span class="normal">3990</span>
<span class="normal">3991</span>
<span class="normal">3992</span>
<span class="normal">3993</span>
<span class="normal">3994</span>
<span class="normal">3995</span>
<span class="normal">3996</span>
<span class="normal">3997</span>
<span class="normal">3998</span>
<span class="normal">3999</span>
<span class="normal">4000</span>
<span class="normal">4001</span>
<span class="normal">4002</span>
<span class="normal">4003</span>
<span class="normal">4004</span>
<span class="normal">4005</span>
<span class="normal">4006</span>
<span class="normal">4007</span>
<span class="normal">4008</span>
<span class="normal">4009</span>
<span class="normal">4010</span>
<span class="normal">4011</span>
<span class="normal">4012</span>
<span class="normal">4013</span>
<span class="normal">4014</span>
<span class="normal">4015</span>
<span class="normal">4016</span>
<span class="normal">4017</span>
<span class="normal">4018</span>
<span class="normal">4019</span>
<span class="normal">4020</span>
<span class="normal">4021</span>
<span class="normal">4022</span>
<span class="normal">4023</span>
<span class="normal">4024</span>
<span class="normal">4025</span>
<span class="normal">4026</span>
<span class="normal">4027</span>
<span class="normal">4028</span>
<span class="normal">4029</span>
<span class="normal">4030</span>
<span class="normal">4031</span>
<span class="normal">4032</span>
<span class="normal">4033</span>
<span class="normal">4034</span>
<span class="normal">4035</span>
<span class="normal">4036</span>
<span class="normal">4037</span>
<span class="normal">4038</span>
<span class="normal">4039</span>
<span class="normal">4040</span>
<span class="normal">4041</span>
<span class="normal">4042</span>
<span class="normal">4043</span>
<span class="normal">4044</span>
<span class="normal">4045</span>
<span class="normal">4046</span>
<span class="normal">4047</span>
<span class="normal">4048</span>
<span class="normal">4049</span>
<span class="normal">4050</span>
<span class="normal">4051</span>
<span class="normal">4052</span>
<span class="normal">4053</span>
<span class="normal">4054</span>
<span class="normal">4055</span>
<span class="normal">4056</span>
<span class="normal">4057</span>
<span class="normal">4058</span>
<span class="normal">4059</span>
<span class="normal">4060</span>
<span class="normal">4061</span>
<span class="normal">4062</span>
<span class="normal">4063</span>
<span class="normal">4064</span>
<span class="normal">4065</span>
<span class="normal">4066</span>
<span class="normal">4067</span>
<span class="normal">4068</span>
<span class="normal">4069</span>
<span class="normal">4070</span>
<span class="normal">4071</span>
<span class="normal">4072</span>
<span class="normal">4073</span>
<span class="normal">4074</span>
<span class="normal">4075</span>
<span class="normal">4076</span>
<span class="normal">4077</span>
<span class="normal">4078</span>
<span class="normal">4079</span>
<span class="normal">4080</span>
<span class="normal">4081</span>
<span class="normal">4082</span>
<span class="normal">4083</span>
<span class="normal">4084</span>
<span class="normal">4085</span>
<span class="normal">4086</span>
<span class="normal">4087</span>
<span class="normal">4088</span>
<span class="normal">4089</span>
<span class="normal">4090</span>
<span class="normal">4091</span>
<span class="normal">4092</span>
<span class="normal">4093</span>
<span class="normal">4094</span>
<span class="normal">4095</span>
<span class="normal">4096</span>
<span class="normal">4097</span>
<span class="normal">4098</span>
<span class="normal">4099</span>
<span class="normal">4100</span>
<span class="normal">4101</span>
<span class="normal">4102</span>
<span class="normal">4103</span>
<span class="normal">4104</span>
<span class="normal">4105</span>
<span class="normal">4106</span>
<span class="normal">4107</span>
<span class="normal">4108</span>
<span class="normal">4109</span>
<span class="normal">4110</span>
<span class="normal">4111</span>
<span class="normal">4112</span>
<span class="normal">4113</span>
<span class="normal">4114</span>
<span class="normal">4115</span>
<span class="normal">4116</span>
<span class="normal">4117</span>
<span class="normal">4118</span>
<span class="normal">4119</span>
<span class="normal">4120</span>
<span class="normal">4121</span>
<span class="normal">4122</span>
<span class="normal">4123</span>
<span class="normal">4124</span>
<span class="normal">4125</span>
<span class="normal">4126</span>
<span class="normal">4127</span>
<span class="normal">4128</span>
<span class="normal">4129</span>
<span class="normal">4130</span>
<span class="normal">4131</span>
<span class="normal">4132</span>
<span class="normal">4133</span>
<span class="normal">4134</span>
<span class="normal">4135</span>
<span class="normal">4136</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TModel</span><span class="p">(</span><span class="n">SeamlessM4TPreTrainedModel</span><span class="p">):</span>
    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_encoder.embed_tokens.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_decoder.embed_tokens.weight&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">current_modality</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4TEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4TSpeechEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span> <span class="o">=</span> <span class="n">SeamlessM4TDecoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">=</span> <span class="n">current_modality</span>
        <span class="k">if</span> <span class="n">current_modality</span> <span class="o">==</span> <span class="s2">&quot;speech&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_features&quot;</span>

        <span class="c1"># these models already call post_init in their initialization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span> <span class="o">=</span> <span class="n">SeamlessM4TTextToUnitForConditionalGeneration</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span> <span class="o">=</span> <span class="n">SeamlessM4TCodeHifiGan</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_modality</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">modality</span> <span class="o">==</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_ids&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">=</span> <span class="s2">&quot;text&quot;</span>
        <span class="k">elif</span> <span class="n">modality</span> <span class="o">==</span> <span class="s2">&quot;speech&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_features&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">=</span> <span class="s2">&quot;speech&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`modality=</span><span class="si">{</span><span class="n">modality</span><span class="si">}</span><span class="s2">` is not a valid modality. It must be `text` or `speech`.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">==</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Seq2SeqLMOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The `use_cache` argument is changed to `False` since `labels` is provided.&quot;</span><span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span>
                    <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`input_ids`,`input_features`, `inputs_embeds` and `encoder_outputs` are all empty. Make sure at least one of them is not.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;`input_ids` is not `None` but `input_features` has been given.&quot;</span>
                    <span class="s2">&quot;`input_features` will be used in priority through the `speech_encoder`. &quot;</span>
                    <span class="s2">&quot;Make sure that `input_features` and `input_ids` are mutually exclusive.&quot;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;`inputs_embeds` is not `None` but `input_features` has been given.&quot;</span>
                    <span class="s2">&quot;`input_features` will be used in priority through `speech_encoder`. &quot;</span>
                    <span class="s2">&quot;`inputs_embeds` will be ignored.&quot;</span>
                <span class="p">)</span>

            <span class="c1"># if encoder_outputs is not None, it&#39;s probably used within a .generate method so no need to warn</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This calls the same method `forward` as `SeamlessM4TForTextToText` and `SeamlessM4TForSpeechToText`&quot;</span>
                <span class="s2">&quot;depending on the input modality. If you want to generate speech, use the `generate` method.&quot;</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;speech&quot;</span><span class="p">)</span>

            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span>
                <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if encoder_outputs is not None, it&#39;s probably used within a .generate method so no need to warn</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This calls the same method `forward` as `SeamlessM4TForTextToText` and `SeamlessM4TForSpeechToText`&quot;</span>
                <span class="s2">&quot;depending on the input modality. If you want to generate speech, use the `generate` method.&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True</span>
        <span class="k">elif</span> <span class="n">return_dict</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">):</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
                <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>
        <span class="c1"># input modality = speech so new attention mask</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">==</span> <span class="s2">&quot;speech&quot;</span> <span class="ow">and</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
            <span class="p">)</span>

        <span class="c1"># decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">decoder_inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">decoder_outputs</span> <span class="o">+</span> <span class="n">encoder_outputs</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">spkr_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">generate_speech</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates translated token ids and/or translated audio waveforms.</span>

<span class="sd">        &lt;Tip&gt;</span>

<span class="sd">        This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">        arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">        that will be passed to one of them.</span>

<span class="sd">        For example, calling `.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)` will successively</span>
<span class="sd">        perform beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">        For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">        guide](./generation_strategies).</span>

<span class="sd">        &lt;/Tip&gt;</span>


<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">                Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">                [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`, *optional*):</span>
<span class="sd">                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">                [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>
<span class="sd">            return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">                If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">                to get translated text alongside the audio. Note that if `generate_speech=True`, this parameter will be</span>
<span class="sd">                ignored.</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language to use as target language for translation.</span>
<span class="sd">            spkr_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">            generate_speech (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                If `False`, will only returns the text tokens and won&#39;t generate speech.</span>

<span class="sd">            kwargs (*optional*):</span>
<span class="sd">                Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">                arguments are of two types:</span>

<span class="sd">                    - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                    except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                    - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                    text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                    This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                    other.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `Union[SeamlessM4TGenerationOutput, Tuple[Tensor], ModelOutput]`:</span>
<span class="sd">            - If `generate_speech` and `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].</span>
<span class="sd">            - If `generate_speech` and not `return_intermediate_token_ids`, returns a tuple composed of waveforms of</span>
<span class="sd">              shape `(batch_size, sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">            - If `generate_speech=False`, it will returns `ModelOutput`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`input_ids`,`input_features` and `inputs_embeds` are all empty. Make sure at least one of them is not.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">generate_speech</span> <span class="ow">and</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># also accept __xxx__</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]:</span>
                <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                        to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                    Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4T supports</span>
<span class="s2">                    more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                    <span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">)))</span>
        <span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
        <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># tgt_lang gets priority over decoder input ids</span>
            <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
            <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

        <span class="c1"># first generation</span>
        <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;speech&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;`input_features` and `input_ids` are both non empty. `input_features` will be used in priority &quot;</span>
                    <span class="s2">&quot;through the speech encoder. Make sure `input_features=None` if you want to use the text encoder.&quot;</span>
                <span class="p">)</span>
            <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>
            <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
        <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">generate_speech</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">text_generation_output</span>

        <span class="c1"># prepare second generation</span>
        <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

        <span class="c1"># get encoder last hidden states</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">==</span> <span class="s2">&quot;speech&quot;</span><span class="p">:</span>
            <span class="c1"># get last_hidden_state from encoder - must do a pass through the speech encoder</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span>
                <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span>
            <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

            <span class="c1"># input modality = speech so new attention mask for the decoder</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">encoder_hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># take care of num_return_sequences</span>
        <span class="c1"># take most probable hidden states per batch of return_sequences</span>
        <span class="c1"># (batch_size*num_return_sequences, ...) -&gt; (batch_size,...)</span>
        <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">idx_most_probable_sequences_per_batch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">+</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
            <span class="p">)</span>
            <span class="n">sequences</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="n">idx_most_probable_sequences_per_batch</span><span class="p">]</span>

        <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
        <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

        <span class="c1"># Compute new attention mask</span>
        <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
        <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

        <span class="c1"># Compute t2u decoder_input_ids</span>
        <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
        <span class="n">t2u_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">t2u_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">,</span> <span class="n">t2u_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_decoder_input_ids</span>

        <span class="c1"># second generation</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">)</span>
        <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># get rid of t2u_decoder_input_ids</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="p">[:,</span> <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:]</span>
        <span class="c1"># replace eos per pad</span>
        <span class="n">unit_ids</span><span class="p">[</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span>
        <span class="c1"># offset of control symbols</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
        <span class="p">)</span>

        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">spkr_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">spkr_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">spkr_id</span><span class="o">=</span><span class="n">spkr_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">(</span>
                <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
                <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
                <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
                <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># cut decoder_input_ids if past is used</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># encoder_outputs is defined. input_ids not needed</span>
            <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">:</span> <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="c1"># cached cross_attention states don&#39;t have to be reordered -&gt; they are always the same</span>
            <span class="n">reordered_past</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">past_state</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">past_state</span> <span class="ow">in</span> <span class="n">layer_past</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">reordered_past</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TModel</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_intermediate_token_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">spkr_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">generate_speech</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates translated token ids and/or translated audio waveforms.</p>
<p><Tip></p>
<p>This method successively calls the <code>.generate</code> function of two different sub-models. You can specify keyword
arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments
that will be passed to one of them.</p>
<p>For example, calling <code>.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)</code> will successively
perform beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</p>
<p>For an overview of generation strategies and code examples, check out the <a href="./generation_strategies">following
guide</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<code>SeamlessM4TTokenizer</code>] or [<code>SeamlessM4TProcessor</code>]. See
[<code>PreTrainedTokenizer.encode</code>] and [<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_features</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input audio features. This should be returnes by the [<code>SeamlessM4TFeatureExtractor</code>] class or the
[<code>SeamlessM4TProcessor</code>] class. See [<code>SeamlessM4TFeatureExtractor.__call__</code>] for details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_intermediate_token_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If <code>True</code>, also returns the intermediate generated text and unit tokens. Set to <code>True</code> if you also want
to get translated text alongside the audio. Note that if <code>generate_speech=True</code>, this parameter will be
ignored.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>spkr_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the speaker used for speech synthesis. Must be lower than <code>config.vocoder_num_spkrs</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>generate_speech</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If <code>False</code>, will only returns the text tokens and won't generate speech.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Remaining dictionary of keyword arguments that will be passed to [<code>GenerationMixin.generate</code>]. Keyword
arguments are of two types:</p>
<div class="highlight"><pre><span></span><code>- Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,
except for `decoder_input_ids` which will only be passed through the text components.
- With a *text_* or *speech_* prefix, they will be input for the `generate` method of the
text model and speech model respectively. It has the priority over the keywords without a prefix.

This means you can, for example, specify a generation strategy for one generation but not for the
other.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput">SeamlessM4TGenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>Union[SeamlessM4TGenerationOutput, Tuple[Tensor], ModelOutput]</code>:</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput">SeamlessM4TGenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If <code>generate_speech</code> and <code>return_intermediate_token_ids</code>, returns [<code>SeamlessM4TGenerationOutput</code>].</li>
</ul>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput">SeamlessM4TGenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If <code>generate_speech</code> and not <code>return_intermediate_token_ids</code>, returns a tuple composed of waveforms of
shape <code>(batch_size, sequence_length)</code>and and <code>waveform_lengths</code> which gives the length of each sample.</li>
</ul>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput" href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput">SeamlessM4TGenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If <code>generate_speech=False</code>, it will returns <code>ModelOutput</code>.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3893</span>
<span class="normal">3894</span>
<span class="normal">3895</span>
<span class="normal">3896</span>
<span class="normal">3897</span>
<span class="normal">3898</span>
<span class="normal">3899</span>
<span class="normal">3900</span>
<span class="normal">3901</span>
<span class="normal">3902</span>
<span class="normal">3903</span>
<span class="normal">3904</span>
<span class="normal">3905</span>
<span class="normal">3906</span>
<span class="normal">3907</span>
<span class="normal">3908</span>
<span class="normal">3909</span>
<span class="normal">3910</span>
<span class="normal">3911</span>
<span class="normal">3912</span>
<span class="normal">3913</span>
<span class="normal">3914</span>
<span class="normal">3915</span>
<span class="normal">3916</span>
<span class="normal">3917</span>
<span class="normal">3918</span>
<span class="normal">3919</span>
<span class="normal">3920</span>
<span class="normal">3921</span>
<span class="normal">3922</span>
<span class="normal">3923</span>
<span class="normal">3924</span>
<span class="normal">3925</span>
<span class="normal">3926</span>
<span class="normal">3927</span>
<span class="normal">3928</span>
<span class="normal">3929</span>
<span class="normal">3930</span>
<span class="normal">3931</span>
<span class="normal">3932</span>
<span class="normal">3933</span>
<span class="normal">3934</span>
<span class="normal">3935</span>
<span class="normal">3936</span>
<span class="normal">3937</span>
<span class="normal">3938</span>
<span class="normal">3939</span>
<span class="normal">3940</span>
<span class="normal">3941</span>
<span class="normal">3942</span>
<span class="normal">3943</span>
<span class="normal">3944</span>
<span class="normal">3945</span>
<span class="normal">3946</span>
<span class="normal">3947</span>
<span class="normal">3948</span>
<span class="normal">3949</span>
<span class="normal">3950</span>
<span class="normal">3951</span>
<span class="normal">3952</span>
<span class="normal">3953</span>
<span class="normal">3954</span>
<span class="normal">3955</span>
<span class="normal">3956</span>
<span class="normal">3957</span>
<span class="normal">3958</span>
<span class="normal">3959</span>
<span class="normal">3960</span>
<span class="normal">3961</span>
<span class="normal">3962</span>
<span class="normal">3963</span>
<span class="normal">3964</span>
<span class="normal">3965</span>
<span class="normal">3966</span>
<span class="normal">3967</span>
<span class="normal">3968</span>
<span class="normal">3969</span>
<span class="normal">3970</span>
<span class="normal">3971</span>
<span class="normal">3972</span>
<span class="normal">3973</span>
<span class="normal">3974</span>
<span class="normal">3975</span>
<span class="normal">3976</span>
<span class="normal">3977</span>
<span class="normal">3978</span>
<span class="normal">3979</span>
<span class="normal">3980</span>
<span class="normal">3981</span>
<span class="normal">3982</span>
<span class="normal">3983</span>
<span class="normal">3984</span>
<span class="normal">3985</span>
<span class="normal">3986</span>
<span class="normal">3987</span>
<span class="normal">3988</span>
<span class="normal">3989</span>
<span class="normal">3990</span>
<span class="normal">3991</span>
<span class="normal">3992</span>
<span class="normal">3993</span>
<span class="normal">3994</span>
<span class="normal">3995</span>
<span class="normal">3996</span>
<span class="normal">3997</span>
<span class="normal">3998</span>
<span class="normal">3999</span>
<span class="normal">4000</span>
<span class="normal">4001</span>
<span class="normal">4002</span>
<span class="normal">4003</span>
<span class="normal">4004</span>
<span class="normal">4005</span>
<span class="normal">4006</span>
<span class="normal">4007</span>
<span class="normal">4008</span>
<span class="normal">4009</span>
<span class="normal">4010</span>
<span class="normal">4011</span>
<span class="normal">4012</span>
<span class="normal">4013</span>
<span class="normal">4014</span>
<span class="normal">4015</span>
<span class="normal">4016</span>
<span class="normal">4017</span>
<span class="normal">4018</span>
<span class="normal">4019</span>
<span class="normal">4020</span>
<span class="normal">4021</span>
<span class="normal">4022</span>
<span class="normal">4023</span>
<span class="normal">4024</span>
<span class="normal">4025</span>
<span class="normal">4026</span>
<span class="normal">4027</span>
<span class="normal">4028</span>
<span class="normal">4029</span>
<span class="normal">4030</span>
<span class="normal">4031</span>
<span class="normal">4032</span>
<span class="normal">4033</span>
<span class="normal">4034</span>
<span class="normal">4035</span>
<span class="normal">4036</span>
<span class="normal">4037</span>
<span class="normal">4038</span>
<span class="normal">4039</span>
<span class="normal">4040</span>
<span class="normal">4041</span>
<span class="normal">4042</span>
<span class="normal">4043</span>
<span class="normal">4044</span>
<span class="normal">4045</span>
<span class="normal">4046</span>
<span class="normal">4047</span>
<span class="normal">4048</span>
<span class="normal">4049</span>
<span class="normal">4050</span>
<span class="normal">4051</span>
<span class="normal">4052</span>
<span class="normal">4053</span>
<span class="normal">4054</span>
<span class="normal">4055</span>
<span class="normal">4056</span>
<span class="normal">4057</span>
<span class="normal">4058</span>
<span class="normal">4059</span>
<span class="normal">4060</span>
<span class="normal">4061</span>
<span class="normal">4062</span>
<span class="normal">4063</span>
<span class="normal">4064</span>
<span class="normal">4065</span>
<span class="normal">4066</span>
<span class="normal">4067</span>
<span class="normal">4068</span>
<span class="normal">4069</span>
<span class="normal">4070</span>
<span class="normal">4071</span>
<span class="normal">4072</span>
<span class="normal">4073</span>
<span class="normal">4074</span>
<span class="normal">4075</span>
<span class="normal">4076</span>
<span class="normal">4077</span>
<span class="normal">4078</span>
<span class="normal">4079</span>
<span class="normal">4080</span>
<span class="normal">4081</span>
<span class="normal">4082</span>
<span class="normal">4083</span>
<span class="normal">4084</span>
<span class="normal">4085</span>
<span class="normal">4086</span>
<span class="normal">4087</span>
<span class="normal">4088</span>
<span class="normal">4089</span>
<span class="normal">4090</span>
<span class="normal">4091</span>
<span class="normal">4092</span>
<span class="normal">4093</span>
<span class="normal">4094</span>
<span class="normal">4095</span>
<span class="normal">4096</span>
<span class="normal">4097</span>
<span class="normal">4098</span>
<span class="normal">4099</span>
<span class="normal">4100</span>
<span class="normal">4101</span>
<span class="normal">4102</span>
<span class="normal">4103</span>
<span class="normal">4104</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">spkr_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">generate_speech</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates translated token ids and/or translated audio waveforms.</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">    arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">    that will be passed to one of them.</span>

<span class="sd">    For example, calling `.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)` will successively</span>
<span class="sd">    perform beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">    For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">    guide](./generation_strategies).</span>

<span class="sd">    &lt;/Tip&gt;</span>


<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">            Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`, *optional*):</span>
<span class="sd">            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">            [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>
<span class="sd">        return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">            If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">            to get translated text alongside the audio. Note that if `generate_speech=True`, this parameter will be</span>
<span class="sd">            ignored.</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        spkr_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">        generate_speech (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            If `False`, will only returns the text tokens and won&#39;t generate speech.</span>

<span class="sd">        kwargs (*optional*):</span>
<span class="sd">            Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">            arguments are of two types:</span>

<span class="sd">                - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                other.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `Union[SeamlessM4TGenerationOutput, Tuple[Tensor], ModelOutput]`:</span>
<span class="sd">        - If `generate_speech` and `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].</span>
<span class="sd">        - If `generate_speech` and not `return_intermediate_token_ids`, returns a tuple composed of waveforms of</span>
<span class="sd">          shape `(batch_size, sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">        - If `generate_speech=False`, it will returns `ModelOutput`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;`input_ids`,`input_features` and `inputs_embeds` are all empty. Make sure at least one of them is not.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">generate_speech</span> <span class="ow">and</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># also accept __xxx__</span>
        <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]:</span>
            <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                    to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4T supports</span>
<span class="s2">                more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                <span class="p">)</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">)))</span>
    <span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
    <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># tgt_lang gets priority over decoder input ids</span>
        <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

    <span class="c1"># first generation</span>
    <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;speech&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;`input_features` and `input_ids` are both non empty. `input_features` will be used in priority &quot;</span>
                <span class="s2">&quot;through the speech encoder. Make sure `input_features=None` if you want to use the text encoder.&quot;</span>
            <span class="p">)</span>
        <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>
        <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">generate_speech</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">text_generation_output</span>

    <span class="c1"># prepare second generation</span>
    <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="c1"># get encoder last hidden states</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">==</span> <span class="s2">&quot;speech&quot;</span><span class="p">:</span>
        <span class="c1"># get last_hidden_state from encoder - must do a pass through the speech encoder</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span>
            <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span>
        <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="c1"># input modality = speech so new attention mask for the decoder</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">encoder_hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># take care of num_return_sequences</span>
    <span class="c1"># take most probable hidden states per batch of return_sequences</span>
    <span class="c1"># (batch_size*num_return_sequences, ...) -&gt; (batch_size,...)</span>
    <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="n">idx_most_probable_sequences_per_batch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">idx_most_probable_sequences_per_batch</span> <span class="o">+</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
        <span class="p">)</span>
        <span class="n">sequences</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="n">idx_most_probable_sequences_per_batch</span><span class="p">]</span>

    <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
    <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

    <span class="c1"># Compute new attention mask</span>
    <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
    <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

    <span class="c1"># Compute t2u decoder_input_ids</span>
    <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
    <span class="n">t2u_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">t2u_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">t2u_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">,</span> <span class="n">t2u_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_decoder_input_ids</span>

    <span class="c1"># second generation</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">)</span>
    <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># get rid of t2u_decoder_input_ids</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="p">[:,</span> <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:]</span>
    <span class="c1"># replace eos per pad</span>
    <span class="n">unit_ids</span><span class="p">[</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span>
    <span class="c1"># offset of control symbols</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
    <span class="p">)</span>

    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">spkr_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">spkr_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">spkr_id</span><span class="o">=</span><span class="n">spkr_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SeamlessM4TGenerationOutput</span><span class="p">(</span>
            <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
            <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
            <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
            <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.modeling_utils.PreTrainedModel" href="../../modeling_utils/#mindnlp.transformers.modeling_utils.PreTrainedModel">PreTrainedModel</a></code></p>


        <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
models.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TPreTrainedModel</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained</span>
<span class="sd">    models.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">config_class</span> <span class="o">=</span> <span class="n">SeamlessM4TConfig</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;seamless_m4t&quot;</span>
    <span class="n">supports_gradient_checkpointing</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;SeamlessM4TEncoderLayer&quot;</span><span class="p">,</span> <span class="s2">&quot;SeamlessM4TDecoderLayer&quot;</span><span class="p">,</span> <span class="s2">&quot;SeamlessM4TConformerEncoderLayer&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the weights&quot;&quot;&quot;</span>
        <span class="n">std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">module</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">SeamlessM4TConformerSelfAttention</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;pos_bias_u&quot;</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">pos_bias_u</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;pos_bias_v&quot;</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">pos_bias_v</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">SeamlessM4TConformerPositionalConvEmbedding</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span>
                <span class="n">module</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">std</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">module</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">in_channels</span><span class="p">)),</span>
            <span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">SeamlessM4TConformerFeatureProjection</span><span class="p">):</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">module</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">in_features</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="n">k</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="n">k</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">)):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">ones_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">k</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">groups</span> <span class="o">/</span> <span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">*</span> <span class="n">module</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="n">k</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
        <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">adaptor_kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">adaptor_stride</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">seq_lens</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">int</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">((</span><span class="n">seq_lens</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">seq_lens</span><span class="o">.</span><span class="n">floor</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">compute_last_hidden_states_per_sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">beam_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the last hidden states.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            hidden_states (`Tuple[Tuple[mindspore.Tensor]]`):</span>
<span class="sd">                The generated hidden states. Tuple (one element for each generated token) of tuples (one element for</span>
<span class="sd">                each layer of the decoder) of mindspore.Tensor of shape (batch_size*num_beams*num_return_sequences,</span>
<span class="sd">                generated_length, hidden_size).</span>
<span class="sd">            beam_indices (`mindspore.Tensor`, *optional*):</span>
<span class="sd">                Beam indices of generated token id at each generation step. `mindspore.Tensor` of shape</span>
<span class="sd">                `(batch_size*num_return_sequences, sequence_length)`. Only required if a `num_beams&gt;1` at</span>
<span class="sd">                generate-time.</span>

<span class="sd">        Return:</span>
<span class="sd">            `mindspore.Tensor`: A `mindspore.Tensor` of shape `(batch_size*num_return_sequences, sequence_length, hidden_size)`</span>
<span class="sd">            containing</span>
<span class="sd">                the last hidden states.</span>
<span class="sd">        ```&quot;&quot;&quot;</span>
        <span class="c1"># 1. First, let&#39;s compute last_hidden_states from hidden_states.</span>
        <span class="c1"># For each generation step, takes the hidden state from the last layer.</span>
        <span class="c1"># shape: (batch_size*vocab_size*num_return_sequences, # generation_steps, hidden_dim)</span>
        <span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">hidden_states</span> <span class="ow">in</span> <span class="n">hidden_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 2. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent</span>
        <span class="c1"># to a beam search approach were the first (and only) beam is always selected</span>
        <span class="c1"># in that case, return directly last_hidden_states</span>
        <span class="k">if</span> <span class="n">beam_indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">last_hidden_states</span>

        <span class="c1"># 3. cut beam_indices to longest beam length</span>
        <span class="n">beam_indices_mask</span> <span class="o">=</span> <span class="n">beam_indices</span> <span class="o">&lt;</span> <span class="mi">0</span>
        <span class="n">max_beam_length</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beam_indices_mask</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">beam_indices</span><span class="o">.</span><span class="n">copy</span><span class="p">()[:,</span> <span class="p">:</span><span class="n">max_beam_length</span><span class="p">]</span>
        <span class="n">beam_indices_mask</span> <span class="o">=</span> <span class="n">beam_indices_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">max_beam_length</span><span class="p">]</span>

        <span class="c1"># 4. Set indices of beams that finished early to 0; such indices will be masked correctly afterwards anyways</span>
        <span class="n">beam_indices</span><span class="p">[</span><span class="n">beam_indices_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># 5. broadcast_to beam_indices to last_hidden_states dim</span>
        <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">beam_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">beam_indices</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">last_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="c1"># 6. select the right candidate for each beam</span>
        <span class="c1"># in other words, new_last_hidden_states[i,j,k] = last_hidden_states[beam_indices[i,j,k], j, k] for all i, j, k</span>
        <span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">last_hidden_states</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">beam_indices</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">last_hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel.compute_last_hidden_states_per_sample" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TPreTrainedModel</span><span class="o">.</span><span class="n">compute_last_hidden_states_per_sample</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">beam_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel.compute_last_hidden_states_per_sample" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Computes the last hidden states.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generated hidden states. Tuple (one element for each generated token) of tuples (one element for
each layer of the decoder) of mindspore.Tensor of shape (batch_size*num_beams*num_return_sequences,
generated_length, hidden_size).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[Tuple[mindspore.Tensor]]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>beam_indices</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Beam indices of generated token id at each generation step. <code>mindspore.Tensor</code> of shape
<code>(batch_size*num_return_sequences, sequence_length)</code>. Only required if a <code>num_beams&gt;1</code> at
generate-time.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p><code>mindspore.Tensor</code>: A <code>mindspore.Tensor</code> of shape <code>(batch_size*num_return_sequences, sequence_length, hidden_size)</code>
containing
    the last hidden states.</p>
</details>        <p>```</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">compute_last_hidden_states_per_sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">beam_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the last hidden states.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        hidden_states (`Tuple[Tuple[mindspore.Tensor]]`):</span>
<span class="sd">            The generated hidden states. Tuple (one element for each generated token) of tuples (one element for</span>
<span class="sd">            each layer of the decoder) of mindspore.Tensor of shape (batch_size*num_beams*num_return_sequences,</span>
<span class="sd">            generated_length, hidden_size).</span>
<span class="sd">        beam_indices (`mindspore.Tensor`, *optional*):</span>
<span class="sd">            Beam indices of generated token id at each generation step. `mindspore.Tensor` of shape</span>
<span class="sd">            `(batch_size*num_return_sequences, sequence_length)`. Only required if a `num_beams&gt;1` at</span>
<span class="sd">            generate-time.</span>

<span class="sd">    Return:</span>
<span class="sd">        `mindspore.Tensor`: A `mindspore.Tensor` of shape `(batch_size*num_return_sequences, sequence_length, hidden_size)`</span>
<span class="sd">        containing</span>
<span class="sd">            the last hidden states.</span>
<span class="sd">    ```&quot;&quot;&quot;</span>
    <span class="c1"># 1. First, let&#39;s compute last_hidden_states from hidden_states.</span>
    <span class="c1"># For each generation step, takes the hidden state from the last layer.</span>
    <span class="c1"># shape: (batch_size*vocab_size*num_return_sequences, # generation_steps, hidden_dim)</span>
    <span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">hidden_states</span> <span class="ow">in</span> <span class="n">hidden_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 2. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent</span>
    <span class="c1"># to a beam search approach were the first (and only) beam is always selected</span>
    <span class="c1"># in that case, return directly last_hidden_states</span>
    <span class="k">if</span> <span class="n">beam_indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">last_hidden_states</span>

    <span class="c1"># 3. cut beam_indices to longest beam length</span>
    <span class="n">beam_indices_mask</span> <span class="o">=</span> <span class="n">beam_indices</span> <span class="o">&lt;</span> <span class="mi">0</span>
    <span class="n">max_beam_length</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beam_indices_mask</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">beam_indices</span><span class="o">.</span><span class="n">copy</span><span class="p">()[:,</span> <span class="p">:</span><span class="n">max_beam_length</span><span class="p">]</span>
    <span class="n">beam_indices_mask</span> <span class="o">=</span> <span class="n">beam_indices_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">max_beam_length</span><span class="p">]</span>

    <span class="c1"># 4. Set indices of beams that finished early to 0; such indices will be masked correctly afterwards anyways</span>
    <span class="n">beam_indices</span><span class="p">[</span><span class="n">beam_indices_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># 5. broadcast_to beam_indices to last_hidden_states dim</span>
    <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">beam_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">beam_indices</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">last_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="c1"># 6. select the right candidate for each beam</span>
    <span class="c1"># in other words, new_last_hidden_states[i,j,k] = last_hidden_states[beam_indices[i,j,k], j, k] for all i, j, k</span>
    <span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">last_hidden_states</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">beam_indices</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">last_hidden_states</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TScaledWordEmbedding" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TScaledWordEmbedding</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TScaledWordEmbedding" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Embedding">Embedding</span></code></p>


        <p>This module overrides nn.Embeddings' forward by multiplying with embeddings scale.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TScaledWordEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This module overrides nn.Embeddings&#39; forward by multiplying with embeddings scale.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embed_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span> <span class="o">=</span> <span class="n">embed_scale</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding</code>


<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This module produces sinusoidal positional embeddings of any length.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TSinusoidalPositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This module produces sinusoidal positional embeddings of any length.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_positions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">offset</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">make_weights</span><span class="p">(</span><span class="n">num_positions</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">emb_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;weights&quot;</span><span class="p">):</span>
            <span class="c1"># in forward put the weights on the correct dtype and device of the param</span>
            <span class="n">emb_weights</span> <span class="o">=</span> <span class="n">emb_weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;weights&quot;</span><span class="p">,</span> <span class="n">emb_weights</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build sinusoidal embeddings.</span>

<span class="sd">        This matches the implementation in tensor2tensor, but differs slightly from the description in Section 3.5 of</span>
<span class="sd">        &quot;Attention Is All You Need&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">half_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="o">-</span><span class="n">emb</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">ops</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">emb</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">emb</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># zero pad</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">emb</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">emb</span><span class="p">[</span><span class="n">padding_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="n">emb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_default_dtype</span><span class="p">())</span>

    <span class="nd">@no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
            <span class="c1"># Create the position ids from the input token ids. Any padded tokens remain padded.</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">create_position_ids_from_input_ids</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_position_ids_from_inputs_embeds</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">)</span>

        <span class="c1"># expand embeddings if needed</span>
        <span class="n">max_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">past_key_values_length</span>
        <span class="k">if</span> <span class="n">max_pos</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_weights</span><span class="p">(</span><span class="n">max_pos</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">create_position_ids_from_inputs_embeds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs_embeds: mindspore.Tensor</span>

<span class="sd">        Returns: mindspore.Tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">+</span> <span class="n">past_key_values_length</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TSinusoidalPositionalEmbedding</span><span class="o">.</span><span class="n">create_position_ids_from_inputs_embeds</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>inputs_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_position_ids_from_inputs_embeds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs_embeds: mindspore.Tensor</span>

<span class="sd">    Returns: mindspore.Tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">position_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">+</span> <span class="n">past_key_values_length</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding.get_embedding" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TSinusoidalPositionalEmbedding</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSinusoidalPositionalEmbedding.get_embedding" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Build sinusoidal embeddings.</p>
<p>This matches the implementation in tensor2tensor, but differs slightly from the description in Section 3.5 of
"Attention Is All You Need".</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">get_embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build sinusoidal embeddings.</span>

<span class="sd">    This matches the implementation in tensor2tensor, but differs slightly from the description in Section 3.5 of</span>
<span class="sd">    &quot;Attention Is All You Need&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">half_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="o">-</span><span class="n">emb</span><span class="p">)</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">ops</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">emb</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">emb</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">embedding_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># zero pad</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">emb</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">emb</span><span class="p">[</span><span class="n">padding_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">return</span> <span class="n">emb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_default_dtype</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.create_position_ids_from_input_ids" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">create_position_ids_from_input_ids</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.create_position_ids_from_input_ids" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols
are ignored. This is modified from fairseq's <code>utils.make_positions</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor x:</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_position_ids_from_input_ids</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols</span>
<span class="sd">    are ignored. This is modified from fairseq&#39;s `utils.make_positions`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: mindspore.Tensor x:</span>

<span class="sd">    Returns: mindspore.Tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">padding_idx</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="n">incremental_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span> <span class="o">+</span> <span class="n">past_key_values_length</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="n">incremental_indices</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="o">+</span> <span class="n">padding_idx</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.format_speech_generation_kwargs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.format_speech_generation_kwargs" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Format kwargs for SeamlessM4T models that generate speech, attribute kwargs to either the text generation or the
speech generation models.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Keyword arguments are of two types:</p>
<div class="highlight"><pre><span></span><code>- Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,
except for `decoder_input_ids` which will only be passed through the text components.
- With a *text_* or *speech_* prefix, they will be input for the `generate` method of the
text model and speech model respectively. It has the priority over the keywords without a prefix.

This means you can, for example, specify a generation strategy for one generation but not for the
other.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`)`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Format kwargs for SeamlessM4T models that generate speech, attribute kwargs to either the text generation or the</span>
<span class="sd">    speech generation models.</span>

<span class="sd">    Args:</span>
<span class="sd">        kwargs (`dict`)`:</span>
<span class="sd">             Keyword arguments are of two types:</span>

<span class="sd">                - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                other.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># attribute kwargs to models</span>
    <span class="n">kwargs_text</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;text_&quot;</span><span class="p">):</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;text_&quot;</span><span class="p">)</span> <span class="p">:]</span>
            <span class="n">kwargs_text</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">elif</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;speech_&quot;</span><span class="p">):</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;speech_&quot;</span><span class="p">)</span> <span class="p">:]</span>
            <span class="n">kwargs_speech</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If the key is already in a specific config, then it&#39;s been set with a</span>
            <span class="c1"># submodules specific value and we don&#39;t override</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs_text</span><span class="p">:</span>
                <span class="n">kwargs_text</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs_speech</span><span class="p">:</span>
                <span class="n">kwargs_speech</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.shift_tokens_right" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">modeling_seamless_m4t</span><span class="o">.</span><span class="n">shift_tokens_right</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">,</span> <span class="n">decoder_start_token_id</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.modeling_seamless_m4t.shift_tokens_right" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Shift input ids one token to the right.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\modeling_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">shift_tokens_right</span><span class="p">(</span><span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">decoder_start_token_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shift input ids one token to the right.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shifted_input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">shifted_input_ids</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">shifted_input_ids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoder_start_token_id</span>

    <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;self.model.config.pad_token_id has to be defined.&quot;</span><span class="p">)</span>
    <span class="c1"># replace possible -100 values in labels by `pad_token_id`</span>
    <span class="n">shifted_input_ids</span> <span class="o">=</span> <span class="n">shifted_input_ids</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">shifted_input_ids</span> <span class="o">==</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">shifted_input_ids</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t</code>


<a href="#mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>SeamlessM4T model configuration</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t.SeamlessM4TConfig" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t.SeamlessM4TConfig</code>


<a href="#mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t.SeamlessM4TConfig" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.configuration_utils.PretrainedConfig" href="../../configuration_utils/#mindnlp.transformers.configuration_utils.PretrainedConfig">PretrainedConfig</a></code></p>


        <p>This is the configuration class to store the configuration of a [<code>~SeamlessM4TModel</code>]. It is used to instantiate an
SeamlessM4T model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the SeamlessM4T
<a href="https://hf-mirror.com/" title="facebook/hf-seamless-m4t-medium">"facebook/hf-seamless-m4t-medium"</a> architecture.</p>
<p>Configuration objects inherit from [<code>PretrainedConfig</code>] and can be used to control the model outputs. Read the
documentation from [<code>PretrainedConfig</code>] for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vocabulary size of the SeamlessM4T model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling [<code>~SeamlessM4TModel</code>], [<code>~SeamlessM4TForTextToSpeech</code>] or
[<code>~SeamlessM4TForTextToText</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 256102</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256102</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Unit vocabulary size of the SeamlessM4T model. Defines the number of different unit tokens that can be
represented by the <code>inputs_ids</code> passed when calling the Text-To-Units sub-model of [<code>~SeamlessM4TModel</code>],
[<code>~SeamlessM4TForSpeechToSpeech</code>] or [<code>~SeamlessM4TForTextToSpeech</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 10082</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10082</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>Parameters</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>args below are Parameters shared across sub-models</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>shared across sub-models</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimensionality of the "intermediate" layers in the architecture.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1024</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1024</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>initializer_range</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.02</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.02</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_norm_eps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The epsilon used by the layer normalization layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 1e-05</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not the model should return the last key/values attentions (not used by all models).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum sequence length that this model text encoder and decoder might ever be used with. Typically set
this to something large just in case (e.g., 512 or 1024 or 2048).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1024</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1024</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_encoder_decoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the model is used as an encoder/decoder or not.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The LayerDrop probability for the encoders. See the <a href="see https://arxiv.org/abs/1909.11556">LayerDrop paper</a>
for more details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.05</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The LayerDrop probability for the decoders. See the <a href="see https://arxiv.org/abs/1909.11556">LayerDrop paper</a>
for more details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.05</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>activation_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The non-linear activation function (function or string) in the decoder and feed-forward layers. If string,
<code>"gelu"</code>, <code>"relu"</code>, <code>"selu"</code>, <code>"swish"</code> and <code>"gelu_new"</code> are supported.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `function`, *optional*, defaults to `&#34;relu&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;relu&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for all fully connected layers in the embeddings, encoder, decoder, and pooler.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for all attention layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>activation_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for all activation layers in the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scale_embedding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Scale embeddings by diving by sqrt(d_model).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>Text</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>args below are text decoder specific parameters</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>encoder and text decoder specific parameters</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the Transformer text encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 24</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer text encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8192</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in the Transformer text encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the Transformer text decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 24</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer text decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8192</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in the Transformer text decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_start_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token. Only
applied in the text decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 3</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_new_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum numbers of text tokens to generate, ignoring the number of tokens in the prompt.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 256</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>padding</em> text token. Only applied to the text-decoder model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>beginning-of-stream</em> text token. Only applied to the text-decoder model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 2</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>end-of-stream</em> text token. Only applied to the text-decoder model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 3</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>Speech</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>args below are Speech encoder specific parameters</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>encoder specific parameters</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the Transformer speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 24</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in the Transformer speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_intermediate_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 4096</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_hidden_act</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The non-linear activation function (function or string) in the speech encoder. If string, <code>"gelu"</code>,
<code>"relu"</code>, <code>"selu"</code>, <code>"swish"</code> and <code>"gelu_new"</code> are supported.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `function`, *optional*, defaults to `&#34;swish&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;swish&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for all layers in the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_adapter</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Add an adapter layer on top of the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The LayerDrop probability for the speech encoder. See the <a href="see
https://arxiv.org/abs/1909.11556">LayerDrop paper</a> for more details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>feature_projection_input_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input dimension of the input feature projection of the speech encoder, i.e the dimension after processing
input audios with [<code>SeamlessM4TFeatureExtractor</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 160</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>160</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_conv_pos_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of convolutional positional embeddings. Defines the kernel size of 1D convolutional positional
embeddings layer of the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 128</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>128</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_conv_pos_embedding_groups</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of groups of 1D convolutional positional embeddings layer of the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Kernel size of the convolutional layers in the adapter network. Only relevant if <code>add_adapter is True</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Stride of the convolutional layers in the adapter network. Only relevant if <code>add_adapter is True</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for all layers in the speech adapter.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_adapter_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of convolutional layers that should be used in the adapter network. Only relevant if <code>add_adapter is
True</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_embeddings_type</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be specified to <code>relative</code> or <code>rotary</code> for relative or rotary position embeddings respectively. If left
<code>None</code> no relative position embedding is applied. Only applied to the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;relative&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;relative&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rotary_embedding_base</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If <code>"rotary"</code> position embeddings are used, defines the size of the embedding base. Only applied to the
speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 10000</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_source_positions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>if <code>"relative"</code> position embeddings are used, defines the maximum source input positions. Only applied to
the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 4096</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>conv_depthwise_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Kernel size of convolutional depthwise 1D layer in Conformer blocks. Only applied to the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 31</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>31</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>Text-To-Unit</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>args below are Text-To-Unit (t2u) model specific parameters</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>t2u) model specific parameters</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>beginning-of-stream</em> unit token. Only applied to the text-to-unit seq2seq model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>padding</em> unit token. Only applied to the text-to-unit seq2seq model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>end-of-stream</em> unit token. Only applied to the text-to-unit seq2seq model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 2</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_start_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token. Only
applied to the text-to-unit seq2seq model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 2</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_max_new_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum numbers of unit tokens to generate, ignoring the number of tokens in the prompt. Only applied
to the text-to-unit seq2seq model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1024</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1024</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the Transformer text-to-unit encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 6</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>6</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer text-to-unit encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8192</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in the Transformer text-to-unit encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the Transformer text-to-unit decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 6</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>6</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer text-to-unit decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8192</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in the Transformer text-to-unit decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum sequence length that this model text-to-unit component might ever be used with. Typically set
this to something large just in case (e.g., 512 or 1024 or 2048).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 2048</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2048</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>Hifi-Gan</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>args below are Hifi-Gan Vocoder specific parameters</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Vocoder specific parameters</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sampling_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sampling rate at which the output audio will be generated, expressed in hertz (Hz).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16000</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_initial_channel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of input channels into the hifi-gan upsampling network. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 512</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>512</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_rates</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tuple of integers defining the stride of each 1D convolutional layer in the vocoder upsampling network.
The length of <em>upsample_rates</em> defines the number of convolutional layers and has to match the length of
<em>upsample_kernel_sizes</em>. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[int]` or `List[int]`, *optional*, defaults to `[5, 4, 4, 2, 2]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[5, 4, 4, 2, 2]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_kernel_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tuple of integers defining the kernel size of each 1D convolutional layer in the vocoder upsampling
network. The length of <em>upsample_kernel_sizes</em> defines the number of convolutional layers and has to match
the length of <em>upsample_rates</em>. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[int]` or `List[int]`, *optional*, defaults to `[11, 8, 8, 4, 4]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[11, 8, 8, 4, 4]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resblock_kernel_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tuple of integers defining the kernel sizes of the vocoder 1D convolutional layers in the multi-receptive
field fusion (MRF) module. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[int]` or `List[int]`, *optional*, defaults to `[3, 7, 11]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[3, 7, 11]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resblock_dilation_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A nested tuple of integers defining the dilation rates of the vocoder dilated 1D convolutional layers in
the multi-receptive field fusion (MRF) module. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[Tuple[int]]` or `List[List[int]]`, *optional*, defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[[1, 3, 5], [1, 3, 5], [1, 3, 5]]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>leaky_relu_slope</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The angle of the negative slope used by the leaky ReLU activation in the vocoder. Applies to the vocoder
only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unit_hifi_gan_vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vocabulary size of the SeamlessM4T vocoder. Defines the number of different unit tokens that can be
represented by the <code>inputs_ids</code> passed when calling the vocoder of [<code>~SeamlessM4TModel</code>],
[<code>~SeamlessM4TForSpeechToSpeech</code>] or [<code>~SeamlessM4TForTextToSpeech</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 10000</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unit_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The projection dimension of the input ids given to the hifi-gan vocoder. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1280</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1280</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lang_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The projection dimension of the target language given to the hifi-gan vocoder. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 256</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>spkr_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The projection dimension of the speaker id given to the hifi-gan vocoder. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 256</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_num_langs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of langs supported by the vocoder. Might be different from <code>t2u_num_langs</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 36</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>36</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_num_spkrs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of speakers supported by the vocoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 200</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>200</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>variance_predictor_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Kernel size of the duration predictor. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 3</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>var_pred_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probabilitiy of the duration predictor. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.5</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.5</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_offset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Offset the unit token ids by this number to account for symbol tokens. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 4</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">SeamlessM4TModel</span><span class="p">,</span> <span class="n">SeamlessM4TConfig</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a SeamlessM4T &quot;facebook/hf-seamless-m4t-medium&quot; style configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">SeamlessM4TConfig</span><span class="p">()</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a model from the &quot;facebook/hf-seamless-m4t-medium&quot; style configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">SeamlessM4TModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Accessing the model configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</code></pre></div>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\configuration_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the configuration class to store the configuration of a [`~SeamlessM4TModel`]. It is used to instantiate an</span>
<span class="sd">    SeamlessM4T model according to the specified arguments, defining the model architecture. Instantiating a</span>
<span class="sd">    configuration with the defaults will yield a similar configuration to that of the SeamlessM4T</span>
<span class="sd">    [&quot;facebook/hf-seamless-m4t-medium&quot;](https://hf-mirror.com/&quot;facebook/hf-seamless-m4t-medium&quot;) architecture.</span>

<span class="sd">    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the</span>
<span class="sd">    documentation from [`PretrainedConfig`] for more information.</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_size (`int`, *optional*, defaults to 256102):</span>
<span class="sd">            Vocabulary size of the SeamlessM4T model. Defines the number of different tokens that can be represented by</span>
<span class="sd">            the `inputs_ids` passed when calling [`~SeamlessM4TModel`], [`~SeamlessM4TForTextToSpeech`] or</span>
<span class="sd">            [`~SeamlessM4TForTextToText`].</span>
<span class="sd">        t2u_vocab_size (`int`, *optional*, defaults to 10082):</span>
<span class="sd">            Unit vocabulary size of the SeamlessM4T model. Defines the number of different unit tokens that can be</span>
<span class="sd">            represented by the `inputs_ids` passed when calling the Text-To-Units sub-model of [`~SeamlessM4TModel`],</span>
<span class="sd">            [`~SeamlessM4TForSpeechToSpeech`] or [`~SeamlessM4TForTextToSpeech`].</span>
<span class="sd">        Parameters shared across sub-models: args below are Parameters shared across sub-models</span>
<span class="sd">        hidden_size (`int`, *optional*, defaults to 1024):</span>
<span class="sd">            Dimensionality of the &quot;intermediate&quot; layers in the architecture.</span>
<span class="sd">        initializer_range (`float`, *optional*, defaults to 0.02):</span>
<span class="sd">            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</span>
<span class="sd">        layer_norm_eps (`float`, *optional*, defaults to 1e-05):</span>
<span class="sd">            The epsilon used by the layer normalization layers.</span>
<span class="sd">        use_cache (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether or not the model should return the last key/values attentions (not used by all models).</span>
<span class="sd">        max_position_embeddings (`int`, *optional*, defaults to 1024):</span>
<span class="sd">            The maximum sequence length that this model text encoder and decoder might ever be used with. Typically set</span>
<span class="sd">            this to something large just in case (e.g., 512 or 1024 or 2048).</span>
<span class="sd">        is_encoder_decoder (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the model is used as an encoder/decoder or not.</span>
<span class="sd">        encoder_layerdrop (`float`, *optional*, defaults to 0.05):</span>
<span class="sd">            The LayerDrop probability for the encoders. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)</span>
<span class="sd">            for more details.</span>
<span class="sd">        decoder_layerdrop (`float`, *optional*, defaults to 0.05):</span>
<span class="sd">            The LayerDrop probability for the decoders. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)</span>
<span class="sd">            for more details.</span>
<span class="sd">        activation_function (`str` or `function`, *optional*, defaults to `&quot;relu&quot;`):</span>
<span class="sd">            The non-linear activation function (function or string) in the decoder and feed-forward layers. If string,</span>
<span class="sd">            `&quot;gelu&quot;`, `&quot;relu&quot;`, `&quot;selu&quot;`, `&quot;swish&quot;` and `&quot;gelu_new&quot;` are supported.</span>
<span class="sd">        dropout (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The dropout probability for all fully connected layers in the embeddings, encoder, decoder, and pooler.</span>
<span class="sd">        attention_dropout (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The dropout probability for all attention layers.</span>
<span class="sd">        activation_dropout (`float`, *optional*, defaults to 0.0):</span>
<span class="sd">            The dropout probability for all activation layers in the model.</span>
<span class="sd">        scale_embedding (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Scale embeddings by diving by sqrt(d_model).</span>
<span class="sd">        Text encoder and text decoder specific parameters:  args below are text decoder specific parameters</span>
<span class="sd">        encoder_layers (`int`, *optional*, defaults to 24):</span>
<span class="sd">            Number of hidden layers in the Transformer text encoder.</span>
<span class="sd">        encoder_ffn_dim (`int`, *optional*, defaults to 8192):</span>
<span class="sd">            Dimension of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer text encoder.</span>
<span class="sd">        encoder_attention_heads (`int`, *optional*, defaults to 16):</span>
<span class="sd">            Number of attention heads for each attention layer in the Transformer text encoder.</span>
<span class="sd">        decoder_layers (`int`, *optional*, defaults to 24):</span>
<span class="sd">            Number of hidden layers in the Transformer text decoder.</span>
<span class="sd">        decoder_ffn_dim (`int`, *optional*, defaults to 8192):</span>
<span class="sd">            Dimension of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer text decoder.</span>
<span class="sd">        decoder_attention_heads (`int`, *optional*, defaults to 16):</span>
<span class="sd">            Number of attention heads for each attention layer in the Transformer text decoder.</span>
<span class="sd">        decoder_start_token_id (`int`, *optional*, defaults to 3):</span>
<span class="sd">            If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token. Only</span>
<span class="sd">            applied in the text decoder.</span>
<span class="sd">        max_new_tokens (`int`, *optional*, defaults to 256):</span>
<span class="sd">            The maximum numbers of text tokens to generate, ignoring the number of tokens in the prompt.</span>
<span class="sd">        pad_token_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the _padding_ text token. Only applied to the text-decoder model.</span>
<span class="sd">        bos_token_id (`int`, *optional*, defaults to 2):</span>
<span class="sd">            The id of the _beginning-of-stream_ text token. Only applied to the text-decoder model.</span>
<span class="sd">        eos_token_id (`int`, *optional*, defaults to 3):</span>
<span class="sd">            The id of the _end-of-stream_ text token. Only applied to the text-decoder model.</span>
<span class="sd">        Speech encoder specific parameters: args below are Speech encoder specific parameters</span>
<span class="sd">        speech_encoder_layers (`int`, *optional*, defaults to 24):</span>
<span class="sd">            Number of hidden layers in the Transformer speech encoder.</span>
<span class="sd">        speech_encoder_attention_heads (`int`, *optional*, defaults to 16):</span>
<span class="sd">            Number of attention heads for each attention layer in the Transformer speech encoder.</span>
<span class="sd">        speech_encoder_intermediate_size (`int`, *optional*, defaults to 4096):</span>
<span class="sd">            Dimension of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer speech encoder.</span>
<span class="sd">        speech_encoder_hidden_act (`str` or `function`, *optional*, defaults to `&quot;swish&quot;`):</span>
<span class="sd">            The non-linear activation function (function or string) in the speech encoder. If string, `&quot;gelu&quot;`,</span>
<span class="sd">            `&quot;relu&quot;`, `&quot;selu&quot;`, `&quot;swish&quot;` and `&quot;gelu_new&quot;` are supported.</span>
<span class="sd">        speech_encoder_dropout (`float`, *optional*, defaults to 0.0):</span>
<span class="sd">            The dropout probability for all layers in the speech encoder.</span>
<span class="sd">        add_adapter (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Add an adapter layer on top of the speech encoder.</span>
<span class="sd">        speech_encoder_layerdrop (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The LayerDrop probability for the speech encoder. See the [LayerDrop paper](see</span>
<span class="sd">            https://arxiv.org/abs/1909.11556) for more details.</span>
<span class="sd">        feature_projection_input_dim (`int`, *optional*, defaults to 160):</span>
<span class="sd">            Input dimension of the input feature projection of the speech encoder, i.e the dimension after processing</span>
<span class="sd">            input audios with [`SeamlessM4TFeatureExtractor`].</span>
<span class="sd">        num_conv_pos_embeddings (`int`, *optional*, defaults to 128):</span>
<span class="sd">            Number of convolutional positional embeddings. Defines the kernel size of 1D convolutional positional</span>
<span class="sd">            embeddings layer of the speech encoder.</span>
<span class="sd">        num_conv_pos_embedding_groups (`int`, *optional*, defaults to 16):</span>
<span class="sd">            Number of groups of 1D convolutional positional embeddings layer of the speech encoder.</span>
<span class="sd">        adaptor_kernel_size (`int`, *optional*, defaults to 8):</span>
<span class="sd">            Kernel size of the convolutional layers in the adapter network. Only relevant if `add_adapter is True`.</span>
<span class="sd">        adaptor_stride (`int`, *optional*, defaults to 8):</span>
<span class="sd">            Stride of the convolutional layers in the adapter network. Only relevant if `add_adapter is True`.</span>
<span class="sd">        adaptor_dropout (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The dropout probability for all layers in the speech adapter.</span>
<span class="sd">        num_adapter_layers (`int`, *optional*, defaults to 1):</span>
<span class="sd">            Number of convolutional layers that should be used in the adapter network. Only relevant if `add_adapter is</span>
<span class="sd">            True`.</span>
<span class="sd">        position_embeddings_type (`str`, *optional*, defaults to `&quot;relative&quot;`):</span>
<span class="sd">            Can be specified to `relative` or `rotary` for relative or rotary position embeddings respectively. If left</span>
<span class="sd">            `None` no relative position embedding is applied. Only applied to the speech encoder.</span>
<span class="sd">        rotary_embedding_base (`int`, *optional*, defaults to 10000):</span>
<span class="sd">            If `&quot;rotary&quot;` position embeddings are used, defines the size of the embedding base. Only applied to the</span>
<span class="sd">            speech encoder.</span>
<span class="sd">        max_source_positions (`int`, *optional*, defaults to 4096):</span>
<span class="sd">            if `&quot;relative&quot;` position embeddings are used, defines the maximum source input positions. Only applied to</span>
<span class="sd">            the speech encoder.</span>
<span class="sd">        conv_depthwise_kernel_size (`int`, *optional*, defaults to 31):</span>
<span class="sd">            Kernel size of convolutional depthwise 1D layer in Conformer blocks. Only applied to the speech encoder.</span>
<span class="sd">        Text-To-Unit (t2u) model specific parameters: args below are Text-To-Unit (t2u) model specific parameters</span>
<span class="sd">        t2u_bos_token_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the _beginning-of-stream_ unit token. Only applied to the text-to-unit seq2seq model.</span>
<span class="sd">        t2u_pad_token_id (`int`, *optional*, defaults to 1):</span>
<span class="sd">            The id of the _padding_ unit token. Only applied to the text-to-unit seq2seq model.</span>
<span class="sd">        t2u_eos_token_id (`int`, *optional*, defaults to 2):</span>
<span class="sd">            The id of the _end-of-stream_ unit token. Only applied to the text-to-unit seq2seq model.</span>
<span class="sd">        t2u_decoder_start_token_id (`int`, *optional*, defaults to 2):</span>
<span class="sd">            If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token. Only</span>
<span class="sd">            applied to the text-to-unit seq2seq model.</span>
<span class="sd">        t2u_max_new_tokens (`int`, *optional*, defaults to 1024):</span>
<span class="sd">            The maximum numbers of unit tokens to generate, ignoring the number of tokens in the prompt. Only applied</span>
<span class="sd">            to the text-to-unit seq2seq model.</span>
<span class="sd">        t2u_encoder_layers (`int`, *optional*, defaults to 6):</span>
<span class="sd">            Number of hidden layers in the Transformer text-to-unit encoder.</span>
<span class="sd">        t2u_encoder_ffn_dim (`int`, *optional*, defaults to 8192):</span>
<span class="sd">            Dimension of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer text-to-unit encoder.</span>
<span class="sd">        t2u_encoder_attention_heads (`int`, *optional*, defaults to 16):</span>
<span class="sd">            Number of attention heads for each attention layer in the Transformer text-to-unit encoder.</span>
<span class="sd">        t2u_decoder_layers (`int`, *optional*, defaults to 6):</span>
<span class="sd">            Number of hidden layers in the Transformer text-to-unit decoder.</span>
<span class="sd">        t2u_decoder_ffn_dim (`int`, *optional*, defaults to 8192):</span>
<span class="sd">            Dimension of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer text-to-unit decoder.</span>
<span class="sd">        t2u_decoder_attention_heads (`int`, *optional*, defaults to 16):</span>
<span class="sd">            Number of attention heads for each attention layer in the Transformer text-to-unit decoder.</span>
<span class="sd">        t2u_max_position_embeddings (`int`, *optional*, defaults to 2048):</span>
<span class="sd">            The maximum sequence length that this model text-to-unit component might ever be used with. Typically set</span>
<span class="sd">            this to something large just in case (e.g., 512 or 1024 or 2048).</span>
<span class="sd">        Hifi-Gan Vocoder specific parameters:  args below are Hifi-Gan Vocoder specific parameters</span>
<span class="sd">        sampling_rate (`int`, *optional*, defaults to 16000):</span>
<span class="sd">            The sampling rate at which the output audio will be generated, expressed in hertz (Hz).</span>
<span class="sd">        upsample_initial_channel (`int`, *optional*, defaults to 512):</span>
<span class="sd">            The number of input channels into the hifi-gan upsampling network. Applies to the vocoder only.</span>
<span class="sd">        upsample_rates (`Tuple[int]` or `List[int]`, *optional*, defaults to `[5, 4, 4, 2, 2]`):</span>
<span class="sd">            A tuple of integers defining the stride of each 1D convolutional layer in the vocoder upsampling network.</span>
<span class="sd">            The length of *upsample_rates* defines the number of convolutional layers and has to match the length of</span>
<span class="sd">            *upsample_kernel_sizes*. Applies to the vocoder only.</span>
<span class="sd">        upsample_kernel_sizes (`Tuple[int]` or `List[int]`, *optional*, defaults to `[11, 8, 8, 4, 4]`):</span>
<span class="sd">            A tuple of integers defining the kernel size of each 1D convolutional layer in the vocoder upsampling</span>
<span class="sd">            network. The length of *upsample_kernel_sizes* defines the number of convolutional layers and has to match</span>
<span class="sd">            the length of *upsample_rates*. Applies to the vocoder only.</span>
<span class="sd">        resblock_kernel_sizes (`Tuple[int]` or `List[int]`, *optional*, defaults to `[3, 7, 11]`):</span>
<span class="sd">            A tuple of integers defining the kernel sizes of the vocoder 1D convolutional layers in the multi-receptive</span>
<span class="sd">            field fusion (MRF) module. Applies to the vocoder only.</span>
<span class="sd">        resblock_dilation_sizes (`Tuple[Tuple[int]]` or `List[List[int]]`, *optional*, defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`):</span>
<span class="sd">            A nested tuple of integers defining the dilation rates of the vocoder dilated 1D convolutional layers in</span>
<span class="sd">            the multi-receptive field fusion (MRF) module. Applies to the vocoder only.</span>
<span class="sd">        leaky_relu_slope (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The angle of the negative slope used by the leaky ReLU activation in the vocoder. Applies to the vocoder</span>
<span class="sd">            only.</span>
<span class="sd">        unit_hifi_gan_vocab_size (`int`, *optional*, defaults to 10000):</span>
<span class="sd">            Vocabulary size of the SeamlessM4T vocoder. Defines the number of different unit tokens that can be</span>
<span class="sd">            represented by the `inputs_ids` passed when calling the vocoder of [`~SeamlessM4TModel`],</span>
<span class="sd">            [`~SeamlessM4TForSpeechToSpeech`] or [`~SeamlessM4TForTextToSpeech`].</span>
<span class="sd">        unit_embed_dim (`int`, *optional*, defaults to 1280):</span>
<span class="sd">            The projection dimension of the input ids given to the hifi-gan vocoder. Applies to the vocoder only.</span>
<span class="sd">        lang_embed_dim (`int`, *optional*, defaults to 256):</span>
<span class="sd">            The projection dimension of the target language given to the hifi-gan vocoder. Applies to the vocoder only.</span>
<span class="sd">        spkr_embed_dim (`int`, *optional*, defaults to 256):</span>
<span class="sd">            The projection dimension of the speaker id given to the hifi-gan vocoder. Applies to the vocoder only.</span>
<span class="sd">        vocoder_num_langs (`int`, *optional*, defaults to 36):</span>
<span class="sd">            Number of langs supported by the vocoder. Might be different from `t2u_num_langs`.</span>
<span class="sd">        vocoder_num_spkrs (`int`, *optional*, defaults to 200):</span>
<span class="sd">            Number of speakers supported by the vocoder.</span>
<span class="sd">        variance_predictor_kernel_size (`int`, *optional*, defaults to 3):</span>
<span class="sd">            Kernel size of the duration predictor. Applies to the vocoder only.</span>
<span class="sd">        var_pred_dropout (`float`, *optional*, defaults to 0.5):</span>
<span class="sd">            The dropout probabilitiy of the duration predictor. Applies to the vocoder only.</span>
<span class="sd">        vocoder_offset (`int`, *optional*, defaults to 4):</span>
<span class="sd">            Offset the unit token ids by this number to account for symbol tokens. Applies to the vocoder only.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import SeamlessM4TModel, SeamlessM4TConfig</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a SeamlessM4T &quot;facebook/hf-seamless-m4t-medium&quot; style configuration</span>
<span class="sd">        &gt;&gt;&gt; configuration = SeamlessM4TConfig()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a model from the &quot;facebook/hf-seamless-m4t-medium&quot; style configuration</span>
<span class="sd">        &gt;&gt;&gt; model = SeamlessM4TModel(configuration)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Accessing the model configuration</span>
<span class="sd">        &gt;&gt;&gt; configuration = model.config</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;seamless_m4t&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="mi">256102</span><span class="p">,</span>
        <span class="n">t2u_vocab_size</span><span class="o">=</span><span class="mi">10082</span><span class="p">,</span>
        <span class="c1"># shared config</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
        <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">encoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
        <span class="n">decoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
        <span class="n">activation_function</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">activation_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">scale_embedding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="c1"># text encoder|decoder</span>
        <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
        <span class="n">encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
        <span class="n">encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">decoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
        <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
        <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="c1"># speech_encoder</span>
        <span class="n">speech_encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
        <span class="n">speech_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">speech_encoder_intermediate_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
        <span class="n">speech_encoder_hidden_act</span><span class="o">=</span><span class="s2">&quot;swish&quot;</span><span class="p">,</span>
        <span class="n">speech_encoder_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">add_adapter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">speech_encoder_layerdrop</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">feature_projection_input_dim</span><span class="o">=</span><span class="mi">160</span><span class="p">,</span>
        <span class="n">num_conv_pos_embeddings</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">num_conv_pos_embedding_groups</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">adaptor_kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">adaptor_stride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">adaptor_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">num_adapter_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">position_embeddings_type</span><span class="o">=</span><span class="s2">&quot;relative&quot;</span><span class="p">,</span>
        <span class="n">rotary_embedding_base</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
        <span class="n">max_source_positions</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
        <span class="n">conv_depthwise_kernel_size</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span>
        <span class="c1"># t2u config</span>
        <span class="n">t2u_bos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">t2u_pad_token_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">t2u_eos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">t2u_decoder_start_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">t2u_max_new_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">t2u_encoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
        <span class="n">t2u_encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
        <span class="n">t2u_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">t2u_decoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
        <span class="n">t2u_decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
        <span class="n">t2u_decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">t2u_max_position_embeddings</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
        <span class="c1"># hifi-gan vocoder config</span>
        <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span>
        <span class="n">upsample_initial_channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="n">upsample_rates</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
        <span class="n">upsample_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
        <span class="n">resblock_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>
        <span class="n">resblock_dilation_sizes</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span>
        <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="c1"># specific to Code Hifi-Gan</span>
        <span class="n">unit_hifi_gan_vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
        <span class="n">unit_embed_dim</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span>
        <span class="n">lang_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">spkr_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">vocoder_num_langs</span><span class="o">=</span><span class="mi">36</span><span class="p">,</span>
        <span class="n">vocoder_num_spkrs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">variance_predictor_kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">var_pred_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">vocoder_offset</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a SeamlessM4TConfig object with the specified configuration parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (object): The instance of the class.</span>
<span class="sd">            vocab_size (int): The size of the vocabulary.</span>
<span class="sd">            t2u_vocab_size (int): The size of the T2U vocabulary.</span>
<span class="sd">            hidden_size (int): The size of the hidden layers.</span>
<span class="sd">            initializer_range (float): The range for weight initialization.</span>
<span class="sd">            layer_norm_eps (float): The epsilon value for layer normalization.</span>
<span class="sd">            use_cache (bool): Flag to indicate whether to use cache.</span>
<span class="sd">            max_position_embeddings (int): The maximum position embeddings.</span>
<span class="sd">            is_encoder_decoder (bool): Flag to indicate if it&#39;s an encoder-decoder model.</span>
<span class="sd">            encoder_layerdrop (float): The layer drop rate for encoder layers.</span>
<span class="sd">            decoder_layerdrop (float): The layer drop rate for decoder layers.</span>
<span class="sd">            activation_function (str): The activation function to use.</span>
<span class="sd">            dropout (float): The dropout rate.</span>
<span class="sd">            attention_dropout (float): The dropout rate for attention layers.</span>
<span class="sd">            activation_dropout (float): The dropout rate for activation layers.</span>
<span class="sd">            scale_embedding (bool): Flag to indicate whether to scale embeddings.</span>
<span class="sd">            encoder_layers (int): The number of encoder layers.</span>
<span class="sd">            encoder_ffn_dim (int): The dimension of the encoder feed-forward network.</span>
<span class="sd">            encoder_attention_heads (int): The number of attention heads for encoder.</span>
<span class="sd">            decoder_layers (int): The number of decoder layers.</span>
<span class="sd">            decoder_ffn_dim (int): The dimension of the decoder feed-forward network.</span>
<span class="sd">            decoder_attention_heads (int): The number of attention heads for decoder.</span>
<span class="sd">            decoder_start_token_id (int): The start token ID for decoder.</span>
<span class="sd">            max_new_tokens (int): The maximum number of new tokens.</span>
<span class="sd">            pad_token_id (int): The ID of the padding token.</span>
<span class="sd">            bos_token_id (int): The ID of the beginning of sentence token.</span>
<span class="sd">            eos_token_id (int): The ID of the end of sentence token.</span>
<span class="sd">            speech_encoder_layers (int): The number of layers in the speech encoder.</span>
<span class="sd">            speech_encoder_attention_heads (int): The number of attention heads for speech encoder.</span>
<span class="sd">            speech_encoder_intermediate_size (int): The size of the intermediate layer in speech encoder.</span>
<span class="sd">            speech_encoder_hidden_act (str): The activation function for the hidden layers in speech encoder.</span>
<span class="sd">            speech_encoder_dropout (float): The dropout rate for the speech encoder.</span>
<span class="sd">            add_adapter (bool): Flag to indicate whether to add adapter layers.</span>
<span class="sd">            speech_encoder_layerdrop (float): The layer drop rate for speech encoder.</span>
<span class="sd">            feature_projection_input_dim (int): The input dimension for feature projection.</span>
<span class="sd">            num_conv_pos_embeddings (int): The number of convolutional positional embeddings.</span>
<span class="sd">            num_conv_pos_embedding_groups (int): The number of groups for convolutional positional embeddings.</span>
<span class="sd">            adaptor_kernel_size (int): The kernel size for the adaptor.</span>
<span class="sd">            adaptor_stride (int): The stride for the adaptor.</span>
<span class="sd">            adaptor_dropout (float): The dropout rate for the adaptor.</span>
<span class="sd">            num_adapter_layers (int): The number of adapter layers.</span>
<span class="sd">            position_embeddings_type (str): The type of position embeddings.</span>
<span class="sd">            rotary_embedding_base (int): The base value for rotary embeddings.</span>
<span class="sd">            max_source_positions (int): The maximum source positions.</span>
<span class="sd">            conv_depthwise_kernel_size (int): The kernel size for depthwise convolution.</span>
<span class="sd">            t2u_bos_token_id (int): The ID of the beginning of sentence token for T2U.</span>
<span class="sd">            t2u_pad_token_id (int): The ID of the padding token for T2U.</span>
<span class="sd">            t2u_eos_token_id (int): The ID of the end of sentence token for T2U.</span>
<span class="sd">            t2u_decoder_start_token_id (int): The start token ID for the T2U decoder.</span>
<span class="sd">            t2u_max_new_tokens (int): The maximum number of new tokens for T2U.</span>
<span class="sd">            t2u_encoder_layers (int): The number of layers in the T2U encoder.</span>
<span class="sd">            t2u_encoder_ffn_dim (int): The dimension of the T2U encoder feed-forward network.</span>
<span class="sd">            t2u_encoder_attention_heads (int): The number of attention heads for T2U encoder.</span>
<span class="sd">            t2u_decoder_layers (int): The number of layers in the T2U decoder.</span>
<span class="sd">            t2u_decoder_ffn_dim (int): The dimension of the T2U decoder feed-forward network.</span>
<span class="sd">            t2u_decoder_attention_heads (int): The number of attention heads for T2U decoder.</span>
<span class="sd">            t2u_max_position_embeddings (int): The maximum position embeddings for T2U.</span>
<span class="sd">            sampling_rate (int): The sampling rate for audio processing.</span>
<span class="sd">            upsample_initial_channel (int): The initial number of channels for upsampling.</span>
<span class="sd">            upsample_rates (list): The rates for upsampling.</span>
<span class="sd">            upsample_kernel_sizes (list): The kernel sizes for upsampling.</span>
<span class="sd">            resblock_kernel_sizes (list): The kernel sizes for the residual blocks.</span>
<span class="sd">            resblock_dilation_sizes (list): The dilation sizes for the residual blocks.</span>
<span class="sd">            leaky_relu_slope (float): The slope for leaky ReLU activation.</span>
<span class="sd">            unit_hifi_gan_vocab_size (int): The vocabulary size for the HiFi-GAN unit.</span>
<span class="sd">            unit_embed_dim (int): The embedding dimension for the HiFi-GAN unit.</span>
<span class="sd">            lang_embed_dim (int): The embedding dimension for language.</span>
<span class="sd">            spkr_embed_dim (int): The embedding dimension for speaker.</span>
<span class="sd">            vocoder_num_langs (int): The number of languages for the vocoder.</span>
<span class="sd">            vocoder_num_spkrs (int): The number of speakers for the vocoder.</span>
<span class="sd">            variance_predictor_kernel_size (int): The kernel size for the variance predictor.</span>
<span class="sd">            var_pred_dropout (float): The dropout rate for the variance predictor.</span>
<span class="sd">            vocoder_offset (int): The offset value for the vocoder.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># overall_config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_vocab_size</span> <span class="o">=</span> <span class="n">t2u_vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_eps</span> <span class="o">=</span> <span class="n">layer_norm_eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="o">=</span> <span class="n">max_new_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layerdrop</span> <span class="o">=</span> <span class="n">encoder_layerdrop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layerdrop</span> <span class="o">=</span> <span class="n">decoder_layerdrop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">activation_function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_dropout</span> <span class="o">=</span> <span class="n">activation_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_embedding</span> <span class="o">=</span> <span class="n">scale_embedding</span>
        <span class="c1"># for proper config init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">decoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">decoder_layers</span>

        <span class="c1"># text|unit encoder|decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">encoder_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_ffn_dim</span> <span class="o">=</span> <span class="n">encoder_ffn_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attention_heads</span> <span class="o">=</span> <span class="n">encoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">decoder_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_ffn_dim</span> <span class="o">=</span> <span class="n">decoder_ffn_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_attention_heads</span> <span class="o">=</span> <span class="n">decoder_attention_heads</span>

        <span class="c1"># speech_encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_layers</span> <span class="o">=</span> <span class="n">speech_encoder_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_hidden_act</span> <span class="o">=</span> <span class="n">speech_encoder_hidden_act</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_dropout</span> <span class="o">=</span> <span class="n">speech_encoder_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_attention_heads</span> <span class="o">=</span> <span class="n">speech_encoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_layerdrop</span> <span class="o">=</span> <span class="n">speech_encoder_layerdrop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_intermediate_size</span> <span class="o">=</span> <span class="n">speech_encoder_intermediate_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_projection_input_dim</span> <span class="o">=</span> <span class="n">feature_projection_input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_conv_pos_embeddings</span> <span class="o">=</span> <span class="n">num_conv_pos_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_conv_pos_embedding_groups</span> <span class="o">=</span> <span class="n">num_conv_pos_embedding_groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_kernel_size</span> <span class="o">=</span> <span class="n">adaptor_kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_stride</span> <span class="o">=</span> <span class="n">adaptor_stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_dropout</span> <span class="o">=</span> <span class="n">adaptor_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_adapter_layers</span> <span class="o">=</span> <span class="n">num_adapter_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="o">=</span> <span class="n">position_embeddings_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_embedding_base</span> <span class="o">=</span> <span class="n">rotary_embedding_base</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_source_positions</span> <span class="o">=</span> <span class="n">max_source_positions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_depthwise_kernel_size</span> <span class="o">=</span> <span class="n">conv_depthwise_kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_adapter</span> <span class="o">=</span> <span class="n">add_adapter</span>

        <span class="c1"># t2u config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_bos_token_id</span> <span class="o">=</span> <span class="n">t2u_bos_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_pad_token_id</span> <span class="o">=</span> <span class="n">t2u_pad_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_eos_token_id</span> <span class="o">=</span> <span class="n">t2u_eos_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_start_token_id</span> <span class="o">=</span> <span class="n">t2u_decoder_start_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_max_new_tokens</span> <span class="o">=</span> <span class="n">t2u_max_new_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_layers</span> <span class="o">=</span> <span class="n">t2u_encoder_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_ffn_dim</span> <span class="o">=</span> <span class="n">t2u_encoder_ffn_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_attention_heads</span> <span class="o">=</span> <span class="n">t2u_encoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_layers</span> <span class="o">=</span> <span class="n">t2u_decoder_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_ffn_dim</span> <span class="o">=</span> <span class="n">t2u_decoder_ffn_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_attention_heads</span> <span class="o">=</span> <span class="n">t2u_decoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_max_position_embeddings</span> <span class="o">=</span> <span class="n">t2u_max_position_embeddings</span>

        <span class="c1"># hifi-gan vocoder config</span>
        <span class="c1"># original parameters specific to Hifi-Gan</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span> <span class="o">=</span> <span class="n">sampling_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upsample_initial_channel</span> <span class="o">=</span> <span class="n">upsample_initial_channel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upsample_rates</span> <span class="o">=</span> <span class="n">upsample_rates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upsample_kernel_sizes</span> <span class="o">=</span> <span class="n">upsample_kernel_sizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resblock_kernel_sizes</span> <span class="o">=</span> <span class="n">resblock_kernel_sizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resblock_dilation_sizes</span> <span class="o">=</span> <span class="n">resblock_dilation_sizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu_slope</span> <span class="o">=</span> <span class="n">leaky_relu_slope</span>

        <span class="c1"># specific to Code Hifi-Gan</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unit_hifi_gan_vocab_size</span> <span class="o">=</span> <span class="n">unit_hifi_gan_vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unit_embed_dim</span> <span class="o">=</span> <span class="n">unit_embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lang_embed_dim</span> <span class="o">=</span> <span class="n">lang_embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spkr_embed_dim</span> <span class="o">=</span> <span class="n">spkr_embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_num_langs</span> <span class="o">=</span> <span class="n">vocoder_num_langs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_num_spkrs</span> <span class="o">=</span> <span class="n">vocoder_num_spkrs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">variance_predictor_kernel_size</span> <span class="o">=</span> <span class="n">variance_predictor_kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_pred_dropout</span> <span class="o">=</span> <span class="n">var_pred_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_offset</span> <span class="o">=</span> <span class="n">vocoder_offset</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
            <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
            <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
            <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="n">decoder_start_token_id</span><span class="p">,</span>
            <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="n">max_position_embeddings</span><span class="o">=</span><span class="n">max_position_embeddings</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t.SeamlessM4TConfig.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">configuration_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TConfig</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">256102</span><span class="p">,</span> <span class="n">t2u_vocab_size</span><span class="o">=</span><span class="mi">10082</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">encoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">decoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">activation_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale_embedding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span> <span class="n">encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">decoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span> <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">speech_encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">speech_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">speech_encoder_intermediate_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">speech_encoder_hidden_act</span><span class="o">=</span><span class="s1">&#39;swish&#39;</span><span class="p">,</span> <span class="n">speech_encoder_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">add_adapter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">speech_encoder_layerdrop</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">feature_projection_input_dim</span><span class="o">=</span><span class="mi">160</span><span class="p">,</span> <span class="n">num_conv_pos_embeddings</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_conv_pos_embedding_groups</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">adaptor_kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">adaptor_stride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">adaptor_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_adapter_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">position_embeddings_type</span><span class="o">=</span><span class="s1">&#39;relative&#39;</span><span class="p">,</span> <span class="n">rotary_embedding_base</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">max_source_positions</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">conv_depthwise_kernel_size</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span> <span class="n">t2u_bos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">t2u_pad_token_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">t2u_eos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">t2u_decoder_start_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">t2u_max_new_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">t2u_encoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">t2u_encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span> <span class="n">t2u_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">t2u_decoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">t2u_decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span> <span class="n">t2u_decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">t2u_max_position_embeddings</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span> <span class="n">upsample_initial_channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">upsample_rates</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">upsample_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">resblock_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span> <span class="n">resblock_dilation_sizes</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span> <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">unit_hifi_gan_vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">unit_embed_dim</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span> <span class="n">lang_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">spkr_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">vocoder_num_langs</span><span class="o">=</span><span class="mi">36</span><span class="p">,</span> <span class="n">vocoder_num_spkrs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">variance_predictor_kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">var_pred_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">vocoder_offset</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.configuration_seamless_m4t.SeamlessM4TConfig.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a SeamlessM4TConfig object with the specified configuration parameters.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the vocabulary.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256102</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the T2U vocabulary.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10082</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the hidden layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1024</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>initializer_range</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The range for weight initialization.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.02</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_norm_eps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The epsilon value for layer normalization.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag to indicate whether to use cache.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum position embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1024</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_encoder_decoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag to indicate if it's an encoder-decoder model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The layer drop rate for encoder layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The layer drop rate for decoder layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>activation_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The activation function to use.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;relu&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate for attention layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>activation_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate for activation layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scale_embedding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag to indicate whether to scale embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of encoder layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the encoder feed-forward network.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads for encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of decoder layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the decoder feed-forward network.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads for decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_start_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The start token ID for decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_new_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum number of new tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ID of the padding token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ID of the beginning of sentence token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ID of the end of sentence token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of layers in the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads for speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_intermediate_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the intermediate layer in speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_hidden_act</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The activation function for the hidden layers in speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;swish&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate for the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_adapter</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag to indicate whether to add adapter layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The layer drop rate for speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>feature_projection_input_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input dimension for feature projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>160</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_conv_pos_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of convolutional positional embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>128</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_conv_pos_embedding_groups</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of groups for convolutional positional embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size for the adaptor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride for the adaptor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate for the adaptor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_adapter_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of adapter layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_embeddings_type</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The type of position embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;relative&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rotary_embedding_base</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The base value for rotary embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_source_positions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum source positions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>conv_depthwise_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size for depthwise convolution.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>31</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ID of the beginning of sentence token for T2U.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ID of the padding token for T2U.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The ID of the end of sentence token for T2U.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_start_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The start token ID for the T2U decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_max_new_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum number of new tokens for T2U.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1024</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of layers in the T2U encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>6</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the T2U encoder feed-forward network.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads for T2U encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of layers in the T2U decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>6</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the T2U decoder feed-forward network.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads for T2U decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum position embeddings for T2U.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2048</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sampling_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sampling rate for audio processing.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_initial_channel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The initial number of channels for upsampling.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>512</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_rates</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The rates for upsampling.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>list</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[5, 4, 4, 2, 2]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_kernel_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel sizes for upsampling.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>list</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[11, 8, 8, 4, 4]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resblock_kernel_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel sizes for the residual blocks.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>list</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[3, 7, 11]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resblock_dilation_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dilation sizes for the residual blocks.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>list</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[[1, 3, 5], [1, 3, 5], [1, 3, 5]]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>leaky_relu_slope</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The slope for leaky ReLU activation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unit_hifi_gan_vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The vocabulary size for the HiFi-GAN unit.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unit_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The embedding dimension for the HiFi-GAN unit.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1280</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lang_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The embedding dimension for language.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>spkr_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The embedding dimension for speaker.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_num_langs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of languages for the vocoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>36</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_num_spkrs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of speakers for the vocoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>200</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>variance_predictor_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size for the variance predictor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>var_pred_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout rate for the variance predictor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.5</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_offset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The offset value for the vocoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\configuration_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">256102</span><span class="p">,</span>
    <span class="n">t2u_vocab_size</span><span class="o">=</span><span class="mi">10082</span><span class="p">,</span>
    <span class="c1"># shared config</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">encoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">decoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">activation_function</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">activation_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">scale_embedding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># text encoder|decoder</span>
    <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
    <span class="n">encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
    <span class="n">encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">decoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
    <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
    <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">bos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">eos_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="c1"># speech_encoder</span>
    <span class="n">speech_encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
    <span class="n">speech_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">speech_encoder_intermediate_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
    <span class="n">speech_encoder_hidden_act</span><span class="o">=</span><span class="s2">&quot;swish&quot;</span><span class="p">,</span>
    <span class="n">speech_encoder_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">add_adapter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">speech_encoder_layerdrop</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">feature_projection_input_dim</span><span class="o">=</span><span class="mi">160</span><span class="p">,</span>
    <span class="n">num_conv_pos_embeddings</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">num_conv_pos_embedding_groups</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">adaptor_kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">adaptor_stride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">adaptor_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">num_adapter_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">position_embeddings_type</span><span class="o">=</span><span class="s2">&quot;relative&quot;</span><span class="p">,</span>
    <span class="n">rotary_embedding_base</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">max_source_positions</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
    <span class="n">conv_depthwise_kernel_size</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span>
    <span class="c1"># t2u config</span>
    <span class="n">t2u_bos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">t2u_pad_token_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">t2u_eos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">t2u_decoder_start_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">t2u_max_new_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">t2u_encoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">t2u_encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
    <span class="n">t2u_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">t2u_decoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">t2u_decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
    <span class="n">t2u_decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">t2u_max_position_embeddings</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="c1"># hifi-gan vocoder config</span>
    <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span>
    <span class="n">upsample_initial_channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">upsample_rates</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="n">upsample_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">resblock_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>
    <span class="n">resblock_dilation_sizes</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span>
    <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="c1"># specific to Code Hifi-Gan</span>
    <span class="n">unit_hifi_gan_vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">unit_embed_dim</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span>
    <span class="n">lang_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">spkr_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">vocoder_num_langs</span><span class="o">=</span><span class="mi">36</span><span class="p">,</span>
    <span class="n">vocoder_num_spkrs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">variance_predictor_kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">var_pred_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">vocoder_offset</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a SeamlessM4TConfig object with the specified configuration parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (object): The instance of the class.</span>
<span class="sd">        vocab_size (int): The size of the vocabulary.</span>
<span class="sd">        t2u_vocab_size (int): The size of the T2U vocabulary.</span>
<span class="sd">        hidden_size (int): The size of the hidden layers.</span>
<span class="sd">        initializer_range (float): The range for weight initialization.</span>
<span class="sd">        layer_norm_eps (float): The epsilon value for layer normalization.</span>
<span class="sd">        use_cache (bool): Flag to indicate whether to use cache.</span>
<span class="sd">        max_position_embeddings (int): The maximum position embeddings.</span>
<span class="sd">        is_encoder_decoder (bool): Flag to indicate if it&#39;s an encoder-decoder model.</span>
<span class="sd">        encoder_layerdrop (float): The layer drop rate for encoder layers.</span>
<span class="sd">        decoder_layerdrop (float): The layer drop rate for decoder layers.</span>
<span class="sd">        activation_function (str): The activation function to use.</span>
<span class="sd">        dropout (float): The dropout rate.</span>
<span class="sd">        attention_dropout (float): The dropout rate for attention layers.</span>
<span class="sd">        activation_dropout (float): The dropout rate for activation layers.</span>
<span class="sd">        scale_embedding (bool): Flag to indicate whether to scale embeddings.</span>
<span class="sd">        encoder_layers (int): The number of encoder layers.</span>
<span class="sd">        encoder_ffn_dim (int): The dimension of the encoder feed-forward network.</span>
<span class="sd">        encoder_attention_heads (int): The number of attention heads for encoder.</span>
<span class="sd">        decoder_layers (int): The number of decoder layers.</span>
<span class="sd">        decoder_ffn_dim (int): The dimension of the decoder feed-forward network.</span>
<span class="sd">        decoder_attention_heads (int): The number of attention heads for decoder.</span>
<span class="sd">        decoder_start_token_id (int): The start token ID for decoder.</span>
<span class="sd">        max_new_tokens (int): The maximum number of new tokens.</span>
<span class="sd">        pad_token_id (int): The ID of the padding token.</span>
<span class="sd">        bos_token_id (int): The ID of the beginning of sentence token.</span>
<span class="sd">        eos_token_id (int): The ID of the end of sentence token.</span>
<span class="sd">        speech_encoder_layers (int): The number of layers in the speech encoder.</span>
<span class="sd">        speech_encoder_attention_heads (int): The number of attention heads for speech encoder.</span>
<span class="sd">        speech_encoder_intermediate_size (int): The size of the intermediate layer in speech encoder.</span>
<span class="sd">        speech_encoder_hidden_act (str): The activation function for the hidden layers in speech encoder.</span>
<span class="sd">        speech_encoder_dropout (float): The dropout rate for the speech encoder.</span>
<span class="sd">        add_adapter (bool): Flag to indicate whether to add adapter layers.</span>
<span class="sd">        speech_encoder_layerdrop (float): The layer drop rate for speech encoder.</span>
<span class="sd">        feature_projection_input_dim (int): The input dimension for feature projection.</span>
<span class="sd">        num_conv_pos_embeddings (int): The number of convolutional positional embeddings.</span>
<span class="sd">        num_conv_pos_embedding_groups (int): The number of groups for convolutional positional embeddings.</span>
<span class="sd">        adaptor_kernel_size (int): The kernel size for the adaptor.</span>
<span class="sd">        adaptor_stride (int): The stride for the adaptor.</span>
<span class="sd">        adaptor_dropout (float): The dropout rate for the adaptor.</span>
<span class="sd">        num_adapter_layers (int): The number of adapter layers.</span>
<span class="sd">        position_embeddings_type (str): The type of position embeddings.</span>
<span class="sd">        rotary_embedding_base (int): The base value for rotary embeddings.</span>
<span class="sd">        max_source_positions (int): The maximum source positions.</span>
<span class="sd">        conv_depthwise_kernel_size (int): The kernel size for depthwise convolution.</span>
<span class="sd">        t2u_bos_token_id (int): The ID of the beginning of sentence token for T2U.</span>
<span class="sd">        t2u_pad_token_id (int): The ID of the padding token for T2U.</span>
<span class="sd">        t2u_eos_token_id (int): The ID of the end of sentence token for T2U.</span>
<span class="sd">        t2u_decoder_start_token_id (int): The start token ID for the T2U decoder.</span>
<span class="sd">        t2u_max_new_tokens (int): The maximum number of new tokens for T2U.</span>
<span class="sd">        t2u_encoder_layers (int): The number of layers in the T2U encoder.</span>
<span class="sd">        t2u_encoder_ffn_dim (int): The dimension of the T2U encoder feed-forward network.</span>
<span class="sd">        t2u_encoder_attention_heads (int): The number of attention heads for T2U encoder.</span>
<span class="sd">        t2u_decoder_layers (int): The number of layers in the T2U decoder.</span>
<span class="sd">        t2u_decoder_ffn_dim (int): The dimension of the T2U decoder feed-forward network.</span>
<span class="sd">        t2u_decoder_attention_heads (int): The number of attention heads for T2U decoder.</span>
<span class="sd">        t2u_max_position_embeddings (int): The maximum position embeddings for T2U.</span>
<span class="sd">        sampling_rate (int): The sampling rate for audio processing.</span>
<span class="sd">        upsample_initial_channel (int): The initial number of channels for upsampling.</span>
<span class="sd">        upsample_rates (list): The rates for upsampling.</span>
<span class="sd">        upsample_kernel_sizes (list): The kernel sizes for upsampling.</span>
<span class="sd">        resblock_kernel_sizes (list): The kernel sizes for the residual blocks.</span>
<span class="sd">        resblock_dilation_sizes (list): The dilation sizes for the residual blocks.</span>
<span class="sd">        leaky_relu_slope (float): The slope for leaky ReLU activation.</span>
<span class="sd">        unit_hifi_gan_vocab_size (int): The vocabulary size for the HiFi-GAN unit.</span>
<span class="sd">        unit_embed_dim (int): The embedding dimension for the HiFi-GAN unit.</span>
<span class="sd">        lang_embed_dim (int): The embedding dimension for language.</span>
<span class="sd">        spkr_embed_dim (int): The embedding dimension for speaker.</span>
<span class="sd">        vocoder_num_langs (int): The number of languages for the vocoder.</span>
<span class="sd">        vocoder_num_spkrs (int): The number of speakers for the vocoder.</span>
<span class="sd">        variance_predictor_kernel_size (int): The kernel size for the variance predictor.</span>
<span class="sd">        var_pred_dropout (float): The dropout rate for the variance predictor.</span>
<span class="sd">        vocoder_offset (int): The offset value for the vocoder.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># overall_config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_vocab_size</span> <span class="o">=</span> <span class="n">t2u_vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_eps</span> <span class="o">=</span> <span class="n">layer_norm_eps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">max_position_embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="o">=</span> <span class="n">max_new_tokens</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layerdrop</span> <span class="o">=</span> <span class="n">encoder_layerdrop</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layerdrop</span> <span class="o">=</span> <span class="n">decoder_layerdrop</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">activation_function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation_dropout</span> <span class="o">=</span> <span class="n">activation_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scale_embedding</span> <span class="o">=</span> <span class="n">scale_embedding</span>
    <span class="c1"># for proper config init</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">decoder_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">decoder_layers</span>

    <span class="c1"># text|unit encoder|decoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">encoder_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_ffn_dim</span> <span class="o">=</span> <span class="n">encoder_ffn_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attention_heads</span> <span class="o">=</span> <span class="n">encoder_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">decoder_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_ffn_dim</span> <span class="o">=</span> <span class="n">decoder_ffn_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_attention_heads</span> <span class="o">=</span> <span class="n">decoder_attention_heads</span>

    <span class="c1"># speech_encoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_layers</span> <span class="o">=</span> <span class="n">speech_encoder_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_hidden_act</span> <span class="o">=</span> <span class="n">speech_encoder_hidden_act</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_dropout</span> <span class="o">=</span> <span class="n">speech_encoder_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_attention_heads</span> <span class="o">=</span> <span class="n">speech_encoder_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_layerdrop</span> <span class="o">=</span> <span class="n">speech_encoder_layerdrop</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_intermediate_size</span> <span class="o">=</span> <span class="n">speech_encoder_intermediate_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">feature_projection_input_dim</span> <span class="o">=</span> <span class="n">feature_projection_input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_conv_pos_embeddings</span> <span class="o">=</span> <span class="n">num_conv_pos_embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_conv_pos_embedding_groups</span> <span class="o">=</span> <span class="n">num_conv_pos_embedding_groups</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_kernel_size</span> <span class="o">=</span> <span class="n">adaptor_kernel_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_stride</span> <span class="o">=</span> <span class="n">adaptor_stride</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_dropout</span> <span class="o">=</span> <span class="n">adaptor_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_adapter_layers</span> <span class="o">=</span> <span class="n">num_adapter_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="o">=</span> <span class="n">position_embeddings_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rotary_embedding_base</span> <span class="o">=</span> <span class="n">rotary_embedding_base</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_source_positions</span> <span class="o">=</span> <span class="n">max_source_positions</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv_depthwise_kernel_size</span> <span class="o">=</span> <span class="n">conv_depthwise_kernel_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_adapter</span> <span class="o">=</span> <span class="n">add_adapter</span>

    <span class="c1"># t2u config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_bos_token_id</span> <span class="o">=</span> <span class="n">t2u_bos_token_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_pad_token_id</span> <span class="o">=</span> <span class="n">t2u_pad_token_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_eos_token_id</span> <span class="o">=</span> <span class="n">t2u_eos_token_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_start_token_id</span> <span class="o">=</span> <span class="n">t2u_decoder_start_token_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_max_new_tokens</span> <span class="o">=</span> <span class="n">t2u_max_new_tokens</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_layers</span> <span class="o">=</span> <span class="n">t2u_encoder_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_ffn_dim</span> <span class="o">=</span> <span class="n">t2u_encoder_ffn_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_attention_heads</span> <span class="o">=</span> <span class="n">t2u_encoder_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_layers</span> <span class="o">=</span> <span class="n">t2u_decoder_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_ffn_dim</span> <span class="o">=</span> <span class="n">t2u_decoder_ffn_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_attention_heads</span> <span class="o">=</span> <span class="n">t2u_decoder_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_max_position_embeddings</span> <span class="o">=</span> <span class="n">t2u_max_position_embeddings</span>

    <span class="c1"># hifi-gan vocoder config</span>
    <span class="c1"># original parameters specific to Hifi-Gan</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span> <span class="o">=</span> <span class="n">sampling_rate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">upsample_initial_channel</span> <span class="o">=</span> <span class="n">upsample_initial_channel</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">upsample_rates</span> <span class="o">=</span> <span class="n">upsample_rates</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">upsample_kernel_sizes</span> <span class="o">=</span> <span class="n">upsample_kernel_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">resblock_kernel_sizes</span> <span class="o">=</span> <span class="n">resblock_kernel_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">resblock_dilation_sizes</span> <span class="o">=</span> <span class="n">resblock_dilation_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu_slope</span> <span class="o">=</span> <span class="n">leaky_relu_slope</span>

    <span class="c1"># specific to Code Hifi-Gan</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unit_hifi_gan_vocab_size</span> <span class="o">=</span> <span class="n">unit_hifi_gan_vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unit_embed_dim</span> <span class="o">=</span> <span class="n">unit_embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lang_embed_dim</span> <span class="o">=</span> <span class="n">lang_embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">spkr_embed_dim</span> <span class="o">=</span> <span class="n">spkr_embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_num_langs</span> <span class="o">=</span> <span class="n">vocoder_num_langs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_num_spkrs</span> <span class="o">=</span> <span class="n">vocoder_num_spkrs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">variance_predictor_kernel_size</span> <span class="o">=</span> <span class="n">variance_predictor_kernel_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">var_pred_dropout</span> <span class="o">=</span> <span class="n">var_pred_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_offset</span> <span class="o">=</span> <span class="n">vocoder_offset</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="n">decoder_start_token_id</span><span class="p">,</span>
        <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
        <span class="n">max_position_embeddings</span><span class="o">=</span><span class="n">max_position_embeddings</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t</code>


<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>Tokenization classes for SeamlessM4T.</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer</code>


<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.tokenization_utils.PreTrainedTokenizer" href="../../tokenization_utils/#mindnlp.transformers.tokenization_utils.PreTrainedTokenizer">PreTrainedTokenizer</a></code></p>


        <p>Construct a SeamlessM4T tokenizer.</p>
<p>Adapted from [<code>RobertaTokenizer</code>] and [<code>XLNetTokenizer</code>]. Based on
<a href="https://github.com/google/sentencepiece">SentencePiece</a>.</p>
<p>The tokenization method is <code>&lt;language code&gt; &lt;tokens&gt; &lt;eos&gt;</code> for source language documents, and <code>&lt;eos&gt; &lt;language
code&gt; &lt;tokens&gt; &lt;eos&gt;</code> for target language documents.</p>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">SeamlessM4TTokenizer</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="o">...</span>     <span class="s2">&quot;facebook/hf-seamless-m4t-medium&quot;</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="s2">&quot;eng&quot;</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="s2">&quot;fra&quot;</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">example_english_phrase</span> <span class="o">=</span> <span class="s2">&quot; UN Chief Says There Is No Military Solution in Syria&quot;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">expected_translation_french</span> <span class="o">=</span> <span class="s2">&quot;Le chef de l&#39;ONU affirme qu&#39;il n&#39;y a pas de solution militaire en Syrie.&quot;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">example_english_phrase</span><span class="p">,</span> <span class="n">text_target</span><span class="o">=</span><span class="n">expected_translation_french</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>
</code></pre></div>
</details>

<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to the vocabulary file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<p><Tip></p>
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>
<p></Tip></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;s&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The end of sequence token.</p>
<p><Tip></p>
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>
<p></Tip></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;/s&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;/s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sep_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;/s&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;/s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cls_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;s&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unk_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;unk&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;unk&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token used for padding, for example when batching sequences of different lengths.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;pad&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;pad&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The path to a tokenizer file to use instead of the vocab file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>src_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as source language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;eng&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;eng&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;fra&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;fra&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sp_model_kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional keyword arguments to pass to the model initialization.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Any]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>additional_special_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tuple or a list of additional special tokens. Can be used to specify the list of languages that will be
supported by the tokenizer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>tuple or list of `str` or `tokenizers.AddedToken`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TTokenizer</span><span class="p">(</span><span class="n">PreTrainedTokenizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct a SeamlessM4T tokenizer.</span>

<span class="sd">    Adapted from [`RobertaTokenizer`] and [`XLNetTokenizer`]. Based on</span>
<span class="sd">    [SentencePiece](https://github.com/google/sentencepiece).</span>

<span class="sd">    The tokenization method is `&lt;language code&gt; &lt;tokens&gt; &lt;eos&gt;` for source language documents, and `&lt;eos&gt; &lt;language</span>
<span class="sd">    code&gt; &lt;tokens&gt; &lt;eos&gt;` for target language documents.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import SeamlessM4TTokenizer</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = SeamlessM4TTokenizer.from_pretrained(</span>
<span class="sd">        ...     &quot;facebook/hf-seamless-m4t-medium&quot;, src_lang=&quot;eng&quot;, tgt_lang=&quot;fra&quot;</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; example_english_phrase = &quot; UN Chief Says There Is No Military Solution in Syria&quot;</span>
<span class="sd">        &gt;&gt;&gt; expected_translation_french = &quot;Le chef de l&#39;ONU affirme qu&#39;il n&#39;y a pas de solution militaire en Syrie.&quot;</span>
<span class="sd">        &gt;&gt;&gt; inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors=&quot;ms&quot;)</span>
<span class="sd">        ```</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_file (`str`):</span>
<span class="sd">            Path to the vocabulary file.</span>
<span class="sd">        bos_token (`str`, *optional*, defaults to `&quot;&lt;s&gt;&quot;`):</span>
<span class="sd">            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</span>

<span class="sd">            &lt;Tip&gt;</span>

<span class="sd">            When building a sequence using special tokens, this is not the token that is used for the beginning of</span>
<span class="sd">            sequence. The token used is the `cls_token`.</span>

<span class="sd">            &lt;/Tip&gt;</span>

<span class="sd">        eos_token (`str`, *optional*, defaults to `&quot;&lt;/s&gt;&quot;`):</span>
<span class="sd">            The end of sequence token.</span>

<span class="sd">            &lt;Tip&gt;</span>

<span class="sd">            When building a sequence using special tokens, this is not the token that is used for the end of sequence.</span>
<span class="sd">            The token used is the `sep_token`.</span>

<span class="sd">            &lt;/Tip&gt;</span>

<span class="sd">        sep_token (`str`, *optional*, defaults to `&quot;&lt;/s&gt;&quot;`):</span>
<span class="sd">            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for</span>
<span class="sd">            sequence classification or for a text and a question for question answering. It is also used as the last</span>
<span class="sd">            token of a sequence built with special tokens.</span>
<span class="sd">        cls_token (`str`, *optional*, defaults to `&quot;&lt;s&gt;&quot;`):</span>
<span class="sd">            The classifier token which is used when doing sequence classification (classification of the whole sequence</span>
<span class="sd">            instead of per-token classification). It is the first token of the sequence when built with special tokens.</span>
<span class="sd">        unk_token (`str`, *optional*, defaults to `&quot;&lt;unk&gt;&quot;`):</span>
<span class="sd">            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this</span>
<span class="sd">            token instead.</span>
<span class="sd">        pad_token (`str`, *optional*, defaults to `&quot;&lt;pad&gt;&quot;`):</span>
<span class="sd">            The token used for padding, for example when batching sequences of different lengths.</span>
<span class="sd">        tokenizer_file (`str`, *optional*):</span>
<span class="sd">            The path to a tokenizer file to use instead of the vocab file.</span>
<span class="sd">        src_lang (`str`, *optional*, defaults to `&quot;eng&quot;`):</span>
<span class="sd">            The language to use as source language for translation.</span>
<span class="sd">        tgt_lang (`str`, *optional*, defaults to `&quot;fra&quot;`):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        sp_model_kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">            Additional keyword arguments to pass to the model initialization.</span>
<span class="sd">        additional_special_tokens (tuple or list of `str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="sd">            A tuple or a list of additional special tokens. Can be used to specify the list of languages that will be</span>
<span class="sd">            supported by the tokenizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">vocab_files_names</span> <span class="o">=</span> <span class="n">VOCAB_FILES_NAMES</span>
    <span class="n">max_model_input_sizes</span> <span class="o">=</span> <span class="n">PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES</span>
    <span class="n">pretrained_vocab_files_map</span> <span class="o">=</span> <span class="n">PRETRAINED_VOCAB_FILES_MAP</span>
    <span class="n">model_input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>

    <span class="n">prefix_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">suffix_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_file</span><span class="p">,</span>
        <span class="n">bos_token</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
        <span class="n">eos_token</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
        <span class="n">sep_token</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
        <span class="n">cls_token</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
        <span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span>
        <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span>
        <span class="n">tokenizer_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">src_lang</span><span class="o">=</span><span class="s2">&quot;eng&quot;</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="o">=</span><span class="s2">&quot;fra&quot;</span><span class="p">,</span>
        <span class="n">sp_model_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">additional_special_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the SeamlessM4TTokenizer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            vocab_file (str): The path to the vocabulary file.</span>
<span class="sd">            bos_token (str, optional): The token representing the beginning of a sequence. Defaults to &#39;&lt;s&gt;&#39;.</span>
<span class="sd">            eos_token (str, optional): The token representing the end of a sequence. Defaults to &#39;&lt;/s&gt;&#39;.</span>
<span class="sd">            sep_token (str, optional): The token used to separate two sequences. Defaults to &#39;&lt;/s&gt;&#39;.</span>
<span class="sd">            cls_token (str, optional): The token representing the classification of a sequence. Defaults to &#39;&lt;s&gt;&#39;.</span>
<span class="sd">            unk_token (str, optional): The token representing an unknown word. Defaults to &#39;&lt;unk&gt;&#39;.</span>
<span class="sd">            pad_token (str, optional): The token used for padding sequences. Defaults to &#39;&lt;pad&gt;&#39;.</span>
<span class="sd">            tokenizer_file (str, optional): The path to the tokenizer file. Defaults to None.</span>
<span class="sd">            src_lang (str, optional): The source language. Defaults to &#39;eng&#39;.</span>
<span class="sd">            tgt_lang (str, optional): The target language. Defaults to &#39;fra&#39;.</span>
<span class="sd">            sp_model_kwargs (Optional[Dict[str, Any]], optional): Additional arguments for the sentencepiece model.</span>
<span class="sd">                Defaults to None.</span>
<span class="sd">            additional_special_tokens (List[str], optional): Additional special tokens. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sp_model_kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">sp_model_kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">sp_model_kwargs</span>
        <span class="c1"># Add this unused argument to keep some important Copied from statements</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">legacy</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span> <span class="o">=</span> <span class="n">vocab_file</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_spm_processor</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;from_slow&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>

        <span class="c1"># Vocab    |    0    |    1    |   2    |    3    |  4   |  5   |  6   |   7  |   8  |  9</span>
        <span class="c1"># -------- | ------- | ------- | ------ | ------- | ---- | ---- | ---- | ---- | ---- | ----</span>
        <span class="c1"># spm  | &#39;&lt;unk&gt;&#39;   | &#39;&lt;s&gt;&#39; | &#39;&lt;/s&gt;&#39; | &#39;an&#39; | &#39;en&#39; | &#39;_d&#39; | &#39;er&#39; | &#39;in&#39; | &#39;_s&#39; | &#39;_a&#39;</span>
        <span class="c1"># fairseq  | &#39;&lt;pad&gt;&#39;   | &#39;&lt;unk&gt;&#39; | &#39;&lt;s&gt;&#39; | &#39;&lt;/s&gt;&#39; | &#39;an&#39; | &#39;en&#39; | &#39;▁d&#39; | &#39;er&#39; | &#39;in&#39; | &#39;▁s&#39;</span>

        <span class="c1"># Mimic fairseq token-to-id alignment for the first 4 token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_added_tokens_decoder</span> <span class="o">=</span> <span class="p">{</span>
            <span class="mi">0</span><span class="p">:</span> <span class="n">AddedToken</span><span class="p">(</span><span class="n">pad_token</span><span class="p">,</span> <span class="n">special</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad_token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">pad_token</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">:</span> <span class="n">AddedToken</span><span class="p">(</span><span class="n">unk_token</span><span class="p">,</span> <span class="n">special</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">unk_token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">unk_token</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">:</span> <span class="n">AddedToken</span><span class="p">(</span><span class="n">bos_token</span><span class="p">,</span> <span class="n">special</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bos_token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">bos_token</span><span class="p">,</span>
            <span class="mi">3</span><span class="p">:</span> <span class="n">AddedToken</span><span class="p">(</span><span class="n">eos_token</span><span class="p">,</span> <span class="n">special</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">eos_token</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># The first &quot;real&quot; token &quot;an&quot; has position 4 in the original fairseq vocab and position 3 in the spm vocab</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fairseq_offset</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sp_model_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">src_lang</span><span class="si">}</span><span class="s2">__&quot;</span> <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">src_lang</span> <span class="k">else</span> <span class="n">src_lang</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">__&quot;</span> <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tgt_lang</span> <span class="k">else</span> <span class="n">tgt_lang</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">bos_token</span><span class="o">=</span><span class="n">bos_token</span><span class="p">,</span>
            <span class="n">eos_token</span><span class="o">=</span><span class="n">eos_token</span><span class="p">,</span>
            <span class="n">unk_token</span><span class="o">=</span><span class="n">unk_token</span><span class="p">,</span>
            <span class="n">sep_token</span><span class="o">=</span><span class="n">sep_token</span><span class="p">,</span>
            <span class="n">cls_token</span><span class="o">=</span><span class="n">cls_token</span><span class="p">,</span>
            <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span>
            <span class="n">tokenizer_file</span><span class="o">=</span><span class="n">tokenizer_file</span><span class="p">,</span>
            <span class="n">src_lang</span><span class="o">=</span><span class="n">src_lang</span><span class="p">,</span>
            <span class="n">tgt_lang</span><span class="o">=</span><span class="n">tgt_lang</span><span class="p">,</span>
            <span class="n">additional_special_tokens</span><span class="o">=</span><span class="n">additional_special_tokens</span><span class="p">,</span>
            <span class="n">sp_model_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model_kwargs</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer.__getstate__</span>
    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the state of the SeamlessM4TTokenizer object.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizer): The instance of the SeamlessM4TTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: A dictionary containing the current state of the object,</span>
<span class="sd">                with the following keys:</span>

<span class="sd">                - &#39;__dict__&#39;: A dictionary containing the object&#39;s instance variables.</span>
<span class="sd">                - &#39;sp_model&#39;: The value of the &#39;sp_model&#39; instance variable set to None.</span>
<span class="sd">                - &#39;sp_model_proto&#39;: The serialized model proto of the &#39;sp_model&#39; instance variable.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;sp_model&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;sp_model_proto&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="o">.</span><span class="n">serialized_model_proto</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer.__setstate__</span>
    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to set the state of the SeamlessM4TTokenizer instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizer): The instance of the SeamlessM4TTokenizer class.</span>
<span class="sd">            d (dict): A dictionary containing the state information to be set on the instance.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">           None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">d</span>

        <span class="c1"># for backward compatibility</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;sp_model_kwargs&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sp_model_kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span> <span class="o">=</span> <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="o">.</span><span class="n">LoadFromSerializedProto</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model_proto</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vocab_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method returns the size of the vocabulary used by the SeamlessM4TTokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the SeamlessM4TTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: The size of the vocabulary used by the SeamlessM4TTokenizer.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_target</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_pair_target</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">src_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            text (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a</span>
<span class="sd">                list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),</span>
<span class="sd">                you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a</span>
<span class="sd">                list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),</span>
<span class="sd">                you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):</span>
<span class="sd">                 Select a strategy to pad the returned sequences (according to the model&#39;s padding side and padding</span>
<span class="sd">                 index) among:</span>

<span class="sd">                - `True` or `&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a single</span>
<span class="sd">                sequence if provided).</span>
<span class="sd">                - `&#39;max_length&#39;`: Pad to a maximum length specified with the argument `max_length` or to the maximum</span>
<span class="sd">                acceptable input length for the model if that argument is not provided.</span>
<span class="sd">                - `False` or `&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of different</span>
<span class="sd">                lengths).</span>
<span class="sd">            pad_to_multiple_of (`int`, *optional*):</span>
<span class="sd">                If set will pad the sequence to a multiple of the provided value.</span>

<span class="sd">                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</span>
<span class="sd">                `&gt;= 7.5` (Volta).</span>
<span class="sd">            src_lang (`str`, *optional*):</span>
<span class="sd">                A string representing the source language. If not specified, the last `src_lang` specified (either</span>
<span class="sd">                during initialization or when calling this tokenizer) will be used.</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                A string representing the target language. If not specified, the last `tgt_lang` specified (either</span>
<span class="sd">                during initialization or when calling this tokenizer) will be used.</span>
<span class="sd">            kwargs (*optional*):</span>
<span class="sd">                Remaining dictionary of keyword arguments that will be passed to [`PreTrainedTokenizer.__call__`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">src_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span>

        <span class="n">output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span>
            <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
            <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
            <span class="n">text_target</span><span class="o">=</span><span class="n">text_target</span><span class="p">,</span>
            <span class="n">text_pair_target</span><span class="o">=</span><span class="n">text_pair_target</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">BatchEncoding</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">tensor_type</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;return_tensors&quot;</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer.src_lang</span>
    <span class="k">def</span> <span class="nf">src_lang</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the source language of the SeamlessM4TTokenizer instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizer): The instance of the SeamlessM4TTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: The source language of the tokenized text.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        This property method returns the source language of the tokenized text. The source language refers to the</span>
<span class="sd">        language in which the original text was written.</span>

<span class="sd">        Note:</span>
<span class="sd">            The source language is stored internally as a private attribute &#39;_src_lang&#39;. This method retrieves the value</span>
<span class="sd">            of &#39;_src_lang&#39; and returns it as a string.</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; tokenizer = SeamlessM4TTokenizer()</span>
<span class="sd">            &gt;&gt;&gt; tokenizer.src_lang</span>
<span class="sd">            &#39;en&#39;</span>
<span class="sd">            ```</span>
<span class="sd">        In the example above, the &#39;src_lang&#39; property method is called on an instance of the SeamlessM4TTokenizer class,</span>
<span class="sd">        returning the source language &#39;en&#39;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span>

    <span class="nd">@src_lang</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">src_lang</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_src_lang</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the source language for the SeamlessM4TTokenizer instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizer): The current instance of the SeamlessM4TTokenizer class.</span>
<span class="sd">            new_src_lang (str): The new source language to be set. It should be a string representing the language code.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method updates the source language attribute of the instance.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">new_src_lang</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">new_src_lang</span><span class="si">}</span><span class="s2">__&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span> <span class="o">=</span> <span class="n">new_src_lang</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tgt_lang</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the target language of the SeamlessM4TTokenizer instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the SeamlessM4TTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: The target language of the tokenizer.</span>
<span class="sd">                It represents the language into which the input text will be translated.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span>

    <span class="nd">@tgt_lang</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">tgt_lang</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_tgt_lang</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the target language for the SeamlessM4TTokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the SeamlessM4TTokenizer class.</span>
<span class="sd">            new_tgt_lang (str): The new target language to set. It should be a string representing the target language code.</span>
<span class="sd">                If the target language does not contain &#39;__&#39; (double underscore), it will be prefixed and suffixed with &#39;__&#39;</span>
<span class="sd">                to indicate that it is a special token. Otherwise, the target language will be set as is.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">new_tgt_lang</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">new_tgt_lang</span><span class="si">}</span><span class="s2">__&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span> <span class="o">=</span> <span class="n">new_tgt_lang</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer.get_special_tokens_mask</span>
    <span class="k">def</span> <span class="nf">get_special_tokens_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding</span>
<span class="sd">        special tokens using the tokenizer `prepare_for_model` method.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (`List[int]`):</span>
<span class="sd">                List of IDs.</span>
<span class="sd">            token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">                Optional second list of IDs for sequence pairs.</span>
<span class="sd">            already_has_special_tokens (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not the token list is already formatted with special tokens for the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">already_has_special_tokens</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span>
                <span class="n">token_ids_0</span><span class="o">=</span><span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="n">token_ids_1</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>

        <span class="n">prefix_ones</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span><span class="p">)</span>
        <span class="n">suffix_ones</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">prefix_ones</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">))</span> <span class="o">+</span> <span class="n">suffix_ones</span>
        <span class="k">return</span> <span class="n">prefix_ones</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">))</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_1</span><span class="p">))</span> <span class="o">+</span> <span class="n">suffix_ones</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer.build_inputs_with_special_tokens</span>
    <span class="k">def</span> <span class="nf">build_inputs_with_special_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and</span>
<span class="sd">        adding special tokens. An NLLB sequence has the following format, where `X` represents the sequence:</span>

<span class="sd">        - `input_ids` (for encoder) `X [eos, src_lang_code]`</span>
<span class="sd">        - `decoder_input_ids`: (for decoder) `X [eos, tgt_lang_code]`</span>

<span class="sd">        BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a</span>
<span class="sd">        separator.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (`List[int]`):</span>
<span class="sd">                List of IDs to which the special tokens will be added.</span>
<span class="sd">            token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">                Optional second list of IDs for sequence pairs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span>
        <span class="c1"># We don&#39;t expect to process pairs, but leave the pair logic for API consistency</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">token_ids_1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer.create_token_type_ids_from_sequences</span>
    <span class="k">def</span> <span class="nf">create_token_type_ids_from_sequences</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a mask from the two sequences passed to be used in a sequence-pair classification task. nllb does not</span>
<span class="sd">        make use of token type ids, therefore a list of zeros is returned.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (`List[int]`):</span>
<span class="sd">                List of IDs.</span>
<span class="sd">            token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">                Optional second list of IDs for sequence pairs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[int]`: List of zeros.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sep</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sep_token_id</span><span class="p">]</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token_id</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span> <span class="o">+</span> <span class="n">sep</span> <span class="o">+</span> <span class="n">token_ids_1</span> <span class="o">+</span> <span class="n">sep</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_build_translation_inputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">raw_inputs</span><span class="p">,</span> <span class="n">return_tensors</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">src_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="o">**</span><span class="n">extra_kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Used by translation pipeline, to prepare inputs for the generate function&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">src_lang</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Translation requires a `src_lang` and a `tgt_lang` for this model.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">raw_inputs</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">extra_kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tgt_lang</span><span class="p">:</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">__&quot;</span>
        <span class="n">tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;forced_bos_token_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tgt_lang_id</span>
        <span class="k">return</span> <span class="n">inputs</span>

    <span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method: get_vocab</span>

<span class="sd">        Description:</span>
<span class="sd">        This method returns the vocabulary for the SeamlessM4TTokenizer instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the SeamlessM4TTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            vocab: A dictionary containing the vocabulary, where the keys are tokens and the values are their</span>
<span class="sd">                corresponding IDs.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fairseq_offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">fairseq_offset</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">vocab</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">vocab</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">unk_token_length</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the length of the unknown token.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizer): An instance of the SeamlessM4TTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: The length of the unknown token.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        This method calculates and returns the length of the unknown token present in the SeamlessM4TTokenizer instance.</span>
<span class="sd">        The unknown token is obtained by encoding the string representation of the &#39;unk_token&#39; attribute using the</span>
<span class="sd">        &#39;sp_model&#39; encoding method. The length of the resulting encoded token is then returned as an integer value.</span>

<span class="sd">        Note that this method takes no additional parameters besides the mandatory &#39;self&#39; parameter, which represents</span>
<span class="sd">        the instance of the SeamlessM4TTokenizer class on which the method is called.</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; tokenizer = SeamlessM4TTokenizer()</span>
<span class="sd">            &gt;&gt;&gt; tokenizer.unk_token = &quot;unknown&quot;</span>
<span class="sd">            &gt;&gt;&gt; tokenizer.unk_token_length()</span>
<span class="sd">            7</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)))</span>

    <span class="c1"># Copied from transformers.models.t5.tokenization_t5.T5Tokenizer.get_spm_processor</span>
    <span class="k">def</span> <span class="nf">get_spm_processor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">from_slow</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieves the SentencePieceProcessor tokenizer for the SeamlessM4TTokenizer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizer): An instance of the SeamlessM4TTokenizer class.</span>
<span class="sd">            from_slow (bool, optional): A flag indicating whether to load the tokenizer from the slow version or not.</span>
<span class="sd">                Defaults to False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model_kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">legacy</span> <span class="ow">or</span> <span class="n">from_slow</span><span class="p">:</span>  <span class="c1"># no dependency on protobuf</span>
            <span class="n">tokenizer</span><span class="o">.</span><span class="n">Load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">tokenizer</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">sp_model</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
            <span class="n">model_pb2</span> <span class="o">=</span> <span class="n">import_protobuf</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The new behaviour of </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> (with `self.legacy = False`)&quot;</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model_pb2</span><span class="o">.</span><span class="n">ModelProto</span><span class="o">.</span><span class="n">FromString</span><span class="p">(</span><span class="n">sp_model</span><span class="p">)</span>
            <span class="n">normalizer_spec</span> <span class="o">=</span> <span class="n">model_pb2</span><span class="o">.</span><span class="n">NormalizerSpec</span><span class="p">()</span>
            <span class="n">normalizer_spec</span><span class="o">.</span><span class="n">add_dummy_prefix</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">model</span><span class="o">.</span><span class="n">normalizer_spec</span><span class="o">.</span><span class="n">MergeFrom</span><span class="p">(</span><span class="n">normalizer_spec</span><span class="p">)</span>
            <span class="n">sp_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
            <span class="n">tokenizer</span><span class="o">.</span><span class="n">LoadFromSerializedProto</span><span class="p">(</span><span class="n">sp_model</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tokenizer</span>

    <span class="c1"># Copied from transformers.models.t5.tokenization_t5.T5Tokenizer.tokenize</span>
    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="s2">&quot;TextInput&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a string to a list of tokens. If `self.legacy` is set to `False`, a prefix token is added unless the</span>
<span class="sd">        first token is special.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">legacy</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">tokens</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">SPIECE_UNDERLINE</span> <span class="o">+</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">SPIECE_UNDERLINE</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">SPIECE_UNDERLINE</span> <span class="ow">and</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">tokens</span>

    <span class="c1"># Copied from transformers.models.t5.tokenization_t5.T5Tokenizer._tokenize</span>
    <span class="k">def</span> <span class="nf">_tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a tokenized string.</span>

<span class="sd">        We de-activated the `add_dummy_prefix` option, thus the sentencepiece internals will always strip any</span>
<span class="sd">        SPIECE_UNDERLINE. For example: `self.sp_model.encode(f&quot;{SPIECE_UNDERLINE}Hey&quot;, out_type = str)` will give</span>
<span class="sd">        `[&#39;H&#39;, &#39;e&#39;, &#39;y&#39;]` instead of `[&#39;▁He&#39;, &#39;y&#39;]`. Thus we always encode `f&quot;{unk_token}text&quot;` and strip the</span>
<span class="sd">        `unk_token`. Here is an example with `unk_token = &quot;&lt;unk&gt;&quot;` and `unk_token_length = 4`.</span>
<span class="sd">        `self.tokenizer.sp_model.encode(&quot;&lt;unk&gt; Hey&quot;, out_type = str)[4:]`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">legacy</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">text</span><span class="o">.</span><span class="n">startswith</span><span class="p">((</span><span class="n">SPIECE_UNDERLINE</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">tokens</span>

        <span class="c1"># 1. Encode string + prefix ex: &quot;&lt;unk&gt; Hey&quot;</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span> <span class="o">+</span> <span class="n">text</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
        <span class="c1"># 2. Remove self.unk_token from [&#39;&lt;&#39;,&#39;unk&#39;,&#39;&gt;&#39;, &#39;▁Hey&#39;]</span>
        <span class="k">return</span> <span class="n">tokens</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token_length</span> <span class="p">:]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_length</span> <span class="k">else</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">_convert_token_to_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Converts a token (str) in an id using the vocab.&quot;&quot;&quot;</span>
        <span class="n">spm_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="o">.</span><span class="n">PieceToId</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

        <span class="c1"># Need to return unknown token if the SP model returned 0</span>
        <span class="k">return</span> <span class="n">spm_id</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">fairseq_offset</span> <span class="k">if</span> <span class="n">spm_id</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_id</span>

    <span class="k">def</span> <span class="nf">_convert_id_to_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Converts an index (integer) in a token (str) using the vocab.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="o">.</span><span class="n">IdToPiece</span><span class="p">(</span><span class="n">index</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">fairseq_offset</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">convert_tokens_to_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Converts a sequence of tokens (strings for sub-words) in a single string.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">SPIECE_UNDERLINE</span><span class="p">):</span>
            <span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">:]</span>

        <span class="n">out_string</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">SPIECE_UNDERLINE</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">out_string</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer.save_vocabulary</span>
    <span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the vocabulary of the SeamlessM4TTokenizer object to a specified directory.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizer): The instance of the SeamlessM4TTokenizer class.</span>
<span class="sd">            save_directory (str): The path of the directory where the vocabulary will be saved.</span>
<span class="sd">            filename_prefix (Optional[str]): An optional prefix to be added to the filename of the saved vocabulary.</span>
<span class="sd">                Default is None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[str]: A tuple containing the path of the saved vocabulary file.</span>

<span class="sd">        Raises:</span>
<span class="sd">            OSError: If the save_directory is not a valid directory.</span>
<span class="sd">            IOError: If the vocabulary file cannot be copied or saved.</span>

<span class="sd">        Note:</span>
<span class="sd">            - The save_directory should be an existing directory.</span>
<span class="sd">            - If the save_directory already contains a file with the same name as the vocabulary file,</span>
<span class="sd">            it will be overwritten.</span>
<span class="sd">            - If the self.vocab_file is not an existing file, the vocabulary will be saved directly to the</span>
<span class="sd">            specified directory.</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; tokenizer = SeamlessM4TTokenizer()</span>
<span class="sd">            &gt;&gt;&gt; save_directory = &#39;/path/to/save_directory&#39;</span>
<span class="sd">            &gt;&gt;&gt; filename_prefix = &#39;my_vocab&#39;</span>
<span class="sd">            &gt;&gt;&gt; saved_file = tokenizer.save_vocabulary(save_directory, filename_prefix)</span>
<span class="sd">            &gt;&gt;&gt; print(saved_file)  # Output: (&#39;/path/to/save_directory/my_vocab-vocab.txt&#39;,)</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary path (</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">) should be a directory&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="n">out_vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">VOCAB_FILES_NAMES</span><span class="p">[</span><span class="s2">&quot;vocab_file&quot;</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">)</span> <span class="o">!=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">out_vocab_file</span><span class="p">)</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">):</span>
            <span class="n">copyfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">out_vocab_file</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">out_vocab_file</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fi</span><span class="p">:</span>
                <span class="n">content_spiece_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="o">.</span><span class="n">serialized_model_proto</span><span class="p">()</span>
                <span class="n">fi</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">content_spiece_model</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">out_vocab_file</span><span class="p">,)</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer.prepare_seq2seq_batch with eng_Latn-&gt;eng, fra_Latn-&gt;fra</span>
    <span class="k">def</span> <span class="nf">prepare_seq2seq_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">src_texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">src_lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;eng&quot;</span><span class="p">,</span>
        <span class="n">tgt_texts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;fra&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare Seq2Seq Batch method in the SeamlessM4TTokenizer class.</span>

<span class="sd">        This method prepares a batch of inputs for sequence-to-sequence models.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            src_texts (List[str]): A list of source texts to be encoded.</span>
<span class="sd">            src_lang (str, optional): The language of the source texts. Defaults to &#39;eng&#39;.</span>
<span class="sd">            tgt_texts (Optional[List[str]], optional): A list of target texts to be encoded. Defaults to None.</span>
<span class="sd">            tgt_lang (str, optional): The language of the target texts. Defaults to &#39;fra&#39;.</span>
<span class="sd">            **kwargs: Additional keyword arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            BatchEncoding: A BatchEncoding object containing the prepared batch of inputs.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">prepare_seq2seq_batch</span><span class="p">(</span><span class="n">src_texts</span><span class="p">,</span> <span class="n">tgt_texts</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer._switch_to_input_mode</span>
    <span class="k">def</span> <span class="nf">_switch_to_input_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Switches the tokenizer to input mode.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizer): An instance of the SeamlessM4TTokenizer class.</span>
<span class="sd">                This parameter is used to access the methods and properties of the class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        Description:</span>
<span class="sd">            This method is used to switch the tokenizer to input mode. In input mode, the tokenizer processes the</span>
<span class="sd">            source language text and prepares it for translation. Switching to input mode involves setting the source</span>
<span class="sd">            language special tokens using the set_src_lang_special_tokens method of the SeamlessM4TTokenizer class.</span>
<span class="sd">            The source language is passed as a parameter to the set_src_lang_special_tokens method to configure the</span>
<span class="sd">            special tokens specific to the source language.</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; tokenizer = SeamlessM4TTokenizer()</span>
<span class="sd">            &gt;&gt;&gt; tokenizer._switch_to_input_mode()</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer._switch_to_target_mode</span>
    <span class="k">def</span> <span class="nf">_switch_to_target_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Switches the tokenizer to the target mode for the SeamlessM4TTokenizer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the SeamlessM4TTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_lang</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_lang</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the special tokens to the source lang setting.</span>
<span class="sd">        Prefix=[src_lang_code], suffix = [eos]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">src_lang</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;src_lang&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">src_lang</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_id</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`src_lang=</span><span class="si">{</span><span class="n">src_lang</span><span class="si">}</span><span class="s2">` has not be found in the vocabulary. Behaviour will probably be unexpected because the language token id will be replaced by the unknown token id.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>

    <span class="c1"># https://github.com/facebookresearch/fairseq2/blob/c53f18e6be6b8b46b722f2249b8397b7eccd7ad3/src/fairseq2/models/nllb/tokenizer.py#L112-L116</span>
    <span class="k">def</span> <span class="nf">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lang</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the special tokens to the target lang setting.</span>
<span class="sd">        Prefix=[eos, tgt_lang_code] and suffix=[eos].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">lang</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;tgt_lang&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lang</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_id</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`tgt_lang=</span><span class="si">{</span><span class="n">lang</span><span class="si">}</span><span class="s2">` has not be found in the vocabulary. Behaviour will probably be unexpected because the language token id will be replaced by the unknown token id.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.src_lang" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">src_lang</span><span class="p">:</span> <span class="nb">str</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
      <small class="doc doc-label doc-label-writable"><code>writable</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.src_lang" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the source language of the SeamlessM4TTokenizer instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the SeamlessM4TTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer" href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer">SeamlessM4TTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>str</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The source language of the tokenized text.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>
        <p>This property method returns the source language of the tokenized text. The source language refers to the
language in which the original text was written.</p>


<details class="note" open>
  <summary>Note</summary>
  <p>The source language is stored internally as a private attribute '_src_lang'. This method retrieves the value
of '_src_lang' and returns it as a string.</p>
</details>

<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SeamlessM4TTokenizer</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">src_lang</span>
<span class="s1">&#39;en&#39;</span>
</code></pre></div>
</details>        <p>In the example above, the 'src_lang' property method is called on an instance of the SeamlessM4TTokenizer class,
returning the source language 'en'.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.tgt_lang" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">tgt_lang</span><span class="p">:</span> <span class="nb">str</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
      <small class="doc doc-label doc-label-writable"><code>writable</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.tgt_lang" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the target language of the SeamlessM4TTokenizer instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the SeamlessM4TTokenizer class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>str</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The target language of the tokenizer.
It represents the language into which the input text will be translated.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.unk_token_length" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">unk_token_length</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.unk_token_length" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the length of the unknown token.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the SeamlessM4TTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer" href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer">SeamlessM4TTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>int</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The length of the unknown token.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <p>This method calculates and returns the length of the unknown token present in the SeamlessM4TTokenizer instance.
The unknown token is obtained by encoding the string representation of the 'unk_token' attribute using the
'sp_model' encoding method. The length of the resulting encoded token is then returned as an integer value.</p>
<p>Note that this method takes no additional parameters besides the mandatory 'self' parameter, which represents
the instance of the SeamlessM4TTokenizer class on which the method is called.</p>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SeamlessM4TTokenizer</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">unk_token</span> <span class="o">=</span> <span class="s2">&quot;unknown&quot;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">unk_token_length</span><span class="p">()</span>
<span class="mi">7</span>
</code></pre></div>
</details>    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.vocab_size" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">vocab_size</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.vocab_size" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method returns the size of the vocabulary used by the SeamlessM4TTokenizer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the SeamlessM4TTokenizer class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>int</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The size of the vocabulary used by the SeamlessM4TTokenizer.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_pair</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_pair_target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__call__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>text</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, `List[str]`, `List[List[str]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_pair</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, `List[str]`, `List[List[str]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_target</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a
list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),
you must set <code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, `List[str]`, `List[List[str]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_pair_target</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a
list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),
you must set <code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, `List[str]`, `List[List[str]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Select a strategy to pad the returned sequences (according to the model's padding side and padding
 index) among:</p>
<ul>
<li><code>True</code> or <code>'longest'</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>'max_length'</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>'do_not_pad'</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_to_multiple_of</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
<code>&gt;= 7.5</code> (Volta).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>src_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A string representing the source language. If not specified, the last <code>src_lang</code> specified (either
during initialization or when calling this tokenizer) will be used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A string representing the target language. If not specified, the last <code>tgt_lang</code> specified (either
during initialization or when calling this tokenizer) will be used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Remaining dictionary of keyword arguments that will be passed to [<code>PreTrainedTokenizer.__call__</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_target</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_pair_target</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">src_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        text (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">        text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">        text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a</span>
<span class="sd">            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),</span>
<span class="sd">            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">        text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a</span>
<span class="sd">            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),</span>
<span class="sd">            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):</span>
<span class="sd">             Select a strategy to pad the returned sequences (according to the model&#39;s padding side and padding</span>
<span class="sd">             index) among:</span>

<span class="sd">            - `True` or `&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a single</span>
<span class="sd">            sequence if provided).</span>
<span class="sd">            - `&#39;max_length&#39;`: Pad to a maximum length specified with the argument `max_length` or to the maximum</span>
<span class="sd">            acceptable input length for the model if that argument is not provided.</span>
<span class="sd">            - `False` or `&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of different</span>
<span class="sd">            lengths).</span>
<span class="sd">        pad_to_multiple_of (`int`, *optional*):</span>
<span class="sd">            If set will pad the sequence to a multiple of the provided value.</span>

<span class="sd">            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</span>
<span class="sd">            `&gt;= 7.5` (Volta).</span>
<span class="sd">        src_lang (`str`, *optional*):</span>
<span class="sd">            A string representing the source language. If not specified, the last `src_lang` specified (either</span>
<span class="sd">            during initialization or when calling this tokenizer) will be used.</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            A string representing the target language. If not specified, the last `tgt_lang` specified (either</span>
<span class="sd">            during initialization or when calling this tokenizer) will be used.</span>
<span class="sd">        kwargs (*optional*):</span>
<span class="sd">            Remaining dictionary of keyword arguments that will be passed to [`PreTrainedTokenizer.__call__`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">src_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span>
        <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
        <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
        <span class="n">text_target</span><span class="o">=</span><span class="n">text_target</span><span class="p">,</span>
        <span class="n">text_pair_target</span><span class="o">=</span><span class="n">text_pair_target</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">BatchEncoding</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">tensor_type</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;return_tensors&quot;</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__getstate__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__getstate__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Return the state of the SeamlessM4TTokenizer object.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the SeamlessM4TTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer" href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer">SeamlessM4TTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>dict</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A dictionary containing the current state of the object,
with the following keys:</p>
<ul>
<li>'<strong>dict</strong>': A dictionary containing the object's instance variables.</li>
<li>'sp_model': The value of the 'sp_model' instance variable set to None.</li>
<li>'sp_model_proto': The serialized model proto of the 'sp_model' instance variable.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the state of the SeamlessM4TTokenizer object.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (SeamlessM4TTokenizer): The instance of the SeamlessM4TTokenizer class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary containing the current state of the object,</span>
<span class="sd">            with the following keys:</span>

<span class="sd">            - &#39;__dict__&#39;: A dictionary containing the object&#39;s instance variables.</span>
<span class="sd">            - &#39;sp_model&#39;: The value of the &#39;sp_model&#39; instance variable set to None.</span>
<span class="sd">            - &#39;sp_model_proto&#39;: The serialized model proto of the &#39;sp_model&#39; instance variable.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;sp_model&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;sp_model_proto&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="o">.</span><span class="n">serialized_model_proto</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">state</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">bos_token</span><span class="o">=</span><span class="s1">&#39;&lt;s&gt;&#39;</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="s1">&#39;&lt;/s&gt;&#39;</span><span class="p">,</span> <span class="n">sep_token</span><span class="o">=</span><span class="s1">&#39;&lt;/s&gt;&#39;</span><span class="p">,</span> <span class="n">cls_token</span><span class="o">=</span><span class="s1">&#39;&lt;s&gt;&#39;</span><span class="p">,</span> <span class="n">unk_token</span><span class="o">=</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s1">&#39;&lt;pad&gt;&#39;</span><span class="p">,</span> <span class="n">tokenizer_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="s1">&#39;eng&#39;</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="s1">&#39;fra&#39;</span><span class="p">,</span> <span class="n">sp_model_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">additional_special_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the SeamlessM4TTokenizer class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The path to the vocabulary file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token representing the beginning of a sequence. Defaults to '<s>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token representing the end of a sequence. Defaults to '</s>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;/s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sep_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token used to separate two sequences. Defaults to '</s>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;/s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cls_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token representing the classification of a sequence. Defaults to '<s>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unk_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token representing an unknown word. Defaults to '<unk>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;unk&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token used for padding sequences. Defaults to '<pad>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;pad&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The path to the tokenizer file. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>src_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The source language. Defaults to 'eng'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;eng&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The target language. Defaults to 'fra'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;fra&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sp_model_kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional arguments for the sentencepiece model.
Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>additional_special_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional special tokens. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_file</span><span class="p">,</span>
    <span class="n">bos_token</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
    <span class="n">eos_token</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
    <span class="n">sep_token</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
    <span class="n">cls_token</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
    <span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span>
    <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span>
    <span class="n">tokenizer_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">src_lang</span><span class="o">=</span><span class="s2">&quot;eng&quot;</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="o">=</span><span class="s2">&quot;fra&quot;</span><span class="p">,</span>
    <span class="n">sp_model_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">additional_special_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the SeamlessM4TTokenizer class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        vocab_file (str): The path to the vocabulary file.</span>
<span class="sd">        bos_token (str, optional): The token representing the beginning of a sequence. Defaults to &#39;&lt;s&gt;&#39;.</span>
<span class="sd">        eos_token (str, optional): The token representing the end of a sequence. Defaults to &#39;&lt;/s&gt;&#39;.</span>
<span class="sd">        sep_token (str, optional): The token used to separate two sequences. Defaults to &#39;&lt;/s&gt;&#39;.</span>
<span class="sd">        cls_token (str, optional): The token representing the classification of a sequence. Defaults to &#39;&lt;s&gt;&#39;.</span>
<span class="sd">        unk_token (str, optional): The token representing an unknown word. Defaults to &#39;&lt;unk&gt;&#39;.</span>
<span class="sd">        pad_token (str, optional): The token used for padding sequences. Defaults to &#39;&lt;pad&gt;&#39;.</span>
<span class="sd">        tokenizer_file (str, optional): The path to the tokenizer file. Defaults to None.</span>
<span class="sd">        src_lang (str, optional): The source language. Defaults to &#39;eng&#39;.</span>
<span class="sd">        tgt_lang (str, optional): The target language. Defaults to &#39;fra&#39;.</span>
<span class="sd">        sp_model_kwargs (Optional[Dict[str, Any]], optional): Additional arguments for the sentencepiece model.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        additional_special_tokens (List[str], optional): Additional special tokens. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sp_model_kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">sp_model_kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">sp_model_kwargs</span>
    <span class="c1"># Add this unused argument to keep some important Copied from statements</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">legacy</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span> <span class="o">=</span> <span class="n">vocab_file</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_spm_processor</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;from_slow&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>

    <span class="c1"># Vocab    |    0    |    1    |   2    |    3    |  4   |  5   |  6   |   7  |   8  |  9</span>
    <span class="c1"># -------- | ------- | ------- | ------ | ------- | ---- | ---- | ---- | ---- | ---- | ----</span>
    <span class="c1"># spm  | &#39;&lt;unk&gt;&#39;   | &#39;&lt;s&gt;&#39; | &#39;&lt;/s&gt;&#39; | &#39;an&#39; | &#39;en&#39; | &#39;_d&#39; | &#39;er&#39; | &#39;in&#39; | &#39;_s&#39; | &#39;_a&#39;</span>
    <span class="c1"># fairseq  | &#39;&lt;pad&gt;&#39;   | &#39;&lt;unk&gt;&#39; | &#39;&lt;s&gt;&#39; | &#39;&lt;/s&gt;&#39; | &#39;an&#39; | &#39;en&#39; | &#39;▁d&#39; | &#39;er&#39; | &#39;in&#39; | &#39;▁s&#39;</span>

    <span class="c1"># Mimic fairseq token-to-id alignment for the first 4 token</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_added_tokens_decoder</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mi">0</span><span class="p">:</span> <span class="n">AddedToken</span><span class="p">(</span><span class="n">pad_token</span><span class="p">,</span> <span class="n">special</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad_token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">pad_token</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">:</span> <span class="n">AddedToken</span><span class="p">(</span><span class="n">unk_token</span><span class="p">,</span> <span class="n">special</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">unk_token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">unk_token</span><span class="p">,</span>
        <span class="mi">2</span><span class="p">:</span> <span class="n">AddedToken</span><span class="p">(</span><span class="n">bos_token</span><span class="p">,</span> <span class="n">special</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bos_token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">bos_token</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">:</span> <span class="n">AddedToken</span><span class="p">(</span><span class="n">eos_token</span><span class="p">,</span> <span class="n">special</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">eos_token</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="c1"># The first &quot;real&quot; token &quot;an&quot; has position 4 in the original fairseq vocab and position 3 in the spm vocab</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fairseq_offset</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">sp_model_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">src_lang</span><span class="si">}</span><span class="s2">__&quot;</span> <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">src_lang</span> <span class="k">else</span> <span class="n">src_lang</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">__&quot;</span> <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tgt_lang</span> <span class="k">else</span> <span class="n">tgt_lang</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">bos_token</span><span class="o">=</span><span class="n">bos_token</span><span class="p">,</span>
        <span class="n">eos_token</span><span class="o">=</span><span class="n">eos_token</span><span class="p">,</span>
        <span class="n">unk_token</span><span class="o">=</span><span class="n">unk_token</span><span class="p">,</span>
        <span class="n">sep_token</span><span class="o">=</span><span class="n">sep_token</span><span class="p">,</span>
        <span class="n">cls_token</span><span class="o">=</span><span class="n">cls_token</span><span class="p">,</span>
        <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span>
        <span class="n">tokenizer_file</span><span class="o">=</span><span class="n">tokenizer_file</span><span class="p">,</span>
        <span class="n">src_lang</span><span class="o">=</span><span class="n">src_lang</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="o">=</span><span class="n">tgt_lang</span><span class="p">,</span>
        <span class="n">additional_special_tokens</span><span class="o">=</span><span class="n">additional_special_tokens</span><span class="p">,</span>
        <span class="n">sp_model_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model_kwargs</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__setstate__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">d</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.__setstate__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Method to set the state of the SeamlessM4TTokenizer instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the SeamlessM4TTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer" href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer">SeamlessM4TTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>d</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary containing the state information to be set on the instance.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>dict</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method to set the state of the SeamlessM4TTokenizer instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (SeamlessM4TTokenizer): The instance of the SeamlessM4TTokenizer class.</span>
<span class="sd">        d (dict): A dictionary containing the state information to be set on the instance.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">       None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">d</span>

    <span class="c1"># for backward compatibility</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;sp_model_kwargs&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sp_model_kwargs</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span> <span class="o">=</span> <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model_kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="o">.</span><span class="n">LoadFromSerializedProto</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model_proto</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.build_inputs_with_special_tokens" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">build_inputs_with_special_tokens</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.build_inputs_with_special_tokens" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An NLLB sequence has the following format, where <code>X</code> represents the sequence:</p>
<ul>
<li><code>input_ids</code> (for encoder) <code>X [eos, src_lang_code]</code></li>
<li><code>decoder_input_ids</code>: (for decoder) <code>X [eos, tgt_lang_code]</code></li>
</ul>
<p>BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a
separator.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_0</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of IDs to which the special tokens will be added.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_1</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional second list of IDs for sequence pairs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.List">List</span>[int]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>List[int]</code>: List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">build_inputs_with_special_tokens</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and</span>
<span class="sd">    adding special tokens. An NLLB sequence has the following format, where `X` represents the sequence:</span>

<span class="sd">    - `input_ids` (for encoder) `X [eos, src_lang_code]`</span>
<span class="sd">    - `decoder_input_ids`: (for decoder) `X [eos, tgt_lang_code]`</span>

<span class="sd">    BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a</span>
<span class="sd">    separator.</span>

<span class="sd">    Args:</span>
<span class="sd">        token_ids_0 (`List[int]`):</span>
<span class="sd">            List of IDs to which the special tokens will be added.</span>
<span class="sd">        token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">            Optional second list of IDs for sequence pairs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span>
    <span class="c1"># We don&#39;t expect to process pairs, but leave the pair logic for API consistency</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">token_ids_1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.convert_tokens_to_string" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_string</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.convert_tokens_to_string" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Converts a sequence of tokens (strings for sub-words) in a single string.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">convert_tokens_to_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Converts a sequence of tokens (strings for sub-words) in a single string.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">SPIECE_UNDERLINE</span><span class="p">):</span>
        <span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">:]</span>

    <span class="n">out_string</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">SPIECE_UNDERLINE</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">out_string</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.create_token_type_ids_from_sequences" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">create_token_type_ids_from_sequences</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.create_token_type_ids_from_sequences" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create a mask from the two sequences passed to be used in a sequence-pair classification task. nllb does not
make use of token type ids, therefore a list of zeros is returned.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_0</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of IDs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_1</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional second list of IDs for sequence pairs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.List">List</span>[int]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>List[int]</code>: List of zeros.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_token_type_ids_from_sequences</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a mask from the two sequences passed to be used in a sequence-pair classification task. nllb does not</span>
<span class="sd">    make use of token type ids, therefore a list of zeros is returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        token_ids_0 (`List[int]`):</span>
<span class="sd">            List of IDs.</span>
<span class="sd">        token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">            Optional second list of IDs for sequence pairs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `List[int]`: List of zeros.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sep</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sep_token_id</span><span class="p">]</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token_id</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span> <span class="o">+</span> <span class="n">sep</span> <span class="o">+</span> <span class="n">token_ids_1</span> <span class="o">+</span> <span class="n">sep</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_special_tokens_mask" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_special_tokens_mask" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_0</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of IDs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_1</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional second list of IDs for sequence pairs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>already_has_special_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not the token list is already formatted with special tokens for the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.List">List</span>[int]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>List[int]</code>: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_special_tokens_mask</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding</span>
<span class="sd">    special tokens using the tokenizer `prepare_for_model` method.</span>

<span class="sd">    Args:</span>
<span class="sd">        token_ids_0 (`List[int]`):</span>
<span class="sd">            List of IDs.</span>
<span class="sd">        token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">            Optional second list of IDs for sequence pairs.</span>
<span class="sd">        already_has_special_tokens (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not the token list is already formatted with special tokens for the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">already_has_special_tokens</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span>
            <span class="n">token_ids_0</span><span class="o">=</span><span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="n">token_ids_1</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="n">prefix_ones</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span><span class="p">)</span>
    <span class="n">suffix_ones</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">prefix_ones</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">))</span> <span class="o">+</span> <span class="n">suffix_ones</span>
    <span class="k">return</span> <span class="n">prefix_ones</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">))</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_1</span><span class="p">))</span> <span class="o">+</span> <span class="n">suffix_ones</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_spm_processor" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">get_spm_processor</span><span class="p">(</span><span class="n">from_slow</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_spm_processor" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Retrieves the SentencePieceProcessor tokenizer for the SeamlessM4TTokenizer class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the SeamlessM4TTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer" href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer">SeamlessM4TTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>from_slow</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A flag indicating whether to load the tokenizer from the slow version or not.
Defaults to False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_spm_processor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">from_slow</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieves the SentencePieceProcessor tokenizer for the SeamlessM4TTokenizer class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (SeamlessM4TTokenizer): An instance of the SeamlessM4TTokenizer class.</span>
<span class="sd">        from_slow (bool, optional): A flag indicating whether to load the tokenizer from the slow version or not.</span>
<span class="sd">            Defaults to False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_model_kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">legacy</span> <span class="ow">or</span> <span class="n">from_slow</span><span class="p">:</span>  <span class="c1"># no dependency on protobuf</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">Load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tokenizer</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">sp_model</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="n">model_pb2</span> <span class="o">=</span> <span class="n">import_protobuf</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The new behaviour of </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> (with `self.legacy = False`)&quot;</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model_pb2</span><span class="o">.</span><span class="n">ModelProto</span><span class="o">.</span><span class="n">FromString</span><span class="p">(</span><span class="n">sp_model</span><span class="p">)</span>
        <span class="n">normalizer_spec</span> <span class="o">=</span> <span class="n">model_pb2</span><span class="o">.</span><span class="n">NormalizerSpec</span><span class="p">()</span>
        <span class="n">normalizer_spec</span><span class="o">.</span><span class="n">add_dummy_prefix</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">model</span><span class="o">.</span><span class="n">normalizer_spec</span><span class="o">.</span><span class="n">MergeFrom</span><span class="p">(</span><span class="n">normalizer_spec</span><span class="p">)</span>
        <span class="n">sp_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">LoadFromSerializedProto</span><span class="p">(</span><span class="n">sp_model</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenizer</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_vocab" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">get_vocab</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.get_vocab" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Description:
This method returns the vocabulary for the SeamlessM4TTokenizer instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the SeamlessM4TTokenizer class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A dictionary containing the vocabulary, where the keys are tokens and the values are their
corresponding IDs.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method: get_vocab</span>

<span class="sd">    Description:</span>
<span class="sd">    This method returns the vocabulary for the SeamlessM4TTokenizer instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the SeamlessM4TTokenizer class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        vocab: A dictionary containing the vocabulary, where the keys are tokens and the values are their</span>
<span class="sd">            corresponding IDs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fairseq_offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">fairseq_offset</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="n">vocab</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vocab</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.prepare_seq2seq_batch" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">prepare_seq2seq_batch</span><span class="p">(</span><span class="n">src_texts</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="s1">&#39;eng&#39;</span><span class="p">,</span> <span class="n">tgt_texts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="s1">&#39;fra&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.prepare_seq2seq_batch" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Prepare Seq2Seq Batch method in the SeamlessM4TTokenizer class.</p>
<p>This method prepares a batch of inputs for sequence-to-sequence models.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>src_texts</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of source texts to be encoded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[str]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>src_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language of the source texts. Defaults to 'eng'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;eng&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_texts</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of target texts to be encoded. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language of the target texts. Defaults to 'fra'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;fra&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional keyword arguments.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>BatchEncoding</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A BatchEncoding object containing the prepared batch of inputs.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.tokenization_utils.BatchEncoding">BatchEncoding</span></code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_seq2seq_batch</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">src_texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">src_lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;eng&quot;</span><span class="p">,</span>
    <span class="n">tgt_texts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;fra&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prepare Seq2Seq Batch method in the SeamlessM4TTokenizer class.</span>

<span class="sd">    This method prepares a batch of inputs for sequence-to-sequence models.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        src_texts (List[str]): A list of source texts to be encoded.</span>
<span class="sd">        src_lang (str, optional): The language of the source texts. Defaults to &#39;eng&#39;.</span>
<span class="sd">        tgt_texts (Optional[List[str]], optional): A list of target texts to be encoded. Defaults to None.</span>
<span class="sd">        tgt_lang (str, optional): The language of the target texts. Defaults to &#39;fra&#39;.</span>
<span class="sd">        **kwargs: Additional keyword arguments.</span>

<span class="sd">    Returns:</span>
<span class="sd">        BatchEncoding: A BatchEncoding object containing the prepared batch of inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">prepare_seq2seq_batch</span><span class="p">(</span><span class="n">src_texts</span><span class="p">,</span> <span class="n">tgt_texts</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.save_vocabulary" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">save_vocabulary</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.save_vocabulary" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Save the vocabulary of the SeamlessM4TTokenizer object to a specified directory.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the SeamlessM4TTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer" href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer">SeamlessM4TTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The path of the directory where the vocabulary will be saved.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>filename_prefix</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An optional prefix to be added to the filename of the saved vocabulary.
Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Tuple">Tuple</span>[str]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Tuple[str]: A tuple containing the path of the saved vocabulary file.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>OSError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the save_directory is not a valid directory.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>IOError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the vocabulary file cannot be copied or saved.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>The save_directory should be an existing directory.</li>
<li>If the save_directory already contains a file with the same name as the vocabulary file,
it will be overwritten.</li>
<li>If the self.vocab_file is not an existing file, the vocabulary will be saved directly to the
specified directory.</li>
</ul>
</details>

<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SeamlessM4TTokenizer</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">save_directory</span> <span class="o">=</span> <span class="s1">&#39;/path/to/save_directory&#39;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">filename_prefix</span> <span class="o">=</span> <span class="s1">&#39;my_vocab&#39;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">saved_file</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">save_vocabulary</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">saved_file</span><span class="p">)</span>  <span class="c1"># Output: (&#39;/path/to/save_directory/my_vocab-vocab.txt&#39;,)</span>
</code></pre></div>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the vocabulary of the SeamlessM4TTokenizer object to a specified directory.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (SeamlessM4TTokenizer): The instance of the SeamlessM4TTokenizer class.</span>
<span class="sd">        save_directory (str): The path of the directory where the vocabulary will be saved.</span>
<span class="sd">        filename_prefix (Optional[str]): An optional prefix to be added to the filename of the saved vocabulary.</span>
<span class="sd">            Default is None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[str]: A tuple containing the path of the saved vocabulary file.</span>

<span class="sd">    Raises:</span>
<span class="sd">        OSError: If the save_directory is not a valid directory.</span>
<span class="sd">        IOError: If the vocabulary file cannot be copied or saved.</span>

<span class="sd">    Note:</span>
<span class="sd">        - The save_directory should be an existing directory.</span>
<span class="sd">        - If the save_directory already contains a file with the same name as the vocabulary file,</span>
<span class="sd">        it will be overwritten.</span>
<span class="sd">        - If the self.vocab_file is not an existing file, the vocabulary will be saved directly to the</span>
<span class="sd">        specified directory.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = SeamlessM4TTokenizer()</span>
<span class="sd">        &gt;&gt;&gt; save_directory = &#39;/path/to/save_directory&#39;</span>
<span class="sd">        &gt;&gt;&gt; filename_prefix = &#39;my_vocab&#39;</span>
<span class="sd">        &gt;&gt;&gt; saved_file = tokenizer.save_vocabulary(save_directory, filename_prefix)</span>
<span class="sd">        &gt;&gt;&gt; print(saved_file)  # Output: (&#39;/path/to/save_directory/my_vocab-vocab.txt&#39;,)</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary path (</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">) should be a directory&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="n">out_vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">VOCAB_FILES_NAMES</span><span class="p">[</span><span class="s2">&quot;vocab_file&quot;</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">)</span> <span class="o">!=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">out_vocab_file</span><span class="p">)</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">):</span>
        <span class="n">copyfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">out_vocab_file</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">):</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">out_vocab_file</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fi</span><span class="p">:</span>
            <span class="n">content_spiece_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sp_model</span><span class="o">.</span><span class="n">serialized_model_proto</span><span class="p">()</span>
            <span class="n">fi</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">content_spiece_model</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">out_vocab_file</span><span class="p">,)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.set_src_lang_special_tokens" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">set_src_lang_special_tokens</span><span class="p">(</span><span class="n">src_lang</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.set_src_lang_special_tokens" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Reset the special tokens to the source lang setting.
Prefix=[src_lang_code], suffix = [eos]</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_lang</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the special tokens to the source lang setting.</span>
<span class="sd">    Prefix=[src_lang_code], suffix = [eos]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">src_lang</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;src_lang&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">src_lang</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_id</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;`src_lang=</span><span class="si">{</span><span class="n">src_lang</span><span class="si">}</span><span class="s2">` has not be found in the vocabulary. Behaviour will probably be unexpected because the language token id will be replaced by the unknown token id.&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.set_tgt_lang_special_tokens" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="n">lang</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.set_tgt_lang_special_tokens" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Reset the special tokens to the target lang setting.
Prefix=[eos, tgt_lang_code] and suffix=[eos].</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lang</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the special tokens to the target lang setting.</span>
<span class="sd">    Prefix=[eos, tgt_lang_code] and suffix=[eos].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">lang</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;tgt_lang&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lang</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_id</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;`tgt_lang=</span><span class="si">{</span><span class="n">lang</span><span class="si">}</span><span class="s2">` has not be found in the vocabulary. Behaviour will probably be unexpected because the language token id will be replaced by the unknown token id.&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.tokenize" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TTokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t.SeamlessM4TTokenizer.tokenize" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Converts a string to a list of tokens. If <code>self.legacy</code> is set to <code>False</code>, a prefix token is added unless the
first token is special.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="s2">&quot;TextInput&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a string to a list of tokens. If `self.legacy` is set to `False`, a prefix token is added unless the</span>
<span class="sd">    first token is special.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">legacy</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">SPIECE_UNDERLINE</span> <span class="o">+</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">SPIECE_UNDERLINE</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">SPIECE_UNDERLINE</span> <span class="ow">and</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast</code>


<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>Fast Tokenization class for SeamlessM4T.</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast</code>


<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.tokenization_utils_fast.PreTrainedTokenizerFast" href="../../tokenization_utils_fast/#mindnlp.transformers.tokenization_utils_fast.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a></code></p>


        <p>Construct a "fast" SeamlessM4T tokenizer (backed by HuggingFace's <em>tokenizers</em> library). Based on
<a href="https://hf-mirror.com/docs/tokenizers/python/latest/components.html?highlight=BPE#models">BPE</a>.</p>
<p>This tokenizer inherits from [<code>PreTrainedTokenizerFast</code>] which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.</p>
<p>The tokenization method is <code>&lt;language code&gt; &lt;tokens&gt; &lt;eos&gt;</code> for source language documents, and <code>&lt;eos&gt; &lt;language
code&gt; &lt;tokens&gt; &lt;eos&gt;</code> for target language documents.</p>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">SeamlessM4TTokenizerFast</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="o">...</span>     <span class="s2">&quot;facebook/hf-seamless-m4t-medium&quot;</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="s2">&quot;eng&quot;</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="s2">&quot;fra&quot;</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">example_english_phrase</span> <span class="o">=</span> <span class="s2">&quot; UN Chief Says There Is No Military Solution in Syria&quot;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">expected_translation_french</span> <span class="o">=</span> <span class="s2">&quot;Le chef de l&#39;ONU affirme qu&#39;il n&#39;y a pas de solution militaire en Syrie.&quot;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">example_english_phrase</span><span class="p">,</span> <span class="n">text_target</span><span class="o">=</span><span class="n">expected_translation_french</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>
</code></pre></div>
</details>

<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to the vocabulary file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The path to a tokenizer file to use instead of the vocab file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<p><Tip></p>
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>
<p></Tip></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;s&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The end of sequence token.</p>
<p><Tip></p>
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>
<p></Tip></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;/s&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;/s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sep_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;/s&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;/s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cls_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;s&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unk_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;unk&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;unk&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token used for padding, for example when batching sequences of different lengths.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;pad&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;pad&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>src_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as source language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;eng&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;eng&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;fra&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;fra&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>additional_special_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tuple or a list of additional special tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>tuple or list of `str` or `tokenizers.AddedToken`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t_fast.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TTokenizerFast</span><span class="p">(</span><span class="n">PreTrainedTokenizerFast</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct a &quot;fast&quot; SeamlessM4T tokenizer (backed by HuggingFace&#39;s *tokenizers* library). Based on</span>
<span class="sd">    [BPE](https://hf-mirror.com/docs/tokenizers/python/latest/components.html?highlight=BPE#models).</span>

<span class="sd">    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should</span>
<span class="sd">    refer to this superclass for more information regarding those methods.</span>

<span class="sd">    The tokenization method is `&lt;language code&gt; &lt;tokens&gt; &lt;eos&gt;` for source language documents, and `&lt;eos&gt; &lt;language</span>
<span class="sd">    code&gt; &lt;tokens&gt; &lt;eos&gt;` for target language documents.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import SeamlessM4TTokenizerFast</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = SeamlessM4TTokenizerFast.from_pretrained(</span>
<span class="sd">        ...     &quot;facebook/hf-seamless-m4t-medium&quot;, src_lang=&quot;eng&quot;, tgt_lang=&quot;fra&quot;</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; example_english_phrase = &quot; UN Chief Says There Is No Military Solution in Syria&quot;</span>
<span class="sd">        &gt;&gt;&gt; expected_translation_french = &quot;Le chef de l&#39;ONU affirme qu&#39;il n&#39;y a pas de solution militaire en Syrie.&quot;</span>
<span class="sd">        &gt;&gt;&gt; inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors=&quot;ms&quot;)</span>
<span class="sd">        ```</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_file (`str`, *optional*):</span>
<span class="sd">            Path to the vocabulary file.</span>
<span class="sd">        tokenizer_file (`str`, *optional*):</span>
<span class="sd">            The path to a tokenizer file to use instead of the vocab file.</span>
<span class="sd">        bos_token (`str`, *optional*, defaults to `&quot;&lt;s&gt;&quot;`):</span>
<span class="sd">            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</span>

<span class="sd">            &lt;Tip&gt;</span>

<span class="sd">            When building a sequence using special tokens, this is not the token that is used for the beginning of</span>
<span class="sd">            sequence. The token used is the `cls_token`.</span>

<span class="sd">            &lt;/Tip&gt;</span>

<span class="sd">        eos_token (`str`, *optional*, defaults to `&quot;&lt;/s&gt;&quot;`):</span>
<span class="sd">            The end of sequence token.</span>

<span class="sd">            &lt;Tip&gt;</span>

<span class="sd">            When building a sequence using special tokens, this is not the token that is used for the end of sequence.</span>
<span class="sd">            The token used is the `sep_token`.</span>

<span class="sd">            &lt;/Tip&gt;</span>

<span class="sd">        sep_token (`str`, *optional*, defaults to `&quot;&lt;/s&gt;&quot;`):</span>
<span class="sd">            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for</span>
<span class="sd">            sequence classification or for a text and a question for question answering. It is also used as the last</span>
<span class="sd">            token of a sequence built with special tokens.</span>
<span class="sd">        cls_token (`str`, *optional*, defaults to `&quot;&lt;s&gt;&quot;`):</span>
<span class="sd">            The classifier token which is used when doing sequence classification (classification of the whole sequence</span>
<span class="sd">            instead of per-token classification). It is the first token of the sequence when built with special tokens.</span>
<span class="sd">        unk_token (`str`, *optional*, defaults to `&quot;&lt;unk&gt;&quot;`):</span>
<span class="sd">            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this</span>
<span class="sd">            token instead.</span>
<span class="sd">        pad_token (`str`, *optional*, defaults to `&quot;&lt;pad&gt;&quot;`):</span>
<span class="sd">            The token used for padding, for example when batching sequences of different lengths.</span>
<span class="sd">        src_lang (`str`, *optional*, defaults to `&quot;eng&quot;`):</span>
<span class="sd">            The language to use as source language for translation.</span>
<span class="sd">        tgt_lang (`str`, *optional*, defaults to `&quot;fra&quot;`):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        additional_special_tokens (tuple or list of `str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="sd">            A tuple or a list of additional special tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">vocab_files_names</span> <span class="o">=</span> <span class="n">VOCAB_FILES_NAMES</span>
    <span class="n">pretrained_vocab_files_map</span> <span class="o">=</span> <span class="n">PRETRAINED_VOCAB_FILES_MAP</span>
    <span class="n">max_model_input_sizes</span> <span class="o">=</span> <span class="n">PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES</span>
    <span class="n">slow_tokenizer_class</span> <span class="o">=</span> <span class="n">SeamlessM4TTokenizer</span>
    <span class="n">model_input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>

    <span class="n">prefix_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">suffix_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">bos_token</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
        <span class="n">eos_token</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
        <span class="n">sep_token</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
        <span class="n">cls_token</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
        <span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span>
        <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span>
        <span class="n">src_lang</span><span class="o">=</span><span class="s2">&quot;eng&quot;</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="o">=</span><span class="s2">&quot;fra&quot;</span><span class="p">,</span>
        <span class="n">additional_special_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the SeamlessM4TTokenizerFast class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the SeamlessM4TTokenizerFast class.</span>
<span class="sd">            vocab_file (str): Path to the vocabulary file.</span>
<span class="sd">            tokenizer_file (str): Path to the tokenizer file.</span>
<span class="sd">            bos_token (str): The beginning of sequence token. Defaults to &#39;&lt;s&gt;&#39;.</span>
<span class="sd">            eos_token (str): The end of sequence token. Defaults to &#39;&lt;/s&gt;&#39;.</span>
<span class="sd">            sep_token (str): The separator token. Defaults to &#39;&lt;/s&gt;&#39;.</span>
<span class="sd">            cls_token (str): The classification token. Defaults to &#39;&lt;s&gt;&#39;.</span>
<span class="sd">            unk_token (str): The unknown token. Defaults to &#39;&lt;unk&gt;&#39;.</span>
<span class="sd">            pad_token (str): The padding token. Defaults to &#39;&lt;pad&gt;&#39;.</span>
<span class="sd">            src_lang (str): The source language. Defaults to &#39;eng&#39;.</span>
<span class="sd">            tgt_lang (str): The target language. Defaults to &#39;fra&#39;.</span>
<span class="sd">            additional_special_tokens (List[str]): A list of additional special tokens. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">vocab_file</span><span class="o">=</span><span class="n">vocab_file</span><span class="p">,</span>
            <span class="n">tokenizer_file</span><span class="o">=</span><span class="n">tokenizer_file</span><span class="p">,</span>
            <span class="n">bos_token</span><span class="o">=</span><span class="n">bos_token</span><span class="p">,</span>
            <span class="n">eos_token</span><span class="o">=</span><span class="n">eos_token</span><span class="p">,</span>
            <span class="n">sep_token</span><span class="o">=</span><span class="n">sep_token</span><span class="p">,</span>
            <span class="n">cls_token</span><span class="o">=</span><span class="n">cls_token</span><span class="p">,</span>
            <span class="n">unk_token</span><span class="o">=</span><span class="n">unk_token</span><span class="p">,</span>
            <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span>
            <span class="n">src_lang</span><span class="o">=</span><span class="n">src_lang</span><span class="p">,</span>
            <span class="n">tgt_lang</span><span class="o">=</span><span class="n">tgt_lang</span><span class="p">,</span>
            <span class="n">additional_special_tokens</span><span class="o">=</span><span class="n">additional_special_tokens</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span> <span class="o">=</span> <span class="n">vocab_file</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">src_lang</span><span class="si">}</span><span class="s2">__&quot;</span> <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">src_lang</span> <span class="k">else</span> <span class="n">src_lang</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">__&quot;</span> <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tgt_lang</span> <span class="k">else</span> <span class="n">tgt_lang</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">can_save_slow_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method checks if the slow tokenizer can be saved.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizerFast): The instance of the SeamlessM4TTokenizerFast class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            bool: Returns True if the vocab_file exists, False otherwise.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span> <span class="k">else</span> <span class="kc">False</span>

    <span class="nd">@property</span>
    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb.NllbTokenizer.src_lang</span>
    <span class="k">def</span> <span class="nf">src_lang</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method returns the source language used for tokenization.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the SeamlessM4TTokenizerFast class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: The source language used for tokenization.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span>

    <span class="nd">@src_lang</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">src_lang</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_src_lang</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        src_lang(self, new_src_lang: str) -&gt; None</span>

<span class="sd">        This method sets the source language for the SeamlessM4TTokenizerFast object.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the SeamlessM4TTokenizerFast class.</span>
<span class="sd">            new_src_lang (str): The new source language to be set. It should be a string representing the language code.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">new_src_lang</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">new_src_lang</span><span class="si">}</span><span class="s2">__&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span> <span class="o">=</span> <span class="n">new_src_lang</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tgt_lang</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        tgt_lang method in the SeamlessM4TTokenizerFast class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: A reference to the current instance of the class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: The language code representing the target language for tokenization.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span>

    <span class="nd">@tgt_lang</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">tgt_lang</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_tgt_lang</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the target language for the SeamlessM4TTokenizerFast object.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizerFast): The instance of the SeamlessM4TTokenizerFast class.</span>
<span class="sd">            new_tgt_lang (str): The new target language to be set. It should be a string representing the language code.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">new_tgt_lang</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">new_tgt_lang</span><span class="si">}</span><span class="s2">__&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span> <span class="o">=</span> <span class="n">new_tgt_lang</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_inputs_with_special_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and</span>
<span class="sd">        adding special tokens. The special tokens depend on calling set_lang.</span>

<span class="sd">        An SeamlessM4T sequence has the following format, where `X` represents the sequence:</span>

<span class="sd">        - `input_ids` (for encoder) `[src_lang_code] X [eos]`</span>
<span class="sd">        - `decoder_input_ids`: (for decoder) `[eos, tgt_lang_code] X [eos]`</span>

<span class="sd">        BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a</span>
<span class="sd">        separator.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (`List[int]`):</span>
<span class="sd">                List of IDs to which the special tokens will be added.</span>
<span class="sd">            token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">                Optional second list of IDs for sequence pairs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span>
        <span class="c1"># We don&#39;t expect to process pairs, but leave the pair logic for API consistency</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">token_ids_1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb_fast.NllbTokenizerFast.create_token_type_ids_from_sequences</span>
    <span class="k">def</span> <span class="nf">create_token_type_ids_from_sequences</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a mask from the two sequences passed to be used in a sequence-pair classification task. nllb does not</span>
<span class="sd">        make use of token type ids, therefore a list of zeros is returned.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (`List[int]`):</span>
<span class="sd">                List of IDs.</span>
<span class="sd">            token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">                Optional second list of IDs for sequence pairs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[int]`: List of zeros.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sep</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sep_token_id</span><span class="p">]</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token_id</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span> <span class="o">+</span> <span class="n">sep</span> <span class="o">+</span> <span class="n">token_ids_1</span> <span class="o">+</span> <span class="n">sep</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_build_translation_inputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">raw_inputs</span><span class="p">,</span> <span class="n">return_tensors</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">src_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="o">**</span><span class="n">extra_kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Used by translation pipeline, to prepare inputs for the generate function&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">src_lang</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Translation requires a `src_lang` and a `tgt_lang` for this model&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">raw_inputs</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">extra_kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tgt_lang</span><span class="p">:</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">__&quot;</span>
        <span class="n">tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;forced_bos_token_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tgt_lang_id</span>
        <span class="k">return</span> <span class="n">inputs</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb_fast.NllbTokenizerFast.prepare_seq2seq_batch with &quot;fra_Latn&quot;-&gt;&quot;fra&quot;, &quot;eng_Latn&quot;-&gt;&quot;eng&quot;</span>
    <span class="k">def</span> <span class="nf">prepare_seq2seq_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">src_texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">src_lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;eng&quot;</span><span class="p">,</span>
        <span class="n">tgt_texts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;fra&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares a batch for sequence-to-sequence tokenization using the SeamlessM4TTokenizerFast class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizerFast): An instance of the SeamlessM4TTokenizerFast class.</span>
<span class="sd">            src_texts (List[str]): A list of source texts to be tokenized.</span>
<span class="sd">            src_lang (str, optional): The language of the source texts. Defaults to &#39;eng&#39;.</span>
<span class="sd">            tgt_texts (List[str], optional): A list of target texts to be tokenized. Defaults to None.</span>
<span class="sd">            tgt_lang (str, optional): The language of the target texts. Defaults to &#39;fra&#39;.</span>
<span class="sd">            **kwargs: Additional keyword arguments that can be passed to the underlying tokenizer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            BatchEncoding: A batch encoding containing the tokenized sequences.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>

<span class="sd">        This method prepares a batch of source texts and, optionally, target texts for tokenization using the</span>
<span class="sd">        SeamlessM4TTokenizerFast class. It takes the source texts, source language, target texts, and target language</span>
<span class="sd">        as input parameters. The method returns a BatchEncoding object, which contains the tokenized sequences.</span>

<span class="sd">        The &#39;self&#39; parameter refers to the instance of the SeamlessM4TTokenizerFast class on which the method is called.</span>

<span class="sd">        The &#39;src_texts&#39; parameter is a list of source texts that need to be tokenized.</span>

<span class="sd">        The &#39;src_lang&#39; parameter specifies the language of the source texts. The default value is &#39;eng&#39;.</span>

<span class="sd">        The &#39;tgt_texts&#39; parameter is an optional list of target texts that need to be tokenized. If not provided,</span>
<span class="sd">        it defaults to None.</span>

<span class="sd">        The &#39;tgt_lang&#39; parameter specifies the language of the target texts. The default value is &#39;fra&#39;.</span>

<span class="sd">        Additional keyword arguments can be passed using the &#39;**kwargs&#39; parameter. These arguments will be forwarded</span>
<span class="sd">        to the underlying tokenizer.</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; tokenizer = SeamlessM4TTokenizerFast()</span>
<span class="sd">            &gt;&gt;&gt; src_texts = [&quot;Hello, world!&quot;, &quot;How are you?&quot;]</span>
<span class="sd">            &gt;&gt;&gt; tgt_texts = [&quot;Bonjour, le monde!&quot;, &quot;Comment ça va?&quot;]</span>
<span class="sd">            &gt;&gt;&gt; batch = tokenizer.prepare_seq2seq_batch(src_texts, src_lang=&#39;eng&#39;, tgt_texts=tgt_texts, tgt_lang=&#39;fra&#39;)</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">prepare_seq2seq_batch</span><span class="p">(</span><span class="n">src_texts</span><span class="p">,</span> <span class="n">tgt_texts</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb_fast.NllbTokenizerFast._switch_to_input_mode</span>
    <span class="k">def</span> <span class="nf">_switch_to_input_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to switch the tokenizer to input mode by setting source language special tokens.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizerFast): The instance of the SeamlessM4TTokenizerFast class.</span>
<span class="sd">                Represents the tokenizer object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb_fast.NllbTokenizerFast._switch_to_target_mode</span>
    <span class="k">def</span> <span class="nf">_switch_to_target_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Switches the tokenizer to target mode for SeamlessM4TTokenizerFast.</span>

<span class="sd">        Args:</span>
<span class="sd">            self:</span>
<span class="sd">                The instance of SeamlessM4TTokenizerFast.</span>

<span class="sd">                - Type: object</span>
<span class="sd">                - Purpose: Represents the instance of the SeamlessM4TTokenizerFast class.</span>
<span class="sd">                - Restrictions: None</span>

<span class="sd">        Returns:</span>
<span class="sd">            None:</span>
<span class="sd">                Indicates that no value is returned from this method.</span>

<span class="sd">                - Type: None</span>
<span class="sd">                - Purpose: The method sets the target language special tokens and does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_lang</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_lang</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the special tokens to the source lang setting.</span>
<span class="sd">        Prefix=[src_lang_code], suffix = [eos]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">src_lang</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_id</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`tgt_lang=</span><span class="si">{</span><span class="n">src_lang</span><span class="si">}</span><span class="s2">` has not be found in the `vocabulary`. Behaviour will probably be unexpected because the language token id will be replaced by the unknown token id.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;src_lang&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">src_lang</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>

        <span class="n">prefix_tokens_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span><span class="p">)</span>
        <span class="n">suffix_tokens_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">processors</span><span class="o">.</span><span class="n">TemplateProcessing</span><span class="p">(</span>
            <span class="n">single</span><span class="o">=</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;$A&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span>
            <span class="n">pair</span><span class="o">=</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;$A&quot;</span><span class="p">,</span> <span class="s2">&quot;$B&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span>
            <span class="n">special_tokens</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span><span class="p">)),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lang</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reset the special tokens to the target lang setting.</span>
<span class="sd">        Prefix=[eos, tgt_lang_code] and suffix=[eos].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">lang</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_id</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`tgt_lang=</span><span class="si">{</span><span class="n">lang</span><span class="si">}</span><span class="s2">` has not be found in the `vocabulary`. Behaviour will probably be unexpected because the language token id will be replaced by the unknown token id.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;tgt_lang&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lang</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>

        <span class="n">prefix_tokens_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span><span class="p">)</span>
        <span class="n">suffix_tokens_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">processors</span><span class="o">.</span><span class="n">TemplateProcessing</span><span class="p">(</span>
            <span class="n">single</span><span class="o">=</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;$A&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span>
            <span class="n">pair</span><span class="o">=</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;$A&quot;</span><span class="p">,</span> <span class="s2">&quot;$B&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span>
            <span class="n">special_tokens</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span><span class="p">)),</span>
        <span class="p">)</span>

    <span class="c1"># Copied from transformers.models.nllb.tokenization_nllb_fast.NllbTokenizerFast.save_vocabulary</span>
    <span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the vocabulary for a slow tokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TTokenizerFast): The instance of the class.</span>
<span class="sd">            save_directory (str): The directory where the vocabulary will be saved.</span>
<span class="sd">            filename_prefix (Optional[str], optional): The prefix to be added to the filename. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[str]: A tuple containing the path to the saved vocabulary file.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the fast tokenizer does not have the necessary information to save the vocabulary</span>
<span class="sd">                for a slow tokenizer.</span>
<span class="sd">            FileNotFoundError: If the save_directory does not exist.</span>
<span class="sd">            IsADirectoryError: If the save_directory is not a directory.</span>

<span class="sd">        Note:</span>
<span class="sd">            The method assumes that the fast tokenizer has the necessary information to save the vocabulary</span>
<span class="sd">            for a slow tokenizer.</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; tokenizer = SeamlessM4TTokenizerFast()</span>
<span class="sd">            &gt;&gt;&gt; save_directory = &#39;/path/to/save/directory&#39;</span>
<span class="sd">            &gt;&gt;&gt; filename_prefix = &#39;vocab&#39;</span>
<span class="sd">            &gt;&gt;&gt; vocab_file = tokenizer.save_vocabulary(save_directory, filename_prefix)</span>
<span class="sd">            &gt;&gt;&gt; # vocab_file is now (&#39;/path/to/save/directory/vocab-file&#39;, )</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">can_save_slow_tokenizer</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Your fast tokenizer does not have the necessary information to save the vocabulary for a slow &quot;</span>
                <span class="s2">&quot;tokenizer.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary path (</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">) should be a directory.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="n">out_vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">VOCAB_FILES_NAMES</span><span class="p">[</span><span class="s2">&quot;vocab_file&quot;</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">)</span> <span class="o">!=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">out_vocab_file</span><span class="p">):</span>
            <span class="n">copyfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">out_vocab_file</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">out_vocab_file</span><span class="p">,)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_pretrained</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">resolved_vocab_files</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
        <span class="n">init_configuration</span><span class="p">,</span>
        <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">_commit_hash</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_is_local</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method _from_pretrained in the class SeamlessM4TTokenizerFast.</span>

<span class="sd">        Args:</span>
<span class="sd">            cls (class): The class itself.</span>
<span class="sd">            resolved_vocab_files (dict): A dictionary containing resolved vocabulary files.</span>
<span class="sd">            pretrained_model_name_or_path (str): The name or path of the pretrained model.</span>
<span class="sd">            init_configuration (dict): Initial configuration settings for the tokenizer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_from_pretrained</span><span class="p">(</span>
            <span class="n">resolved_vocab_files</span><span class="p">,</span>
            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
            <span class="n">init_configuration</span><span class="p">,</span>
            <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">_commit_hash</span><span class="o">=</span><span class="n">_commit_hash</span><span class="p">,</span>
            <span class="n">_is_local</span><span class="o">=</span><span class="n">_is_local</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># ensure also set after from pretrained</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">set_src_lang_special_tokens</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">_src_lang</span><span class="p">)</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">_tgt_lang</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">tokenizer</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_target</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_pair_target</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">src_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            text (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a</span>
<span class="sd">                list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),</span>
<span class="sd">                you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a</span>
<span class="sd">                list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),</span>
<span class="sd">                you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):</span>
<span class="sd">                 Select a strategy to pad the returned sequences (according to the model&#39;s padding side and padding</span>
<span class="sd">                 index) among:</span>

<span class="sd">                - `True` or `&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a single</span>
<span class="sd">                sequence if provided).</span>
<span class="sd">                - `&#39;max_length&#39;`: Pad to a maximum length specified with the argument `max_length` or to the maximum</span>
<span class="sd">                acceptable input length for the model if that argument is not provided.</span>
<span class="sd">                - `False` or `&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of different</span>
<span class="sd">                lengths).</span>
<span class="sd">            pad_to_multiple_of (`int`, *optional*):</span>
<span class="sd">                If set will pad the sequence to a multiple of the provided value.</span>

<span class="sd">                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</span>
<span class="sd">                `&gt;= 7.5` (Volta).</span>
<span class="sd">            src_lang (`str`, *optional*):</span>
<span class="sd">                A string representing the source language. If not specified, the last `src_lang` specified (either</span>
<span class="sd">                during initialization or when calling this tokenizer) will be used.</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                A string representing the target language. If not specified, the last `tgt_lang` specified (either</span>
<span class="sd">                during initialization or when calling this tokenizer) will be used.</span>
<span class="sd">            kwargs (*optional*):</span>
<span class="sd">                Remaining dictionary of keyword arguments that will be passed to [`PreTrainedTokenizerFast.__call__`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">src_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span>

        <span class="n">output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span>
            <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
            <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
            <span class="n">text_target</span><span class="o">=</span><span class="n">text_target</span><span class="p">,</span>
            <span class="n">text_pair_target</span><span class="o">=</span><span class="n">text_pair_target</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.can_save_slow_tokenizer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t_fast</span><span class="o">.</span><span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="n">can_save_slow_tokenizer</span><span class="p">:</span> <span class="nb">bool</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.can_save_slow_tokenizer" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method checks if the slow tokenizer can be saved.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the SeamlessM4TTokenizerFast class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast" href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast">SeamlessM4TTokenizerFast</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>bool</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Returns True if the vocab_file exists, False otherwise.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.src_lang" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t_fast</span><span class="o">.</span><span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="n">src_lang</span><span class="p">:</span> <span class="nb">str</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
      <small class="doc doc-label doc-label-writable"><code>writable</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.src_lang" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method returns the source language used for tokenization.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the SeamlessM4TTokenizerFast class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>str</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The source language used for tokenization.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.tgt_lang" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t_fast</span><span class="o">.</span><span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="n">tgt_lang</span><span class="p">:</span> <span class="nb">str</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
      <small class="doc doc-label doc-label-writable"><code>writable</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.tgt_lang" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>tgt_lang method in the SeamlessM4TTokenizerFast class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A reference to the current instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>str</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The language code representing the target language for tokenization.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>
    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t_fast</span><span class="o">.</span><span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_pair</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_pair_target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.__call__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>text</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, `List[str]`, `List[List[str]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_pair</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, `List[str]`, `List[List[str]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_target</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a
list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),
you must set <code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, `List[str]`, `List[List[str]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_pair_target</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a
list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),
you must set <code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, `List[str]`, `List[List[str]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Select a strategy to pad the returned sequences (according to the model's padding side and padding
 index) among:</p>
<ul>
<li><code>True</code> or <code>'longest'</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>'max_length'</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>'do_not_pad'</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_to_multiple_of</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
<code>&gt;= 7.5</code> (Volta).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>src_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A string representing the source language. If not specified, the last <code>src_lang</code> specified (either
during initialization or when calling this tokenizer) will be used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A string representing the target language. If not specified, the last <code>tgt_lang</code> specified (either
during initialization or when calling this tokenizer) will be used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Remaining dictionary of keyword arguments that will be passed to [<code>PreTrainedTokenizerFast.__call__</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t_fast.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_target</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_pair_target</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">src_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        text (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">        text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">        text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a</span>
<span class="sd">            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),</span>
<span class="sd">            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">        text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a</span>
<span class="sd">            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),</span>
<span class="sd">            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):</span>
<span class="sd">             Select a strategy to pad the returned sequences (according to the model&#39;s padding side and padding</span>
<span class="sd">             index) among:</span>

<span class="sd">            - `True` or `&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a single</span>
<span class="sd">            sequence if provided).</span>
<span class="sd">            - `&#39;max_length&#39;`: Pad to a maximum length specified with the argument `max_length` or to the maximum</span>
<span class="sd">            acceptable input length for the model if that argument is not provided.</span>
<span class="sd">            - `False` or `&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of different</span>
<span class="sd">            lengths).</span>
<span class="sd">        pad_to_multiple_of (`int`, *optional*):</span>
<span class="sd">            If set will pad the sequence to a multiple of the provided value.</span>

<span class="sd">            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</span>
<span class="sd">            `&gt;= 7.5` (Volta).</span>
<span class="sd">        src_lang (`str`, *optional*):</span>
<span class="sd">            A string representing the source language. If not specified, the last `src_lang` specified (either</span>
<span class="sd">            during initialization or when calling this tokenizer) will be used.</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            A string representing the target language. If not specified, the last `tgt_lang` specified (either</span>
<span class="sd">            during initialization or when calling this tokenizer) will be used.</span>
<span class="sd">        kwargs (*optional*):</span>
<span class="sd">            Remaining dictionary of keyword arguments that will be passed to [`PreTrainedTokenizerFast.__call__`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">src_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span>
        <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
        <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
        <span class="n">text_target</span><span class="o">=</span><span class="n">text_target</span><span class="p">,</span>
        <span class="n">text_pair_target</span><span class="o">=</span><span class="n">text_pair_target</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t_fast</span><span class="o">.</span><span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">vocab_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokenizer_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bos_token</span><span class="o">=</span><span class="s1">&#39;&lt;s&gt;&#39;</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="s1">&#39;&lt;/s&gt;&#39;</span><span class="p">,</span> <span class="n">sep_token</span><span class="o">=</span><span class="s1">&#39;&lt;/s&gt;&#39;</span><span class="p">,</span> <span class="n">cls_token</span><span class="o">=</span><span class="s1">&#39;&lt;s&gt;&#39;</span><span class="p">,</span> <span class="n">unk_token</span><span class="o">=</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s1">&#39;&lt;pad&gt;&#39;</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="s1">&#39;eng&#39;</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="s1">&#39;fra&#39;</span><span class="p">,</span> <span class="n">additional_special_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes the SeamlessM4TTokenizerFast class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the SeamlessM4TTokenizerFast class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to the vocabulary file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to the tokenizer file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The beginning of sequence token. Defaults to '<s>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The end of sequence token. Defaults to '</s>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;/s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sep_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The separator token. Defaults to '</s>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;/s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cls_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The classification token. Defaults to '<s>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unk_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The unknown token. Defaults to '<unk>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;unk&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The padding token. Defaults to '<pad>'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;pad&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>src_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The source language. Defaults to 'eng'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;eng&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The target language. Defaults to 'fra'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;fra&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>additional_special_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of additional special tokens. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t_fast.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bos_token</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
    <span class="n">eos_token</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
    <span class="n">sep_token</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
    <span class="n">cls_token</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
    <span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span>
    <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span>
    <span class="n">src_lang</span><span class="o">=</span><span class="s2">&quot;eng&quot;</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="o">=</span><span class="s2">&quot;fra&quot;</span><span class="p">,</span>
    <span class="n">additional_special_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the SeamlessM4TTokenizerFast class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: An instance of the SeamlessM4TTokenizerFast class.</span>
<span class="sd">        vocab_file (str): Path to the vocabulary file.</span>
<span class="sd">        tokenizer_file (str): Path to the tokenizer file.</span>
<span class="sd">        bos_token (str): The beginning of sequence token. Defaults to &#39;&lt;s&gt;&#39;.</span>
<span class="sd">        eos_token (str): The end of sequence token. Defaults to &#39;&lt;/s&gt;&#39;.</span>
<span class="sd">        sep_token (str): The separator token. Defaults to &#39;&lt;/s&gt;&#39;.</span>
<span class="sd">        cls_token (str): The classification token. Defaults to &#39;&lt;s&gt;&#39;.</span>
<span class="sd">        unk_token (str): The unknown token. Defaults to &#39;&lt;unk&gt;&#39;.</span>
<span class="sd">        pad_token (str): The padding token. Defaults to &#39;&lt;pad&gt;&#39;.</span>
<span class="sd">        src_lang (str): The source language. Defaults to &#39;eng&#39;.</span>
<span class="sd">        tgt_lang (str): The target language. Defaults to &#39;fra&#39;.</span>
<span class="sd">        additional_special_tokens (List[str]): A list of additional special tokens. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">vocab_file</span><span class="o">=</span><span class="n">vocab_file</span><span class="p">,</span>
        <span class="n">tokenizer_file</span><span class="o">=</span><span class="n">tokenizer_file</span><span class="p">,</span>
        <span class="n">bos_token</span><span class="o">=</span><span class="n">bos_token</span><span class="p">,</span>
        <span class="n">eos_token</span><span class="o">=</span><span class="n">eos_token</span><span class="p">,</span>
        <span class="n">sep_token</span><span class="o">=</span><span class="n">sep_token</span><span class="p">,</span>
        <span class="n">cls_token</span><span class="o">=</span><span class="n">cls_token</span><span class="p">,</span>
        <span class="n">unk_token</span><span class="o">=</span><span class="n">unk_token</span><span class="p">,</span>
        <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span>
        <span class="n">src_lang</span><span class="o">=</span><span class="n">src_lang</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="o">=</span><span class="n">tgt_lang</span><span class="p">,</span>
        <span class="n">additional_special_tokens</span><span class="o">=</span><span class="n">additional_special_tokens</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span> <span class="o">=</span> <span class="n">vocab_file</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">src_lang</span><span class="si">}</span><span class="s2">__&quot;</span> <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">src_lang</span> <span class="k">else</span> <span class="n">src_lang</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;__</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">__&quot;</span> <span class="k">if</span> <span class="s2">&quot;__&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tgt_lang</span> <span class="k">else</span> <span class="n">tgt_lang</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_src_lang</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tgt_lang</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.build_inputs_with_special_tokens" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t_fast</span><span class="o">.</span><span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="n">build_inputs_with_special_tokens</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.build_inputs_with_special_tokens" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. The special tokens depend on calling set_lang.</p>
<p>An SeamlessM4T sequence has the following format, where <code>X</code> represents the sequence:</p>
<ul>
<li><code>input_ids</code> (for encoder) <code>[src_lang_code] X [eos]</code></li>
<li><code>decoder_input_ids</code>: (for decoder) <code>[eos, tgt_lang_code] X [eos]</code></li>
</ul>
<p>BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a
separator.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_0</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of IDs to which the special tokens will be added.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_1</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional second list of IDs for sequence pairs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.List">List</span>[int]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>List[int]</code>: list of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t_fast.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">build_inputs_with_special_tokens</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and</span>
<span class="sd">    adding special tokens. The special tokens depend on calling set_lang.</span>

<span class="sd">    An SeamlessM4T sequence has the following format, where `X` represents the sequence:</span>

<span class="sd">    - `input_ids` (for encoder) `[src_lang_code] X [eos]`</span>
<span class="sd">    - `decoder_input_ids`: (for decoder) `[eos, tgt_lang_code] X [eos]`</span>

<span class="sd">    BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a</span>
<span class="sd">    separator.</span>

<span class="sd">    Args:</span>
<span class="sd">        token_ids_0 (`List[int]`):</span>
<span class="sd">            List of IDs to which the special tokens will be added.</span>
<span class="sd">        token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">            Optional second list of IDs for sequence pairs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `List[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span>
    <span class="c1"># We don&#39;t expect to process pairs, but leave the pair logic for API consistency</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">token_ids_1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.create_token_type_ids_from_sequences" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t_fast</span><span class="o">.</span><span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="n">create_token_type_ids_from_sequences</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.create_token_type_ids_from_sequences" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Create a mask from the two sequences passed to be used in a sequence-pair classification task. nllb does not
make use of token type ids, therefore a list of zeros is returned.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_0</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of IDs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token_ids_1</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional second list of IDs for sequence pairs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.List">List</span>[int]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>List[int]</code>: List of zeros.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t_fast.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_token_type_ids_from_sequences</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a mask from the two sequences passed to be used in a sequence-pair classification task. nllb does not</span>
<span class="sd">    make use of token type ids, therefore a list of zeros is returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        token_ids_0 (`List[int]`):</span>
<span class="sd">            List of IDs.</span>
<span class="sd">        token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">            Optional second list of IDs for sequence pairs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `List[int]`: List of zeros.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sep</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sep_token_id</span><span class="p">]</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token_id</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span> <span class="o">+</span> <span class="n">sep</span> <span class="o">+</span> <span class="n">token_ids_1</span> <span class="o">+</span> <span class="n">sep</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.prepare_seq2seq_batch" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t_fast</span><span class="o">.</span><span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="n">prepare_seq2seq_batch</span><span class="p">(</span><span class="n">src_texts</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="s1">&#39;eng&#39;</span><span class="p">,</span> <span class="n">tgt_texts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="s1">&#39;fra&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.prepare_seq2seq_batch" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Prepares a batch for sequence-to-sequence tokenization using the SeamlessM4TTokenizerFast class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the SeamlessM4TTokenizerFast class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast" href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast">SeamlessM4TTokenizerFast</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>src_texts</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of source texts to be tokenized.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[str]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>src_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language of the source texts. Defaults to 'eng'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;eng&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_texts</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of target texts to be tokenized. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language of the target texts. Defaults to 'fra'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;fra&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional keyword arguments that can be passed to the underlying tokenizer.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>BatchEncoding</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A batch encoding containing the tokenized sequences.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.tokenization_utils.BatchEncoding">BatchEncoding</span></code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>
        <p>This method prepares a batch of source texts and, optionally, target texts for tokenization using the
SeamlessM4TTokenizerFast class. It takes the source texts, source language, target texts, and target language
as input parameters. The method returns a BatchEncoding object, which contains the tokenized sequences.</p>
<p>The 'self' parameter refers to the instance of the SeamlessM4TTokenizerFast class on which the method is called.</p>
<p>The 'src_texts' parameter is a list of source texts that need to be tokenized.</p>
<p>The 'src_lang' parameter specifies the language of the source texts. The default value is 'eng'.</p>
<p>The 'tgt_texts' parameter is an optional list of target texts that need to be tokenized. If not provided,
it defaults to None.</p>
<p>The 'tgt_lang' parameter specifies the language of the target texts. The default value is 'fra'.</p>
<p>Additional keyword arguments can be passed using the '**kwargs' parameter. These arguments will be forwarded
to the underlying tokenizer.</p>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SeamlessM4TTokenizerFast</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">src_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Hello, world!&quot;</span><span class="p">,</span> <span class="s2">&quot;How are you?&quot;</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tgt_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Bonjour, le monde!&quot;</span><span class="p">,</span> <span class="s2">&quot;Comment ça va?&quot;</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">prepare_seq2seq_batch</span><span class="p">(</span><span class="n">src_texts</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="s1">&#39;eng&#39;</span><span class="p">,</span> <span class="n">tgt_texts</span><span class="o">=</span><span class="n">tgt_texts</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="s1">&#39;fra&#39;</span><span class="p">)</span>
</code></pre></div>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t_fast.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_seq2seq_batch</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">src_texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">src_lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;eng&quot;</span><span class="p">,</span>
    <span class="n">tgt_texts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;fra&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prepares a batch for sequence-to-sequence tokenization using the SeamlessM4TTokenizerFast class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (SeamlessM4TTokenizerFast): An instance of the SeamlessM4TTokenizerFast class.</span>
<span class="sd">        src_texts (List[str]): A list of source texts to be tokenized.</span>
<span class="sd">        src_lang (str, optional): The language of the source texts. Defaults to &#39;eng&#39;.</span>
<span class="sd">        tgt_texts (List[str], optional): A list of target texts to be tokenized. Defaults to None.</span>
<span class="sd">        tgt_lang (str, optional): The language of the target texts. Defaults to &#39;fra&#39;.</span>
<span class="sd">        **kwargs: Additional keyword arguments that can be passed to the underlying tokenizer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        BatchEncoding: A batch encoding containing the tokenized sequences.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>

<span class="sd">    This method prepares a batch of source texts and, optionally, target texts for tokenization using the</span>
<span class="sd">    SeamlessM4TTokenizerFast class. It takes the source texts, source language, target texts, and target language</span>
<span class="sd">    as input parameters. The method returns a BatchEncoding object, which contains the tokenized sequences.</span>

<span class="sd">    The &#39;self&#39; parameter refers to the instance of the SeamlessM4TTokenizerFast class on which the method is called.</span>

<span class="sd">    The &#39;src_texts&#39; parameter is a list of source texts that need to be tokenized.</span>

<span class="sd">    The &#39;src_lang&#39; parameter specifies the language of the source texts. The default value is &#39;eng&#39;.</span>

<span class="sd">    The &#39;tgt_texts&#39; parameter is an optional list of target texts that need to be tokenized. If not provided,</span>
<span class="sd">    it defaults to None.</span>

<span class="sd">    The &#39;tgt_lang&#39; parameter specifies the language of the target texts. The default value is &#39;fra&#39;.</span>

<span class="sd">    Additional keyword arguments can be passed using the &#39;**kwargs&#39; parameter. These arguments will be forwarded</span>
<span class="sd">    to the underlying tokenizer.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = SeamlessM4TTokenizerFast()</span>
<span class="sd">        &gt;&gt;&gt; src_texts = [&quot;Hello, world!&quot;, &quot;How are you?&quot;]</span>
<span class="sd">        &gt;&gt;&gt; tgt_texts = [&quot;Bonjour, le monde!&quot;, &quot;Comment ça va?&quot;]</span>
<span class="sd">        &gt;&gt;&gt; batch = tokenizer.prepare_seq2seq_batch(src_texts, src_lang=&#39;eng&#39;, tgt_texts=tgt_texts, tgt_lang=&#39;fra&#39;)</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">prepare_seq2seq_batch</span><span class="p">(</span><span class="n">src_texts</span><span class="p">,</span> <span class="n">tgt_texts</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.save_vocabulary" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t_fast</span><span class="o">.</span><span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="n">save_vocabulary</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.save_vocabulary" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Save the vocabulary for a slow tokenizer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast" href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast">SeamlessM4TTokenizerFast</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The directory where the vocabulary will be saved.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>filename_prefix</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The prefix to be added to the filename. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Tuple">Tuple</span>[str]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Tuple[str]: A tuple containing the path to the saved vocabulary file.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the fast tokenizer does not have the necessary information to save the vocabulary
for a slow tokenizer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>FileNotFoundError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the save_directory does not exist.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>IsADirectoryError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the save_directory is not a directory.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <p>The method assumes that the fast tokenizer has the necessary information to save the vocabulary
for a slow tokenizer.</p>
</details>

<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SeamlessM4TTokenizerFast</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">save_directory</span> <span class="o">=</span> <span class="s1">&#39;/path/to/save/directory&#39;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">filename_prefix</span> <span class="o">=</span> <span class="s1">&#39;vocab&#39;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vocab_file</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">save_vocabulary</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># vocab_file is now (&#39;/path/to/save/directory/vocab-file&#39;, )</span>
</code></pre></div>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t_fast.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the vocabulary for a slow tokenizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (SeamlessM4TTokenizerFast): The instance of the class.</span>
<span class="sd">        save_directory (str): The directory where the vocabulary will be saved.</span>
<span class="sd">        filename_prefix (Optional[str], optional): The prefix to be added to the filename. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[str]: A tuple containing the path to the saved vocabulary file.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the fast tokenizer does not have the necessary information to save the vocabulary</span>
<span class="sd">            for a slow tokenizer.</span>
<span class="sd">        FileNotFoundError: If the save_directory does not exist.</span>
<span class="sd">        IsADirectoryError: If the save_directory is not a directory.</span>

<span class="sd">    Note:</span>
<span class="sd">        The method assumes that the fast tokenizer has the necessary information to save the vocabulary</span>
<span class="sd">        for a slow tokenizer.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = SeamlessM4TTokenizerFast()</span>
<span class="sd">        &gt;&gt;&gt; save_directory = &#39;/path/to/save/directory&#39;</span>
<span class="sd">        &gt;&gt;&gt; filename_prefix = &#39;vocab&#39;</span>
<span class="sd">        &gt;&gt;&gt; vocab_file = tokenizer.save_vocabulary(save_directory, filename_prefix)</span>
<span class="sd">        &gt;&gt;&gt; # vocab_file is now (&#39;/path/to/save/directory/vocab-file&#39;, )</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">can_save_slow_tokenizer</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Your fast tokenizer does not have the necessary information to save the vocabulary for a slow &quot;</span>
            <span class="s2">&quot;tokenizer.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary path (</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">) should be a directory.&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="n">out_vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">VOCAB_FILES_NAMES</span><span class="p">[</span><span class="s2">&quot;vocab_file&quot;</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">)</span> <span class="o">!=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">out_vocab_file</span><span class="p">):</span>
        <span class="n">copyfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">out_vocab_file</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">out_vocab_file</span><span class="p">,)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.set_src_lang_special_tokens" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t_fast</span><span class="o">.</span><span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="n">set_src_lang_special_tokens</span><span class="p">(</span><span class="n">src_lang</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.set_src_lang_special_tokens" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Reset the special tokens to the source lang setting.
Prefix=[src_lang_code], suffix = [eos]</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t_fast.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_src_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_lang</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the special tokens to the source lang setting.</span>
<span class="sd">    Prefix=[src_lang_code], suffix = [eos]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">src_lang</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_id</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;`tgt_lang=</span><span class="si">{</span><span class="n">src_lang</span><span class="si">}</span><span class="s2">` has not be found in the `vocabulary`. Behaviour will probably be unexpected because the language token id will be replaced by the unknown token id.&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;src_lang&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">src_lang</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>

    <span class="n">prefix_tokens_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span><span class="p">)</span>
    <span class="n">suffix_tokens_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">processors</span><span class="o">.</span><span class="n">TemplateProcessing</span><span class="p">(</span>
        <span class="n">single</span><span class="o">=</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;$A&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span>
        <span class="n">pair</span><span class="o">=</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;$A&quot;</span><span class="p">,</span> <span class="s2">&quot;$B&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span>
        <span class="n">special_tokens</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span><span class="p">)),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.set_tgt_lang_special_tokens" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">tokenization_seamless_m4t_fast</span><span class="o">.</span><span class="n">SeamlessM4TTokenizerFast</span><span class="o">.</span><span class="n">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="n">lang</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.tokenization_seamless_m4t_fast.SeamlessM4TTokenizerFast.set_tgt_lang_special_tokens" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Reset the special tokens to the target lang setting.
Prefix=[eos, tgt_lang_code] and suffix=[eos].</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\tokenization_seamless_m4t_fast.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_tgt_lang_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lang</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reset the special tokens to the target lang setting.</span>
<span class="sd">    Prefix=[eos, tgt_lang_code] and suffix=[eos].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">lang</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_id</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;`tgt_lang=</span><span class="si">{</span><span class="n">lang</span><span class="si">}</span><span class="s2">` has not be found in the `vocabulary`. Behaviour will probably be unexpected because the language token id will be replaced by the unknown token id.&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;tgt_lang&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lang</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_lang_code</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>

    <span class="n">prefix_tokens_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span><span class="p">)</span>
    <span class="n">suffix_tokens_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">processors</span><span class="o">.</span><span class="n">TemplateProcessing</span><span class="p">(</span>
        <span class="n">single</span><span class="o">=</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;$A&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span>
        <span class="n">pair</span><span class="o">=</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;$A&quot;</span><span class="p">,</span> <span class="s2">&quot;$B&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span>
        <span class="n">special_tokens</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">prefix_tokens_str</span> <span class="o">+</span> <span class="n">suffix_tokens_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix_tokens</span><span class="p">)),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t</code>


<a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>Feature extractor class for SeamlessM4T</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor</code>


<a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.transformers.feature_extraction_sequence_utils.SequenceFeatureExtractor">SequenceFeatureExtractor</span></code></p>


        <p>Constructs a SeamlessM4T feature extractor.</p>
<p>This feature extractor inherits from [<code>SequenceFeatureExtractor</code>] which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.</p>
<p>This class extracts mel-filter bank features from raw speech.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>feature_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The feature dimension of the extracted features.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 80</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>80</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sampling_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16000</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_mel_bins</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of Mel-frequency bins.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 80</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>80</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value that is used to fill the padding vectors.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Stride used to reshape audios from shape (batch_size,num_frames,num_mel_bins) to
(batch_size,num_frames//stride,num_mel_bins*stride).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 2</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\feature_extraction_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TFeatureExtractor</span><span class="p">(</span><span class="n">SequenceFeatureExtractor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a SeamlessM4T feature extractor.</span>

<span class="sd">    This feature extractor inherits from [`SequenceFeatureExtractor`] which contains most of the main methods. Users</span>
<span class="sd">    should refer to this superclass for more information regarding those methods.</span>

<span class="sd">    This class extracts mel-filter bank features from raw speech.</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_size (`int`, *optional*, defaults to 80):</span>
<span class="sd">            The feature dimension of the extracted features.</span>
<span class="sd">        sampling_rate (`int`, *optional*, defaults to 16000):</span>
<span class="sd">            The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).</span>
<span class="sd">        num_mel_bins (`int`, *optional*, defaults to 80):</span>
<span class="sd">            Number of Mel-frequency bins.</span>
<span class="sd">        padding_value (`float`, *optional*, defaults to 0.0):</span>
<span class="sd">            The value that is used to fill the padding vectors.</span>
<span class="sd">        stride (`int`, *optional*, defaults to 2):</span>
<span class="sd">            Stride used to reshape audios from shape (batch_size,num_frames,num_mel_bins) to</span>
<span class="sd">            (batch_size,num_frames//stride,num_mel_bins*stride).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;input_features&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">feature_size</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
        <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span>
        <span class="n">num_mel_bins</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
        <span class="n">padding_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the SeamlessM4TFeatureExtractor class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TFeatureExtractor): The instance of the class.</span>
<span class="sd">            feature_size (int, optional): The size of the extracted feature. Defaults to 80.</span>
<span class="sd">            sampling_rate (int, optional): The sampling rate of the audio. Defaults to 16000.</span>
<span class="sd">            num_mel_bins (int, optional): The number of mel bins for mel-frequency cepstral coefficients (MFCC).</span>
<span class="sd">                Defaults to 80.</span>
<span class="sd">            padding_value (float, optional): The value used for padding. Defaults to 0.0.</span>
<span class="sd">            stride (int, optional): The stride for feature extraction. Defaults to 2.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mel_bins</span> <span class="o">=</span> <span class="n">num_mel_bins</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>

        <span class="n">mel_filters</span> <span class="o">=</span> <span class="n">mel_filter_bank</span><span class="p">(</span>
            <span class="n">num_frequency_bins</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
            <span class="n">num_mel_filters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_mel_bins</span><span class="p">,</span>
            <span class="n">min_frequency</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
            <span class="n">max_frequency</span><span class="o">=</span><span class="n">sampling_rate</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">sampling_rate</span><span class="o">=</span><span class="n">sampling_rate</span><span class="p">,</span>
            <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">mel_scale</span><span class="o">=</span><span class="s2">&quot;kaldi&quot;</span><span class="p">,</span>
            <span class="n">triangularize_in_mel_space</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mel_filters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">mel_filters</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window</span> <span class="o">=</span> <span class="n">window_function</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="s2">&quot;povey&quot;</span><span class="p">,</span> <span class="n">periodic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">feature_size</span><span class="o">=</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="n">sampling_rate</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="n">padding_value</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="c1"># Copied from transformers.models.wav2vec2.feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor.zero_mean_unit_var_norm</span>
    <span class="k">def</span> <span class="nf">zero_mean_unit_var_norm</span><span class="p">(</span>
        <span class="n">input_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">padding_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Every array in the list is normalized to have zero mean and unit variance</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">normed_input_values</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">vector</span><span class="p">,</span> <span class="n">length</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_values</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
                <span class="n">normed_slice</span> <span class="o">=</span> <span class="p">(</span><span class="n">vector</span> <span class="o">-</span> <span class="n">vector</span><span class="p">[:</span><span class="n">length</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vector</span><span class="p">[:</span><span class="n">length</span><span class="p">]</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">length</span> <span class="o">&lt;</span> <span class="n">normed_slice</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="n">normed_slice</span><span class="p">[</span><span class="n">length</span><span class="p">:]</span> <span class="o">=</span> <span class="n">padding_value</span>

                <span class="n">normed_input_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">normed_slice</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">normed_input_values</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_values</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">normed_input_values</span>

    <span class="k">def</span> <span class="nf">_extract_fbank_features</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">waveform</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get mel-filter bank features using TorchAudio. Note that TorchAudio requires 16-bit signed integers as inputs</span>
<span class="sd">        and hence the waveform should not be normalized before feature extraction.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># by default, it extracts the left channel if stereo</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">waveform</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">waveform</span> <span class="o">=</span> <span class="n">waveform</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">waveform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">waveform</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">15</span><span class="p">)</span>  <span class="c1"># Kaldi compliance: 16-bit signed integers</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">spectrogram</span><span class="p">(</span>
            <span class="n">waveform</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">window</span><span class="p">,</span>
            <span class="n">frame_length</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
            <span class="n">hop_length</span><span class="o">=</span><span class="mi">160</span><span class="p">,</span>
            <span class="n">fft_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
            <span class="n">power</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
            <span class="n">center</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">preemphasis</span><span class="o">=</span><span class="mf">0.97</span><span class="p">,</span>
            <span class="n">mel_filters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mel_filters</span><span class="p">,</span>
            <span class="n">log_mel</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">,</span>
            <span class="n">mel_floor</span><span class="o">=</span><span class="mf">1.192092955078125e-07</span><span class="p">,</span>
            <span class="n">remove_dc_offset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="k">return</span> <span class="n">features</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">raw_speech</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]],</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sampling_rate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">do_normalize_per_mel_bins</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchFeature</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Main method to featurize and prepare for the model one or several sequence(s).</span>

<span class="sd">        Args:</span>
<span class="sd">            raw_speech (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`, `List[List[List[float]]]`):</span>
<span class="sd">                The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float</span>
<span class="sd">                values, a list of numpy arrays, a list of list of float values or a list of a list of list of float</span>
<span class="sd">                values. If `raw_speech` is a one-dimensional `np.ndarray` or a `List[float]`, `raw_speech` is</span>
<span class="sd">                considered a single-channel, single-sample sound. In all other cases, the first dimension of</span>
<span class="sd">                `raw_speech`, whether from an `np.ndarray` or a `List[...]`, corresponds to the number of samples in</span>
<span class="sd">                the batch, and the number of channels (i.e. mono or stereo character) is derived from the other</span>
<span class="sd">                dimensions (1D -&gt; single-channel waveform batches; 2D-&gt; stereo-channel waveform batches).</span>
<span class="sd">            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):</span>
<span class="sd">                Select a strategy to pad the returned sequences (according to the model&#39;s padding side and padding</span>
<span class="sd">                index) among:</span>

<span class="sd">                - `True` or `&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a single</span>
<span class="sd">                sequence if provided).</span>
<span class="sd">                - `&#39;max_length&#39;`: Pad to a maximum length specified with the argument `max_length` or to the maximum</span>
<span class="sd">                acceptable input length for the model if that argument is not provided.</span>
<span class="sd">                - `False` or `&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of different</span>
<span class="sd">                lengths).</span>
<span class="sd">            pad_to_multiple_of (`int`, *optional*, defaults to 2):</span>
<span class="sd">                If set will pad the sequence to a multiple of the provided value.</span>

<span class="sd">                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</span>
<span class="sd">                `&gt;= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</span>
<span class="sd">            max_length (`int`, *optional*):</span>
<span class="sd">                Maximum length of the returned list and optionally padding length (see above).</span>
<span class="sd">            truncation (`bool`):</span>
<span class="sd">                Activates truncation to cut input sequences longer than *max_length* to *max_length*.</span>
<span class="sd">            return_attention_mask (`bool`, *optional*):</span>
<span class="sd">                Whether to return the attention mask. If left to the default, will return the attention mask according</span>
<span class="sd">                to the specific feature_extractor&#39;s default.</span>

<span class="sd">                [What are attention masks?](../glossary#attention-mask)</span>

<span class="sd">                &lt;Tip&gt;</span>

<span class="sd">                For SeamlessM4T models, `attention_mask` should always be passed for batched inference, to avoid subtle</span>
<span class="sd">                bugs.</span>

<span class="sd">                &lt;/Tip&gt;</span>

<span class="sd">            return_tensors (`str` or [`~utils.TensorType`], *optional*):</span>
<span class="sd">                If set, will return tensors instead of list of python integers. Acceptable values are:</span>

<span class="sd">                - `&#39;tf&#39;`: Return TensorFlow `tf.constant` objects.</span>
<span class="sd">                - `&#39;pt&#39;`: Return PyTorch `torch.Tensor` objects.</span>
<span class="sd">                - `&#39;np&#39;`: Return Numpy `np.ndarray` objects.</span>
<span class="sd">            sampling_rate (`int`, *optional*):</span>
<span class="sd">                The sampling rate at which the `raw_speech` input was sampled. It is strongly recommended to pass</span>
<span class="sd">                `sampling_rate` at the forward call to prevent silent errors.</span>
<span class="sd">            do_normalize_per_mel_bins (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether or not to zero-mean unit-variance normalize the input per mel-channel.</span>
<span class="sd">            kwargs (*optional*):</span>
<span class="sd">                Remaining dictionary of keyword arguments that will be passed to the tokenizer or the feature</span>
<span class="sd">                extractor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sampling_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sampling_rate</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The model corresponding to this feature extractor: </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> was trained using a sampling rate of&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span><span class="si">}</span><span class="s2">. Please make sure that the provided `raw_speech` input was sampled with&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span><span class="si">}</span><span class="s2"> and not </span><span class="si">{</span><span class="n">sampling_rate</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;It is strongly recommended to pass the `sampling_rate` argument to this function. &quot;</span>
                <span class="s2">&quot;Failing to do so can result in silent errors that might be hard to debug.&quot;</span>
            <span class="p">)</span>

        <span class="n">is_batched_numpy</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw_speech</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">is_batched_numpy</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw_speech</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Only mono-channel or stereo-channel audio is supported for input to </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">is_batched</span> <span class="o">=</span> <span class="n">is_batched_numpy</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)))</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">is_batched</span><span class="p">:</span>
            <span class="n">raw_speech</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">speech</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">speech</span> <span class="ow">in</span> <span class="n">raw_speech</span><span class="p">]</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">is_batched</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">raw_speech</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">raw_speech</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
            <span class="n">raw_speech</span> <span class="o">=</span> <span class="n">raw_speech</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># always return batch</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_batched</span><span class="p">:</span>
            <span class="n">raw_speech</span> <span class="o">=</span> <span class="p">[</span><span class="n">raw_speech</span><span class="p">]</span>

        <span class="c1"># extract fbank features</span>
        <span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_extract_fbank_features</span><span class="p">(</span><span class="n">waveform</span><span class="p">)</span> <span class="k">for</span> <span class="n">waveform</span> <span class="ow">in</span> <span class="n">raw_speech</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">do_normalize_per_mel_bins</span><span class="p">:</span>
            <span class="c1"># torch defaults to ddof=1, and numpy defaults to ddof=0</span>
            <span class="n">features</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">features</span>
            <span class="p">]</span>

        <span class="c1"># convert into correct format for padding</span>
        <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="n">BatchFeature</span><span class="p">({</span><span class="s2">&quot;input_features&quot;</span><span class="p">:</span> <span class="n">features</span><span class="p">})</span>

        <span class="n">padded_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
            <span class="n">encoded_inputs</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># SeamlessM4T needs to process extracted features</span>
        <span class="n">input_features</span> <span class="o">=</span> <span class="n">padded_inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_features&quot;</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">padded_inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">)</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_frames</span><span class="p">,</span> <span class="n">num_channels</span> <span class="o">=</span> <span class="n">input_features</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">remainder</span> <span class="o">=</span> <span class="n">num_frames</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
        <span class="k">if</span> <span class="n">remainder</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_frames</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_frames</span><span class="p">]</span>

        <span class="n">input_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">input_features</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_frames</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="n">num_channels</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_frames</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="n">indices</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>

        <span class="n">padded_inputs</span><span class="p">[</span><span class="s2">&quot;input_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_features</span>
        <span class="n">padded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">attention_mask</span>

        <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">padded_inputs</span> <span class="o">=</span> <span class="n">padded_inputs</span><span class="o">.</span><span class="n">convert_to_tensors</span><span class="p">(</span><span class="n">return_tensors</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">padded_inputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">feature_extraction_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TFeatureExtractor</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">do_normalize_per_mel_bins</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.__call__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Main method to featurize and prepare for the model one or several sequence(s).</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>raw_speech</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float
values, a list of numpy arrays, a list of list of float values or a list of a list of list of float
values. If <code>raw_speech</code> is a one-dimensional <code>np.ndarray</code> or a <code>List[float]</code>, <code>raw_speech</code> is
considered a single-channel, single-sample sound. In all other cases, the first dimension of
<code>raw_speech</code>, whether from an <code>np.ndarray</code> or a <code>List[...]</code>, corresponds to the number of samples in
the batch, and the number of channels (i.e. mono or stereo character) is derived from the other
dimensions (1D -&gt; single-channel waveform batches; 2D-&gt; stereo-channel waveform batches).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`, `List[List[List[float]]]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Select a strategy to pad the returned sequences (according to the model's padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>'longest'</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>'max_length'</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>'do_not_pad'</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_to_multiple_of</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
<code>&gt;= 7.5</code> (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 2</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_length</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Maximum length of the returned list and optionally padding length (see above).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>truncation</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Activates truncation to cut input sequences longer than <em>max_length</em> to <em>max_length</em>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor's default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p><Tip></p>
<p>For SeamlessM4T models, <code>attention_mask</code> should always be passed for batched inference, to avoid subtle
bugs.</p>
<p></Tip></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_tensors</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>'tf'</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>'pt'</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>'np'</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or [`~utils.TensorType`], *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sampling_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sampling rate at which the <code>raw_speech</code> input was sampled. It is strongly recommended to pass
<code>sampling_rate</code> at the forward call to prevent silent errors.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>do_normalize_per_mel_bins</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to zero-mean unit-variance normalize the input per mel-channel.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Remaining dictionary of keyword arguments that will be passed to the tokenizer or the feature
extractor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\feature_extraction_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">raw_speech</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]],</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">truncation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sampling_rate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">do_normalize_per_mel_bins</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchFeature</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Main method to featurize and prepare for the model one or several sequence(s).</span>

<span class="sd">    Args:</span>
<span class="sd">        raw_speech (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`, `List[List[List[float]]]`):</span>
<span class="sd">            The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float</span>
<span class="sd">            values, a list of numpy arrays, a list of list of float values or a list of a list of list of float</span>
<span class="sd">            values. If `raw_speech` is a one-dimensional `np.ndarray` or a `List[float]`, `raw_speech` is</span>
<span class="sd">            considered a single-channel, single-sample sound. In all other cases, the first dimension of</span>
<span class="sd">            `raw_speech`, whether from an `np.ndarray` or a `List[...]`, corresponds to the number of samples in</span>
<span class="sd">            the batch, and the number of channels (i.e. mono or stereo character) is derived from the other</span>
<span class="sd">            dimensions (1D -&gt; single-channel waveform batches; 2D-&gt; stereo-channel waveform batches).</span>
<span class="sd">        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):</span>
<span class="sd">            Select a strategy to pad the returned sequences (according to the model&#39;s padding side and padding</span>
<span class="sd">            index) among:</span>

<span class="sd">            - `True` or `&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a single</span>
<span class="sd">            sequence if provided).</span>
<span class="sd">            - `&#39;max_length&#39;`: Pad to a maximum length specified with the argument `max_length` or to the maximum</span>
<span class="sd">            acceptable input length for the model if that argument is not provided.</span>
<span class="sd">            - `False` or `&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of different</span>
<span class="sd">            lengths).</span>
<span class="sd">        pad_to_multiple_of (`int`, *optional*, defaults to 2):</span>
<span class="sd">            If set will pad the sequence to a multiple of the provided value.</span>

<span class="sd">            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</span>
<span class="sd">            `&gt;= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</span>
<span class="sd">        max_length (`int`, *optional*):</span>
<span class="sd">            Maximum length of the returned list and optionally padding length (see above).</span>
<span class="sd">        truncation (`bool`):</span>
<span class="sd">            Activates truncation to cut input sequences longer than *max_length* to *max_length*.</span>
<span class="sd">        return_attention_mask (`bool`, *optional*):</span>
<span class="sd">            Whether to return the attention mask. If left to the default, will return the attention mask according</span>
<span class="sd">            to the specific feature_extractor&#39;s default.</span>

<span class="sd">            [What are attention masks?](../glossary#attention-mask)</span>

<span class="sd">            &lt;Tip&gt;</span>

<span class="sd">            For SeamlessM4T models, `attention_mask` should always be passed for batched inference, to avoid subtle</span>
<span class="sd">            bugs.</span>

<span class="sd">            &lt;/Tip&gt;</span>

<span class="sd">        return_tensors (`str` or [`~utils.TensorType`], *optional*):</span>
<span class="sd">            If set, will return tensors instead of list of python integers. Acceptable values are:</span>

<span class="sd">            - `&#39;tf&#39;`: Return TensorFlow `tf.constant` objects.</span>
<span class="sd">            - `&#39;pt&#39;`: Return PyTorch `torch.Tensor` objects.</span>
<span class="sd">            - `&#39;np&#39;`: Return Numpy `np.ndarray` objects.</span>
<span class="sd">        sampling_rate (`int`, *optional*):</span>
<span class="sd">            The sampling rate at which the `raw_speech` input was sampled. It is strongly recommended to pass</span>
<span class="sd">            `sampling_rate` at the forward call to prevent silent errors.</span>
<span class="sd">        do_normalize_per_mel_bins (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether or not to zero-mean unit-variance normalize the input per mel-channel.</span>
<span class="sd">        kwargs (*optional*):</span>
<span class="sd">            Remaining dictionary of keyword arguments that will be passed to the tokenizer or the feature</span>
<span class="sd">            extractor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sampling_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sampling_rate</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The model corresponding to this feature extractor: </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> was trained using a sampling rate of&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span><span class="si">}</span><span class="s2">. Please make sure that the provided `raw_speech` input was sampled with&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span><span class="si">}</span><span class="s2"> and not </span><span class="si">{</span><span class="n">sampling_rate</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;It is strongly recommended to pass the `sampling_rate` argument to this function. &quot;</span>
            <span class="s2">&quot;Failing to do so can result in silent errors that might be hard to debug.&quot;</span>
        <span class="p">)</span>

    <span class="n">is_batched_numpy</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw_speech</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">is_batched_numpy</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw_speech</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Only mono-channel or stereo-channel audio is supported for input to </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">is_batched</span> <span class="o">=</span> <span class="n">is_batched_numpy</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)))</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">is_batched</span><span class="p">:</span>
        <span class="n">raw_speech</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">speech</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">speech</span> <span class="ow">in</span> <span class="n">raw_speech</span><span class="p">]</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">is_batched</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">raw_speech</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_speech</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">raw_speech</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="n">raw_speech</span> <span class="o">=</span> <span class="n">raw_speech</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># always return batch</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_batched</span><span class="p">:</span>
        <span class="n">raw_speech</span> <span class="o">=</span> <span class="p">[</span><span class="n">raw_speech</span><span class="p">]</span>

    <span class="c1"># extract fbank features</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_extract_fbank_features</span><span class="p">(</span><span class="n">waveform</span><span class="p">)</span> <span class="k">for</span> <span class="n">waveform</span> <span class="ow">in</span> <span class="n">raw_speech</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">do_normalize_per_mel_bins</span><span class="p">:</span>
        <span class="c1"># torch defaults to ddof=1, and numpy defaults to ddof=0</span>
        <span class="n">features</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">features</span>
        <span class="p">]</span>

    <span class="c1"># convert into correct format for padding</span>
    <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="n">BatchFeature</span><span class="p">({</span><span class="s2">&quot;input_features&quot;</span><span class="p">:</span> <span class="n">features</span><span class="p">})</span>

    <span class="n">padded_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
        <span class="n">encoded_inputs</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># SeamlessM4T needs to process extracted features</span>
    <span class="n">input_features</span> <span class="o">=</span> <span class="n">padded_inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_features&quot;</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">padded_inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">)</span>

    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_frames</span><span class="p">,</span> <span class="n">num_channels</span> <span class="o">=</span> <span class="n">input_features</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">remainder</span> <span class="o">=</span> <span class="n">num_frames</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
    <span class="k">if</span> <span class="n">remainder</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_frames</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_frames</span><span class="p">]</span>

    <span class="n">input_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">input_features</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_frames</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="n">num_channels</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_frames</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="n">indices</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>

    <span class="n">padded_inputs</span><span class="p">[</span><span class="s2">&quot;input_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_features</span>
    <span class="n">padded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">attention_mask</span>

    <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">padded_inputs</span> <span class="o">=</span> <span class="n">padded_inputs</span><span class="o">.</span><span class="n">convert_to_tensors</span><span class="p">(</span><span class="n">return_tensors</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">padded_inputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">feature_extraction_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TFeatureExtractor</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">feature_size</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span> <span class="n">num_mel_bins</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the SeamlessM4TFeatureExtractor class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor" href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor">SeamlessM4TFeatureExtractor</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>feature_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the extracted feature. Defaults to 80.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>80</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sampling_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sampling rate of the audio. Defaults to 16000.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_mel_bins</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of mel bins for mel-frequency cepstral coefficients (MFCC).
Defaults to 80.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>80</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value used for padding. Defaults to 0.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride for feature extraction. Defaults to 2.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\feature_extraction_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">feature_size</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
    <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span>
    <span class="n">num_mel_bins</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
    <span class="n">padding_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the SeamlessM4TFeatureExtractor class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (SeamlessM4TFeatureExtractor): The instance of the class.</span>
<span class="sd">        feature_size (int, optional): The size of the extracted feature. Defaults to 80.</span>
<span class="sd">        sampling_rate (int, optional): The sampling rate of the audio. Defaults to 16000.</span>
<span class="sd">        num_mel_bins (int, optional): The number of mel bins for mel-frequency cepstral coefficients (MFCC).</span>
<span class="sd">            Defaults to 80.</span>
<span class="sd">        padding_value (float, optional): The value used for padding. Defaults to 0.0.</span>
<span class="sd">        stride (int, optional): The stride for feature extraction. Defaults to 2.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_mel_bins</span> <span class="o">=</span> <span class="n">num_mel_bins</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>

    <span class="n">mel_filters</span> <span class="o">=</span> <span class="n">mel_filter_bank</span><span class="p">(</span>
        <span class="n">num_frequency_bins</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">num_mel_filters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_mel_bins</span><span class="p">,</span>
        <span class="n">min_frequency</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">max_frequency</span><span class="o">=</span><span class="n">sampling_rate</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">sampling_rate</span><span class="o">=</span><span class="n">sampling_rate</span><span class="p">,</span>
        <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">mel_scale</span><span class="o">=</span><span class="s2">&quot;kaldi&quot;</span><span class="p">,</span>
        <span class="n">triangularize_in_mel_space</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mel_filters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">mel_filters</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">window</span> <span class="o">=</span> <span class="n">window_function</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="s2">&quot;povey&quot;</span><span class="p">,</span> <span class="n">periodic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">feature_size</span><span class="o">=</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="n">sampling_rate</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="n">padding_value</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.zero_mean_unit_var_norm" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">feature_extraction_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TFeatureExtractor</span><span class="o">.</span><span class="n">zero_mean_unit_var_norm</span><span class="p">(</span><span class="n">input_values</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t.feature_extraction_seamless_m4t.SeamlessM4TFeatureExtractor.zero_mean_unit_var_norm" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Every array in the list is normalized to have zero mean and unit variance</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\feature_extraction_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="c1"># Copied from transformers.models.wav2vec2.feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor.zero_mean_unit_var_norm</span>
<span class="k">def</span> <span class="nf">zero_mean_unit_var_norm</span><span class="p">(</span>
    <span class="n">input_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">padding_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Every array in the list is normalized to have zero mean and unit variance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">normed_input_values</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">vector</span><span class="p">,</span> <span class="n">length</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_values</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
            <span class="n">normed_slice</span> <span class="o">=</span> <span class="p">(</span><span class="n">vector</span> <span class="o">-</span> <span class="n">vector</span><span class="p">[:</span><span class="n">length</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vector</span><span class="p">[:</span><span class="n">length</span><span class="p">]</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">length</span> <span class="o">&lt;</span> <span class="n">normed_slice</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">normed_slice</span><span class="p">[</span><span class="n">length</span><span class="p">:]</span> <span class="o">=</span> <span class="n">padding_value</span>

            <span class="n">normed_input_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">normed_slice</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">normed_input_values</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_values</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">normed_input_values</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t</code>


<a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>Audio/Text processor class for SeamlessM4T</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor</code>


<a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.transformers.processing_utils.ProcessorMixin">ProcessorMixin</span></code></p>


        <p>Constructs a SeamlessM4T processor which wraps a SeamlessM4T feature extractor and a SeamlessM4T tokenizer into a
single processor.</p>
<p>[<code>SeamlessM4TProcessor</code>] offers all the functionalities of [<code>SeamlessM4TFeatureExtractor</code>] and
[<code>SeamlessM4TTokenizerFast</code>]. See the [<code>~SeamlessM4TProcessor.__call__</code>] and [<code>~SeamlessM4TProcessor.decode</code>] for
more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>feature_extractor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The audio processor is a required input.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`SeamlessM4TFeatureExtractor`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The tokenizer is a required input.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`SeamlessM4TTokenizerFast`]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\processing_seamless_m4t.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4TProcessor</span><span class="p">(</span><span class="n">ProcessorMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a SeamlessM4T processor which wraps a SeamlessM4T feature extractor and a SeamlessM4T tokenizer into a</span>
<span class="sd">    single processor.</span>

<span class="sd">    [`SeamlessM4TProcessor`] offers all the functionalities of [`SeamlessM4TFeatureExtractor`] and</span>
<span class="sd">    [`SeamlessM4TTokenizerFast`]. See the [`~SeamlessM4TProcessor.__call__`] and [`~SeamlessM4TProcessor.decode`] for</span>
<span class="sd">    more information.</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_extractor ([`SeamlessM4TFeatureExtractor`]):</span>
<span class="sd">            The audio processor is a required input.</span>
<span class="sd">        tokenizer ([`SeamlessM4TTokenizerFast`]):</span>
<span class="sd">            The tokenizer is a required input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">feature_extractor_class</span> <span class="o">=</span> <span class="s2">&quot;SeamlessM4TFeatureExtractor&quot;</span>
    <span class="n">tokenizer_class</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;SeamlessM4TTokenizer&quot;</span><span class="p">,</span> <span class="s2">&quot;SeamlessM4TTokenizerFast&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_extractor</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a SeamlessM4TProcessor instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TProcessor): The instance of the SeamlessM4TProcessor class.</span>
<span class="sd">            feature_extractor (object): The feature extractor object used for processing.</span>
<span class="sd">            tokenizer (object): The tokenizer object used for processing.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">feature_extractor</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">audios</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`</span>
<span class="sd">        and `kwargs` arguments to SeamlessM4TTokenizerFast&#39;s [`~SeamlessM4TTokenizerFast.__call__`] if `text` is not</span>
<span class="sd">        `None` to encode the text. To prepare the audio(s), this method forwards the `audios` and `kwrags` arguments to</span>
<span class="sd">        SeamlessM4TFeatureExtractor&#39;s [`~SeamlessM4TFeatureExtractor.__call__`] if `audios` is not `None`. Please refer</span>
<span class="sd">        to the doctsring of the above two methods for more information.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (`str`, `List[str]`, `List[List[str]]`):</span>
<span class="sd">                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            audios (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):</span>
<span class="sd">                The audio or batch of audios to be prepared. Each audio can be NumPy array or PyTorch tensor. In case</span>
<span class="sd">                of a NumPy array/PyTorch tensor, each audio should be of shape (C, T), where C is a number of channels,</span>
<span class="sd">                and T the sample length of the audio.</span>
<span class="sd">            src_lang (`str`, *optional*):</span>
<span class="sd">                The language code of the input texts/audios. If not specified, the last `src_lang` specified will be</span>
<span class="sd">                used.</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The code of the target language. If not specified, the last `tgt_lang` specified will be used.</span>
<span class="sd">            kwargs (*optional*):</span>
<span class="sd">                Remaining dictionary of keyword arguments that will be passed to the feature extractor and/or the</span>
<span class="sd">                tokenizer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`BatchEncoding`]:</span>
<span class="sd">                A [`BatchEncoding`] with the following fields:</span>

<span class="sd">                - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.</span>
<span class="sd">                - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when</span>
<span class="sd">                `return_attention_mask=True` or if *&quot;attention_mask&quot;* is in `self.model_input_names` and if `text` is not</span>
<span class="sd">                `None`).</span>
<span class="sd">                - **input_features** -- Audio input features to be fed to a model. Returned when `audios` is not `None`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sampling_rate</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;sampling_rate&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">audios</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either text or audios. Both cannot be none.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">audios</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Text and audios are mututally exclusive when passed to `SeamlessM4T`. Specify one or another.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span>
            <span class="k">if</span> <span class="n">src_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
            <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="p">(</span><span class="n">audios</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="n">sampling_rate</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">encoding</span>

    <span class="k">def</span> <span class="nf">batch_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method forwards all its arguments to SeamlessM4TTokenizerFast&#39;s [`~PreTrainedTokenizer.batch_decode`].</span>
<span class="sd">        Please refer to the docstring of this method for more information.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method forwards all its arguments to SeamlessM4TTokenizerFast&#39;s [`~PreTrainedTokenizer.decode`]. Please</span>
<span class="sd">        refer to the docstring of this method for more information.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">model_input_names</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a list of unique model input names required by the SeamlessM4TProcessor.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (SeamlessM4TProcessor): An instance of the SeamlessM4TProcessor class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>

<span class="sd">        This method retrieves the model input names from the tokenizer and feature extractor used by the</span>
<span class="sd">        SeamlessM4TProcessor. It then combines these names into a single list and removes any duplicates,</span>
<span class="sd">        returning the final list of model input names.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tokenizer_input_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_input_names</span>
        <span class="n">feature_extractor_input_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">model_input_names</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="n">tokenizer_input_names</span> <span class="o">+</span> <span class="n">feature_extractor_input_names</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.model_input_names" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">processing_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TProcessor</span><span class="o">.</span><span class="n">model_input_names</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.model_input_names" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns a list of unique model input names required by the SeamlessM4TProcessor.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the SeamlessM4TProcessor class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor" href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor">SeamlessM4TProcessor</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <p>This method retrieves the model input names from the tokenizer and feature extractor used by the
SeamlessM4TProcessor. It then combines these names into a single list and removes any duplicates,
returning the final list of model input names.</p>
    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">processing_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TProcessor</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">audios</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.__call__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the <code>text</code>
and <code>kwargs</code> arguments to SeamlessM4TTokenizerFast's [<code>~SeamlessM4TTokenizerFast.__call__</code>] if <code>text</code> is not
<code>None</code> to encode the text. To prepare the audio(s), this method forwards the <code>audios</code> and <code>kwrags</code> arguments to
SeamlessM4TFeatureExtractor's [<code>~SeamlessM4TFeatureExtractor.__call__</code>] if <code>audios</code> is not <code>None</code>. Please refer
to the doctsring of the above two methods for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>text</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, `List[str]`, `List[List[str]]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>audios</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The audio or batch of audios to be prepared. Each audio can be NumPy array or PyTorch tensor. In case
of a NumPy array/PyTorch tensor, each audio should be of shape (C, T), where C is a number of channels,
and T the sample length of the audio.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>src_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language code of the input texts/audios. If not specified, the last <code>src_lang</code> specified will be
used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The code of the target language. If not specified, the last <code>tgt_lang</code> specified will be used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Remaining dictionary of keyword arguments that will be passed to the feature extractor and/or the
tokenizer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>[<code>BatchEncoding</code>]:
A [<code>BatchEncoding</code>] with the following fields:</p>
<ul>
<li><strong>input_ids</strong> -- List of token ids to be fed to a model. Returned when <code>text</code> is not <code>None</code>.</li>
<li><strong>attention_mask</strong> -- List of indices specifying which tokens should be attended to by the model (when
<code>return_attention_mask=True</code> or if <em>"attention_mask"</em> is in <code>self.model_input_names</code> and if <code>text</code> is not
<code>None</code>).</li>
<li><strong>input_features</strong> -- Audio input features to be fed to a model. Returned when <code>audios</code> is not <code>None</code>.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\processing_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">audios</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`</span>
<span class="sd">    and `kwargs` arguments to SeamlessM4TTokenizerFast&#39;s [`~SeamlessM4TTokenizerFast.__call__`] if `text` is not</span>
<span class="sd">    `None` to encode the text. To prepare the audio(s), this method forwards the `audios` and `kwrags` arguments to</span>
<span class="sd">    SeamlessM4TFeatureExtractor&#39;s [`~SeamlessM4TFeatureExtractor.__call__`] if `audios` is not `None`. Please refer</span>
<span class="sd">    to the doctsring of the above two methods for more information.</span>

<span class="sd">    Args:</span>
<span class="sd">        text (`str`, `List[str]`, `List[List[str]]`):</span>
<span class="sd">            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">        audios (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):</span>
<span class="sd">            The audio or batch of audios to be prepared. Each audio can be NumPy array or PyTorch tensor. In case</span>
<span class="sd">            of a NumPy array/PyTorch tensor, each audio should be of shape (C, T), where C is a number of channels,</span>
<span class="sd">            and T the sample length of the audio.</span>
<span class="sd">        src_lang (`str`, *optional*):</span>
<span class="sd">            The language code of the input texts/audios. If not specified, the last `src_lang` specified will be</span>
<span class="sd">            used.</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The code of the target language. If not specified, the last `tgt_lang` specified will be used.</span>
<span class="sd">        kwargs (*optional*):</span>
<span class="sd">            Remaining dictionary of keyword arguments that will be passed to the feature extractor and/or the</span>
<span class="sd">            tokenizer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        [`BatchEncoding`]:</span>
<span class="sd">            A [`BatchEncoding`] with the following fields:</span>

<span class="sd">            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.</span>
<span class="sd">            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when</span>
<span class="sd">            `return_attention_mask=True` or if *&quot;attention_mask&quot;* is in `self.model_input_names` and if `text` is not</span>
<span class="sd">            `None`).</span>
<span class="sd">            - **input_features** -- Audio input features to be fed to a model. Returned when `audios` is not `None`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sampling_rate</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;sampling_rate&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">audios</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either text or audios. Both cannot be none.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">audios</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Text and audios are mututally exclusive when passed to `SeamlessM4T`. Specify one or another.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span>
        <span class="k">if</span> <span class="n">src_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">src_lang</span>
        <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="p">(</span><span class="n">audios</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="n">sampling_rate</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">encoding</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">processing_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TProcessor</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">feature_extractor</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a SeamlessM4TProcessor instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the SeamlessM4TProcessor class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor" href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor">SeamlessM4TProcessor</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>feature_extractor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The feature extractor object used for processing.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The tokenizer object used for processing.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\processing_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_extractor</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a SeamlessM4TProcessor instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (SeamlessM4TProcessor): The instance of the SeamlessM4TProcessor class.</span>
<span class="sd">        feature_extractor (object): The feature extractor object used for processing.</span>
<span class="sd">        tokenizer (object): The tokenizer object used for processing.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">feature_extractor</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.batch_decode" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">processing_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TProcessor</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.batch_decode" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method forwards all its arguments to SeamlessM4TTokenizerFast's [<code>~PreTrainedTokenizer.batch_decode</code>].
Please refer to the docstring of this method for more information.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\processing_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">batch_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method forwards all its arguments to SeamlessM4TTokenizerFast&#39;s [`~PreTrainedTokenizer.batch_decode`].</span>
<span class="sd">    Please refer to the docstring of this method for more information.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.decode" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t</span><span class="o">.</span><span class="n">processing_seamless_m4t</span><span class="o">.</span><span class="n">SeamlessM4TProcessor</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t.processing_seamless_m4t.SeamlessM4TProcessor.decode" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method forwards all its arguments to SeamlessM4TTokenizerFast's [<code>~PreTrainedTokenizer.decode</code>]. Please
refer to the docstring of this method for more information.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t\processing_seamless_m4t.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method forwards all its arguments to SeamlessM4TTokenizerFast&#39;s [`~PreTrainedTokenizer.decode`]. Please</span>
<span class="sd">    refer to the docstring of this method for more information.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../sam/" class="md-footer__link md-footer__link--prev" aria-label="Previous: sam">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                sam
              </div>
            </div>
          </a>
        
        
          
          <a href="../seamless_m4t_v2/" class="md-footer__link md-footer__link--next" aria-label="Next: seamless_m4t_v2">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                seamless_m4t_v2
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 MindSpore Lab and CQU NLP Team.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:lvyufeng@cqu.edu.cn" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindnlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/lu-yu-feng-46-1" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>