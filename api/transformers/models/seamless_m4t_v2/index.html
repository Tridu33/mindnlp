
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../seamless_m4t/">
      
      
        <link rel="next" href="../segformer/">
      
      
      <link rel="icon" href="../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>seamless_m4t_v2 - MindNLP Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="MindNLP Docs" class="md-header__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindNLP Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              seamless_m4t_v2
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="./" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../../../zh/api/transformers/models/seamless_m4t_v2/" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../tutorials/quick_start/" class="md-tabs__link">
          
  
    
  
  Tutorials

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../supported_models/" class="md-tabs__link">
        
  
    
  
  Supported Models

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../contribute/" class="md-tabs__link">
        
  
    
  
  How-To Contribute

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../accelerate/" class="md-tabs__link">
          
  
    
  
  API Reference

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../notes/changelog/" class="md-tabs__link">
          
  
    
  
  Notes

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="MindNLP Docs" class="md-nav__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    MindNLP Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindnlp" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/data_preprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocess
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_mirror/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Mirror
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../supported_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Supported Models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How-To Contribute
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../accelerate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/load_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    load_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/BaseMapFunction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BaseMapFunction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transforms
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engine
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Engine
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_1" >
        
          
          <label class="md-nav__link" for="__nav_5_4_1" id="__nav_5_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    train_args
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_1">
            <span class="md-nav__icon md-icon"></span>
            train_args
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seq2seq
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_2" >
        
          
          <label class="md-nav__link" for="__nav_5_4_2" id="__nav_5_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_2">
            <span class="md-nav__icon md-icon"></span>
            trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/default_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    default_func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../peft/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_2" >
        
          
          <label class="md-nav__link" for="__nav_5_7_2" id="__nav_5_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tuners
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_2">
            <span class="md-nav__icon md-icon"></span>
            tuners
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adalora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AdaLoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adaption_prompt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adaption_Prompt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/ia3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lokr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoKr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/prompt_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt tuning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_3" >
        
          
          <label class="md-nav__link" for="__nav_5_7_3" id="__nav_5_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/utils/merge_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    merge_utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/peft_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    peft_model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sentence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_2" >
        
          
          <label class="md-nav__link" for="__nav_5_9_2" id="__nav_5_9_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_2">
            <span class="md-nav__icon md-icon"></span>
            generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/generation/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/logits_process/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/stopping_criteria/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stopping_criteria
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/streamers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    streamers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_3" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9_3" id="__nav_5_9_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9_3">
            <span class="md-nav__icon md-icon"></span>
            models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../albert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    albert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    align
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../altclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    altclip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../audio_spectrogram_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    audio_spectrogram_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../auto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    auto
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    autoformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../baichuan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    baichuan
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../barthez/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    barthez
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bartpho/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bartpho
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../beit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bertweet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bertweet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bge_m3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bge_m3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../big_bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    big_bird
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bigbird_pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bigbird_pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../biogpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    biogpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot_small/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot_small
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip_2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bloom
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bridgetower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bros/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bros
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../byt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    byt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../camembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    camembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../canine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    canine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../codegen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    codegen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cogvlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cogvlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cohere/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convnext/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convnext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpmant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpmbee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ctrl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctrl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cvt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cvt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data2vec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data2vec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../decision_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decision_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distilbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    distilbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../electra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    electra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../encodec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encodec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie_m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie_m
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../esm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    esm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    flava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../funnel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    funnel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gemma
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_bigcode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_bigcode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_pangu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_pangu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gptj/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gptj
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../graphormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    graphormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../groupvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    groupvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hubert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hubert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../imagegpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    imagegpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../internlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    internlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jetmoe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jetmoe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlmv2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlmv2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../led/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    led
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava_next/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava_next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../luke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    luke
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../marian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    marian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../maskformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maskformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mbart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mbart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minicpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minicpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minigpt4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minigpt4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mixtral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilevit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilevit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../moss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    moss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen_melody/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen_melody
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mvp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mvp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nezha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nezha
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nystromformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nystromformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../olmo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    olmo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../openelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    openelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    opt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../owlvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    owlvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../poolformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    poolformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pop2piano/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pop2piano
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2_moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    resnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roc_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roc_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rwkv
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seamless_m4t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_seamless_m4t_v2
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_seamless_m4t_v2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Attention" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2Attention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Attention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2CodeHifiGan" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2CodeHifiGan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2CodeHifiGan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2CodeHifiGan.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerConvolutionModule" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ConformerConvolutionModule
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ConformerEncoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerEncoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ConformerEncoderLayer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerSelfAttention" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ConformerSelfAttention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Decoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2Decoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2Decoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Decoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2DecoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2DecoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2DecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2DecoderLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Encoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2Encoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2Encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Encoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2EncoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2EncoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2EncoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2EncoderLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToSpeech" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ForSpeechToSpeech
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2ForSpeechToSpeech">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToSpeech.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToText" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ForSpeechToText
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2ForSpeechToText">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToText.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToSpeech" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ForTextToSpeech
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2ForTextToSpeech">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToSpeech.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToText" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ForTextToText
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2ForTextToText">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToText.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2GenerationOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2HifiGan" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2HifiGan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2HifiGan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2HifiGan.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Model" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2Model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Model.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2PreTrainedModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ScaledWordEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ScaledWordEmbedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2SinusoidalPositionalEmbedding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2SinusoidalPositionalEmbedding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds" class="md-nav__link">
    <span class="md-ellipsis">
      create_position_ids_from_inputs_embeds
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding.get_embedding" class="md-nav__link">
    <span class="md-ellipsis">
      get_embedding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2TextToUnitDecoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2TextToUnitDecoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2TextToUnitDecoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2TextToUnitDecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderOutput" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2TextToUnitDecoderOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitOutput" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2TextToUnitOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.create_position_ids_from_input_ids" class="md-nav__link">
    <span class="md-ellipsis">
      create_position_ids_from_input_ids
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.format_speech_generation_kwargs" class="md-nav__link">
    <span class="md-ellipsis">
      format_speech_generation_kwargs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.pad_sequence" class="md-nav__link">
    <span class="md-ellipsis">
      pad_sequence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.shift_tokens_right" class="md-nav__link">
    <span class="md-ellipsis">
      shift_tokens_right
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_seamless_m4t_v2
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_seamless_m4t_v2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2.SeamlessM4Tv2Config" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2Config
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2Config">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2.SeamlessM4Tv2Config.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../segformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    segformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seggpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seggpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_encoder_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_encoder_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_to_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_to_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../squeezebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    squeezebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stablelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stablelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    starcoder2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../swiftformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    swiftformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../switch_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    switch_transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../t5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    t5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../table_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    table_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../timesformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    timesformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tinybert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tinybert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../van/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    van
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vipllava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vipllava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vision_text_dual_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision_text_dual_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../visual_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visual_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_conformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_conformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_with_lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_with_lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wavlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    whisper
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../x_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    x_clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta_xl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta_xl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlnet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_4" >
        
          
          <label class="md-nav__link" for="__nav_5_9_4" id="__nav_5_9_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_4">
            <span class="md-nav__icon md-icon"></span>
            pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforemrs/pipeline/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/automatic_speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    automatic_speech_recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/document_question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    document_question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/fill_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fill_mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text2text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text2text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/zero_shot_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_shot_classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../configuration_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    configuration_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modeling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_fast/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_fast
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TRL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Notes
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Change Log
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code of Conduct
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_seamless_m4t_v2
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_seamless_m4t_v2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Attention" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2Attention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Attention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2CodeHifiGan" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2CodeHifiGan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2CodeHifiGan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2CodeHifiGan.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerConvolutionModule" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ConformerConvolutionModule
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ConformerEncoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerEncoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ConformerEncoderLayer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerSelfAttention" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ConformerSelfAttention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Decoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2Decoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2Decoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Decoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2DecoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2DecoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2DecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2DecoderLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Encoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2Encoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2Encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Encoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2EncoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2EncoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2EncoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2EncoderLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToSpeech" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ForSpeechToSpeech
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2ForSpeechToSpeech">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToSpeech.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToText" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ForSpeechToText
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2ForSpeechToText">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToText.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToSpeech" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ForTextToSpeech
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2ForTextToSpeech">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToSpeech.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToText" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ForTextToText
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2ForTextToText">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToText.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2GenerationOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2HifiGan" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2HifiGan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2HifiGan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2HifiGan.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Model" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2Model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Model.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2PreTrainedModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ScaledWordEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2ScaledWordEmbedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2SinusoidalPositionalEmbedding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2SinusoidalPositionalEmbedding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds" class="md-nav__link">
    <span class="md-ellipsis">
      create_position_ids_from_inputs_embeds
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding.get_embedding" class="md-nav__link">
    <span class="md-ellipsis">
      get_embedding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2TextToUnitDecoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2TextToUnitDecoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2TextToUnitDecoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2TextToUnitDecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderLayer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderOutput" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2TextToUnitDecoderOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitOutput" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2TextToUnitOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.create_position_ids_from_input_ids" class="md-nav__link">
    <span class="md-ellipsis">
      create_position_ids_from_input_ids
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.format_speech_generation_kwargs" class="md-nav__link">
    <span class="md-ellipsis">
      format_speech_generation_kwargs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.pad_sequence" class="md-nav__link">
    <span class="md-ellipsis">
      pad_sequence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.shift_tokens_right" class="md-nav__link">
    <span class="md-ellipsis">
      shift_tokens_right
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_seamless_m4t_v2
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_seamless_m4t_v2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2.SeamlessM4Tv2Config" class="md-nav__link">
    <span class="md-ellipsis">
      SeamlessM4Tv2Config
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SeamlessM4Tv2Config">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2.SeamlessM4Tv2Config.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindnlp/edit/master/docs/en/api/transformers/models/seamless_m4t_v2.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindnlp/raw/master/docs/en/api/transformers/models/seamless_m4t_v2.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>seamless_m4t_v2</h1>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>MindSpore SeamlessM4Tv2 model.</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Attention" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Attention</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Attention" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Multi-headed attention from 'Attention Is All You Need' paper</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span>
<span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span>
<span class="normal">967</span>
<span class="normal">968</span>
<span class="normal">969</span>
<span class="normal">970</span>
<span class="normal">971</span>
<span class="normal">972</span>
<span class="normal">973</span>
<span class="normal">974</span>
<span class="normal">975</span>
<span class="normal">976</span>
<span class="normal">977</span>
<span class="normal">978</span>
<span class="normal">979</span>
<span class="normal">980</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2Attention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multi-headed attention from &#39;Attention Is All You Need&#39; paper&quot;&quot;&quot;</span>

    <span class="c1"># Copied from transformers.models.bart.modeling_bart.BartAttention.__init__ with Bart-&gt;SeamlessM4Tv2</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">is_decoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SeamlessM4Tv2Config</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;embed_dim must be divisible by num_heads (got `embed_dim`: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; and `num_heads`: </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="o">**-</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span> <span class="o">=</span> <span class="n">is_decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_causal</span> <span class="o">=</span> <span class="n">is_causal</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">projection</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">new_projection_shape</span> <span class="o">=</span> <span class="n">projection</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="c1"># move heads to 2nd position (B, T, H * D) -&gt; (B, T, H, D) -&gt; (B, H, T, D)</span>
        <span class="n">new_projection</span> <span class="o">=</span> <span class="n">projection</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">new_projection_shape</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_projection</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Input shape: Batch x Time x Channel&quot;&quot;&quot;</span>

        <span class="n">is_cross_attention</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>

        <span class="c1"># use encoder_hidden_states if cross attention</span>
        <span class="n">current_states</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">hidden_states</span>
        <span class="c1"># checking that the `sequence_length` of the `past_key_value` is the same as the he provided</span>
        <span class="c1"># `encoder_hidden_states` to support prefix tuning</span>
        <span class="k">if</span> <span class="n">is_cross_attention</span> <span class="ow">and</span> <span class="n">past_key_value</span> <span class="ow">and</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">current_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="c1"># reuse k,v, cross_attentions</span>
            <span class="n">key_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">value_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">current_states</span><span class="p">))</span>
            <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">current_states</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_cross_attention</span><span class="p">:</span>
                <span class="c1"># reuse k, v, self_attention</span>
                <span class="n">key_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">key_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">value_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">value_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">)</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
            <span class="c1"># if cross_attention save Tuple(mindspore.Tensor, mindspore.Tensor) of all cross attention key/value_states.</span>
            <span class="c1"># Further calls to cross_attention layer can then reuse all cross-attention</span>
            <span class="c1"># key/value_states (first &quot;if&quot; case)</span>
            <span class="c1"># if uni-directional self-attention (decoder) save Tuple(mindspore.Tensor, mindspore.Tensor) of</span>
            <span class="c1"># all previous decoder key/value_states. Further calls to uni-directional self-attention</span>
            <span class="c1"># can concat previous decoder key/value_states to current projected key/value_states (third &quot;elif&quot; case)</span>
            <span class="c1"># if encoder bi-directional self-attention `past_key_value` is always `None`</span>
            <span class="n">past_key_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span> <span class="o">+</span> <span class="n">attention_mask</span>

        <span class="c1"># (batch_size, n_heads, seq_length, key_length)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1">#  attn_output = ops.bmm(attn_probs, value_states) ?</span>
        <span class="n">context_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>
        <span class="c1"># attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim) ?</span>
        <span class="n">context_states</span> <span class="o">=</span> <span class="n">context_states</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">context_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">past_key_value</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">past_key_value</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Attention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2Attention</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Attention.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Input shape: Batch x Time x Channel</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span>
<span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span>
<span class="normal">967</span>
<span class="normal">968</span>
<span class="normal">969</span>
<span class="normal">970</span>
<span class="normal">971</span>
<span class="normal">972</span>
<span class="normal">973</span>
<span class="normal">974</span>
<span class="normal">975</span>
<span class="normal">976</span>
<span class="normal">977</span>
<span class="normal">978</span>
<span class="normal">979</span>
<span class="normal">980</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Input shape: Batch x Time x Channel&quot;&quot;&quot;</span>

    <span class="n">is_cross_attention</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>

    <span class="c1"># use encoder_hidden_states if cross attention</span>
    <span class="n">current_states</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">hidden_states</span>
    <span class="c1"># checking that the `sequence_length` of the `past_key_value` is the same as the he provided</span>
    <span class="c1"># `encoder_hidden_states` to support prefix tuning</span>
    <span class="k">if</span> <span class="n">is_cross_attention</span> <span class="ow">and</span> <span class="n">past_key_value</span> <span class="ow">and</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">current_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="c1"># reuse k,v, cross_attentions</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">current_states</span><span class="p">))</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">current_states</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_cross_attention</span><span class="p">:</span>
            <span class="c1"># reuse k, v, self_attention</span>
            <span class="n">key_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">key_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">value_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">value_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">)</span>
    <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
        <span class="c1"># if cross_attention save Tuple(mindspore.Tensor, mindspore.Tensor) of all cross attention key/value_states.</span>
        <span class="c1"># Further calls to cross_attention layer can then reuse all cross-attention</span>
        <span class="c1"># key/value_states (first &quot;if&quot; case)</span>
        <span class="c1"># if uni-directional self-attention (decoder) save Tuple(mindspore.Tensor, mindspore.Tensor) of</span>
        <span class="c1"># all previous decoder key/value_states. Further calls to uni-directional self-attention</span>
        <span class="c1"># can concat previous decoder key/value_states to current projected key/value_states (third &quot;elif&quot; case)</span>
        <span class="c1"># if encoder bi-directional self-attention `past_key_value` is always `None`</span>
        <span class="n">past_key_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span> <span class="o">+</span> <span class="n">attention_mask</span>

    <span class="c1"># (batch_size, n_heads, seq_length, key_length)</span>
    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

    <span class="c1">#  attn_output = ops.bmm(attn_probs, value_states) ?</span>
    <span class="n">context_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>
    <span class="c1"># attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim) ?</span>
    <span class="n">context_states</span> <span class="o">=</span> <span class="n">context_states</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">context_states</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">past_key_value</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">past_key_value</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2CodeHifiGan" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2CodeHifiGan</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2CodeHifiGan" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.modeling_utils.PreTrainedModel" href="../../modeling_utils/#mindnlp.transformers.modeling_utils.PreTrainedModel">PreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span>
<span class="normal">2559</span>
<span class="normal">2560</span>
<span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span>
<span class="normal">2569</span>
<span class="normal">2570</span>
<span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2CodeHifiGan</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Config</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_embeds&quot;</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span>
        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">unit_embed_dim</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">variance_predictor_kernel_size</span>
        <span class="n">var_pred_dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">var_pred_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dur_predictor</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2VariancePredictor</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">var_pred_dropout</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">unit_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">unit_hifi_gan_vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">unit_embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speaker_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_num_spkrs</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">spkr_embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">language_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_num_langs</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">lang_embed_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2HifiGan</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan._get_dur_output_lengths</span>
    <span class="k">def</span> <span class="nf">_get_dur_output_lengths</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the output length after the duration layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">unit_lengths</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_ids</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># take care of edge cases where no padding or too many padding</span>
        <span class="n">unit_lengths</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">unit_lengths</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dur_out</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">cumulative_dur_out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dur_out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">unit_lengths</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">cumulative_dur_out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">unit_lengths</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">unit_lengths</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan._get_output_hifigan_lengths</span>
    <span class="k">def</span> <span class="nf">_get_output_hifigan_lengths</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the output length of the hifigan convolutional layers</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">_conv_out_length</span><span class="p">(</span><span class="n">input_length</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
            <span class="c1"># 1D convolutional layer output length formula taken</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">input_length</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="s2">&quot;floor&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">)</span>

        <span class="k">def</span> <span class="nf">_transpose_conv_out_length</span><span class="p">(</span><span class="n">input_length</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">input_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">+</span> <span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># conv_pre</span>
        <span class="n">input_lengths</span> <span class="o">=</span> <span class="n">_conv_out_length</span><span class="p">(</span><span class="n">input_lengths</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="c1"># upsampler</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">upsample_rate</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_rates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_kernel_sizes</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">input_lengths</span> <span class="o">=</span> <span class="n">_transpose_conv_out_length</span><span class="p">(</span>
                <span class="n">input_lengths</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">upsample_rate</span><span class="p">,</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="n">upsample_rate</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="p">)</span>

        <span class="c1"># resblock</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_rates</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">resblock_kernel_sizes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">resblock_dilation_sizes</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">dil</span> <span class="ow">in</span> <span class="n">dilation</span><span class="p">:</span>
                    <span class="n">input_lengths</span> <span class="o">=</span> <span class="n">_conv_out_length</span><span class="p">(</span>
                        <span class="n">input_lengths</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dil</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dil</span>
                    <span class="p">)</span>

                <span class="k">for</span> <span class="n">dil</span> <span class="ow">in</span> <span class="n">dilation</span><span class="p">:</span>
                    <span class="n">input_lengths</span> <span class="o">=</span> <span class="n">_conv_out_length</span><span class="p">(</span><span class="n">input_lengths</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># conv_post</span>
        <span class="n">input_lengths</span> <span class="o">=</span> <span class="n">_conv_out_length</span><span class="p">(</span><span class="n">input_lengths</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">input_lengths</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.forward with SeamlessM4T-&gt;SeamlessM4Tv2, spkr_id-&gt;speaker_id</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">speaker_id</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lang_id</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">                Indices can be obtained using [`SeamlessM4Tv2TextToUnitForConditionalGeneration`]. [What are input</span>
<span class="sd">                IDs?](../glossary#input-ids)</span>
<span class="sd">            speaker_id (`int`, *optional*):</span>
<span class="sd">                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language id to use as target language for translation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unit_embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">spkr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speaker_embedding</span><span class="p">(</span><span class="n">speaker_id</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">lang</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_embedding</span><span class="p">(</span><span class="n">lang_id</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">log_dur_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dur_predictor</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">dur_out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_dur_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># B x C x T</span>
        <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if batched sample, need to interleave per sample, and pad -&gt; loss of parallelism</span>
            <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;`self.training=True` and you use batching. You lose parallelism during the hifigan</span>
<span class="sd">                               forward pass because the samples are interleaved.&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">duration</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">duration</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">)</span>
            <span class="p">]</span>

            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">spkr</span> <span class="o">=</span> <span class="n">spkr</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">lang</span> <span class="o">=</span> <span class="n">lang</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">lang</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">spkr</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">unit_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dur_output_lengths</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">)</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_output_hifigan_lengths</span><span class="p">(</span><span class="n">unit_lengths</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">lengths</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan._init_weights</span>
    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the weights.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose1d</span><span class="p">)):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">module</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.apply_weight_norm</span>
    <span class="k">def</span> <span class="nf">apply_weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">conv_pre</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">upsampler</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">resblocks</span><span class="p">:</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">apply_weight_norm</span><span class="p">()</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">conv_post</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.remove_weight_norm</span>
    <span class="k">def</span> <span class="nf">remove_weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">conv_pre</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">upsampler</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">resblocks</span><span class="p">:</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">()</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="o">.</span><span class="n">conv_post</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2CodeHifiGan.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2CodeHifiGan</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">speaker_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2CodeHifiGan.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<code>SeamlessM4Tv2TextToUnitForConditionalGeneration</code>]. <a href="../glossary#input-ids">What are input
IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speaker_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the speaker used for speech synthesis. Must be lower than <code>config.vocoder_num_spkrs</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language id to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">speaker_id</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lang_id</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">            Indices can be obtained using [`SeamlessM4Tv2TextToUnitForConditionalGeneration`]. [What are input</span>
<span class="sd">            IDs?](../glossary#input-ids)</span>
<span class="sd">        speaker_id (`int`, *optional*):</span>
<span class="sd">            The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language id to use as target language for translation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unit_embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">spkr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speaker_embedding</span><span class="p">(</span><span class="n">speaker_id</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">lang</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_embedding</span><span class="p">(</span><span class="n">lang_id</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">log_dur_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dur_predictor</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">dur_out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_dur_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># B x C x T</span>
    <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># if batched sample, need to interleave per sample, and pad -&gt; loss of parallelism</span>
        <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;`self.training=True` and you use batching. You lose parallelism during the hifigan</span>
<span class="sd">                           forward pass because the samples are interleaved.&quot;&quot;&quot;</span>
            <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">duration</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">duration</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">spkr</span> <span class="o">=</span> <span class="n">spkr</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">lang</span> <span class="o">=</span> <span class="n">lang</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">lang</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">spkr</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hifi_gan</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">unit_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dur_output_lengths</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_output_hifigan_lengths</span><span class="p">(</span><span class="n">unit_lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">lengths</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerConvolutionModule" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerConvolutionModule</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerConvolutionModule" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Convolution block used in the conformer block. Uses a causal depthwise convolution similar to that
described in Section 2.1 of `https://doi.org/10.48550/arxiv.1609.03499</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2ConformerConvolutionModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convolution block used in the conformer block. Uses a causal depthwise convolution similar to that</span>
<span class="sd">    described in Section 2.1 of `https://doi.org/10.48550/arxiv.1609.03499&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">conv_depthwise_kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`config.conv_depthwise_kernel_size` should be a odd number for &#39;SAME&#39; padding&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">glu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GLU</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">conv_depthwise_kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_hidden_act</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># Ensure that we do not leak padded positions in depthwise convolution.</span>
        <span class="c1"># Put 0 where necessary</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>

        <span class="c1"># exchange the temporal dimension and the feature dimension</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># GLU mechanism</span>
        <span class="c1"># =&gt; (batch, 2*channel, dim)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv1</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># =&gt; (batch, channel, dim)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># Pad the sequence entirely on the left because of causal convolution.</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

        <span class="c1"># 1D Depthwise Conv</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv2</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerEncoder" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerEncoder</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerEncoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2ConformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">SeamlessM4Tv2ConformerEncoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_layers</span><span class="p">)]</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_apply_chunk_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a chunk attention mask. It creates a mask to prevent attention across chunks, ensuring that each</span>
<span class="sd">        position attends only to positions within its own chunk. If a left chunk overlap is specified</span>
<span class="sd">        (`speech_encoder_chunk_size` in the configuration), the attention mask is adjusted accordingly to allow each</span>
<span class="sd">        position to also attends the `speech_encoder_chunk_size - 1` previous chunks.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sequence_len</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">chunk_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_len</span><span class="p">)</span>
        <span class="n">chunk_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">chunk_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_chunk_size</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

        <span class="n">start_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">chunk_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_left_chunk_num</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">start_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">chunk_indices</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_left_chunk_num</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">start_indices</span> <span class="o">=</span> <span class="n">start_indices</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_chunk_size</span>
        <span class="n">start_indices</span> <span class="o">=</span> <span class="n">start_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">))</span>

        <span class="n">end_indices</span> <span class="o">=</span> <span class="p">((</span><span class="n">chunk_indices</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_chunk_size</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">max</span><span class="o">=</span><span class="n">sequence_len</span><span class="p">)</span>

        <span class="n">end_indices</span> <span class="o">=</span> <span class="n">end_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">))</span>

        <span class="n">indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="n">sequence_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">chunk_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices</span> <span class="o">&lt;</span> <span class="n">start_indices</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span> <span class="o">|</span> <span class="p">(</span><span class="n">indices</span> <span class="o">&gt;=</span> <span class="n">end_indices</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="n">chunk_mask</span> <span class="o">=</span> <span class="n">chunk_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">chunk_mask</span> <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">int</span><span class="p">()</span> <span class="o">|</span> <span class="n">chunk_mask</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_mask</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_self_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="n">conv_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># make sure padded tokens output 0</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="c1"># extend attention_mask</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                <span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_chunk_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_chunk_attention</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

            <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
            <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">([])</span>

            <span class="n">skip_the_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_layerdrop</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">skip_the_layer</span><span class="p">:</span>
                <span class="c1"># under deepspeed zero3 all gpus must run in sync</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                    <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
                        <span class="n">layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span>
                        <span class="n">hidden_states</span><span class="p">,</span>
                        <span class="n">attention_mask</span><span class="p">,</span>
                        <span class="n">output_attentions</span><span class="p">,</span>
                        <span class="n">conv_attention_mask</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span>
                        <span class="n">hidden_states</span><span class="p">,</span>
                        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                        <span class="n">conv_attention_mask</span><span class="o">=</span><span class="n">conv_attention_mask</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">skip_the_layer</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_self_attentions</span> <span class="o">=</span> <span class="n">all_self_attentions</span> <span class="o">+</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attentions</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerEncoderLayer" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerEncoderLayer</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerEncoderLayer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Conformer block based on https://arxiv.org/abs/2005.08100.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2ConformerEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Conformer block based on https://arxiv.org/abs/2005.08100.&quot;&quot;&quot;</span>

    <span class="c1"># Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerEncoderLayer.__init__ with Wav2Vec2-&gt;SeamlessM4Tv2, attention_dropout-&gt;speech_encoder_dropout, nn-&gt;nn</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_dropout</span>

        <span class="c1"># Feed-forward 1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn1_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn1</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2ConformerFeedForward</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Self-Attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2ConformerSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Conformer Convolution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2ConformerConvolutionModule</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Feed-forward 2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn2_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn2</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2ConformerFeedForward</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">conv_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># 1. Feed-Forward 1 layer</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn1_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn1</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">residual</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="c1"># 2. Self-Attention layer</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="c1"># 3. Convolutional Layer</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">conv_attention_mask</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="c1"># 4. Feed-Forward 2 Layer</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn2_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn2</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">residual</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerSelfAttention" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerSelfAttention</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ConformerSelfAttention" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Construct a SeamlessM4Tv2ConformerSelfAttention object.
Can be enhanced with relative position embeddings.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2ConformerSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a SeamlessM4Tv2ConformerSelfAttention object.</span>
<span class="sd">    Can be enhanced with relative position embeddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">use_position_embeddings</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="k">if</span> <span class="n">use_position_embeddings</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">speech_encoder_dropout</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="o">==</span> <span class="s2">&quot;relative_key&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">left_max_position_embeddings</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">left_max_position_embeddings</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">right_max_position_embeddings</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">right_max_position_embeddings</span>
            <span class="n">num_positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">left_max_position_embeddings</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">right_max_position_embeddings</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">distance_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_positions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="c1"># self-attention mechanism</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># make sure query/key states can be != value states</span>
        <span class="n">query_key_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="c1"># project query_key_states and value_states</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">query_key_states</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">query_key_states</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">value_states</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>

        <span class="c1"># =&gt; (batch, head, time1, d_k)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="o">==</span> <span class="s2">&quot;relative_key&quot;</span><span class="p">:</span>
            <span class="n">query_length</span><span class="p">,</span> <span class="n">key_length</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

            <span class="n">position_ids_l</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">query_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">position_ids_r</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">key_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">distance</span> <span class="o">=</span> <span class="n">position_ids_r</span> <span class="o">-</span> <span class="n">position_ids_l</span>
            <span class="n">distance</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">distance</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">left_max_position_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">right_max_position_embeddings</span><span class="p">)</span>

            <span class="n">positional_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance_embedding</span><span class="p">(</span><span class="n">distance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">left_max_position_embeddings</span><span class="p">)</span>
            <span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">positional_embedding</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># fp16 compatibility</span>

            <span class="n">relative_position_attn_weights</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhld,lrd-&gt;bhlr&quot;</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">positional_embedding</span><span class="p">)</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">+</span> <span class="p">(</span><span class="n">relative_position_attn_weights</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">))</span>

        <span class="c1"># apply attention_mask if necessary</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">+</span> <span class="n">attention_mask</span>

        <span class="c1"># =&gt; (batch, head, time1, time2)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="c1"># =&gt; (batch, head, time1, d_k)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="c1"># =&gt; (batch, time1, hidden_size)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Decoder" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Decoder</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Decoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel">SeamlessM4Tv2PreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2Decoder</span><span class="p">(</span><span class="n">SeamlessM4Tv2PreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4Tv2Config</span><span class="p">,</span>
        <span class="n">embed_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">decoder_layerdrop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_target_positions</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>
        <span class="n">embed_scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_embedding</span> <span class="k">else</span> <span class="mf">1.0</span>

        <span class="k">if</span> <span class="n">embed_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if embed_tokens defined, use its shape instead</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2ScaledWordEmbedding</span><span class="p">(</span>
                <span class="n">embed_tokens</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embed_tokens</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">embed_scale</span><span class="o">=</span><span class="n">embed_scale</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">embed_tokens</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2ScaledWordEmbedding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">embed_scale</span><span class="o">=</span><span class="n">embed_scale</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2SinusoidalPositionalEmbedding</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_target_positions</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">SeamlessM4Tv2DecoderLayer</span><span class="p">(</span>
                    <span class="n">config</span><span class="p">,</span>
                    <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
                    <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_ffn_dim</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutputWithPastAndCrossAttentions</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you</span>
<span class="sd">                provide it.</span>

<span class="sd">                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">                [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class="sd">                - 1 for tokens that are **not masked**,</span>
<span class="sd">                - 0 for tokens that are **masked**.</span>

<span class="sd">                [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">            encoder_hidden_states (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention</span>
<span class="sd">                of the decoder.</span>
<span class="sd">            encoder_attention_mask (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):</span>
<span class="sd">                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values</span>
<span class="sd">                selected in `[0, 1]`:</span>

<span class="sd">                - 1 for tokens that are **not masked**,</span>
<span class="sd">                - 0 for tokens that are **masked**.</span>

<span class="sd">                [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">            past_key_values (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">                Tuple of `tuple(mindspore.Tensor)` of length `config.n_layers`, with each tuple having 2 tensors of</span>
<span class="sd">                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of</span>
<span class="sd">                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.</span>

<span class="sd">                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the</span>
<span class="sd">                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.</span>

<span class="sd">                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those</span>
<span class="sd">                that don&#39;t have their past key value states given to this model) of shape `(batch_size, 1)` instead of</span>
<span class="sd">                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.</span>
<span class="sd">            inputs_embeds (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.</span>
<span class="sd">                This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class="sd">                than the model&#39;s internal embedding lookup matrix.</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">                returned tensors for more detail.</span>
<span class="sd">            output_hidden_states (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class="sd">                for more detail.</span>
<span class="sd">            return_dict (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="c1"># retrieve input_ids and inputs_embeds</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">input_ids</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either decoder_input_ids or decoder_inputs_embeds&quot;</span><span class="p">)</span>

        <span class="c1"># past_key_values_length</span>
        <span class="n">past_key_values_length</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_causal_attention_mask</span><span class="p">(</span>
            <span class="n">attention_mask</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span>
        <span class="p">)</span>

        <span class="c1"># expand encoder attention mask</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">encoder_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_attention_mask</span><span class="p">(</span>
                <span class="n">encoder_attention_mask</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">tgt_len</span><span class="o">=</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># embed positions</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="o">=</span><span class="n">past_key_values_length</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">positions</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                    <span class="s2">&quot;`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...&quot;</span>
                <span class="p">)</span>
                <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># decoder layers</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">output_attentions</span> <span class="ow">and</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">next_decoder_cache</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">decoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">([])</span>
                <span class="k">if</span> <span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">:</span>
                    <span class="k">continue</span>

            <span class="n">past_key_value</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
                    <span class="n">decoder_layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">encoder_attention_mask</span><span class="p">,</span>
                    <span class="kc">None</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="p">,</span>
                    <span class="n">use_cache</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">decoder_layer</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                    <span class="n">past_key_value</span><span class="o">=</span><span class="n">past_key_value</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                    <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">next_decoder_cache</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],)</span>

                <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">all_cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">3</span><span class="p">],)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># add hidden states from the last decoder layer</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="n">next_cache</span> <span class="o">=</span> <span class="n">next_decoder_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
                <span class="n">v</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">next_cache</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attns</span><span class="p">,</span> <span class="n">all_cross_attentions</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">BaseModelOutputWithPastAndCrossAttentions</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">next_cache</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attns</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">all_cross_attentions</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Decoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2Decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Decoder.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
provide it.</p>
<p>Indices can be obtained using [<code>AutoTokenizer</code>]. See [<code>PreTrainedTokenizer.encode</code>] and
[<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
of the decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values
selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>tuple(mindspore.Tensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
cross-attention blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those
that don't have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of
all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inputs_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors
than the model's internal embedding lookup matrix.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return a [<code>~utils.ModelOutput</code>] instead of a plain tuple.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutputWithPastAndCrossAttentions</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you</span>
<span class="sd">            provide it.</span>

<span class="sd">            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">            [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class="sd">            - 1 for tokens that are **not masked**,</span>
<span class="sd">            - 0 for tokens that are **masked**.</span>

<span class="sd">            [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">        encoder_hidden_states (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention</span>
<span class="sd">            of the decoder.</span>
<span class="sd">        encoder_attention_mask (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):</span>
<span class="sd">            Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values</span>
<span class="sd">            selected in `[0, 1]`:</span>

<span class="sd">            - 1 for tokens that are **not masked**,</span>
<span class="sd">            - 0 for tokens that are **masked**.</span>

<span class="sd">            [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">        past_key_values (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">            Tuple of `tuple(mindspore.Tensor)` of length `config.n_layers`, with each tuple having 2 tensors of</span>
<span class="sd">            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of</span>
<span class="sd">            shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.</span>

<span class="sd">            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the</span>
<span class="sd">            cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.</span>

<span class="sd">            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those</span>
<span class="sd">            that don&#39;t have their past key value states given to this model) of shape `(batch_size, 1)` instead of</span>
<span class="sd">            all `decoder_input_ids` of shape `(batch_size, sequence_length)`.</span>
<span class="sd">        inputs_embeds (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.</span>
<span class="sd">            This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class="sd">            than the model&#39;s internal embedding lookup matrix.</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">            returned tensors for more detail.</span>
<span class="sd">        output_hidden_states (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class="sd">            for more detail.</span>
<span class="sd">        return_dict (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="c1"># retrieve input_ids and inputs_embeds</span>
    <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">input_ids</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either decoder_input_ids or decoder_inputs_embeds&quot;</span><span class="p">)</span>

    <span class="c1"># past_key_values_length</span>
    <span class="n">past_key_values_length</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_causal_attention_mask</span><span class="p">(</span>
        <span class="n">attention_mask</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span>
    <span class="p">)</span>

    <span class="c1"># expand encoder attention mask</span>
    <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">encoder_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>
        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_attention_mask</span><span class="p">(</span>
            <span class="n">encoder_attention_mask</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">tgt_len</span><span class="o">=</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="c1"># embed positions</span>
    <span class="n">positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="o">=</span><span class="n">past_key_values_length</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">positions</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                <span class="s2">&quot;`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...&quot;</span>
            <span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># decoder layers</span>
    <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">output_attentions</span> <span class="ow">and</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">next_decoder_cache</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">decoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
        <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">([])</span>
            <span class="k">if</span> <span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">:</span>
                <span class="k">continue</span>

        <span class="n">past_key_value</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
                <span class="n">decoder_layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="kc">None</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">decoder_layer</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="n">past_key_value</span><span class="o">=</span><span class="n">past_key_value</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">next_decoder_cache</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],)</span>

            <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">all_cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">3</span><span class="p">],)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="c1"># add hidden states from the last decoder layer</span>
    <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
        <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

    <span class="n">next_cache</span> <span class="o">=</span> <span class="n">next_decoder_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">v</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">next_cache</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attns</span><span class="p">,</span> <span class="n">all_cross_attentions</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">BaseModelOutputWithPastAndCrossAttentions</span><span class="p">(</span>
        <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">next_cache</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
        <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attns</span><span class="p">,</span>
        <span class="n">cross_attentions</span><span class="o">=</span><span class="n">all_cross_attentions</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2DecoderLayer" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2DecoderLayer</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2DecoderLayer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4Tv2Config</span><span class="p">,</span> <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">decoder_ffn_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">decoder_ffn_dim</span> <span class="k">if</span> <span class="n">decoder_ffn_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">decoder_ffn_dim</span>
        <span class="n">decoder_attention_heads</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">decoder_attention_heads</span> <span class="k">if</span> <span class="n">decoder_attention_heads</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">decoder_attention_heads</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Attention</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
            <span class="n">is_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">activation_function</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Attention</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">decoder_attention_heads</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span> <span class="n">is_decoder</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2FeedForwardNetwork</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="o">=</span><span class="n">decoder_ffn_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">activation_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor`):</span>
<span class="sd">                input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">            attention_mask (`mindspore.Tensor`):</span>
<span class="sd">                attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very</span>
<span class="sd">                large negative values.</span>
<span class="sd">            encoder_hidden_states (`mindspore.Tensor`):</span>
<span class="sd">                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">            encoder_attention_mask (`mindspore.Tensor`):</span>
<span class="sd">                encoder attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by</span>
<span class="sd">                very large negative values.</span>
<span class="sd">            past_key_value (`Tuple(mindspore.Tensor)`):</span>
<span class="sd">                cached past key and value projection states</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">                returned tensors for more detail.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># Self Attention</span>
        <span class="c1"># decoder uni-directional self-attention cached key/values tuple is at positions 1,2</span>
        <span class="n">self_attn_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="c1"># add present self-attn cache to positions 1,2 of present_key_value tuple</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">past_key_value</span><span class="o">=</span><span class="n">self_attn_past_key_value</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="c1"># Cross-Attention Block</span>
        <span class="n">cross_attn_present_key_value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">cross_attn_weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

            <span class="c1"># cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple</span>
            <span class="n">cross_attn_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span> <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">cross_attn_weights</span><span class="p">,</span> <span class="n">cross_attn_present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">past_key_value</span><span class="o">=</span><span class="n">cross_attn_past_key_value</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

            <span class="c1"># add cross-attn to positions 3,4 of present_key_value tuple</span>
            <span class="n">present_key_value</span> <span class="o">+=</span> <span class="n">cross_attn_present_key_value</span>

        <span class="c1"># Fully Connected</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_value</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">cross_attn_weights</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2DecoderLayer.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2DecoderLayer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2DecoderLayer.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>input to the layer of shape <code>(batch, seq_len, embed_dim)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>attention mask of size <code>(batch, 1, tgt_len, src_len)</code> where padding elements are indicated by very
large negative values.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>cross attention input to the layer of shape <code>(batch, seq_len, embed_dim)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>encoder attention mask of size <code>(batch, 1, tgt_len, src_len)</code> where padding elements are indicated by
very large negative values.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>cached past key and value projection states</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple(mindspore.Tensor)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor`):</span>
<span class="sd">            input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">        attention_mask (`mindspore.Tensor`):</span>
<span class="sd">            attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very</span>
<span class="sd">            large negative values.</span>
<span class="sd">        encoder_hidden_states (`mindspore.Tensor`):</span>
<span class="sd">            cross attention input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">        encoder_attention_mask (`mindspore.Tensor`):</span>
<span class="sd">            encoder attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by</span>
<span class="sd">            very large negative values.</span>
<span class="sd">        past_key_value (`Tuple(mindspore.Tensor)`):</span>
<span class="sd">            cached past key and value projection states</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">            returned tensors for more detail.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="c1"># Self Attention</span>
    <span class="c1"># decoder uni-directional self-attention cached key/values tuple is at positions 1,2</span>
    <span class="n">self_attn_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="c1"># add present self-attn cache to positions 1,2 of present_key_value tuple</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="o">=</span><span class="n">self_attn_past_key_value</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="c1"># Cross-Attention Block</span>
    <span class="n">cross_attn_present_key_value</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">cross_attn_weights</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple</span>
        <span class="n">cross_attn_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span> <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">cross_attn_weights</span><span class="p">,</span> <span class="n">cross_attn_present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">past_key_value</span><span class="o">=</span><span class="n">cross_attn_past_key_value</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="c1"># add cross-attn to positions 3,4 of present_key_value tuple</span>
        <span class="n">present_key_value</span> <span class="o">+=</span> <span class="n">cross_attn_present_key_value</span>

    <span class="c1"># Fully Connected</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_value</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">cross_attn_weights</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Encoder" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Encoder</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel">SeamlessM4Tv2PreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2Encoder</span><span class="p">(</span><span class="n">SeamlessM4Tv2PreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4Tv2Config</span><span class="p">,</span>
        <span class="n">embed_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_t2u_encoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">encoder_layerdrop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span> <span class="o">=</span> <span class="n">is_t2u_encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_source_positions</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span><span class="p">:</span>
            <span class="n">embed_scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_embedding</span> <span class="k">else</span> <span class="mf">1.0</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2ScaledWordEmbedding</span><span class="p">(</span>
                <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">embed_scale</span><span class="o">=</span><span class="n">embed_scale</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">embed_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">embed_tokens</span><span class="o">.</span><span class="n">weight</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2SinusoidalPositionalEmbedding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_source_positions</span><span class="p">,</span>
                <span class="n">embed_dim</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">SeamlessM4Tv2EncoderLayer</span><span class="p">(</span>
                    <span class="n">config</span><span class="p">,</span>
                    <span class="n">encoder_attention_heads</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">encoder_attention_heads</span><span class="p">,</span>
                    <span class="n">encoder_ffn_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">encoder_ffn_dim</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you</span>
<span class="sd">                provide it.</span>

<span class="sd">                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">                [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class="sd">                - 1 for tokens that are **not masked**,</span>
<span class="sd">                - 0 for tokens that are **masked**.</span>

<span class="sd">                [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">            inputs_embeds (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.</span>
<span class="sd">                This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class="sd">                than the model&#39;s internal embedding lookup matrix.</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">                returned tensors for more detail.</span>
<span class="sd">            output_hidden_states (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class="sd">                for more detail.</span>
<span class="sd">            return_dict (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You cannot pass input_ids to the encoder of the text_to_units model. Pass inputs_embeds instead.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># retrieve input_ids and inputs_embeds</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">input_ids</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either input_ids or inputs_embeds&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span><span class="p">:</span>
            <span class="n">embed_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">embed_pos</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># expand attention_mask</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">encoder_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">encoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">encoder_states</span> <span class="o">=</span> <span class="n">encoder_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
            <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
            <span class="n">to_drop</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">([])</span>
                <span class="k">if</span> <span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">:</span>  <span class="c1"># skip the layer</span>
                    <span class="n">to_drop</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="k">if</span> <span class="n">to_drop</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                    <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
                        <span class="n">encoder_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span>
                        <span class="n">hidden_states</span><span class="p">,</span>
                        <span class="n">attention_mask</span><span class="p">,</span>
                        <span class="n">output_attentions</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">encoder_layer</span><span class="p">(</span>
                        <span class="n">hidden_states</span><span class="p">,</span>
                        <span class="n">attention_mask</span><span class="p">,</span>
                        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_attentions</span> <span class="o">=</span> <span class="n">all_attentions</span> <span class="o">+</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">encoder_states</span> <span class="o">=</span> <span class="n">encoder_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_states</span><span class="p">,</span> <span class="n">all_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_states</span><span class="p">,</span> <span class="n">attentions</span><span class="o">=</span><span class="n">all_attentions</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Encoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2Encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Encoder.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
provide it.</p>
<p>Indices can be obtained using [<code>AutoTokenizer</code>]. See [<code>PreTrainedTokenizer.encode</code>] and
[<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inputs_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors
than the model's internal embedding lookup matrix.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return a [<code>~utils.ModelOutput</code>] instead of a plain tuple.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you</span>
<span class="sd">            provide it.</span>

<span class="sd">            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">            [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class="sd">            - 1 for tokens that are **not masked**,</span>
<span class="sd">            - 0 for tokens that are **masked**.</span>

<span class="sd">            [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">        inputs_embeds (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.</span>
<span class="sd">            This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class="sd">            than the model&#39;s internal embedding lookup matrix.</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">            returned tensors for more detail.</span>
<span class="sd">        output_hidden_states (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class="sd">            for more detail.</span>
<span class="sd">        return_dict (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You cannot pass input_ids to the encoder of the text_to_units model. Pass inputs_embeds instead.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># retrieve input_ids and inputs_embeds</span>
    <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">input_ids</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either input_ids or inputs_embeds&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_t2u_encoder</span><span class="p">:</span>
        <span class="n">embed_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">embed_pos</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

    <span class="c1"># expand attention_mask</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">encoder_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">encoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">encoder_states</span> <span class="o">=</span> <span class="n">encoder_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
        <span class="n">to_drop</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">([])</span>
            <span class="k">if</span> <span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">:</span>  <span class="c1"># skip the layer</span>
                <span class="n">to_drop</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">to_drop</span><span class="p">:</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
                    <span class="n">encoder_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">encoder_layer</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">all_attentions</span> <span class="o">=</span> <span class="n">all_attentions</span> <span class="o">+</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
        <span class="n">encoder_states</span> <span class="o">=</span> <span class="n">encoder_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_states</span><span class="p">,</span> <span class="n">all_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
        <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_states</span><span class="p">,</span> <span class="n">attentions</span><span class="o">=</span><span class="n">all_attentions</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2EncoderLayer" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2EncoderLayer</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2EncoderLayer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4Tv2Config</span><span class="p">,</span> <span class="n">encoder_ffn_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_attention_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">encoder_ffn_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">encoder_ffn_dim</span> <span class="k">if</span> <span class="n">encoder_ffn_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_ffn_dim</span>
        <span class="n">encoder_attention_heads</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">encoder_attention_heads</span> <span class="k">if</span> <span class="n">encoder_attention_heads</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_attention_heads</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Attention</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">encoder_attention_heads</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2FeedForwardNetwork</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">ffn_dim</span><span class="o">=</span><span class="n">encoder_ffn_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">activation_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor`):</span>
<span class="sd">                input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">            attention_mask (`mindspore.Tensor`):</span>
<span class="sd">                attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very</span>
<span class="sd">                large negative values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">attn_weights</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2EncoderLayer.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2EncoderLayer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2EncoderLayer.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>input to the layer of shape <code>(batch, seq_len, embed_dim)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>attention mask of size <code>(batch, 1, tgt_len, src_len)</code> where padding elements are indicated by very
large negative values.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor`):</span>
<span class="sd">            input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">        attention_mask (`mindspore.Tensor`):</span>
<span class="sd">            attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very</span>
<span class="sd">            large negative values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">attn_weights</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToSpeech" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToSpeech</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToSpeech" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel">SeamlessM4Tv2PreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3652</span>
<span class="normal">3653</span>
<span class="normal">3654</span>
<span class="normal">3655</span>
<span class="normal">3656</span>
<span class="normal">3657</span>
<span class="normal">3658</span>
<span class="normal">3659</span>
<span class="normal">3660</span>
<span class="normal">3661</span>
<span class="normal">3662</span>
<span class="normal">3663</span>
<span class="normal">3664</span>
<span class="normal">3665</span>
<span class="normal">3666</span>
<span class="normal">3667</span>
<span class="normal">3668</span>
<span class="normal">3669</span>
<span class="normal">3670</span>
<span class="normal">3671</span>
<span class="normal">3672</span>
<span class="normal">3673</span>
<span class="normal">3674</span>
<span class="normal">3675</span>
<span class="normal">3676</span>
<span class="normal">3677</span>
<span class="normal">3678</span>
<span class="normal">3679</span>
<span class="normal">3680</span>
<span class="normal">3681</span>
<span class="normal">3682</span>
<span class="normal">3683</span>
<span class="normal">3684</span>
<span class="normal">3685</span>
<span class="normal">3686</span>
<span class="normal">3687</span>
<span class="normal">3688</span>
<span class="normal">3689</span>
<span class="normal">3690</span>
<span class="normal">3691</span>
<span class="normal">3692</span>
<span class="normal">3693</span>
<span class="normal">3694</span>
<span class="normal">3695</span>
<span class="normal">3696</span>
<span class="normal">3697</span>
<span class="normal">3698</span>
<span class="normal">3699</span>
<span class="normal">3700</span>
<span class="normal">3701</span>
<span class="normal">3702</span>
<span class="normal">3703</span>
<span class="normal">3704</span>
<span class="normal">3705</span>
<span class="normal">3706</span>
<span class="normal">3707</span>
<span class="normal">3708</span>
<span class="normal">3709</span>
<span class="normal">3710</span>
<span class="normal">3711</span>
<span class="normal">3712</span>
<span class="normal">3713</span>
<span class="normal">3714</span>
<span class="normal">3715</span>
<span class="normal">3716</span>
<span class="normal">3717</span>
<span class="normal">3718</span>
<span class="normal">3719</span>
<span class="normal">3720</span>
<span class="normal">3721</span>
<span class="normal">3722</span>
<span class="normal">3723</span>
<span class="normal">3724</span>
<span class="normal">3725</span>
<span class="normal">3726</span>
<span class="normal">3727</span>
<span class="normal">3728</span>
<span class="normal">3729</span>
<span class="normal">3730</span>
<span class="normal">3731</span>
<span class="normal">3732</span>
<span class="normal">3733</span>
<span class="normal">3734</span>
<span class="normal">3735</span>
<span class="normal">3736</span>
<span class="normal">3737</span>
<span class="normal">3738</span>
<span class="normal">3739</span>
<span class="normal">3740</span>
<span class="normal">3741</span>
<span class="normal">3742</span>
<span class="normal">3743</span>
<span class="normal">3744</span>
<span class="normal">3745</span>
<span class="normal">3746</span>
<span class="normal">3747</span>
<span class="normal">3748</span>
<span class="normal">3749</span>
<span class="normal">3750</span>
<span class="normal">3751</span>
<span class="normal">3752</span>
<span class="normal">3753</span>
<span class="normal">3754</span>
<span class="normal">3755</span>
<span class="normal">3756</span>
<span class="normal">3757</span>
<span class="normal">3758</span>
<span class="normal">3759</span>
<span class="normal">3760</span>
<span class="normal">3761</span>
<span class="normal">3762</span>
<span class="normal">3763</span>
<span class="normal">3764</span>
<span class="normal">3765</span>
<span class="normal">3766</span>
<span class="normal">3767</span>
<span class="normal">3768</span>
<span class="normal">3769</span>
<span class="normal">3770</span>
<span class="normal">3771</span>
<span class="normal">3772</span>
<span class="normal">3773</span>
<span class="normal">3774</span>
<span class="normal">3775</span>
<span class="normal">3776</span>
<span class="normal">3777</span>
<span class="normal">3778</span>
<span class="normal">3779</span>
<span class="normal">3780</span>
<span class="normal">3781</span>
<span class="normal">3782</span>
<span class="normal">3783</span>
<span class="normal">3784</span>
<span class="normal">3785</span>
<span class="normal">3786</span>
<span class="normal">3787</span>
<span class="normal">3788</span>
<span class="normal">3789</span>
<span class="normal">3790</span>
<span class="normal">3791</span>
<span class="normal">3792</span>
<span class="normal">3793</span>
<span class="normal">3794</span>
<span class="normal">3795</span>
<span class="normal">3796</span>
<span class="normal">3797</span>
<span class="normal">3798</span>
<span class="normal">3799</span>
<span class="normal">3800</span>
<span class="normal">3801</span>
<span class="normal">3802</span>
<span class="normal">3803</span>
<span class="normal">3804</span>
<span class="normal">3805</span>
<span class="normal">3806</span>
<span class="normal">3807</span>
<span class="normal">3808</span>
<span class="normal">3809</span>
<span class="normal">3810</span>
<span class="normal">3811</span>
<span class="normal">3812</span>
<span class="normal">3813</span>
<span class="normal">3814</span>
<span class="normal">3815</span>
<span class="normal">3816</span>
<span class="normal">3817</span>
<span class="normal">3818</span>
<span class="normal">3819</span>
<span class="normal">3820</span>
<span class="normal">3821</span>
<span class="normal">3822</span>
<span class="normal">3823</span>
<span class="normal">3824</span>
<span class="normal">3825</span>
<span class="normal">3826</span>
<span class="normal">3827</span>
<span class="normal">3828</span>
<span class="normal">3829</span>
<span class="normal">3830</span>
<span class="normal">3831</span>
<span class="normal">3832</span>
<span class="normal">3833</span>
<span class="normal">3834</span>
<span class="normal">3835</span>
<span class="normal">3836</span>
<span class="normal">3837</span>
<span class="normal">3838</span>
<span class="normal">3839</span>
<span class="normal">3840</span>
<span class="normal">3841</span>
<span class="normal">3842</span>
<span class="normal">3843</span>
<span class="normal">3844</span>
<span class="normal">3845</span>
<span class="normal">3846</span>
<span class="normal">3847</span>
<span class="normal">3848</span>
<span class="normal">3849</span>
<span class="normal">3850</span>
<span class="normal">3851</span>
<span class="normal">3852</span>
<span class="normal">3853</span>
<span class="normal">3854</span>
<span class="normal">3855</span>
<span class="normal">3856</span>
<span class="normal">3857</span>
<span class="normal">3858</span>
<span class="normal">3859</span>
<span class="normal">3860</span>
<span class="normal">3861</span>
<span class="normal">3862</span>
<span class="normal">3863</span>
<span class="normal">3864</span>
<span class="normal">3865</span>
<span class="normal">3866</span>
<span class="normal">3867</span>
<span class="normal">3868</span>
<span class="normal">3869</span>
<span class="normal">3870</span>
<span class="normal">3871</span>
<span class="normal">3872</span>
<span class="normal">3873</span>
<span class="normal">3874</span>
<span class="normal">3875</span>
<span class="normal">3876</span>
<span class="normal">3877</span>
<span class="normal">3878</span>
<span class="normal">3879</span>
<span class="normal">3880</span>
<span class="normal">3881</span>
<span class="normal">3882</span>
<span class="normal">3883</span>
<span class="normal">3884</span>
<span class="normal">3885</span>
<span class="normal">3886</span>
<span class="normal">3887</span>
<span class="normal">3888</span>
<span class="normal">3889</span>
<span class="normal">3890</span>
<span class="normal">3891</span>
<span class="normal">3892</span>
<span class="normal">3893</span>
<span class="normal">3894</span>
<span class="normal">3895</span>
<span class="normal">3896</span>
<span class="normal">3897</span>
<span class="normal">3898</span>
<span class="normal">3899</span>
<span class="normal">3900</span>
<span class="normal">3901</span>
<span class="normal">3902</span>
<span class="normal">3903</span>
<span class="normal">3904</span>
<span class="normal">3905</span>
<span class="normal">3906</span>
<span class="normal">3907</span>
<span class="normal">3908</span>
<span class="normal">3909</span>
<span class="normal">3910</span>
<span class="normal">3911</span>
<span class="normal">3912</span>
<span class="normal">3913</span>
<span class="normal">3914</span>
<span class="normal">3915</span>
<span class="normal">3916</span>
<span class="normal">3917</span>
<span class="normal">3918</span>
<span class="normal">3919</span>
<span class="normal">3920</span>
<span class="normal">3921</span>
<span class="normal">3922</span>
<span class="normal">3923</span>
<span class="normal">3924</span>
<span class="normal">3925</span>
<span class="normal">3926</span>
<span class="normal">3927</span>
<span class="normal">3928</span>
<span class="normal">3929</span>
<span class="normal">3930</span>
<span class="normal">3931</span>
<span class="normal">3932</span>
<span class="normal">3933</span>
<span class="normal">3934</span>
<span class="normal">3935</span>
<span class="normal">3936</span>
<span class="normal">3937</span>
<span class="normal">3938</span>
<span class="normal">3939</span>
<span class="normal">3940</span>
<span class="normal">3941</span>
<span class="normal">3942</span>
<span class="normal">3943</span>
<span class="normal">3944</span>
<span class="normal">3945</span>
<span class="normal">3946</span>
<span class="normal">3947</span>
<span class="normal">3948</span>
<span class="normal">3949</span>
<span class="normal">3950</span>
<span class="normal">3951</span>
<span class="normal">3952</span>
<span class="normal">3953</span>
<span class="normal">3954</span>
<span class="normal">3955</span>
<span class="normal">3956</span>
<span class="normal">3957</span>
<span class="normal">3958</span>
<span class="normal">3959</span>
<span class="normal">3960</span>
<span class="normal">3961</span>
<span class="normal">3962</span>
<span class="normal">3963</span>
<span class="normal">3964</span>
<span class="normal">3965</span>
<span class="normal">3966</span>
<span class="normal">3967</span>
<span class="normal">3968</span>
<span class="normal">3969</span>
<span class="normal">3970</span>
<span class="normal">3971</span>
<span class="normal">3972</span>
<span class="normal">3973</span>
<span class="normal">3974</span>
<span class="normal">3975</span>
<span class="normal">3976</span>
<span class="normal">3977</span>
<span class="normal">3978</span>
<span class="normal">3979</span>
<span class="normal">3980</span>
<span class="normal">3981</span>
<span class="normal">3982</span>
<span class="normal">3983</span>
<span class="normal">3984</span>
<span class="normal">3985</span>
<span class="normal">3986</span>
<span class="normal">3987</span>
<span class="normal">3988</span>
<span class="normal">3989</span>
<span class="normal">3990</span>
<span class="normal">3991</span>
<span class="normal">3992</span>
<span class="normal">3993</span>
<span class="normal">3994</span>
<span class="normal">3995</span>
<span class="normal">3996</span>
<span class="normal">3997</span>
<span class="normal">3998</span>
<span class="normal">3999</span>
<span class="normal">4000</span>
<span class="normal">4001</span>
<span class="normal">4002</span>
<span class="normal">4003</span>
<span class="normal">4004</span>
<span class="normal">4005</span>
<span class="normal">4006</span>
<span class="normal">4007</span>
<span class="normal">4008</span>
<span class="normal">4009</span>
<span class="normal">4010</span>
<span class="normal">4011</span>
<span class="normal">4012</span>
<span class="normal">4013</span>
<span class="normal">4014</span>
<span class="normal">4015</span>
<span class="normal">4016</span>
<span class="normal">4017</span>
<span class="normal">4018</span>
<span class="normal">4019</span>
<span class="normal">4020</span>
<span class="normal">4021</span>
<span class="normal">4022</span>
<span class="normal">4023</span>
<span class="normal">4024</span>
<span class="normal">4025</span>
<span class="normal">4026</span>
<span class="normal">4027</span>
<span class="normal">4028</span>
<span class="normal">4029</span>
<span class="normal">4030</span>
<span class="normal">4031</span>
<span class="normal">4032</span>
<span class="normal">4033</span>
<span class="normal">4034</span>
<span class="normal">4035</span>
<span class="normal">4036</span>
<span class="normal">4037</span>
<span class="normal">4038</span>
<span class="normal">4039</span>
<span class="normal">4040</span>
<span class="normal">4041</span>
<span class="normal">4042</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2ForSpeechToSpeech</span><span class="p">(</span><span class="n">SeamlessM4Tv2PreTrainedModel</span><span class="p">):</span>
    <span class="n">_keys_to_ignore_on_load_missing</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;text_encoder&quot;</span><span class="p">]</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_features&quot;</span>

    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_decoder.embed_tokens.weight&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.__init__ with SeamlessM4T-&gt;SeamlessM4Tv2</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2SpeechEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Decoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2TextToUnitForConditionalGeneration</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2CodeHifiGan</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.get_encoder</span>
    <span class="k">def</span> <span class="nf">get_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.get_decoder</span>
    <span class="k">def</span> <span class="nf">get_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.get_output_embeddings</span>
    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.set_output_embeddings</span>
    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.get_input_embeddings</span>
    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.set_input_embeddings</span>
    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech._tie_weights</span>
    <span class="k">def</span> <span class="nf">_tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.forward with SeamlessM4T-&gt;SeamlessM4Tv2</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_features</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Seq2SeqLMOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The `use_cache` argument is changed to `False` since `labels` is provided.&quot;</span><span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span>
                    <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
                <span class="p">)</span>

        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if encoder_outputs is not None, it&#39;s probably used within a .generate method so no need to warn</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This is the same forward method as `SeamlessM4Tv2ForSpeechToText`. It doesn&#39;t use `self.t2u_model`.&quot;</span>
                <span class="s2">&quot;If you want to generate speech, use the `generate` method.&quot;</span>
            <span class="p">)</span>

            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span>
                <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True</span>
        <span class="k">elif</span> <span class="n">return_dict</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">):</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
                <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
            <span class="p">)</span>

        <span class="c1"># decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">decoder_inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">decoder_outputs</span> <span class="o">+</span> <span class="n">encoder_outputs</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speaker_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates translated audio waveforms.</span>

<span class="sd">        &lt;Tip&gt;</span>

<span class="sd">        This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">        arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">        that will be passed to one of them.</span>

<span class="sd">        For example, calling `.generate(input_features, num_beams=4, speech_do_sample=True)` will successively perform</span>
<span class="sd">        beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">        For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">        guide](./generation_strategies).</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`):</span>
<span class="sd">                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">                [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>
<span class="sd">            return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">                If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">                to get translated text alongside the audio.</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language to use as target language for translation.</span>
<span class="sd">            speaker_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>

<span class="sd">            kwargs (*optional*):</span>
<span class="sd">                Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">                arguments are of two types:</span>

<span class="sd">                    - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                    except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                    - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                    text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                    This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                    other.</span>


<span class="sd">        Returns:</span>
<span class="sd">            `Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]`:</span>
<span class="sd">            - If `return_intermediate_token_ids`, returns [`SeamlessM4Tv2GenerationOutput`].</span>
<span class="sd">            - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,</span>
<span class="sd">              sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># also accept __xxx__</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]:</span>
                <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                        to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                    Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4Tv2 supports</span>
<span class="s2">                    more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                    <span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
        <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
        <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

        <span class="c1"># first generation</span>
        <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
        <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

        <span class="c1"># prepare second generation</span>
        <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

        <span class="c1"># get last_hidden_state from encoder</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># input modality = speech so new attention mask for the decoder</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
            <span class="p">)</span>

            <span class="c1"># repeat attention mask alongside batch dimension</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># repeat attention mask alongside batch dimension</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
        <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Manually trim the final EOS token</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

        <span class="c1"># Compute new attention mask</span>
        <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
        <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

        <span class="c1"># REMOVE EOS and lang_id</span>
        <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># replace every other EOS</span>
        <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
            <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_input_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span>
        <span class="p">)</span>

        <span class="c1"># compute t2u_char_input_ids</span>
        <span class="n">t2u_subwords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_indices_to_subwords</span><span class="p">(</span><span class="n">t2u_input_ids</span><span class="p">)</span>
        <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_character_length_in_subword</span><span class="p">(</span>
            <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
        <span class="p">)</span>

        <span class="c1"># Add pads for lang, EOS tokens as per NLLB &quot;source&quot; tokenizer mode.</span>
        <span class="n">pad_zero</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pad_zero</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_zero</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t2u_char_input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_char_input_ids</span><span class="p">(</span>
            <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
        <span class="p">)</span>

        <span class="c1"># second pass</span>
        <span class="n">t2u_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="p">(</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span>
            <span class="n">char_input_ids</span><span class="o">=</span><span class="n">t2u_char_input_ids</span><span class="p">,</span>
            <span class="n">char_count_per_id</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

        <span class="c1"># The text-to-unit model is non auto-regressive. We keep the ability to use sampling with temperature</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">temperature</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;do_sample&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_logits</span> <span class="o">/</span> <span class="n">temperature</span>
            <span class="c1"># apply softmax</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># reshape to 2D: (batch_size, seq_len, t2u_vocab_size) -&gt; (batch_size*seq_len, t2u_vocab_size)</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
            <span class="c1"># multinomial then reshape : (batch_size*seq_len)-&gt; (batch_size,seq_len)</span>
            <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">t2u_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="n">replace_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span> <span class="o">|</span> <span class="p">(</span><span class="o">~</span><span class="n">padding_mask</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="c1"># replace eos per pad</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">replace_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">)</span>

        <span class="c1"># offset of control symbols</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
        <span class="p">)</span>

        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">speaker_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">speaker_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">speaker_id</span><span class="o">=</span><span class="n">speaker_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">(</span>
                <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
                <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
                <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
                <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>

    <span class="nd">@staticmethod</span>
    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech._reorder_cache</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="c1"># cached cross_attention states don&#39;t have to be reordered -&gt; they are always the same</span>
            <span class="n">reordered_past</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">past_state</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">past_state</span> <span class="ow">in</span> <span class="n">layer_past</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">reordered_past</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.prepare_inputs_for_generation</span>
    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># cut decoder_input_ids if past is used</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># encoder_outputs is defined. input_ids not needed</span>
            <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">:</span> <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
        <span class="p">}</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToSpeech.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2ForSpeechToSpeech</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_intermediate_token_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">speaker_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToSpeech.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates translated audio waveforms.</p>
<p><Tip></p>
<p>This method successively calls the <code>.generate</code> function of two different sub-models. You can specify keyword
arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments
that will be passed to one of them.</p>
<p>For example, calling <code>.generate(input_features, num_beams=4, speech_do_sample=True)</code> will successively perform
beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</p>
<p>For an overview of generation strategies and code examples, check out the <a href="./generation_strategies">following
guide</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_features</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input audio features. This should be returnes by the [<code>SeamlessM4TFeatureExtractor</code>] class or the
[<code>SeamlessM4TProcessor</code>] class. See [<code>SeamlessM4TFeatureExtractor.__call__</code>] for details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_intermediate_token_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If <code>True</code>, also returns the intermediate generated text and unit tokens. Set to <code>True</code> if you also want
to get translated text alongside the audio.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speaker_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the speaker used for speech synthesis. Must be lower than <code>config.vocoder_num_spkrs</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Remaining dictionary of keyword arguments that will be passed to [<code>GenerationMixin.generate</code>]. Keyword
arguments are of two types:</p>
<div class="highlight"><pre><span></span><code>- Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,
except for `decoder_input_ids` which will only be passed through the text components.
- With a *text_* or *speech_* prefix, they will be input for the `generate` method of the
text model and speech model respectively. It has the priority over the keywords without a prefix.

This means you can, for example, specify a generation strategy for one generation but not for the
other.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput">SeamlessM4Tv2GenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]</code>:</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput">SeamlessM4Tv2GenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If <code>return_intermediate_token_ids</code>, returns [<code>SeamlessM4Tv2GenerationOutput</code>].</li>
</ul>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput">SeamlessM4Tv2GenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If not <code>return_intermediate_token_ids</code>, returns a tuple composed of waveforms of shape <code>(batch_size,
sequence_length)</code>and and <code>waveform_lengths</code> which gives the length of each sample.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3808</span>
<span class="normal">3809</span>
<span class="normal">3810</span>
<span class="normal">3811</span>
<span class="normal">3812</span>
<span class="normal">3813</span>
<span class="normal">3814</span>
<span class="normal">3815</span>
<span class="normal">3816</span>
<span class="normal">3817</span>
<span class="normal">3818</span>
<span class="normal">3819</span>
<span class="normal">3820</span>
<span class="normal">3821</span>
<span class="normal">3822</span>
<span class="normal">3823</span>
<span class="normal">3824</span>
<span class="normal">3825</span>
<span class="normal">3826</span>
<span class="normal">3827</span>
<span class="normal">3828</span>
<span class="normal">3829</span>
<span class="normal">3830</span>
<span class="normal">3831</span>
<span class="normal">3832</span>
<span class="normal">3833</span>
<span class="normal">3834</span>
<span class="normal">3835</span>
<span class="normal">3836</span>
<span class="normal">3837</span>
<span class="normal">3838</span>
<span class="normal">3839</span>
<span class="normal">3840</span>
<span class="normal">3841</span>
<span class="normal">3842</span>
<span class="normal">3843</span>
<span class="normal">3844</span>
<span class="normal">3845</span>
<span class="normal">3846</span>
<span class="normal">3847</span>
<span class="normal">3848</span>
<span class="normal">3849</span>
<span class="normal">3850</span>
<span class="normal">3851</span>
<span class="normal">3852</span>
<span class="normal">3853</span>
<span class="normal">3854</span>
<span class="normal">3855</span>
<span class="normal">3856</span>
<span class="normal">3857</span>
<span class="normal">3858</span>
<span class="normal">3859</span>
<span class="normal">3860</span>
<span class="normal">3861</span>
<span class="normal">3862</span>
<span class="normal">3863</span>
<span class="normal">3864</span>
<span class="normal">3865</span>
<span class="normal">3866</span>
<span class="normal">3867</span>
<span class="normal">3868</span>
<span class="normal">3869</span>
<span class="normal">3870</span>
<span class="normal">3871</span>
<span class="normal">3872</span>
<span class="normal">3873</span>
<span class="normal">3874</span>
<span class="normal">3875</span>
<span class="normal">3876</span>
<span class="normal">3877</span>
<span class="normal">3878</span>
<span class="normal">3879</span>
<span class="normal">3880</span>
<span class="normal">3881</span>
<span class="normal">3882</span>
<span class="normal">3883</span>
<span class="normal">3884</span>
<span class="normal">3885</span>
<span class="normal">3886</span>
<span class="normal">3887</span>
<span class="normal">3888</span>
<span class="normal">3889</span>
<span class="normal">3890</span>
<span class="normal">3891</span>
<span class="normal">3892</span>
<span class="normal">3893</span>
<span class="normal">3894</span>
<span class="normal">3895</span>
<span class="normal">3896</span>
<span class="normal">3897</span>
<span class="normal">3898</span>
<span class="normal">3899</span>
<span class="normal">3900</span>
<span class="normal">3901</span>
<span class="normal">3902</span>
<span class="normal">3903</span>
<span class="normal">3904</span>
<span class="normal">3905</span>
<span class="normal">3906</span>
<span class="normal">3907</span>
<span class="normal">3908</span>
<span class="normal">3909</span>
<span class="normal">3910</span>
<span class="normal">3911</span>
<span class="normal">3912</span>
<span class="normal">3913</span>
<span class="normal">3914</span>
<span class="normal">3915</span>
<span class="normal">3916</span>
<span class="normal">3917</span>
<span class="normal">3918</span>
<span class="normal">3919</span>
<span class="normal">3920</span>
<span class="normal">3921</span>
<span class="normal">3922</span>
<span class="normal">3923</span>
<span class="normal">3924</span>
<span class="normal">3925</span>
<span class="normal">3926</span>
<span class="normal">3927</span>
<span class="normal">3928</span>
<span class="normal">3929</span>
<span class="normal">3930</span>
<span class="normal">3931</span>
<span class="normal">3932</span>
<span class="normal">3933</span>
<span class="normal">3934</span>
<span class="normal">3935</span>
<span class="normal">3936</span>
<span class="normal">3937</span>
<span class="normal">3938</span>
<span class="normal">3939</span>
<span class="normal">3940</span>
<span class="normal">3941</span>
<span class="normal">3942</span>
<span class="normal">3943</span>
<span class="normal">3944</span>
<span class="normal">3945</span>
<span class="normal">3946</span>
<span class="normal">3947</span>
<span class="normal">3948</span>
<span class="normal">3949</span>
<span class="normal">3950</span>
<span class="normal">3951</span>
<span class="normal">3952</span>
<span class="normal">3953</span>
<span class="normal">3954</span>
<span class="normal">3955</span>
<span class="normal">3956</span>
<span class="normal">3957</span>
<span class="normal">3958</span>
<span class="normal">3959</span>
<span class="normal">3960</span>
<span class="normal">3961</span>
<span class="normal">3962</span>
<span class="normal">3963</span>
<span class="normal">3964</span>
<span class="normal">3965</span>
<span class="normal">3966</span>
<span class="normal">3967</span>
<span class="normal">3968</span>
<span class="normal">3969</span>
<span class="normal">3970</span>
<span class="normal">3971</span>
<span class="normal">3972</span>
<span class="normal">3973</span>
<span class="normal">3974</span>
<span class="normal">3975</span>
<span class="normal">3976</span>
<span class="normal">3977</span>
<span class="normal">3978</span>
<span class="normal">3979</span>
<span class="normal">3980</span>
<span class="normal">3981</span>
<span class="normal">3982</span>
<span class="normal">3983</span>
<span class="normal">3984</span>
<span class="normal">3985</span>
<span class="normal">3986</span>
<span class="normal">3987</span>
<span class="normal">3988</span>
<span class="normal">3989</span>
<span class="normal">3990</span>
<span class="normal">3991</span>
<span class="normal">3992</span>
<span class="normal">3993</span>
<span class="normal">3994</span>
<span class="normal">3995</span>
<span class="normal">3996</span>
<span class="normal">3997</span>
<span class="normal">3998</span>
<span class="normal">3999</span>
<span class="normal">4000</span>
<span class="normal">4001</span>
<span class="normal">4002</span>
<span class="normal">4003</span>
<span class="normal">4004</span>
<span class="normal">4005</span>
<span class="normal">4006</span>
<span class="normal">4007</span>
<span class="normal">4008</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">speaker_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates translated audio waveforms.</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">    arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">    that will be passed to one of them.</span>

<span class="sd">    For example, calling `.generate(input_features, num_beams=4, speech_do_sample=True)` will successively perform</span>
<span class="sd">    beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">    For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">    guide](./generation_strategies).</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`):</span>
<span class="sd">            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">            [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>
<span class="sd">        return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">            If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">            to get translated text alongside the audio.</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        speaker_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>

<span class="sd">        kwargs (*optional*):</span>
<span class="sd">            Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">            arguments are of two types:</span>

<span class="sd">                - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                other.</span>


<span class="sd">    Returns:</span>
<span class="sd">        `Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]`:</span>
<span class="sd">        - If `return_intermediate_token_ids`, returns [`SeamlessM4Tv2GenerationOutput`].</span>
<span class="sd">        - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,</span>
<span class="sd">          sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># also accept __xxx__</span>
        <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]:</span>
            <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                    to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4Tv2 supports</span>
<span class="s2">                more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                <span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
    <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
    <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

    <span class="c1"># first generation</span>
    <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

    <span class="c1"># prepare second generation</span>
    <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="c1"># get last_hidden_state from encoder</span>
    <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># input modality = speech so new attention mask for the decoder</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
        <span class="p">)</span>

        <span class="c1"># repeat attention mask alongside batch dimension</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># repeat attention mask alongside batch dimension</span>
    <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
    <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Manually trim the final EOS token</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

    <span class="c1"># Compute new attention mask</span>
    <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
    <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

    <span class="c1"># REMOVE EOS and lang_id</span>
    <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># replace every other EOS</span>
    <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
        <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_input_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span>
    <span class="p">)</span>

    <span class="c1"># compute t2u_char_input_ids</span>
    <span class="n">t2u_subwords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_indices_to_subwords</span><span class="p">(</span><span class="n">t2u_input_ids</span><span class="p">)</span>
    <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_character_length_in_subword</span><span class="p">(</span>
        <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
    <span class="p">)</span>

    <span class="c1"># Add pads for lang, EOS tokens as per NLLB &quot;source&quot; tokenizer mode.</span>
    <span class="n">pad_zero</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pad_zero</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_zero</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t2u_char_input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_char_input_ids</span><span class="p">(</span>
        <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
    <span class="p">)</span>

    <span class="c1"># second pass</span>
    <span class="n">t2u_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="p">(</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span>
        <span class="n">char_input_ids</span><span class="o">=</span><span class="n">t2u_char_input_ids</span><span class="p">,</span>
        <span class="n">char_count_per_id</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

    <span class="c1"># The text-to-unit model is non auto-regressive. We keep the ability to use sampling with temperature</span>
    <span class="n">temperature</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">temperature</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;do_sample&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_logits</span> <span class="o">/</span> <span class="n">temperature</span>
        <span class="c1"># apply softmax</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># reshape to 2D: (batch_size, seq_len, t2u_vocab_size) -&gt; (batch_size*seq_len, t2u_vocab_size)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
        <span class="c1"># multinomial then reshape : (batch_size*seq_len)-&gt; (batch_size,seq_len)</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">t2u_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="n">replace_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span> <span class="o">|</span> <span class="p">(</span><span class="o">~</span><span class="n">padding_mask</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="c1"># replace eos per pad</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">replace_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">)</span>

    <span class="c1"># offset of control symbols</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
    <span class="p">)</span>

    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">speaker_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">speaker_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">speaker_id</span><span class="o">=</span><span class="n">speaker_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">(</span>
            <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
            <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
            <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
            <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToText" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToText</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToText" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel">SeamlessM4Tv2PreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span>
<span class="normal">2976</span>
<span class="normal">2977</span>
<span class="normal">2978</span>
<span class="normal">2979</span>
<span class="normal">2980</span>
<span class="normal">2981</span>
<span class="normal">2982</span>
<span class="normal">2983</span>
<span class="normal">2984</span>
<span class="normal">2985</span>
<span class="normal">2986</span>
<span class="normal">2987</span>
<span class="normal">2988</span>
<span class="normal">2989</span>
<span class="normal">2990</span>
<span class="normal">2991</span>
<span class="normal">2992</span>
<span class="normal">2993</span>
<span class="normal">2994</span>
<span class="normal">2995</span>
<span class="normal">2996</span>
<span class="normal">2997</span>
<span class="normal">2998</span>
<span class="normal">2999</span>
<span class="normal">3000</span>
<span class="normal">3001</span>
<span class="normal">3002</span>
<span class="normal">3003</span>
<span class="normal">3004</span>
<span class="normal">3005</span>
<span class="normal">3006</span>
<span class="normal">3007</span>
<span class="normal">3008</span>
<span class="normal">3009</span>
<span class="normal">3010</span>
<span class="normal">3011</span>
<span class="normal">3012</span>
<span class="normal">3013</span>
<span class="normal">3014</span>
<span class="normal">3015</span>
<span class="normal">3016</span>
<span class="normal">3017</span>
<span class="normal">3018</span>
<span class="normal">3019</span>
<span class="normal">3020</span>
<span class="normal">3021</span>
<span class="normal">3022</span>
<span class="normal">3023</span>
<span class="normal">3024</span>
<span class="normal">3025</span>
<span class="normal">3026</span>
<span class="normal">3027</span>
<span class="normal">3028</span>
<span class="normal">3029</span>
<span class="normal">3030</span>
<span class="normal">3031</span>
<span class="normal">3032</span>
<span class="normal">3033</span>
<span class="normal">3034</span>
<span class="normal">3035</span>
<span class="normal">3036</span>
<span class="normal">3037</span>
<span class="normal">3038</span>
<span class="normal">3039</span>
<span class="normal">3040</span>
<span class="normal">3041</span>
<span class="normal">3042</span>
<span class="normal">3043</span>
<span class="normal">3044</span>
<span class="normal">3045</span>
<span class="normal">3046</span>
<span class="normal">3047</span>
<span class="normal">3048</span>
<span class="normal">3049</span>
<span class="normal">3050</span>
<span class="normal">3051</span>
<span class="normal">3052</span>
<span class="normal">3053</span>
<span class="normal">3054</span>
<span class="normal">3055</span>
<span class="normal">3056</span>
<span class="normal">3057</span>
<span class="normal">3058</span>
<span class="normal">3059</span>
<span class="normal">3060</span>
<span class="normal">3061</span>
<span class="normal">3062</span>
<span class="normal">3063</span>
<span class="normal">3064</span>
<span class="normal">3065</span>
<span class="normal">3066</span>
<span class="normal">3067</span>
<span class="normal">3068</span>
<span class="normal">3069</span>
<span class="normal">3070</span>
<span class="normal">3071</span>
<span class="normal">3072</span>
<span class="normal">3073</span>
<span class="normal">3074</span>
<span class="normal">3075</span>
<span class="normal">3076</span>
<span class="normal">3077</span>
<span class="normal">3078</span>
<span class="normal">3079</span>
<span class="normal">3080</span>
<span class="normal">3081</span>
<span class="normal">3082</span>
<span class="normal">3083</span>
<span class="normal">3084</span>
<span class="normal">3085</span>
<span class="normal">3086</span>
<span class="normal">3087</span>
<span class="normal">3088</span>
<span class="normal">3089</span>
<span class="normal">3090</span>
<span class="normal">3091</span>
<span class="normal">3092</span>
<span class="normal">3093</span>
<span class="normal">3094</span>
<span class="normal">3095</span>
<span class="normal">3096</span>
<span class="normal">3097</span>
<span class="normal">3098</span>
<span class="normal">3099</span>
<span class="normal">3100</span>
<span class="normal">3101</span>
<span class="normal">3102</span>
<span class="normal">3103</span>
<span class="normal">3104</span>
<span class="normal">3105</span>
<span class="normal">3106</span>
<span class="normal">3107</span>
<span class="normal">3108</span>
<span class="normal">3109</span>
<span class="normal">3110</span>
<span class="normal">3111</span>
<span class="normal">3112</span>
<span class="normal">3113</span>
<span class="normal">3114</span>
<span class="normal">3115</span>
<span class="normal">3116</span>
<span class="normal">3117</span>
<span class="normal">3118</span>
<span class="normal">3119</span>
<span class="normal">3120</span>
<span class="normal">3121</span>
<span class="normal">3122</span>
<span class="normal">3123</span>
<span class="normal">3124</span>
<span class="normal">3125</span>
<span class="normal">3126</span>
<span class="normal">3127</span>
<span class="normal">3128</span>
<span class="normal">3129</span>
<span class="normal">3130</span>
<span class="normal">3131</span>
<span class="normal">3132</span>
<span class="normal">3133</span>
<span class="normal">3134</span>
<span class="normal">3135</span>
<span class="normal">3136</span>
<span class="normal">3137</span>
<span class="normal">3138</span>
<span class="normal">3139</span>
<span class="normal">3140</span>
<span class="normal">3141</span>
<span class="normal">3142</span>
<span class="normal">3143</span>
<span class="normal">3144</span>
<span class="normal">3145</span>
<span class="normal">3146</span>
<span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span>
<span class="normal">3157</span>
<span class="normal">3158</span>
<span class="normal">3159</span>
<span class="normal">3160</span>
<span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span>
<span class="normal">3192</span>
<span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span>
<span class="normal">3199</span>
<span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span>
<span class="normal">3204</span>
<span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span>
<span class="normal">3225</span>
<span class="normal">3226</span>
<span class="normal">3227</span>
<span class="normal">3228</span>
<span class="normal">3229</span>
<span class="normal">3230</span>
<span class="normal">3231</span>
<span class="normal">3232</span>
<span class="normal">3233</span>
<span class="normal">3234</span>
<span class="normal">3235</span>
<span class="normal">3236</span>
<span class="normal">3237</span>
<span class="normal">3238</span>
<span class="normal">3239</span>
<span class="normal">3240</span>
<span class="normal">3241</span>
<span class="normal">3242</span>
<span class="normal">3243</span>
<span class="normal">3244</span>
<span class="normal">3245</span>
<span class="normal">3246</span>
<span class="normal">3247</span>
<span class="normal">3248</span>
<span class="normal">3249</span>
<span class="normal">3250</span>
<span class="normal">3251</span>
<span class="normal">3252</span>
<span class="normal">3253</span>
<span class="normal">3254</span>
<span class="normal">3255</span>
<span class="normal">3256</span>
<span class="normal">3257</span>
<span class="normal">3258</span>
<span class="normal">3259</span>
<span class="normal">3260</span>
<span class="normal">3261</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2ForSpeechToText</span><span class="p">(</span><span class="n">SeamlessM4Tv2PreTrainedModel</span><span class="p">):</span>
    <span class="n">_keys_to_ignore_on_load_missing</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;text_decoder&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_model&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder&quot;</span><span class="p">]</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_features&quot;</span>

    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_decoder.embed_tokens.weight&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.__init__ with SeamlessM4T-&gt;SeamlessM4Tv2</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4Tv2Config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2SpeechEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Decoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.get_encoder</span>
    <span class="k">def</span> <span class="nf">get_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.get_decoder</span>
    <span class="k">def</span> <span class="nf">get_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.get_output_embeddings</span>
    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.set_output_embeddings</span>
    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.get_input_embeddings</span>
    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.set_input_embeddings</span>
    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText._tie_weights</span>
    <span class="k">def</span> <span class="nf">_tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.forward</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_features</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Seq2SeqLMOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The `use_cache` argument is changed to `False` since `labels` is provided.&quot;</span><span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span>
                    <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
                <span class="p">)</span>

        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span>
                <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True</span>
        <span class="k">elif</span> <span class="n">return_dict</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">):</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
                <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
            <span class="p">)</span>

        <span class="c1"># decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">decoder_inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">decoder_outputs</span> <span class="o">+</span> <span class="n">encoder_outputs</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.generate</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the</span>
<span class="sd">        model&#39;s default generation configuration. You can override any `generation_config` by passing the corresponding</span>
<span class="sd">        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.</span>

<span class="sd">        For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">        guide](./generation_strategies).</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`):</span>
<span class="sd">                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">                [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>

<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language to use as target language for translation.</span>
<span class="sd">            generation_config (`~generation.GenerationConfig`, *optional*):</span>
<span class="sd">                The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">                passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">                `generation_config` is not provided, the default will be used, which had the following loading</span>
<span class="sd">                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">                default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`, *optional*):</span>
<span class="sd">                Custom logits processors that complement the default logits processors built from arguments and</span>
<span class="sd">                generation config. If a logit processor is passed that is already created with the arguments or a</span>
<span class="sd">                generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`, *optional*):</span>
<span class="sd">                Custom stopping criteria that complement the default stopping criteria built from arguments and a</span>
<span class="sd">                generation config. If a stopping criteria is passed that is already created with the arguments or a</span>
<span class="sd">                generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">            prefix_allowed_tokens_fn (`Callable[[int, mindspore.Tensor], List[int]]`, *optional*):</span>
<span class="sd">                If provided, this function constraints the beam search to allowed tokens only at each step. If not</span>
<span class="sd">                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and</span>
<span class="sd">                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned</span>
<span class="sd">                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful</span>
<span class="sd">                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity</span>
<span class="sd">                Retrieval](https://arxiv.org/abs/2010.00904).</span>
<span class="sd">            synced_gpus (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">            kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">                Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be</span>
<span class="sd">                forwarded to the `forward` function of the model.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`~utils.ModelOutput`] or `mindspore.Tensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`</span>
<span class="sd">            or when `config.return_dict_in_generate=True`) or a `mindspore.Tensor`. The possible</span>
<span class="sd">            [`~utils.ModelOutput`] types are:</span>
<span class="sd">                - [`~generation.GenerateEncoderDecoderOutput`],</span>
<span class="sd">                - [`~generation.GenerateBeamEncoderDecoderOutput`]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
        <span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_embeds&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">input_features</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">inputs</span>
                <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">})[</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">):</span>
                <span class="c1"># also accept __xxx__</span>
                <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model. Please specify a `tgt_lang` in</span>
<span class="s2">                        </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="c1"># tgt_lang gets priority over decoder input ids</span>
                <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
                <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;This model generation config doesn&#39;t have a `text_decoder_lang_to_code_id` key which maps</span>
<span class="sd">                    the target language to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># only a warning, otherwise errors appear in the tests</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;You must either specify a `tgt_lang` or pass a correct `text_decoder_input_ids` to get</span>
<span class="sd">                a correct generation, otherwise the generation will probably make no sense.&quot;&quot;&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
            <span class="n">input_features</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="p">,</span>
            <span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="p">,</span>
            <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">text_decoder_input_ids</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.prepare_inputs_for_generation</span>
    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># cut decoder_input_ids if past is used</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># encoder_outputs is defined. input_ids not needed</span>
            <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">:</span> <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText._reorder_cache</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="c1"># cached cross_attention states don&#39;t have to be reordered -&gt; they are always the same</span>
            <span class="n">reordered_past</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">past_state</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">past_state</span> <span class="ow">in</span> <span class="n">layer_past</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">reordered_past</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToText.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2ForSpeechToText</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForSpeechToText.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates sequences of token ids.</p>
<p><Tip warning={true}></p>
<p>Most generation-controlling parameters are set in <code>generation_config</code> which, if not passed, will be set to the
model's default generation configuration. You can override any <code>generation_config</code> by passing the corresponding
parameters to generate(), e.g. <code>.generate(inputs, num_beams=4, do_sample=True)</code>.</p>
<p>For an overview of generation strategies and code examples, check out the <a href="./generation_strategies">following
guide</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_features</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input audio features. This should be returnes by the [<code>SeamlessM4TFeatureExtractor</code>] class or the
[<code>SeamlessM4TProcessor</code>] class. See [<code>SeamlessM4TFeatureExtractor.__call__</code>] for details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>generation_config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generation configuration to be used as base parametrization for the generation call. <code>**kwargs</code>
passed to generate matching the attributes of <code>generation_config</code> will override them. If
<code>generation_config</code> is not provided, the default will be used, which had the following loading
priority: 1) from the <code>generation_config.json</code> model file, if it exists; 2) from the model
configuration. Please note that unspecified parameters will inherit [<code>~generation.GenerationConfig</code>]'s
default values, whose documentation should be checked to parameterize generation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`~generation.GenerationConfig`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logits_processor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom logits processors that complement the default logits processors built from arguments and
generation config. If a logit processor is passed that is already created with the arguments or a
generation config an error is thrown. This feature is intended for advanced users.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`LogitsProcessorList`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stopping_criteria</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom stopping criteria that complement the default stopping criteria built from arguments and a
generation config. If a stopping criteria is passed that is already created with the arguments or a
generation config an error is thrown. This feature is intended for advanced users.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`StoppingCriteriaList`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prefix_allowed_tokens_fn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments: the batch ID <code>batch_id</code> and
<code>input_ids</code>. It has to return a list with the allowed tokens for the next generation step conditioned
on the batch ID <code>batch_id</code> and the previously generated tokens <code>inputs_ids</code>. This argument is useful
for constrained generation conditioned on the prefix, as described in <a href="https://arxiv.org/abs/2010.00904">Autoregressive Entity
Retrieval</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable[[int, mindspore.Tensor], List[int]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>synced_gpus</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Ad hoc parametrization of <code>generate_config</code> and/or additional model-specific kwargs that will be
forwarded to the <code>forward</code> function of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Any]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p>[<code>~utils.ModelOutput</code>] or <code>mindspore.Tensor</code>: A [<code>~utils.ModelOutput</code>] (if <code>return_dict_in_generate=True</code>
or when <code>config.return_dict_in_generate=True</code>) or a <code>mindspore.Tensor</code>. The possible
[<code>~utils.ModelOutput</code>] types are:
    - [<code>~generation.GenerateEncoderDecoderOutput</code>],
    - [<code>~generation.GenerateBeamEncoderDecoderOutput</code>]</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3117</span>
<span class="normal">3118</span>
<span class="normal">3119</span>
<span class="normal">3120</span>
<span class="normal">3121</span>
<span class="normal">3122</span>
<span class="normal">3123</span>
<span class="normal">3124</span>
<span class="normal">3125</span>
<span class="normal">3126</span>
<span class="normal">3127</span>
<span class="normal">3128</span>
<span class="normal">3129</span>
<span class="normal">3130</span>
<span class="normal">3131</span>
<span class="normal">3132</span>
<span class="normal">3133</span>
<span class="normal">3134</span>
<span class="normal">3135</span>
<span class="normal">3136</span>
<span class="normal">3137</span>
<span class="normal">3138</span>
<span class="normal">3139</span>
<span class="normal">3140</span>
<span class="normal">3141</span>
<span class="normal">3142</span>
<span class="normal">3143</span>
<span class="normal">3144</span>
<span class="normal">3145</span>
<span class="normal">3146</span>
<span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span>
<span class="normal">3157</span>
<span class="normal">3158</span>
<span class="normal">3159</span>
<span class="normal">3160</span>
<span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span>
<span class="normal">3192</span>
<span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span>
<span class="normal">3199</span>
<span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span>
<span class="normal">3204</span>
<span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span>
<span class="normal">3225</span>
<span class="normal">3226</span>
<span class="normal">3227</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates sequences of token ids.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the</span>
<span class="sd">    model&#39;s default generation configuration. You can override any `generation_config` by passing the corresponding</span>
<span class="sd">    parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.</span>

<span class="sd">    For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">    guide](./generation_strategies).</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`):</span>
<span class="sd">            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">            [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>

<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        generation_config (`~generation.GenerationConfig`, *optional*):</span>
<span class="sd">            The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">            passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">            `generation_config` is not provided, the default will be used, which had the following loading</span>
<span class="sd">            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">            default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">        logits_processor (`LogitsProcessorList`, *optional*):</span>
<span class="sd">            Custom logits processors that complement the default logits processors built from arguments and</span>
<span class="sd">            generation config. If a logit processor is passed that is already created with the arguments or a</span>
<span class="sd">            generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">        stopping_criteria (`StoppingCriteriaList`, *optional*):</span>
<span class="sd">            Custom stopping criteria that complement the default stopping criteria built from arguments and a</span>
<span class="sd">            generation config. If a stopping criteria is passed that is already created with the arguments or a</span>
<span class="sd">            generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">        prefix_allowed_tokens_fn (`Callable[[int, mindspore.Tensor], List[int]]`, *optional*):</span>
<span class="sd">            If provided, this function constraints the beam search to allowed tokens only at each step. If not</span>
<span class="sd">            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and</span>
<span class="sd">            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned</span>
<span class="sd">            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful</span>
<span class="sd">            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity</span>
<span class="sd">            Retrieval](https://arxiv.org/abs/2010.00904).</span>
<span class="sd">        synced_gpus (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">        kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">            Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be</span>
<span class="sd">            forwarded to the `forward` function of the model.</span>

<span class="sd">    Return:</span>
<span class="sd">        [`~utils.ModelOutput`] or `mindspore.Tensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`</span>
<span class="sd">        or when `config.return_dict_in_generate=True`) or a `mindspore.Tensor`. The possible</span>
<span class="sd">        [`~utils.ModelOutput`] types are:</span>
<span class="sd">            - [`~generation.GenerateEncoderDecoderOutput`],</span>
<span class="sd">            - [`~generation.GenerateBeamEncoderDecoderOutput`]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
    <span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_embeds&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">input_features</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">inputs</span>
            <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">})[</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">):</span>
            <span class="c1"># also accept __xxx__</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model. Please specify a `tgt_lang` in</span>
<span class="s2">                    </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="c1"># tgt_lang gets priority over decoder input ids</span>
            <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
            <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;This model generation config doesn&#39;t have a `text_decoder_lang_to_code_id` key which maps</span>
<span class="sd">                the target language to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># only a warning, otherwise errors appear in the tests</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;You must either specify a `tgt_lang` or pass a correct `text_decoder_input_ids` to get</span>
<span class="sd">            a correct generation, otherwise the generation will probably make no sense.&quot;&quot;&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">input_features</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">text_decoder_input_ids</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToSpeech" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToSpeech</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToSpeech" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel">SeamlessM4Tv2PreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3264</span>
<span class="normal">3265</span>
<span class="normal">3266</span>
<span class="normal">3267</span>
<span class="normal">3268</span>
<span class="normal">3269</span>
<span class="normal">3270</span>
<span class="normal">3271</span>
<span class="normal">3272</span>
<span class="normal">3273</span>
<span class="normal">3274</span>
<span class="normal">3275</span>
<span class="normal">3276</span>
<span class="normal">3277</span>
<span class="normal">3278</span>
<span class="normal">3279</span>
<span class="normal">3280</span>
<span class="normal">3281</span>
<span class="normal">3282</span>
<span class="normal">3283</span>
<span class="normal">3284</span>
<span class="normal">3285</span>
<span class="normal">3286</span>
<span class="normal">3287</span>
<span class="normal">3288</span>
<span class="normal">3289</span>
<span class="normal">3290</span>
<span class="normal">3291</span>
<span class="normal">3292</span>
<span class="normal">3293</span>
<span class="normal">3294</span>
<span class="normal">3295</span>
<span class="normal">3296</span>
<span class="normal">3297</span>
<span class="normal">3298</span>
<span class="normal">3299</span>
<span class="normal">3300</span>
<span class="normal">3301</span>
<span class="normal">3302</span>
<span class="normal">3303</span>
<span class="normal">3304</span>
<span class="normal">3305</span>
<span class="normal">3306</span>
<span class="normal">3307</span>
<span class="normal">3308</span>
<span class="normal">3309</span>
<span class="normal">3310</span>
<span class="normal">3311</span>
<span class="normal">3312</span>
<span class="normal">3313</span>
<span class="normal">3314</span>
<span class="normal">3315</span>
<span class="normal">3316</span>
<span class="normal">3317</span>
<span class="normal">3318</span>
<span class="normal">3319</span>
<span class="normal">3320</span>
<span class="normal">3321</span>
<span class="normal">3322</span>
<span class="normal">3323</span>
<span class="normal">3324</span>
<span class="normal">3325</span>
<span class="normal">3326</span>
<span class="normal">3327</span>
<span class="normal">3328</span>
<span class="normal">3329</span>
<span class="normal">3330</span>
<span class="normal">3331</span>
<span class="normal">3332</span>
<span class="normal">3333</span>
<span class="normal">3334</span>
<span class="normal">3335</span>
<span class="normal">3336</span>
<span class="normal">3337</span>
<span class="normal">3338</span>
<span class="normal">3339</span>
<span class="normal">3340</span>
<span class="normal">3341</span>
<span class="normal">3342</span>
<span class="normal">3343</span>
<span class="normal">3344</span>
<span class="normal">3345</span>
<span class="normal">3346</span>
<span class="normal">3347</span>
<span class="normal">3348</span>
<span class="normal">3349</span>
<span class="normal">3350</span>
<span class="normal">3351</span>
<span class="normal">3352</span>
<span class="normal">3353</span>
<span class="normal">3354</span>
<span class="normal">3355</span>
<span class="normal">3356</span>
<span class="normal">3357</span>
<span class="normal">3358</span>
<span class="normal">3359</span>
<span class="normal">3360</span>
<span class="normal">3361</span>
<span class="normal">3362</span>
<span class="normal">3363</span>
<span class="normal">3364</span>
<span class="normal">3365</span>
<span class="normal">3366</span>
<span class="normal">3367</span>
<span class="normal">3368</span>
<span class="normal">3369</span>
<span class="normal">3370</span>
<span class="normal">3371</span>
<span class="normal">3372</span>
<span class="normal">3373</span>
<span class="normal">3374</span>
<span class="normal">3375</span>
<span class="normal">3376</span>
<span class="normal">3377</span>
<span class="normal">3378</span>
<span class="normal">3379</span>
<span class="normal">3380</span>
<span class="normal">3381</span>
<span class="normal">3382</span>
<span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span>
<span class="normal">3414</span>
<span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span>
<span class="normal">3444</span>
<span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span>
<span class="normal">3476</span>
<span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span>
<span class="normal">3508</span>
<span class="normal">3509</span>
<span class="normal">3510</span>
<span class="normal">3511</span>
<span class="normal">3512</span>
<span class="normal">3513</span>
<span class="normal">3514</span>
<span class="normal">3515</span>
<span class="normal">3516</span>
<span class="normal">3517</span>
<span class="normal">3518</span>
<span class="normal">3519</span>
<span class="normal">3520</span>
<span class="normal">3521</span>
<span class="normal">3522</span>
<span class="normal">3523</span>
<span class="normal">3524</span>
<span class="normal">3525</span>
<span class="normal">3526</span>
<span class="normal">3527</span>
<span class="normal">3528</span>
<span class="normal">3529</span>
<span class="normal">3530</span>
<span class="normal">3531</span>
<span class="normal">3532</span>
<span class="normal">3533</span>
<span class="normal">3534</span>
<span class="normal">3535</span>
<span class="normal">3536</span>
<span class="normal">3537</span>
<span class="normal">3538</span>
<span class="normal">3539</span>
<span class="normal">3540</span>
<span class="normal">3541</span>
<span class="normal">3542</span>
<span class="normal">3543</span>
<span class="normal">3544</span>
<span class="normal">3545</span>
<span class="normal">3546</span>
<span class="normal">3547</span>
<span class="normal">3548</span>
<span class="normal">3549</span>
<span class="normal">3550</span>
<span class="normal">3551</span>
<span class="normal">3552</span>
<span class="normal">3553</span>
<span class="normal">3554</span>
<span class="normal">3555</span>
<span class="normal">3556</span>
<span class="normal">3557</span>
<span class="normal">3558</span>
<span class="normal">3559</span>
<span class="normal">3560</span>
<span class="normal">3561</span>
<span class="normal">3562</span>
<span class="normal">3563</span>
<span class="normal">3564</span>
<span class="normal">3565</span>
<span class="normal">3566</span>
<span class="normal">3567</span>
<span class="normal">3568</span>
<span class="normal">3569</span>
<span class="normal">3570</span>
<span class="normal">3571</span>
<span class="normal">3572</span>
<span class="normal">3573</span>
<span class="normal">3574</span>
<span class="normal">3575</span>
<span class="normal">3576</span>
<span class="normal">3577</span>
<span class="normal">3578</span>
<span class="normal">3579</span>
<span class="normal">3580</span>
<span class="normal">3581</span>
<span class="normal">3582</span>
<span class="normal">3583</span>
<span class="normal">3584</span>
<span class="normal">3585</span>
<span class="normal">3586</span>
<span class="normal">3587</span>
<span class="normal">3588</span>
<span class="normal">3589</span>
<span class="normal">3590</span>
<span class="normal">3591</span>
<span class="normal">3592</span>
<span class="normal">3593</span>
<span class="normal">3594</span>
<span class="normal">3595</span>
<span class="normal">3596</span>
<span class="normal">3597</span>
<span class="normal">3598</span>
<span class="normal">3599</span>
<span class="normal">3600</span>
<span class="normal">3601</span>
<span class="normal">3602</span>
<span class="normal">3603</span>
<span class="normal">3604</span>
<span class="normal">3605</span>
<span class="normal">3606</span>
<span class="normal">3607</span>
<span class="normal">3608</span>
<span class="normal">3609</span>
<span class="normal">3610</span>
<span class="normal">3611</span>
<span class="normal">3612</span>
<span class="normal">3613</span>
<span class="normal">3614</span>
<span class="normal">3615</span>
<span class="normal">3616</span>
<span class="normal">3617</span>
<span class="normal">3618</span>
<span class="normal">3619</span>
<span class="normal">3620</span>
<span class="normal">3621</span>
<span class="normal">3622</span>
<span class="normal">3623</span>
<span class="normal">3624</span>
<span class="normal">3625</span>
<span class="normal">3626</span>
<span class="normal">3627</span>
<span class="normal">3628</span>
<span class="normal">3629</span>
<span class="normal">3630</span>
<span class="normal">3631</span>
<span class="normal">3632</span>
<span class="normal">3633</span>
<span class="normal">3634</span>
<span class="normal">3635</span>
<span class="normal">3636</span>
<span class="normal">3637</span>
<span class="normal">3638</span>
<span class="normal">3639</span>
<span class="normal">3640</span>
<span class="normal">3641</span>
<span class="normal">3642</span>
<span class="normal">3643</span>
<span class="normal">3644</span>
<span class="normal">3645</span>
<span class="normal">3646</span>
<span class="normal">3647</span>
<span class="normal">3648</span>
<span class="normal">3649</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2ForTextToSpeech</span><span class="p">(</span><span class="n">SeamlessM4Tv2PreTrainedModel</span><span class="p">):</span>
    <span class="n">_keys_to_ignore_on_load_missing</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;speech_encoder&quot;</span><span class="p">]</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_ids&quot;</span>

    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_encoder.embed_tokens.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_decoder.embed_tokens.weight&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.__init__ with SeamlessM4T-&gt;SeamlessM4Tv2</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4Tv2Config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Encoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Decoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2TextToUnitForConditionalGeneration</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2CodeHifiGan</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.get_encoder</span>
    <span class="k">def</span> <span class="nf">get_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.get_decoder</span>
    <span class="k">def</span> <span class="nf">get_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.get_output_embeddings</span>
    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.set_output_embeddings</span>
    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.get_input_embeddings</span>
    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.set_input_embeddings</span>
    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">value</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech._tie_weights</span>
    <span class="k">def</span> <span class="nf">_tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.forward with SeamlessM4T-&gt;SeamlessM4Tv2</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Seq2SeqLMOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The `use_cache` argument is changed to `False` since `labels` is provided.&quot;</span><span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span>
                    <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
                <span class="p">)</span>

        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if encoder_outputs is not None, it&#39;s probably used within a .generate method so no need to warn</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This is the same forward method as `SeamlessM4Tv2ForTextToText`.&quot;</span>
                <span class="s2">&quot;It doesn&#39;t use the text-to-unit model `SeamlessM4Tv2TextToUnitForConditionalGeneration`.&quot;</span>
                <span class="s2">&quot;If you want to generate speech, use the `.generate` method.&quot;</span>
            <span class="p">)</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True</span>
        <span class="k">elif</span> <span class="n">return_dict</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">):</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
                <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>

        <span class="c1"># decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">decoder_inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">decoder_outputs</span> <span class="o">+</span> <span class="n">encoder_outputs</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speaker_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates translated audio waveforms.</span>

<span class="sd">        &lt;Tip&gt;</span>

<span class="sd">        This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">        arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">        that will be passed to one of them.</span>

<span class="sd">        For example, calling `.generate(input_ids, num_beams=4, speech_do_sample=True)` will successively perform</span>
<span class="sd">        beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">        For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">        guide](./generation_strategies).</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">                Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">                [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">                If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">                to get translated text alongside the audio.</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language to use as target language for translation.</span>
<span class="sd">            speaker_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">            kwargs (*optional*):</span>
<span class="sd">                Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">                arguments are of two types:</span>

<span class="sd">                    - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                    except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                    - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                    text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                    This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                    other.</span>


<span class="sd">        Returns:</span>
<span class="sd">            `Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]`:</span>
<span class="sd">            - If `return_intermediate_token_ids`, returns [`SeamlessM4Tv2GenerationOutput`].</span>
<span class="sd">            - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,</span>
<span class="sd">              sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># also accept __xxx__</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]:</span>
                <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                        to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                    Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4Tv2 supports</span>
<span class="s2">                    more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                    <span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>

        <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
        <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

        <span class="c1"># first generation</span>
        <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
        <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

        <span class="c1"># prepare second generation</span>
        <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># repeat attention mask alongside batch dimension</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">encoder_hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># repeat attention mask alongside batch dimension</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
        <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Manually trim the final EOS token</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

        <span class="c1"># Compute new attention mask</span>
        <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
        <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

        <span class="c1"># REMOVE EOS and lang_id</span>
        <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># replace every other EOS</span>
        <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
            <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_input_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span>
        <span class="p">)</span>

        <span class="c1"># compute t2u_char_input_ids</span>
        <span class="n">t2u_subwords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_indices_to_subwords</span><span class="p">(</span><span class="n">t2u_input_ids</span><span class="p">)</span>
        <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_character_length_in_subword</span><span class="p">(</span>
            <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
        <span class="p">)</span>

        <span class="c1"># Add pads for lang, EOS tokens as per NLLB &quot;source&quot; tokenizer mode.</span>
        <span class="n">pad_zero</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pad_zero</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_zero</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t2u_char_input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_char_input_ids</span><span class="p">(</span>
            <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
        <span class="p">)</span>

        <span class="c1"># second pass</span>
        <span class="n">t2u_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="p">(</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span>
            <span class="n">char_input_ids</span><span class="o">=</span><span class="n">t2u_char_input_ids</span><span class="p">,</span>
            <span class="n">char_count_per_id</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

        <span class="c1"># The text-to-unit model is non auto-regressive. We keep the ability to use sampling with temperature</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">temperature</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;do_sample&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_logits</span> <span class="o">/</span> <span class="n">temperature</span>
            <span class="c1"># apply softmax</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># reshape to 2D: (batch_size, seq_len, t2u_vocab_size) -&gt; (batch_size*seq_len, t2u_vocab_size)</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
            <span class="c1"># multinomial then reshape : (batch_size*seq_len)-&gt; (batch_size,seq_len)</span>
            <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">t2u_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="n">replace_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span> <span class="o">|</span> <span class="p">(</span><span class="o">~</span><span class="n">padding_mask</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="c1"># replace eos per pad</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">replace_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">)</span>

        <span class="c1"># offset of control symbols</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
        <span class="p">)</span>

        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">speaker_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">speaker_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">speaker_id</span><span class="o">=</span><span class="n">speaker_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">(</span>
                <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
                <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
                <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
                <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.prepare_inputs_for_generation</span>
    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># cut decoder_input_ids if past is used</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># encoder_outputs is defined. input_ids not needed</span>
            <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">:</span> <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech._reorder_cache</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="c1"># cached cross_attention states don&#39;t have to be reordered -&gt; they are always the same</span>
            <span class="n">reordered_past</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">past_state</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">past_state</span> <span class="ow">in</span> <span class="n">layer_past</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">reordered_past</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToSpeech.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2ForTextToSpeech</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_intermediate_token_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">speaker_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToSpeech.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates translated audio waveforms.</p>
<p><Tip></p>
<p>This method successively calls the <code>.generate</code> function of two different sub-models. You can specify keyword
arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments
that will be passed to one of them.</p>
<p>For example, calling <code>.generate(input_ids, num_beams=4, speech_do_sample=True)</code> will successively perform
beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</p>
<p>For an overview of generation strategies and code examples, check out the <a href="./generation_strategies">following
guide</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<code>SeamlessM4TTokenizer</code>] or [<code>SeamlessM4TProcessor</code>]. See
[<code>PreTrainedTokenizer.encode</code>] and [<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_intermediate_token_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If <code>True</code>, also returns the intermediate generated text and unit tokens. Set to <code>True</code> if you also want
to get translated text alongside the audio.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speaker_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the speaker used for speech synthesis. Must be lower than <code>config.vocoder_num_spkrs</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Remaining dictionary of keyword arguments that will be passed to [<code>GenerationMixin.generate</code>]. Keyword
arguments are of two types:</p>
<div class="highlight"><pre><span></span><code>- Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,
except for `decoder_input_ids` which will only be passed through the text components.
- With a *text_* or *speech_* prefix, they will be input for the `generate` method of the
text model and speech model respectively. It has the priority over the keywords without a prefix.

This means you can, for example, specify a generation strategy for one generation but not for the
other.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput">SeamlessM4Tv2GenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]</code>:</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput">SeamlessM4Tv2GenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If <code>return_intermediate_token_ids</code>, returns [<code>SeamlessM4Tv2GenerationOutput</code>].</li>
</ul>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput">SeamlessM4Tv2GenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If not <code>return_intermediate_token_ids</code>, returns a tuple composed of waveforms of shape <code>(batch_size,
sequence_length)</code>and and <code>waveform_lengths</code> which gives the length of each sample.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span>
<span class="normal">3444</span>
<span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span>
<span class="normal">3476</span>
<span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span>
<span class="normal">3508</span>
<span class="normal">3509</span>
<span class="normal">3510</span>
<span class="normal">3511</span>
<span class="normal">3512</span>
<span class="normal">3513</span>
<span class="normal">3514</span>
<span class="normal">3515</span>
<span class="normal">3516</span>
<span class="normal">3517</span>
<span class="normal">3518</span>
<span class="normal">3519</span>
<span class="normal">3520</span>
<span class="normal">3521</span>
<span class="normal">3522</span>
<span class="normal">3523</span>
<span class="normal">3524</span>
<span class="normal">3525</span>
<span class="normal">3526</span>
<span class="normal">3527</span>
<span class="normal">3528</span>
<span class="normal">3529</span>
<span class="normal">3530</span>
<span class="normal">3531</span>
<span class="normal">3532</span>
<span class="normal">3533</span>
<span class="normal">3534</span>
<span class="normal">3535</span>
<span class="normal">3536</span>
<span class="normal">3537</span>
<span class="normal">3538</span>
<span class="normal">3539</span>
<span class="normal">3540</span>
<span class="normal">3541</span>
<span class="normal">3542</span>
<span class="normal">3543</span>
<span class="normal">3544</span>
<span class="normal">3545</span>
<span class="normal">3546</span>
<span class="normal">3547</span>
<span class="normal">3548</span>
<span class="normal">3549</span>
<span class="normal">3550</span>
<span class="normal">3551</span>
<span class="normal">3552</span>
<span class="normal">3553</span>
<span class="normal">3554</span>
<span class="normal">3555</span>
<span class="normal">3556</span>
<span class="normal">3557</span>
<span class="normal">3558</span>
<span class="normal">3559</span>
<span class="normal">3560</span>
<span class="normal">3561</span>
<span class="normal">3562</span>
<span class="normal">3563</span>
<span class="normal">3564</span>
<span class="normal">3565</span>
<span class="normal">3566</span>
<span class="normal">3567</span>
<span class="normal">3568</span>
<span class="normal">3569</span>
<span class="normal">3570</span>
<span class="normal">3571</span>
<span class="normal">3572</span>
<span class="normal">3573</span>
<span class="normal">3574</span>
<span class="normal">3575</span>
<span class="normal">3576</span>
<span class="normal">3577</span>
<span class="normal">3578</span>
<span class="normal">3579</span>
<span class="normal">3580</span>
<span class="normal">3581</span>
<span class="normal">3582</span>
<span class="normal">3583</span>
<span class="normal">3584</span>
<span class="normal">3585</span>
<span class="normal">3586</span>
<span class="normal">3587</span>
<span class="normal">3588</span>
<span class="normal">3589</span>
<span class="normal">3590</span>
<span class="normal">3591</span>
<span class="normal">3592</span>
<span class="normal">3593</span>
<span class="normal">3594</span>
<span class="normal">3595</span>
<span class="normal">3596</span>
<span class="normal">3597</span>
<span class="normal">3598</span>
<span class="normal">3599</span>
<span class="normal">3600</span>
<span class="normal">3601</span>
<span class="normal">3602</span>
<span class="normal">3603</span>
<span class="normal">3604</span>
<span class="normal">3605</span>
<span class="normal">3606</span>
<span class="normal">3607</span>
<span class="normal">3608</span>
<span class="normal">3609</span>
<span class="normal">3610</span>
<span class="normal">3611</span>
<span class="normal">3612</span>
<span class="normal">3613</span>
<span class="normal">3614</span>
<span class="normal">3615</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">speaker_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates translated audio waveforms.</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">    arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">    that will be passed to one of them.</span>

<span class="sd">    For example, calling `.generate(input_ids, num_beams=4, speech_do_sample=True)` will successively perform</span>
<span class="sd">    beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">    For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">    guide](./generation_strategies).</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">            Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">            If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">            to get translated text alongside the audio.</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        speaker_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">        kwargs (*optional*):</span>
<span class="sd">            Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">            arguments are of two types:</span>

<span class="sd">                - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                other.</span>


<span class="sd">    Returns:</span>
<span class="sd">        `Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]`:</span>
<span class="sd">        - If `return_intermediate_token_ids`, returns [`SeamlessM4Tv2GenerationOutput`].</span>
<span class="sd">        - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,</span>
<span class="sd">          sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># also accept __xxx__</span>
        <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]:</span>
            <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                    to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4Tv2 supports</span>
<span class="s2">                more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                <span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>

    <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
    <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

    <span class="c1"># first generation</span>
    <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

    <span class="c1"># prepare second generation</span>
    <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># repeat attention mask alongside batch dimension</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">encoder_hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># repeat attention mask alongside batch dimension</span>
    <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
    <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Manually trim the final EOS token</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

    <span class="c1"># Compute new attention mask</span>
    <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
    <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

    <span class="c1"># REMOVE EOS and lang_id</span>
    <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># replace every other EOS</span>
    <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
        <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_input_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span>
    <span class="p">)</span>

    <span class="c1"># compute t2u_char_input_ids</span>
    <span class="n">t2u_subwords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_indices_to_subwords</span><span class="p">(</span><span class="n">t2u_input_ids</span><span class="p">)</span>
    <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_character_length_in_subword</span><span class="p">(</span>
        <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
    <span class="p">)</span>

    <span class="c1"># Add pads for lang, EOS tokens as per NLLB &quot;source&quot; tokenizer mode.</span>
    <span class="n">pad_zero</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pad_zero</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_zero</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t2u_char_input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_char_input_ids</span><span class="p">(</span>
        <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
    <span class="p">)</span>

    <span class="c1"># second pass</span>
    <span class="n">t2u_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="p">(</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span>
        <span class="n">char_input_ids</span><span class="o">=</span><span class="n">t2u_char_input_ids</span><span class="p">,</span>
        <span class="n">char_count_per_id</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

    <span class="c1"># The text-to-unit model is non auto-regressive. We keep the ability to use sampling with temperature</span>
    <span class="n">temperature</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">temperature</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;do_sample&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_logits</span> <span class="o">/</span> <span class="n">temperature</span>
        <span class="c1"># apply softmax</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># reshape to 2D: (batch_size, seq_len, t2u_vocab_size) -&gt; (batch_size*seq_len, t2u_vocab_size)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
        <span class="c1"># multinomial then reshape : (batch_size*seq_len)-&gt; (batch_size,seq_len)</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">t2u_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="n">replace_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span> <span class="o">|</span> <span class="p">(</span><span class="o">~</span><span class="n">padding_mask</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="c1"># replace eos per pad</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">replace_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">)</span>

    <span class="c1"># offset of control symbols</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
    <span class="p">)</span>

    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">speaker_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">speaker_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">speaker_id</span><span class="o">=</span><span class="n">speaker_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">(</span>
            <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
            <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
            <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
            <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToText" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToText</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToText" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel">SeamlessM4Tv2PreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span>
<span class="normal">2694</span>
<span class="normal">2695</span>
<span class="normal">2696</span>
<span class="normal">2697</span>
<span class="normal">2698</span>
<span class="normal">2699</span>
<span class="normal">2700</span>
<span class="normal">2701</span>
<span class="normal">2702</span>
<span class="normal">2703</span>
<span class="normal">2704</span>
<span class="normal">2705</span>
<span class="normal">2706</span>
<span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span>
<span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span>
<span class="normal">2756</span>
<span class="normal">2757</span>
<span class="normal">2758</span>
<span class="normal">2759</span>
<span class="normal">2760</span>
<span class="normal">2761</span>
<span class="normal">2762</span>
<span class="normal">2763</span>
<span class="normal">2764</span>
<span class="normal">2765</span>
<span class="normal">2766</span>
<span class="normal">2767</span>
<span class="normal">2768</span>
<span class="normal">2769</span>
<span class="normal">2770</span>
<span class="normal">2771</span>
<span class="normal">2772</span>
<span class="normal">2773</span>
<span class="normal">2774</span>
<span class="normal">2775</span>
<span class="normal">2776</span>
<span class="normal">2777</span>
<span class="normal">2778</span>
<span class="normal">2779</span>
<span class="normal">2780</span>
<span class="normal">2781</span>
<span class="normal">2782</span>
<span class="normal">2783</span>
<span class="normal">2784</span>
<span class="normal">2785</span>
<span class="normal">2786</span>
<span class="normal">2787</span>
<span class="normal">2788</span>
<span class="normal">2789</span>
<span class="normal">2790</span>
<span class="normal">2791</span>
<span class="normal">2792</span>
<span class="normal">2793</span>
<span class="normal">2794</span>
<span class="normal">2795</span>
<span class="normal">2796</span>
<span class="normal">2797</span>
<span class="normal">2798</span>
<span class="normal">2799</span>
<span class="normal">2800</span>
<span class="normal">2801</span>
<span class="normal">2802</span>
<span class="normal">2803</span>
<span class="normal">2804</span>
<span class="normal">2805</span>
<span class="normal">2806</span>
<span class="normal">2807</span>
<span class="normal">2808</span>
<span class="normal">2809</span>
<span class="normal">2810</span>
<span class="normal">2811</span>
<span class="normal">2812</span>
<span class="normal">2813</span>
<span class="normal">2814</span>
<span class="normal">2815</span>
<span class="normal">2816</span>
<span class="normal">2817</span>
<span class="normal">2818</span>
<span class="normal">2819</span>
<span class="normal">2820</span>
<span class="normal">2821</span>
<span class="normal">2822</span>
<span class="normal">2823</span>
<span class="normal">2824</span>
<span class="normal">2825</span>
<span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span>
<span class="normal">2842</span>
<span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span>
<span class="normal">2847</span>
<span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span>
<span class="normal">2885</span>
<span class="normal">2886</span>
<span class="normal">2887</span>
<span class="normal">2888</span>
<span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2ForTextToText</span><span class="p">(</span><span class="n">SeamlessM4Tv2PreTrainedModel</span><span class="p">):</span>
    <span class="n">_keys_to_ignore_on_load_missing</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;speech_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_model&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder&quot;</span><span class="p">]</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_ids&quot;</span>

    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_encoder.embed_tokens.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_decoder.embed_tokens.weight&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4Tv2Config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Encoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Decoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span>

    <span class="k">def</span> <span class="nf">get_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Seq2SeqLMOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The `use_cache` argument is changed to `False` since `labels` is provided.&quot;</span><span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span>
                    <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
                <span class="p">)</span>

        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True</span>
        <span class="k">elif</span> <span class="n">return_dict</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">):</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
                <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>

        <span class="c1"># decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">decoder_inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">decoder_outputs</span> <span class="o">+</span> <span class="n">encoder_outputs</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the</span>
<span class="sd">        model&#39;s default generation configuration. You can override any `generation_config` by passing the corresponding</span>
<span class="sd">        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.</span>

<span class="sd">        For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">        guide](./generation_strategies).</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of varying shape depending on the modality, *optional*):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">                Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">                [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language to use as target language for translation.</span>
<span class="sd">            generation_config (`~generation.GenerationConfig`, *optional*):</span>
<span class="sd">                The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">                passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">                `generation_config` is not provided, the default will be used, which had the following loading</span>
<span class="sd">                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">                default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`, *optional*):</span>
<span class="sd">                Custom logits processors that complement the default logits processors built from arguments and</span>
<span class="sd">                generation config. If a logit processor is passed that is already created with the arguments or a</span>
<span class="sd">                generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`, *optional*):</span>
<span class="sd">                Custom stopping criteria that complement the default stopping criteria built from arguments and a</span>
<span class="sd">                generation config. If a stopping criteria is passed that is already created with the arguments or a</span>
<span class="sd">                generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">            prefix_allowed_tokens_fn (`Callable[[int, mindspore.Tensor], List[int]]`, *optional*):</span>
<span class="sd">                If provided, this function constraints the beam search to allowed tokens only at each step. If not</span>
<span class="sd">                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and</span>
<span class="sd">                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned</span>
<span class="sd">                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful</span>
<span class="sd">                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity</span>
<span class="sd">                Retrieval](https://arxiv.org/abs/2010.00904).</span>
<span class="sd">            synced_gpus (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">            kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">                Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be</span>
<span class="sd">                forwarded to the `forward` function of the model.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`~utils.ModelOutput`] or `mindspore.Tensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`</span>
<span class="sd">            or when `config.return_dict_in_generate=True`) or a `mindspore.Tensor`. The possible</span>
<span class="sd">            [`~utils.ModelOutput`] types are:</span>
<span class="sd">                - [`~generation.GenerateEncoderDecoderOutput`],</span>
<span class="sd">                - [`~generation.GenerateBeamEncoderDecoderOutput`]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># prepare text_decoder_input_ids</span>
        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">):</span>
                <span class="c1"># also accept __xxx__</span>
                <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model. Please specify a `tgt_lang` in</span>
<span class="s2">                        </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="c1"># tgt_lang gets priority over decoder input ids</span>
                <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
                <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;This model generation config doesn&#39;t have a `text_decoder_lang_to_code_id` key which maps</span>
<span class="sd">                    the target language to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># only a warning, otherwise errors appear in the tests</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;You must either specify a `tgt_lang` or pass a correct `text_decoder_input_ids` to get</span>
<span class="sd">                a correct generation, otherwise the generation will probably make no sense.&quot;&quot;&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="p">,</span>
            <span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="p">,</span>
            <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">text_decoder_input_ids</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># cut decoder_input_ids if past is used</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># encoder_outputs is defined. input_ids not needed</span>
            <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">:</span> <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="c1"># cached cross_attention states don&#39;t have to be reordered -&gt; they are always the same</span>
            <span class="n">reordered_past</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">past_state</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">past_state</span> <span class="ow">in</span> <span class="n">layer_past</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">reordered_past</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToText.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2ForTextToText</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ForTextToText.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates sequences of token ids.</p>
<p><Tip warning={true}></p>
<p>Most generation-controlling parameters are set in <code>generation_config</code> which, if not passed, will be set to the
model's default generation configuration. You can override any <code>generation_config</code> by passing the corresponding
parameters to generate(), e.g. <code>.generate(inputs, num_beams=4, do_sample=True)</code>.</p>
<p>For an overview of generation strategies and code examples, check out the <a href="./generation_strategies">following
guide</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<code>SeamlessM4TTokenizer</code>] or [<code>SeamlessM4TProcessor</code>]. See
[<code>PreTrainedTokenizer.encode</code>] and [<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of varying shape depending on the modality, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>generation_config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generation configuration to be used as base parametrization for the generation call. <code>**kwargs</code>
passed to generate matching the attributes of <code>generation_config</code> will override them. If
<code>generation_config</code> is not provided, the default will be used, which had the following loading
priority: 1) from the <code>generation_config.json</code> model file, if it exists; 2) from the model
configuration. Please note that unspecified parameters will inherit [<code>~generation.GenerationConfig</code>]'s
default values, whose documentation should be checked to parameterize generation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`~generation.GenerationConfig`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logits_processor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom logits processors that complement the default logits processors built from arguments and
generation config. If a logit processor is passed that is already created with the arguments or a
generation config an error is thrown. This feature is intended for advanced users.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`LogitsProcessorList`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stopping_criteria</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom stopping criteria that complement the default stopping criteria built from arguments and a
generation config. If a stopping criteria is passed that is already created with the arguments or a
generation config an error is thrown. This feature is intended for advanced users.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`StoppingCriteriaList`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prefix_allowed_tokens_fn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments: the batch ID <code>batch_id</code> and
<code>input_ids</code>. It has to return a list with the allowed tokens for the next generation step conditioned
on the batch ID <code>batch_id</code> and the previously generated tokens <code>inputs_ids</code>. This argument is useful
for constrained generation conditioned on the prefix, as described in <a href="https://arxiv.org/abs/2010.00904">Autoregressive Entity
Retrieval</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable[[int, mindspore.Tensor], List[int]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>synced_gpus</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Ad hoc parametrization of <code>generate_config</code> and/or additional model-specific kwargs that will be
forwarded to the <code>forward</code> function of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Any]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p>[<code>~utils.ModelOutput</code>] or <code>mindspore.Tensor</code>: A [<code>~utils.ModelOutput</code>] (if <code>return_dict_in_generate=True</code>
or when <code>config.return_dict_in_generate=True</code>) or a <code>mindspore.Tensor</code>. The possible
[<code>~utils.ModelOutput</code>] types are:
    - [<code>~generation.GenerateEncoderDecoderOutput</code>],
    - [<code>~generation.GenerateBeamEncoderDecoderOutput</code>]</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span>
<span class="normal">2842</span>
<span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span>
<span class="normal">2847</span>
<span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span>
<span class="normal">2885</span>
<span class="normal">2886</span>
<span class="normal">2887</span>
<span class="normal">2888</span>
<span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates sequences of token ids.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the</span>
<span class="sd">    model&#39;s default generation configuration. You can override any `generation_config` by passing the corresponding</span>
<span class="sd">    parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.</span>

<span class="sd">    For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">    guide](./generation_strategies).</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of varying shape depending on the modality, *optional*):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">            Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        generation_config (`~generation.GenerationConfig`, *optional*):</span>
<span class="sd">            The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">            passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">            `generation_config` is not provided, the default will be used, which had the following loading</span>
<span class="sd">            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">            default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">        logits_processor (`LogitsProcessorList`, *optional*):</span>
<span class="sd">            Custom logits processors that complement the default logits processors built from arguments and</span>
<span class="sd">            generation config. If a logit processor is passed that is already created with the arguments or a</span>
<span class="sd">            generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">        stopping_criteria (`StoppingCriteriaList`, *optional*):</span>
<span class="sd">            Custom stopping criteria that complement the default stopping criteria built from arguments and a</span>
<span class="sd">            generation config. If a stopping criteria is passed that is already created with the arguments or a</span>
<span class="sd">            generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">        prefix_allowed_tokens_fn (`Callable[[int, mindspore.Tensor], List[int]]`, *optional*):</span>
<span class="sd">            If provided, this function constraints the beam search to allowed tokens only at each step. If not</span>
<span class="sd">            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and</span>
<span class="sd">            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned</span>
<span class="sd">            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful</span>
<span class="sd">            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity</span>
<span class="sd">            Retrieval](https://arxiv.org/abs/2010.00904).</span>
<span class="sd">        synced_gpus (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">        kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">            Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be</span>
<span class="sd">            forwarded to the `forward` function of the model.</span>

<span class="sd">    Return:</span>
<span class="sd">        [`~utils.ModelOutput`] or `mindspore.Tensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`</span>
<span class="sd">        or when `config.return_dict_in_generate=True`) or a `mindspore.Tensor`. The possible</span>
<span class="sd">        [`~utils.ModelOutput`] types are:</span>
<span class="sd">            - [`~generation.GenerateEncoderDecoderOutput`],</span>
<span class="sd">            - [`~generation.GenerateBeamEncoderDecoderOutput`]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># prepare text_decoder_input_ids</span>
    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">):</span>
            <span class="c1"># also accept __xxx__</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model. Please specify a `tgt_lang` in</span>
<span class="s2">                    </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="c1"># tgt_lang gets priority over decoder input ids</span>
            <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
            <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;This model generation config doesn&#39;t have a `text_decoder_lang_to_code_id` key which maps</span>
<span class="sd">                the target language to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># only a warning, otherwise errors appear in the tests</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;You must either specify a `tgt_lang` or pass a correct `text_decoder_input_ids` to get</span>
<span class="sd">            a correct generation, otherwise the generation will probably make no sense.&quot;&quot;&quot;</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">text_decoder_input_ids</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.utils.ModelOutput">ModelOutput</span></code></p>


        <p>Class defining the generated outputs from [<code>SeamlessM4Tv2Model</code>], [<code>SeamlessM4Tv2ForTextToText</code>],
[<code>SeamlessM4Tv2ForTextToSpeech</code>], [<code>SeamlessM4Tv2ForSpeechToSpeech</code>] and [<code>SeamlessM4Tv2ForTextToSpeech</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>waveform</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The final audio waveform predicted by the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>waveform_lengths</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The length in samples of each element in the <code>waveform</code> batch.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size,)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sequences</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generated translated sequences. This is the output of the text-to-text or the speech-to-text models.
The second dimension (sequence_length) is either equal to <code>max_length</code> or shorter if all batches finished
early due to the <code>eos_token_id</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unit_sequences</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generated translated unit sequences. This is the output of the text-to-units model. The second
dimension (unit_sequence_length) is either equal to <code>t2u_max_length</code> or shorter if all batches finished
early due to the <code>t2u_eos_token_id</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, unit_sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput with SeamlessM4T-&gt;SeamlessM4Tv2</span>
<span class="k">class</span> <span class="nc">SeamlessM4Tv2GenerationOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class defining the generated outputs from [`SeamlessM4Tv2Model`], [`SeamlessM4Tv2ForTextToText`],</span>
<span class="sd">    [`SeamlessM4Tv2ForTextToSpeech`], [`SeamlessM4Tv2ForSpeechToSpeech`] and [`SeamlessM4Tv2ForTextToSpeech`].</span>

<span class="sd">    Args:</span>
<span class="sd">        waveform (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            The final audio waveform predicted by the model.</span>
<span class="sd">        waveform_lengths (`mindspore.Tensor` of shape `(batch_size,)`, *optional*):</span>
<span class="sd">            The length in samples of each element in the `waveform` batch.</span>
<span class="sd">        sequences (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            The generated translated sequences. This is the output of the text-to-text or the speech-to-text models.</span>
<span class="sd">            The second dimension (sequence_length) is either equal to `max_length` or shorter if all batches finished</span>
<span class="sd">            early due to the `eos_token_id`.</span>
<span class="sd">        unit_sequences (`mindspore.Tensor` of shape `(batch_size, unit_sequence_length)`, *optional*):</span>
<span class="sd">            The generated translated unit sequences. This is the output of the text-to-units model. The second</span>
<span class="sd">            dimension (unit_sequence_length) is either equal to `t2u_max_length` or shorter if all batches finished</span>
<span class="sd">            early due to the `t2u_eos_token_id`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">waveform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">waveform_lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">unit_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2HifiGan" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2HifiGan</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2HifiGan" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span>
<span class="normal">2485</span>
<span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2HifiGan</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4Tv2Config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">model_in_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">unit_embed_dim</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">lang_embed_dim</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">spkr_embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu_slope</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">leaky_relu_slope</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">resblock_kernel_sizes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_upsamples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_rates</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_pre</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">model_in_dim</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">upsample_initial_channel</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">upsampler</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">upsample_rate</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_rates</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">upsample_kernel_sizes</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">upsampler</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose1d</span><span class="p">(</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">upsample_initial_channel</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span><span class="p">),</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">upsample_initial_channel</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span>
                    <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                    <span class="n">stride</span><span class="o">=</span><span class="n">upsample_rate</span><span class="p">,</span>
                    <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="n">upsample_rate</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">upsampler</span><span class="p">)):</span>
            <span class="n">channels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">upsample_initial_channel</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">resblock_kernel_sizes</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">resblock_dilation_sizes</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HifiGanResidualBlock</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">leaky_relu_slope</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv_post</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_embeds</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a log-mel spectrogram into a speech waveform. Passing a batch of log-mel spectrograms returns a batch</span>
<span class="sd">        of speech waveforms. Passing a single, un-batched log-mel spectrogram returns a single, un-batched speech</span>
<span class="sd">        waveform.</span>

<span class="sd">        Args:</span>
<span class="sd">            spectrogram (`mindspore.Tensor`):</span>
<span class="sd">                Tensor containing the log-mel spectrograms. Can be batched and of shape `(batch_size, sequence_length,</span>
<span class="sd">                model_in_dim)`, or un-batched and of shape `(sequence_length, model_in_dim)`. Note that `model_in_dim`</span>
<span class="sd">                is the sum of `config.unit_embed_dim`, `config.lang_embed_dim` and `config.spkr_embed_dim`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `mindspore.Tensor`: Tensor containing the speech waveform. If the input spectrogram is batched, will be of</span>
<span class="sd">            shape `(batch_size, num_frames,)`. If un-batched, will be of shape `(num_frames,)`.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_pre</span><span class="p">(</span><span class="n">input_embeds</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_upsamples</span><span class="p">):</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu_slope</span><span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsampler</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

            <span class="n">res_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span><span class="p">):</span>
                <span class="n">res_state</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span> <span class="o">+</span> <span class="n">j</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">res_state</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_post</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># remove seq-len dim since this collapses to 1</span>
        <span class="n">waveform</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">waveform</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2HifiGan.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2HifiGan</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_embeds</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2HifiGan.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Converts a log-mel spectrogram into a speech waveform. Passing a batch of log-mel spectrograms returns a batch
of speech waveforms. Passing a single, un-batched log-mel spectrogram returns a single, un-batched speech
waveform.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>spectrogram</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tensor containing the log-mel spectrograms. Can be batched and of shape <code>(batch_size, sequence_length,
model_in_dim)</code>, or un-batched and of shape <code>(sequence_length, model_in_dim)</code>. Note that <code>model_in_dim</code>
is the sum of <code>config.unit_embed_dim</code>, <code>config.lang_embed_dim</code> and <code>config.spkr_embed_dim</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindspore.Tensor">Tensor</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>mindspore.Tensor</code>: Tensor containing the speech waveform. If the input spectrogram is batched, will be of</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindspore.Tensor">Tensor</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>shape <code>(batch_size, num_frames,)</code>. If un-batched, will be of shape <code>(num_frames,)</code>.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_embeds</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a log-mel spectrogram into a speech waveform. Passing a batch of log-mel spectrograms returns a batch</span>
<span class="sd">    of speech waveforms. Passing a single, un-batched log-mel spectrogram returns a single, un-batched speech</span>
<span class="sd">    waveform.</span>

<span class="sd">    Args:</span>
<span class="sd">        spectrogram (`mindspore.Tensor`):</span>
<span class="sd">            Tensor containing the log-mel spectrograms. Can be batched and of shape `(batch_size, sequence_length,</span>
<span class="sd">            model_in_dim)`, or un-batched and of shape `(sequence_length, model_in_dim)`. Note that `model_in_dim`</span>
<span class="sd">            is the sum of `config.unit_embed_dim`, `config.lang_embed_dim` and `config.spkr_embed_dim`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `mindspore.Tensor`: Tensor containing the speech waveform. If the input spectrogram is batched, will be of</span>
<span class="sd">        shape `(batch_size, num_frames,)`. If un-batched, will be of shape `(num_frames,)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_pre</span><span class="p">(</span><span class="n">input_embeds</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_upsamples</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu_slope</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsampler</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">res_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span><span class="p">):</span>
            <span class="n">res_state</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resblocks</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span> <span class="o">+</span> <span class="n">j</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">res_state</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_post</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="c1"># remove seq-len dim since this collapses to 1</span>
    <span class="n">waveform</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">waveform</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Model" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Model</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Model" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel">SeamlessM4Tv2PreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4045</span>
<span class="normal">4046</span>
<span class="normal">4047</span>
<span class="normal">4048</span>
<span class="normal">4049</span>
<span class="normal">4050</span>
<span class="normal">4051</span>
<span class="normal">4052</span>
<span class="normal">4053</span>
<span class="normal">4054</span>
<span class="normal">4055</span>
<span class="normal">4056</span>
<span class="normal">4057</span>
<span class="normal">4058</span>
<span class="normal">4059</span>
<span class="normal">4060</span>
<span class="normal">4061</span>
<span class="normal">4062</span>
<span class="normal">4063</span>
<span class="normal">4064</span>
<span class="normal">4065</span>
<span class="normal">4066</span>
<span class="normal">4067</span>
<span class="normal">4068</span>
<span class="normal">4069</span>
<span class="normal">4070</span>
<span class="normal">4071</span>
<span class="normal">4072</span>
<span class="normal">4073</span>
<span class="normal">4074</span>
<span class="normal">4075</span>
<span class="normal">4076</span>
<span class="normal">4077</span>
<span class="normal">4078</span>
<span class="normal">4079</span>
<span class="normal">4080</span>
<span class="normal">4081</span>
<span class="normal">4082</span>
<span class="normal">4083</span>
<span class="normal">4084</span>
<span class="normal">4085</span>
<span class="normal">4086</span>
<span class="normal">4087</span>
<span class="normal">4088</span>
<span class="normal">4089</span>
<span class="normal">4090</span>
<span class="normal">4091</span>
<span class="normal">4092</span>
<span class="normal">4093</span>
<span class="normal">4094</span>
<span class="normal">4095</span>
<span class="normal">4096</span>
<span class="normal">4097</span>
<span class="normal">4098</span>
<span class="normal">4099</span>
<span class="normal">4100</span>
<span class="normal">4101</span>
<span class="normal">4102</span>
<span class="normal">4103</span>
<span class="normal">4104</span>
<span class="normal">4105</span>
<span class="normal">4106</span>
<span class="normal">4107</span>
<span class="normal">4108</span>
<span class="normal">4109</span>
<span class="normal">4110</span>
<span class="normal">4111</span>
<span class="normal">4112</span>
<span class="normal">4113</span>
<span class="normal">4114</span>
<span class="normal">4115</span>
<span class="normal">4116</span>
<span class="normal">4117</span>
<span class="normal">4118</span>
<span class="normal">4119</span>
<span class="normal">4120</span>
<span class="normal">4121</span>
<span class="normal">4122</span>
<span class="normal">4123</span>
<span class="normal">4124</span>
<span class="normal">4125</span>
<span class="normal">4126</span>
<span class="normal">4127</span>
<span class="normal">4128</span>
<span class="normal">4129</span>
<span class="normal">4130</span>
<span class="normal">4131</span>
<span class="normal">4132</span>
<span class="normal">4133</span>
<span class="normal">4134</span>
<span class="normal">4135</span>
<span class="normal">4136</span>
<span class="normal">4137</span>
<span class="normal">4138</span>
<span class="normal">4139</span>
<span class="normal">4140</span>
<span class="normal">4141</span>
<span class="normal">4142</span>
<span class="normal">4143</span>
<span class="normal">4144</span>
<span class="normal">4145</span>
<span class="normal">4146</span>
<span class="normal">4147</span>
<span class="normal">4148</span>
<span class="normal">4149</span>
<span class="normal">4150</span>
<span class="normal">4151</span>
<span class="normal">4152</span>
<span class="normal">4153</span>
<span class="normal">4154</span>
<span class="normal">4155</span>
<span class="normal">4156</span>
<span class="normal">4157</span>
<span class="normal">4158</span>
<span class="normal">4159</span>
<span class="normal">4160</span>
<span class="normal">4161</span>
<span class="normal">4162</span>
<span class="normal">4163</span>
<span class="normal">4164</span>
<span class="normal">4165</span>
<span class="normal">4166</span>
<span class="normal">4167</span>
<span class="normal">4168</span>
<span class="normal">4169</span>
<span class="normal">4170</span>
<span class="normal">4171</span>
<span class="normal">4172</span>
<span class="normal">4173</span>
<span class="normal">4174</span>
<span class="normal">4175</span>
<span class="normal">4176</span>
<span class="normal">4177</span>
<span class="normal">4178</span>
<span class="normal">4179</span>
<span class="normal">4180</span>
<span class="normal">4181</span>
<span class="normal">4182</span>
<span class="normal">4183</span>
<span class="normal">4184</span>
<span class="normal">4185</span>
<span class="normal">4186</span>
<span class="normal">4187</span>
<span class="normal">4188</span>
<span class="normal">4189</span>
<span class="normal">4190</span>
<span class="normal">4191</span>
<span class="normal">4192</span>
<span class="normal">4193</span>
<span class="normal">4194</span>
<span class="normal">4195</span>
<span class="normal">4196</span>
<span class="normal">4197</span>
<span class="normal">4198</span>
<span class="normal">4199</span>
<span class="normal">4200</span>
<span class="normal">4201</span>
<span class="normal">4202</span>
<span class="normal">4203</span>
<span class="normal">4204</span>
<span class="normal">4205</span>
<span class="normal">4206</span>
<span class="normal">4207</span>
<span class="normal">4208</span>
<span class="normal">4209</span>
<span class="normal">4210</span>
<span class="normal">4211</span>
<span class="normal">4212</span>
<span class="normal">4213</span>
<span class="normal">4214</span>
<span class="normal">4215</span>
<span class="normal">4216</span>
<span class="normal">4217</span>
<span class="normal">4218</span>
<span class="normal">4219</span>
<span class="normal">4220</span>
<span class="normal">4221</span>
<span class="normal">4222</span>
<span class="normal">4223</span>
<span class="normal">4224</span>
<span class="normal">4225</span>
<span class="normal">4226</span>
<span class="normal">4227</span>
<span class="normal">4228</span>
<span class="normal">4229</span>
<span class="normal">4230</span>
<span class="normal">4231</span>
<span class="normal">4232</span>
<span class="normal">4233</span>
<span class="normal">4234</span>
<span class="normal">4235</span>
<span class="normal">4236</span>
<span class="normal">4237</span>
<span class="normal">4238</span>
<span class="normal">4239</span>
<span class="normal">4240</span>
<span class="normal">4241</span>
<span class="normal">4242</span>
<span class="normal">4243</span>
<span class="normal">4244</span>
<span class="normal">4245</span>
<span class="normal">4246</span>
<span class="normal">4247</span>
<span class="normal">4248</span>
<span class="normal">4249</span>
<span class="normal">4250</span>
<span class="normal">4251</span>
<span class="normal">4252</span>
<span class="normal">4253</span>
<span class="normal">4254</span>
<span class="normal">4255</span>
<span class="normal">4256</span>
<span class="normal">4257</span>
<span class="normal">4258</span>
<span class="normal">4259</span>
<span class="normal">4260</span>
<span class="normal">4261</span>
<span class="normal">4262</span>
<span class="normal">4263</span>
<span class="normal">4264</span>
<span class="normal">4265</span>
<span class="normal">4266</span>
<span class="normal">4267</span>
<span class="normal">4268</span>
<span class="normal">4269</span>
<span class="normal">4270</span>
<span class="normal">4271</span>
<span class="normal">4272</span>
<span class="normal">4273</span>
<span class="normal">4274</span>
<span class="normal">4275</span>
<span class="normal">4276</span>
<span class="normal">4277</span>
<span class="normal">4278</span>
<span class="normal">4279</span>
<span class="normal">4280</span>
<span class="normal">4281</span>
<span class="normal">4282</span>
<span class="normal">4283</span>
<span class="normal">4284</span>
<span class="normal">4285</span>
<span class="normal">4286</span>
<span class="normal">4287</span>
<span class="normal">4288</span>
<span class="normal">4289</span>
<span class="normal">4290</span>
<span class="normal">4291</span>
<span class="normal">4292</span>
<span class="normal">4293</span>
<span class="normal">4294</span>
<span class="normal">4295</span>
<span class="normal">4296</span>
<span class="normal">4297</span>
<span class="normal">4298</span>
<span class="normal">4299</span>
<span class="normal">4300</span>
<span class="normal">4301</span>
<span class="normal">4302</span>
<span class="normal">4303</span>
<span class="normal">4304</span>
<span class="normal">4305</span>
<span class="normal">4306</span>
<span class="normal">4307</span>
<span class="normal">4308</span>
<span class="normal">4309</span>
<span class="normal">4310</span>
<span class="normal">4311</span>
<span class="normal">4312</span>
<span class="normal">4313</span>
<span class="normal">4314</span>
<span class="normal">4315</span>
<span class="normal">4316</span>
<span class="normal">4317</span>
<span class="normal">4318</span>
<span class="normal">4319</span>
<span class="normal">4320</span>
<span class="normal">4321</span>
<span class="normal">4322</span>
<span class="normal">4323</span>
<span class="normal">4324</span>
<span class="normal">4325</span>
<span class="normal">4326</span>
<span class="normal">4327</span>
<span class="normal">4328</span>
<span class="normal">4329</span>
<span class="normal">4330</span>
<span class="normal">4331</span>
<span class="normal">4332</span>
<span class="normal">4333</span>
<span class="normal">4334</span>
<span class="normal">4335</span>
<span class="normal">4336</span>
<span class="normal">4337</span>
<span class="normal">4338</span>
<span class="normal">4339</span>
<span class="normal">4340</span>
<span class="normal">4341</span>
<span class="normal">4342</span>
<span class="normal">4343</span>
<span class="normal">4344</span>
<span class="normal">4345</span>
<span class="normal">4346</span>
<span class="normal">4347</span>
<span class="normal">4348</span>
<span class="normal">4349</span>
<span class="normal">4350</span>
<span class="normal">4351</span>
<span class="normal">4352</span>
<span class="normal">4353</span>
<span class="normal">4354</span>
<span class="normal">4355</span>
<span class="normal">4356</span>
<span class="normal">4357</span>
<span class="normal">4358</span>
<span class="normal">4359</span>
<span class="normal">4360</span>
<span class="normal">4361</span>
<span class="normal">4362</span>
<span class="normal">4363</span>
<span class="normal">4364</span>
<span class="normal">4365</span>
<span class="normal">4366</span>
<span class="normal">4367</span>
<span class="normal">4368</span>
<span class="normal">4369</span>
<span class="normal">4370</span>
<span class="normal">4371</span>
<span class="normal">4372</span>
<span class="normal">4373</span>
<span class="normal">4374</span>
<span class="normal">4375</span>
<span class="normal">4376</span>
<span class="normal">4377</span>
<span class="normal">4378</span>
<span class="normal">4379</span>
<span class="normal">4380</span>
<span class="normal">4381</span>
<span class="normal">4382</span>
<span class="normal">4383</span>
<span class="normal">4384</span>
<span class="normal">4385</span>
<span class="normal">4386</span>
<span class="normal">4387</span>
<span class="normal">4388</span>
<span class="normal">4389</span>
<span class="normal">4390</span>
<span class="normal">4391</span>
<span class="normal">4392</span>
<span class="normal">4393</span>
<span class="normal">4394</span>
<span class="normal">4395</span>
<span class="normal">4396</span>
<span class="normal">4397</span>
<span class="normal">4398</span>
<span class="normal">4399</span>
<span class="normal">4400</span>
<span class="normal">4401</span>
<span class="normal">4402</span>
<span class="normal">4403</span>
<span class="normal">4404</span>
<span class="normal">4405</span>
<span class="normal">4406</span>
<span class="normal">4407</span>
<span class="normal">4408</span>
<span class="normal">4409</span>
<span class="normal">4410</span>
<span class="normal">4411</span>
<span class="normal">4412</span>
<span class="normal">4413</span>
<span class="normal">4414</span>
<span class="normal">4415</span>
<span class="normal">4416</span>
<span class="normal">4417</span>
<span class="normal">4418</span>
<span class="normal">4419</span>
<span class="normal">4420</span>
<span class="normal">4421</span>
<span class="normal">4422</span>
<span class="normal">4423</span>
<span class="normal">4424</span>
<span class="normal">4425</span>
<span class="normal">4426</span>
<span class="normal">4427</span>
<span class="normal">4428</span>
<span class="normal">4429</span>
<span class="normal">4430</span>
<span class="normal">4431</span>
<span class="normal">4432</span>
<span class="normal">4433</span>
<span class="normal">4434</span>
<span class="normal">4435</span>
<span class="normal">4436</span>
<span class="normal">4437</span>
<span class="normal">4438</span>
<span class="normal">4439</span>
<span class="normal">4440</span>
<span class="normal">4441</span>
<span class="normal">4442</span>
<span class="normal">4443</span>
<span class="normal">4444</span>
<span class="normal">4445</span>
<span class="normal">4446</span>
<span class="normal">4447</span>
<span class="normal">4448</span>
<span class="normal">4449</span>
<span class="normal">4450</span>
<span class="normal">4451</span>
<span class="normal">4452</span>
<span class="normal">4453</span>
<span class="normal">4454</span>
<span class="normal">4455</span>
<span class="normal">4456</span>
<span class="normal">4457</span>
<span class="normal">4458</span>
<span class="normal">4459</span>
<span class="normal">4460</span>
<span class="normal">4461</span>
<span class="normal">4462</span>
<span class="normal">4463</span>
<span class="normal">4464</span>
<span class="normal">4465</span>
<span class="normal">4466</span>
<span class="normal">4467</span>
<span class="normal">4468</span>
<span class="normal">4469</span>
<span class="normal">4470</span>
<span class="normal">4471</span>
<span class="normal">4472</span>
<span class="normal">4473</span>
<span class="normal">4474</span>
<span class="normal">4475</span>
<span class="normal">4476</span>
<span class="normal">4477</span>
<span class="normal">4478</span>
<span class="normal">4479</span>
<span class="normal">4480</span>
<span class="normal">4481</span>
<span class="normal">4482</span>
<span class="normal">4483</span>
<span class="normal">4484</span>
<span class="normal">4485</span>
<span class="normal">4486</span>
<span class="normal">4487</span>
<span class="normal">4488</span>
<span class="normal">4489</span>
<span class="normal">4490</span>
<span class="normal">4491</span>
<span class="normal">4492</span>
<span class="normal">4493</span>
<span class="normal">4494</span>
<span class="normal">4495</span>
<span class="normal">4496</span>
<span class="normal">4497</span>
<span class="normal">4498</span>
<span class="normal">4499</span>
<span class="normal">4500</span>
<span class="normal">4501</span>
<span class="normal">4502</span>
<span class="normal">4503</span>
<span class="normal">4504</span>
<span class="normal">4505</span>
<span class="normal">4506</span>
<span class="normal">4507</span>
<span class="normal">4508</span>
<span class="normal">4509</span>
<span class="normal">4510</span>
<span class="normal">4511</span>
<span class="normal">4512</span>
<span class="normal">4513</span>
<span class="normal">4514</span>
<span class="normal">4515</span>
<span class="normal">4516</span>
<span class="normal">4517</span>
<span class="normal">4518</span>
<span class="normal">4519</span>
<span class="normal">4520</span>
<span class="normal">4521</span>
<span class="normal">4522</span>
<span class="normal">4523</span>
<span class="normal">4524</span>
<span class="normal">4525</span>
<span class="normal">4526</span>
<span class="normal">4527</span>
<span class="normal">4528</span>
<span class="normal">4529</span>
<span class="normal">4530</span>
<span class="normal">4531</span>
<span class="normal">4532</span>
<span class="normal">4533</span>
<span class="normal">4534</span>
<span class="normal">4535</span>
<span class="normal">4536</span>
<span class="normal">4537</span>
<span class="normal">4538</span>
<span class="normal">4539</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2Model</span><span class="p">(</span><span class="n">SeamlessM4Tv2PreTrainedModel</span><span class="p">):</span>
    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_encoder.embed_tokens.weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;text_decoder.embed_tokens.weight&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.__init__ with SeamlessM4T-&gt;SeamlessM4Tv2</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">current_modality</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Encoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2SpeechEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Decoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">=</span> <span class="n">current_modality</span>
        <span class="k">if</span> <span class="n">current_modality</span> <span class="o">==</span> <span class="s2">&quot;speech&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_features&quot;</span>

        <span class="c1"># these models already call post_init in their initialization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2TextToUnitForConditionalGeneration</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2CodeHifiGan</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.set_modality</span>
    <span class="k">def</span> <span class="nf">set_modality</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">modality</span> <span class="o">==</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_ids&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">=</span> <span class="s2">&quot;text&quot;</span>
        <span class="k">elif</span> <span class="n">modality</span> <span class="o">==</span> <span class="s2">&quot;speech&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_features&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">=</span> <span class="s2">&quot;speech&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`modality=</span><span class="si">{</span><span class="n">modality</span><span class="si">}</span><span class="s2">` is not a valid modality. It must be `text` or `speech`.&quot;</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.get_encoder</span>
    <span class="k">def</span> <span class="nf">get_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">==</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.get_output_embeddings</span>
    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.set_output_embeddings</span>
    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.get_input_embeddings</span>
    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.set_input_embeddings</span>
    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">value</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel._tie_weights</span>
    <span class="k">def</span> <span class="nf">_tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.forward with SeamlessM4T-&gt;SeamlessM4Tv2</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Seq2SeqLMOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The `use_cache` argument is changed to `False` since `labels` is provided.&quot;</span><span class="p">)</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span>
                    <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`input_ids`,`input_features`, `inputs_embeds` and `encoder_outputs` are all empty. Make sure at least one of them is not.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;`input_ids` is not `None` but `input_features` has been given.&quot;</span>
                    <span class="s2">&quot;`input_features` will be used in priority through the `speech_encoder`. &quot;</span>
                    <span class="s2">&quot;Make sure that `input_features` and `input_ids` are mutually exclusive.&quot;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;`inputs_embeds` is not `None` but `input_features` has been given.&quot;</span>
                    <span class="s2">&quot;`input_features` will be used in priority through `speech_encoder`. &quot;</span>
                    <span class="s2">&quot;`inputs_embeds` will be ignored.&quot;</span>
                <span class="p">)</span>

            <span class="c1"># if encoder_outputs is not None, it&#39;s probably used within a .generate method so no need to warn</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This calls the same method `forward` as `SeamlessM4Tv2ForTextToText` and `SeamlessM4Tv2ForSpeechToText`&quot;</span>
                <span class="s2">&quot;depending on the input modality. If you want to generate speech, use the `generate` method.&quot;</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;speech&quot;</span><span class="p">)</span>

            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span>
                <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if encoder_outputs is not None, it&#39;s probably used within a .generate method so no need to warn</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This calls the same method `forward` as `SeamlessM4Tv2ForTextToText` and `SeamlessM4Tv2ForSpeechToText`&quot;</span>
                <span class="s2">&quot;depending on the input modality. If you want to generate speech, use the `generate` method.&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True</span>
        <span class="k">elif</span> <span class="n">return_dict</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">):</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
                <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>
        <span class="c1"># input modality = speech so new attention mask</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">==</span> <span class="s2">&quot;speech&quot;</span> <span class="ow">and</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
            <span class="p">)</span>

        <span class="c1"># decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">decoder_inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">decoder_outputs</span> <span class="o">+</span> <span class="n">encoder_outputs</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speaker_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">generate_speech</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates translated token ids and/or translated audio waveforms.</span>

<span class="sd">        &lt;Tip&gt;</span>

<span class="sd">        This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">        arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">        that will be passed to one of them.</span>

<span class="sd">        For example, calling `.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)` will successively</span>
<span class="sd">        perform beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">        For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">        guide](./generation_strategies).</span>

<span class="sd">        &lt;/Tip&gt;</span>


<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">                Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">                [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`, *optional*):</span>
<span class="sd">                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">                [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>
<span class="sd">            return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">                If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">                to get translated text alongside the audio. Note that if `generate_speech=True`, this parameter will be</span>
<span class="sd">                ignored.</span>
<span class="sd">            tgt_lang (`str`, *optional*):</span>
<span class="sd">                The language to use as target language for translation.</span>
<span class="sd">            speaker_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">            generate_speech (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                If `False`, will only returns the text tokens and won&#39;t generate speech.</span>

<span class="sd">            kwargs (*optional*):</span>
<span class="sd">                Remaining dictioy of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">                arguments are of two types:</span>

<span class="sd">                    - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                    except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                    - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                    text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                    This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                    other.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor], ModelOutput]`:</span>
<span class="sd">            - If `generate_speech` and `return_intermediate_token_ids`, returns [`SeamlessM4Tv2GenerationOutput`].</span>
<span class="sd">            - If `generate_speech` and not `return_intermediate_token_ids`, returns a tuple composed of waveforms of</span>
<span class="sd">              shape `(batch_size, sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">            - If `generate_speech=False`, it will returns `ModelOutput`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`input_ids`,`input_features` and `inputs_embeds` are all empty. Make sure at least one of them is not.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">generate_speech</span> <span class="ow">and</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># also accept __xxx__</span>
            <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">generate_speech</span><span class="p">:</span>
                <span class="n">keys_to_check</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">keys_to_check</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys_to_check</span><span class="p">:</span>
                <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                        to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                    Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4Tv2 supports</span>
<span class="s2">                    more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                    <span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">)))</span>
        <span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
        <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
        <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># tgt_lang gets priority over decoder input ids</span>
            <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
            <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

        <span class="c1"># first generation</span>
        <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;speech&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;`input_features` and `input_ids` are both non empty. `input_features` will be used in priority &quot;</span>
                    <span class="s2">&quot;through the speech encoder. Make sure `input_features=None` if you want to use the text encoder.&quot;</span>
                <span class="p">)</span>
            <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>
            <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
        <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">generate_speech</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">text_generation_output</span>

        <span class="c1"># prepare second generation</span>
        <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

        <span class="c1"># get encoder last hidden states</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">==</span> <span class="s2">&quot;speech&quot;</span><span class="p">:</span>
            <span class="c1"># get last_hidden_state from encoder - must do a pass through the speech encoder</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span>
                <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span>
            <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

            <span class="c1"># input modality = speech so new attention mask for the decoder</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">encoder_hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># repeat attention mask alongside batch dimension</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># repeat attention mask alongside batch dimension</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
        <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Manually trim the final EOS token</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

        <span class="c1"># Compute new attention mask</span>
        <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
        <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

        <span class="c1"># REMOVE EOS and lang_id</span>
        <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># replace every other EOS</span>
        <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
            <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_input_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span>
        <span class="p">)</span>

        <span class="c1"># compute t2u_char_input_ids</span>
        <span class="n">t2u_subwords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_indices_to_subwords</span><span class="p">(</span><span class="n">t2u_input_ids</span><span class="p">)</span>
        <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_character_length_in_subword</span><span class="p">(</span>
            <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
        <span class="p">)</span>

        <span class="c1"># Add pads for lang, EOS tokens as per NLLB &quot;source&quot; tokenizer mode.</span>
        <span class="n">pad_zero</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pad_zero</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_zero</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t2u_char_input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_char_input_ids</span><span class="p">(</span>
            <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
        <span class="p">)</span>

        <span class="c1"># second pass</span>
        <span class="n">t2u_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="p">(</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span>
            <span class="n">char_input_ids</span><span class="o">=</span><span class="n">t2u_char_input_ids</span><span class="p">,</span>
            <span class="n">char_count_per_id</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

        <span class="c1"># The text-to-unit model is non auto-regressive. We keep the ability to use sampling with temperature</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">temperature</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;do_sample&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_logits</span> <span class="o">/</span> <span class="n">temperature</span>
            <span class="c1"># apply softmax</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># reshape to 2D: (batch_size, seq_len, t2u_vocab_size) -&gt; (batch_size*seq_len, t2u_vocab_size)</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
            <span class="c1"># multinomial then reshape : (batch_size*seq_len)-&gt; (batch_size,seq_len)</span>
            <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">t2u_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="n">replace_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span> <span class="o">|</span> <span class="p">(</span><span class="o">~</span><span class="n">padding_mask</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="c1"># replace eos per pad</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">replace_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">)</span>

        <span class="c1"># offset of control symbols</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
        <span class="p">)</span>

        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">speaker_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">speaker_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

        <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">speaker_id</span><span class="o">=</span><span class="n">speaker_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">(</span>
                <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
                <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
                <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
                <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.prepare_inputs_for_generation</span>
    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decoder_input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># cut decoder_input_ids if past is used</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># encoder_outputs is defined. input_ids not needed</span>
            <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">:</span> <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel._reorder_cache</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="c1"># cached cross_attention states don&#39;t have to be reordered -&gt; they are always the same</span>
            <span class="n">reordered_past</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">past_state</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">past_state</span> <span class="ow">in</span> <span class="n">layer_past</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">reordered_past</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Model.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2Model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_intermediate_token_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_lang</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">speaker_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">generate_speech</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2Model.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates translated token ids and/or translated audio waveforms.</p>
<p><Tip></p>
<p>This method successively calls the <code>.generate</code> function of two different sub-models. You can specify keyword
arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments
that will be passed to one of them.</p>
<p>For example, calling <code>.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)</code> will successively
perform beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</p>
<p>For an overview of generation strategies and code examples, check out the <a href="./generation_strategies">following
guide</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<code>SeamlessM4TTokenizer</code>] or [<code>SeamlessM4TProcessor</code>]. See
[<code>PreTrainedTokenizer.encode</code>] and [<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_features</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input audio features. This should be returnes by the [<code>SeamlessM4TFeatureExtractor</code>] class or the
[<code>SeamlessM4TProcessor</code>] class. See [<code>SeamlessM4TFeatureExtractor.__call__</code>] for details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_intermediate_token_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If <code>True</code>, also returns the intermediate generated text and unit tokens. Set to <code>True</code> if you also want
to get translated text alongside the audio. Note that if <code>generate_speech=True</code>, this parameter will be
ignored.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tgt_lang</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The language to use as target language for translation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speaker_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the speaker used for speech synthesis. Must be lower than <code>config.vocoder_num_spkrs</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>generate_speech</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If <code>False</code>, will only returns the text tokens and won't generate speech.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Remaining dictioy of keyword arguments that will be passed to [<code>GenerationMixin.generate</code>]. Keyword
arguments are of two types:</p>
<div class="highlight"><pre><span></span><code>- Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,
except for `decoder_input_ids` which will only be passed through the text components.
- With a *text_* or *speech_* prefix, they will be input for the `generate` method of the
text model and speech model respectively. It has the priority over the keywords without a prefix.

This means you can, for example, specify a generation strategy for one generation but not for the
other.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput">SeamlessM4Tv2GenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor], ModelOutput]</code>:</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput">SeamlessM4Tv2GenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If <code>generate_speech</code> and <code>return_intermediate_token_ids</code>, returns [<code>SeamlessM4Tv2GenerationOutput</code>].</li>
</ul>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput">SeamlessM4Tv2GenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If <code>generate_speech</code> and not <code>return_intermediate_token_ids</code>, returns a tuple composed of waveforms of
shape <code>(batch_size, sequence_length)</code>and and <code>waveform_lengths</code> which gives the length of each sample.</li>
</ul>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="mindspore.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2GenerationOutput">SeamlessM4Tv2GenerationOutput</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <ul>
<li>If <code>generate_speech=False</code>, it will returns <code>ModelOutput</code>.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4256</span>
<span class="normal">4257</span>
<span class="normal">4258</span>
<span class="normal">4259</span>
<span class="normal">4260</span>
<span class="normal">4261</span>
<span class="normal">4262</span>
<span class="normal">4263</span>
<span class="normal">4264</span>
<span class="normal">4265</span>
<span class="normal">4266</span>
<span class="normal">4267</span>
<span class="normal">4268</span>
<span class="normal">4269</span>
<span class="normal">4270</span>
<span class="normal">4271</span>
<span class="normal">4272</span>
<span class="normal">4273</span>
<span class="normal">4274</span>
<span class="normal">4275</span>
<span class="normal">4276</span>
<span class="normal">4277</span>
<span class="normal">4278</span>
<span class="normal">4279</span>
<span class="normal">4280</span>
<span class="normal">4281</span>
<span class="normal">4282</span>
<span class="normal">4283</span>
<span class="normal">4284</span>
<span class="normal">4285</span>
<span class="normal">4286</span>
<span class="normal">4287</span>
<span class="normal">4288</span>
<span class="normal">4289</span>
<span class="normal">4290</span>
<span class="normal">4291</span>
<span class="normal">4292</span>
<span class="normal">4293</span>
<span class="normal">4294</span>
<span class="normal">4295</span>
<span class="normal">4296</span>
<span class="normal">4297</span>
<span class="normal">4298</span>
<span class="normal">4299</span>
<span class="normal">4300</span>
<span class="normal">4301</span>
<span class="normal">4302</span>
<span class="normal">4303</span>
<span class="normal">4304</span>
<span class="normal">4305</span>
<span class="normal">4306</span>
<span class="normal">4307</span>
<span class="normal">4308</span>
<span class="normal">4309</span>
<span class="normal">4310</span>
<span class="normal">4311</span>
<span class="normal">4312</span>
<span class="normal">4313</span>
<span class="normal">4314</span>
<span class="normal">4315</span>
<span class="normal">4316</span>
<span class="normal">4317</span>
<span class="normal">4318</span>
<span class="normal">4319</span>
<span class="normal">4320</span>
<span class="normal">4321</span>
<span class="normal">4322</span>
<span class="normal">4323</span>
<span class="normal">4324</span>
<span class="normal">4325</span>
<span class="normal">4326</span>
<span class="normal">4327</span>
<span class="normal">4328</span>
<span class="normal">4329</span>
<span class="normal">4330</span>
<span class="normal">4331</span>
<span class="normal">4332</span>
<span class="normal">4333</span>
<span class="normal">4334</span>
<span class="normal">4335</span>
<span class="normal">4336</span>
<span class="normal">4337</span>
<span class="normal">4338</span>
<span class="normal">4339</span>
<span class="normal">4340</span>
<span class="normal">4341</span>
<span class="normal">4342</span>
<span class="normal">4343</span>
<span class="normal">4344</span>
<span class="normal">4345</span>
<span class="normal">4346</span>
<span class="normal">4347</span>
<span class="normal">4348</span>
<span class="normal">4349</span>
<span class="normal">4350</span>
<span class="normal">4351</span>
<span class="normal">4352</span>
<span class="normal">4353</span>
<span class="normal">4354</span>
<span class="normal">4355</span>
<span class="normal">4356</span>
<span class="normal">4357</span>
<span class="normal">4358</span>
<span class="normal">4359</span>
<span class="normal">4360</span>
<span class="normal">4361</span>
<span class="normal">4362</span>
<span class="normal">4363</span>
<span class="normal">4364</span>
<span class="normal">4365</span>
<span class="normal">4366</span>
<span class="normal">4367</span>
<span class="normal">4368</span>
<span class="normal">4369</span>
<span class="normal">4370</span>
<span class="normal">4371</span>
<span class="normal">4372</span>
<span class="normal">4373</span>
<span class="normal">4374</span>
<span class="normal">4375</span>
<span class="normal">4376</span>
<span class="normal">4377</span>
<span class="normal">4378</span>
<span class="normal">4379</span>
<span class="normal">4380</span>
<span class="normal">4381</span>
<span class="normal">4382</span>
<span class="normal">4383</span>
<span class="normal">4384</span>
<span class="normal">4385</span>
<span class="normal">4386</span>
<span class="normal">4387</span>
<span class="normal">4388</span>
<span class="normal">4389</span>
<span class="normal">4390</span>
<span class="normal">4391</span>
<span class="normal">4392</span>
<span class="normal">4393</span>
<span class="normal">4394</span>
<span class="normal">4395</span>
<span class="normal">4396</span>
<span class="normal">4397</span>
<span class="normal">4398</span>
<span class="normal">4399</span>
<span class="normal">4400</span>
<span class="normal">4401</span>
<span class="normal">4402</span>
<span class="normal">4403</span>
<span class="normal">4404</span>
<span class="normal">4405</span>
<span class="normal">4406</span>
<span class="normal">4407</span>
<span class="normal">4408</span>
<span class="normal">4409</span>
<span class="normal">4410</span>
<span class="normal">4411</span>
<span class="normal">4412</span>
<span class="normal">4413</span>
<span class="normal">4414</span>
<span class="normal">4415</span>
<span class="normal">4416</span>
<span class="normal">4417</span>
<span class="normal">4418</span>
<span class="normal">4419</span>
<span class="normal">4420</span>
<span class="normal">4421</span>
<span class="normal">4422</span>
<span class="normal">4423</span>
<span class="normal">4424</span>
<span class="normal">4425</span>
<span class="normal">4426</span>
<span class="normal">4427</span>
<span class="normal">4428</span>
<span class="normal">4429</span>
<span class="normal">4430</span>
<span class="normal">4431</span>
<span class="normal">4432</span>
<span class="normal">4433</span>
<span class="normal">4434</span>
<span class="normal">4435</span>
<span class="normal">4436</span>
<span class="normal">4437</span>
<span class="normal">4438</span>
<span class="normal">4439</span>
<span class="normal">4440</span>
<span class="normal">4441</span>
<span class="normal">4442</span>
<span class="normal">4443</span>
<span class="normal">4444</span>
<span class="normal">4445</span>
<span class="normal">4446</span>
<span class="normal">4447</span>
<span class="normal">4448</span>
<span class="normal">4449</span>
<span class="normal">4450</span>
<span class="normal">4451</span>
<span class="normal">4452</span>
<span class="normal">4453</span>
<span class="normal">4454</span>
<span class="normal">4455</span>
<span class="normal">4456</span>
<span class="normal">4457</span>
<span class="normal">4458</span>
<span class="normal">4459</span>
<span class="normal">4460</span>
<span class="normal">4461</span>
<span class="normal">4462</span>
<span class="normal">4463</span>
<span class="normal">4464</span>
<span class="normal">4465</span>
<span class="normal">4466</span>
<span class="normal">4467</span>
<span class="normal">4468</span>
<span class="normal">4469</span>
<span class="normal">4470</span>
<span class="normal">4471</span>
<span class="normal">4472</span>
<span class="normal">4473</span>
<span class="normal">4474</span>
<span class="normal">4475</span>
<span class="normal">4476</span>
<span class="normal">4477</span>
<span class="normal">4478</span>
<span class="normal">4479</span>
<span class="normal">4480</span>
<span class="normal">4481</span>
<span class="normal">4482</span>
<span class="normal">4483</span>
<span class="normal">4484</span>
<span class="normal">4485</span>
<span class="normal">4486</span>
<span class="normal">4487</span>
<span class="normal">4488</span>
<span class="normal">4489</span>
<span class="normal">4490</span>
<span class="normal">4491</span>
<span class="normal">4492</span>
<span class="normal">4493</span>
<span class="normal">4494</span>
<span class="normal">4495</span>
<span class="normal">4496</span>
<span class="normal">4497</span>
<span class="normal">4498</span>
<span class="normal">4499</span>
<span class="normal">4500</span>
<span class="normal">4501</span>
<span class="normal">4502</span>
<span class="normal">4503</span>
<span class="normal">4504</span>
<span class="normal">4505</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_intermediate_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_lang</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">speaker_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">generate_speech</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates translated token ids and/or translated audio waveforms.</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    This method successively calls the `.generate` function of two different sub-models. You can specify keyword</span>
<span class="sd">    arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments</span>
<span class="sd">    that will be passed to one of them.</span>

<span class="sd">    For example, calling `.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)` will successively</span>
<span class="sd">    perform beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.</span>

<span class="sd">    For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">    guide](./generation_strategies).</span>

<span class="sd">    &lt;/Tip&gt;</span>


<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">            Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See</span>
<span class="sd">            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        input_features (`mindspore.Tensor` of shape `(batch_size, sequence_length, num_banks)`, *optional*):</span>
<span class="sd">            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the</span>
<span class="sd">            [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.</span>
<span class="sd">        return_intermediate_token_ids (`bool`, *optional*):</span>
<span class="sd">            If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want</span>
<span class="sd">            to get translated text alongside the audio. Note that if `generate_speech=True`, this parameter will be</span>
<span class="sd">            ignored.</span>
<span class="sd">        tgt_lang (`str`, *optional*):</span>
<span class="sd">            The language to use as target language for translation.</span>
<span class="sd">        speaker_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.</span>
<span class="sd">        generate_speech (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            If `False`, will only returns the text tokens and won&#39;t generate speech.</span>

<span class="sd">        kwargs (*optional*):</span>
<span class="sd">            Remaining dictioy of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword</span>
<span class="sd">            arguments are of two types:</span>

<span class="sd">                - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                other.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor], ModelOutput]`:</span>
<span class="sd">        - If `generate_speech` and `return_intermediate_token_ids`, returns [`SeamlessM4Tv2GenerationOutput`].</span>
<span class="sd">        - If `generate_speech` and not `return_intermediate_token_ids`, returns a tuple composed of waveforms of</span>
<span class="sd">          shape `(batch_size, sequence_length)`and and `waveform_lengths` which gives the length of each sample.</span>
<span class="sd">        - If `generate_speech=False`, it will returns `ModelOutput`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;`input_ids`,`input_features` and `inputs_embeds` are all empty. Make sure at least one of them is not.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">generate_speech</span> <span class="ow">and</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify a `tgt_lang` to generate translated speech.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># also accept __xxx__</span>
        <span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">tgt_lang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">generate_speech</span><span class="p">:</span>
            <span class="n">keys_to_check</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">,</span> <span class="s2">&quot;t2u_lang_code_to_id&quot;</span><span class="p">,</span> <span class="s2">&quot;vocoder_lang_code_to_id&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">keys_to_check</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;text_decoder_lang_to_code_id&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys_to_check</span><span class="p">:</span>
            <span class="n">lang_code_to_id</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">lang_code_to_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;This model generation config doesn&#39;t have a `</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` key which maps the target language</span>
<span class="s2">                    to the right token id. Make sure to load the right generation config.&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">tgt_lang</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lang_code_to_id</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;`tgt_lang=</span><span class="si">{</span><span class="n">tgt_lang</span><span class="si">}</span><span class="s2">` is not supported by this model.</span>
<span class="s2">                Please specify a `tgt_lang` in </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lang_code_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. Note that SeamlessM4Tv2 supports</span>
<span class="s2">                more languages for text translation than for speech synthesis.&quot;&quot;&quot;</span>
                <span class="p">)</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">)))</span>
    <span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;return_dict_in_generate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;output_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
    <span class="c1"># overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.</span>
    <span class="k">if</span> <span class="n">tgt_lang</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># tgt_lang gets priority over decoder input ids</span>
        <span class="n">text_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">text_decoder_lang_to_code_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
        <span class="n">text_decoder_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">text_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="n">kwargs_text</span><span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_decoder_input_ids</span>

    <span class="c1"># first generation</span>
    <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;speech&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;`input_features` and `input_ids` are both non empty. `input_features` will be used in priority &quot;</span>
                <span class="s2">&quot;through the speech encoder. Make sure `input_features=None` if you want to use the text encoder.&quot;</span>
            <span class="p">)</span>
        <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_modality</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>
        <span class="n">text_generation_output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_text</span><span class="p">)</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">sequences</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">generate_speech</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">text_generation_output</span>

    <span class="c1"># prepare second generation</span>
    <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="n">kwargs_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="c1"># get encoder last hidden states</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_modality</span> <span class="o">==</span> <span class="s2">&quot;speech&quot;</span><span class="p">:</span>
        <span class="c1"># get last_hidden_state from encoder - must do a pass through the speech encoder</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder</span><span class="p">(</span>
            <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span>
        <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="c1"># input modality = speech so new attention mask for the decoder</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sub_sampled_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">seq_lens</span><span class="o">=</span><span class="n">sub_sampled_lengths</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">text_generation_output</span><span class="o">.</span><span class="n">encoder_hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># repeat attention mask alongside batch dimension</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># repeat attention mask alongside batch dimension</span>
    <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># get decoder last hidden state - must do a pass through the text decoder</span>
    <span class="n">t2u_input_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_decoder</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Manually trim the final EOS token</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span>

    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>

    <span class="c1"># Compute new attention mask</span>
    <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t2u_model_attention_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">t2u_input_embeds</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>
    <span class="n">kwargs_speech</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t2u_model_attention_mask</span>

    <span class="c1"># REMOVE EOS and lang_id</span>
    <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># replace every other EOS</span>
    <span class="n">t2u_input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
        <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_input_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span>
    <span class="p">)</span>

    <span class="c1"># compute t2u_char_input_ids</span>
    <span class="n">t2u_subwords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_indices_to_subwords</span><span class="p">(</span><span class="n">t2u_input_ids</span><span class="p">)</span>
    <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_character_length_in_subword</span><span class="p">(</span>
        <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
    <span class="p">)</span>

    <span class="c1"># Add pads for lang, EOS tokens as per NLLB &quot;source&quot; tokenizer mode.</span>
    <span class="n">pad_zero</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">t2u_char_count_per_id</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pad_zero</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_zero</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t2u_char_input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_char_input_ids</span><span class="p">(</span>
        <span class="n">t2u_input_ids</span><span class="p">,</span> <span class="n">t2u_subwords</span><span class="p">,</span> <span class="n">t2u_char_count_per_id</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span>
    <span class="p">)</span>

    <span class="c1"># second pass</span>
    <span class="n">t2u_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2u_model</span><span class="p">(</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">t2u_input_embeds</span><span class="p">,</span>
        <span class="n">char_input_ids</span><span class="o">=</span><span class="n">t2u_char_input_ids</span><span class="p">,</span>
        <span class="n">char_count_per_id</span><span class="o">=</span><span class="n">t2u_char_count_per_id</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs_speech</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">t2u_output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

    <span class="c1"># The text-to-unit model is non auto-regressive. We keep the ability to use sampling with temperature</span>
    <span class="n">temperature</span> <span class="o">=</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">temperature</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">kwargs_speech</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;do_sample&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">t2u_logits</span> <span class="o">=</span> <span class="n">t2u_logits</span> <span class="o">/</span> <span class="n">temperature</span>
        <span class="c1"># apply softmax</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">t2u_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># reshape to 2D: (batch_size, seq_len, t2u_vocab_size) -&gt; (batch_size*seq_len, t2u_vocab_size)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
        <span class="c1"># multinomial then reshape : (batch_size*seq_len)-&gt; (batch_size,seq_len)</span>
        <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">t2u_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">output_unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="n">replace_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_eos_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span> <span class="o">|</span> <span class="p">(</span><span class="o">~</span><span class="n">padding_mask</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="c1"># replace eos per pad</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">unit_ids</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">replace_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">)</span>

    <span class="c1"># offset of control symbols</span>
    <span class="n">unit_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">unit_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">t2u_pad_token_id</span><span class="p">,</span> <span class="n">unit_ids</span><span class="p">,</span> <span class="n">unit_ids</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocoder_offset</span>
    <span class="p">)</span>

    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocoder_lang_code_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tgt_lang</span><span class="p">)</span>
    <span class="n">vocoder_tgt_lang_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">vocoder_tgt_lang_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">speaker_id</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">speaker_id</span><span class="p">]]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">unit_ids</span><span class="p">))</span>

    <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">unit_ids</span><span class="p">,</span> <span class="n">speaker_id</span><span class="o">=</span><span class="n">speaker_id</span><span class="p">,</span> <span class="n">lang_id</span><span class="o">=</span><span class="n">vocoder_tgt_lang_id</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">return_intermediate_token_ids</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SeamlessM4Tv2GenerationOutput</span><span class="p">(</span>
            <span class="n">waveform</span><span class="o">=</span><span class="n">waveform</span><span class="p">,</span>
            <span class="n">waveform_lengths</span><span class="o">=</span><span class="n">waveform_lengths</span><span class="p">,</span>
            <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
            <span class="n">unit_sequences</span><span class="o">=</span><span class="n">output_unit_ids</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">waveform</span><span class="p">,</span> <span class="n">waveform_lengths</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.modeling_utils.PreTrainedModel" href="../../modeling_utils/#mindnlp.transformers.modeling_utils.PreTrainedModel">PreTrainedModel</a></code></p>


        <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
models.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2PreTrainedModel</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained</span>
<span class="sd">    models.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">config_class</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Config</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;seamless_m4t_v2&quot;</span>
    <span class="n">supports_gradient_checkpointing</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;SeamlessM4Tv2EncoderLayer&quot;</span><span class="p">,</span>
        <span class="s2">&quot;SeamlessM4Tv2DecoderLayer&quot;</span><span class="p">,</span>
        <span class="s2">&quot;SeamlessM4Tv2ConformerEncoderLayer&quot;</span><span class="p">,</span>
        <span class="s2">&quot;SeamlessM4Tv2TextToUnitDecoderLayer&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the weights&quot;&quot;&quot;</span>
        <span class="n">std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">module</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">SeamlessM4Tv2ConformerSelfAttention</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;pos_bias_u&quot;</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">pos_bias_u</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;pos_bias_v&quot;</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">pos_bias_v</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">SeamlessM4Tv2ConformerFeatureProjection</span><span class="p">):</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">module</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">in_features</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="n">k</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="n">k</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">)):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">ones_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose1d</span><span class="p">)):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">k</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">groups</span> <span class="o">/</span> <span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">*</span> <span class="n">module</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="n">k</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

    <span class="c1"># Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel._compute_sub_sample_lengths_from_attention_mask</span>
    <span class="k">def</span> <span class="nf">_compute_sub_sample_lengths_from_attention_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
        <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">adaptor_kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">adaptor_stride</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">seq_lens</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">int</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">((</span><span class="n">seq_lens</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">seq_lens</span><span class="o">.</span><span class="n">floor</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_indices_to_subwords</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the corresponding text string for each input id.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;id_to_text&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;This model generation config doesn&#39;t have a `id_to_text` key which maps</span>
<span class="sd">                token ids to subwords. Make sure to load the right generation config.&quot;&quot;&quot;</span>
            <span class="p">)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">subwords_batch</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">batch_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">subwords</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequence_len</span><span class="p">):</span>
                <span class="n">subword</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">id_to_text</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="n">batch_id</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
                <span class="n">subwords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">subword</span><span class="p">))</span>
            <span class="n">subwords_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">subwords</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">subwords_batch</span>

    <span class="k">def</span> <span class="nf">_count_character_length_in_subword</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">subwords_batch</span><span class="p">,</span>
        <span class="n">merge_space_with_prev_subword</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">unk_token_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">space</span><span class="o">=</span><span class="s2">&quot;▁&quot;</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Counts the number of characters per text string associated with the input token id.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>
<span class="sd">            subwords_batch (`List[List[str]]` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Corresponding text string for each input id.</span>
<span class="sd">            merge_space_with_prev_subword (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Indicates if the space character is merged with the previous subword. If `False`, it will be merged</span>
<span class="sd">                with the next subword.</span>
<span class="sd">            pad_token_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">                The id of the _padding_ text token. If it is encountered when calculating the length of a subword</span>
<span class="sd">                sample, the lengths of subsequent subwords will be set to 0.</span>
<span class="sd">            unk_token_id (`int`, *optional*, defaults to 1):</span>
<span class="sd">                The id of the _unknown_ text token. Associated to a subword of length 1.</span>
<span class="sd">            space (`str`, *optional*, defaults to `&quot;▁&quot;`):</span>
<span class="sd">                The space character.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">char_count_per_id</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">subword_lens</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">batch_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># We slice out the tensor till the padding index.</span>
            <span class="n">subword_indices</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">batch_id</span><span class="p">,</span> <span class="p">:</span> <span class="n">subword_lens</span><span class="p">[</span><span class="n">batch_id</span><span class="p">]]</span>
            <span class="n">subwords</span> <span class="o">=</span> <span class="n">subwords_batch</span><span class="p">[</span><span class="n">batch_id</span><span class="p">][:</span> <span class="n">subword_lens</span><span class="p">[</span><span class="n">batch_id</span><span class="p">]]</span>

            <span class="n">is_next_start_with_space</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">subwords</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">subwords</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">space</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">subwords</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">False</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">subwords</span><span class="p">))</span>
            <span class="p">]</span>
            <span class="n">is_punc</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">subwords</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="n">subwords</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="n">subwords</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">isnumeric</span><span class="p">()</span>
                <span class="ow">and</span> <span class="n">subwords</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">space</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">subwords</span><span class="p">))</span>
            <span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">subword_idx</span><span class="p">,</span> <span class="n">subword</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">subword_indices</span><span class="p">,</span> <span class="n">subwords</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">subword_idx</span> <span class="o">==</span> <span class="n">pad_token_id</span><span class="p">:</span>
                    <span class="k">break</span>

                <span class="k">if</span> <span class="n">subword_idx</span> <span class="o">==</span> <span class="n">unk_token_id</span><span class="p">:</span>
                    <span class="c1"># We set char_len to 1 for an unk token.</span>
                    <span class="n">char_len</span> <span class="o">=</span> <span class="mi">1</span>

                    <span class="k">if</span> <span class="n">merge_space_with_prev_subword</span> <span class="ow">and</span> <span class="n">is_next_start_with_space</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                        <span class="n">char_len</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># By default, spaces are merged with the next subword.</span>
                    <span class="c1"># char_len includes the space.</span>
                    <span class="n">char_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">subword</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">merge_space_with_prev_subword</span><span class="p">:</span>
                        <span class="c1"># Add the space for the next subword.</span>
                        <span class="k">if</span> <span class="n">is_next_start_with_space</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                            <span class="n">char_len</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="c1"># Subtract the space for the current subword.</span>
                        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">is_next_start_with_space</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]:</span>
                            <span class="n">char_len</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Merge space with punctuation mark by default.</span>
                        <span class="k">if</span> <span class="n">is_punc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">and</span> <span class="n">is_next_start_with_space</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                            <span class="n">char_len</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="c1"># Subtract the space for the subword succeeding the punctuation mark.</span>
                        <span class="k">elif</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">is_punc</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">is_next_start_with_space</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]:</span>
                            <span class="n">char_len</span> <span class="o">-=</span> <span class="mi">1</span>

                <span class="n">char_count_per_id</span><span class="p">[</span><span class="n">batch_id</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">char_len</span>

        <span class="k">return</span> <span class="n">char_count_per_id</span>

    <span class="k">def</span> <span class="nf">_get_char_input_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">subwords_batch</span><span class="p">,</span> <span class="n">char_count_per_id</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">unk_token_id</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the corresponding character input id for each character of `subwords_batch`.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>
<span class="sd">            subwords_batch (`List[List[str]]` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Corresponding text string for each input id.</span>
<span class="sd">            char_count_per_id (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                Number of characters per input id.</span>
<span class="sd">            pad_token_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">                The id of the _padding_ text token. If it is encountered when calculating the length of a subword</span>
<span class="sd">                sample, the lengths of subsequent subwords will be set to 0.</span>
<span class="sd">            unk_token_id (`int`, *optional*, defaults to 1):</span>
<span class="sd">                The id of the _unknown_ text token. Associated to a subword of length 1.</span>
<span class="sd">        Returns:</span>
<span class="sd">            `mindspore.Tensor`: Tensor of shape `(batch_size, char_sequence_length)` containing the id of each character.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;char_to_id&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;This model generation config doesn&#39;t have a `char_to_id` key which maps</span>
<span class="sd">                characters to character ids. Make sure to load the right generation config.&quot;&quot;&quot;</span>
            <span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">max_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">char_count_per_id</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">char_seqs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_len</span><span class="p">),</span> <span class="n">pad_token_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">subword_lens</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">batch_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">subword_indices</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">batch_id</span><span class="p">,</span> <span class="p">:</span> <span class="n">subword_lens</span><span class="p">[</span><span class="n">batch_id</span><span class="p">]]</span>
            <span class="n">subwords</span> <span class="o">=</span> <span class="n">subwords_batch</span><span class="p">[</span><span class="n">batch_id</span><span class="p">][:</span> <span class="n">subword_lens</span><span class="p">[</span><span class="n">batch_id</span><span class="p">]]</span>
            <span class="k">for</span> <span class="n">subword_idx</span><span class="p">,</span> <span class="n">subword</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">subword_indices</span><span class="p">,</span> <span class="n">subwords</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">subword_idx</span> <span class="o">==</span> <span class="n">unk_token_id</span><span class="p">:</span>
                    <span class="n">char_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">unk_token_id</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Get char token indices corresponding to the subwords.</span>
                    <span class="n">char_ids</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">char_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ch</span><span class="p">,</span> <span class="n">unk_token_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">subword</span><span class="p">)]</span>
                <span class="n">char_seq_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">char_ids</span><span class="p">)</span>
                <span class="n">char_seqs</span><span class="p">[</span><span class="n">batch_id</span><span class="p">,</span> <span class="n">total</span> <span class="p">:</span> <span class="n">total</span> <span class="o">+</span> <span class="n">char_seq_len</span><span class="p">]</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">char_ids</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">char_seqs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">char_seq_len</span>
        <span class="k">return</span> <span class="n">char_seqs</span>

    <span class="k">def</span> <span class="nf">_hard_upsample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">durations</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Repeats the time dimension of each sample in the batch based on the corresponding duration.</span>

<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor` of shape `(batch_size, sequence_length, *)`, *optional*):</span>
<span class="sd">                The sequence to repeat, where `*` is any number of sequence-specific dimensions including none.</span>
<span class="sd">            durations (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Indicates how many times to repeat time segments.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">durations</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if batched sample, need to interleave per sample, and pad -&gt; loss of parallelism</span>
            <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;`self.training=True` and you use batching. You lose parallelism during the hifigan</span>
<span class="sd">                               forward pass because the samples are interleaved.&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">duration</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">duration</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">durations</span><span class="p">)</span>
            <span class="p">]</span>

            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ScaledWordEmbedding" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ScaledWordEmbedding</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2ScaledWordEmbedding" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Embedding">Embedding</span></code></p>


        <p>This module overrides nn.Embeddings' forward by multiplying with embeddings scale.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2ScaledWordEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This module overrides nn.Embeddings&#39; forward by multiplying with embeddings scale.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embed_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span> <span class="o">=</span> <span class="n">embed_scale</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This module produces sinusoidal positional embeddings of any length.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2SinusoidalPositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This module produces sinusoidal positional embeddings of any length.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_positions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">offset</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">make_weights</span><span class="p">(</span><span class="n">num_positions</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">emb_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;weights&quot;</span><span class="p">):</span>
            <span class="c1"># in forward put the weights on the correct dtype and device of the param</span>
            <span class="n">emb_weights</span> <span class="o">=</span> <span class="n">emb_weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;weights&quot;</span><span class="p">,</span> <span class="n">emb_weights</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build sinusoidal embeddings.</span>

<span class="sd">        This matches the implementation in tensor2tensor, but differs slightly from the description in Section 3.5 of</span>
<span class="sd">        &quot;Attention Is All You Need&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">half_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="o">-</span><span class="n">emb</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">ops</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">emb</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">emb</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># zero pad</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">emb</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">emb</span><span class="p">[</span><span class="n">padding_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="n">emb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_default_dtype</span><span class="p">())</span>

    <span class="nd">@no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
            <span class="c1"># Create the position ids from the input token ids. Any padded tokens remain padded.</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">create_position_ids_from_input_ids</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_position_ids_from_inputs_embeds</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">)</span>

        <span class="c1"># expand embeddings if needed</span>
        <span class="n">max_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">past_key_values_length</span>
        <span class="k">if</span> <span class="n">max_pos</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_weights</span><span class="p">(</span><span class="n">max_pos</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">create_position_ids_from_inputs_embeds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs_embeds: mindspore.Tensor</span>

<span class="sd">        Returns: mindspore.Tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">+</span> <span class="n">past_key_values_length</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2SinusoidalPositionalEmbedding</span><span class="o">.</span><span class="n">create_position_ids_from_inputs_embeds</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>inputs_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_position_ids_from_inputs_embeds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs_embeds: mindspore.Tensor</span>

<span class="sd">    Returns: mindspore.Tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">position_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">+</span> <span class="n">past_key_values_length</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding.get_embedding" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2SinusoidalPositionalEmbedding</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2SinusoidalPositionalEmbedding.get_embedding" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Build sinusoidal embeddings.</p>
<p>This matches the implementation in tensor2tensor, but differs slightly from the description in Section 3.5 of
"Attention Is All You Need".</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">get_embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build sinusoidal embeddings.</span>

<span class="sd">    This matches the implementation in tensor2tensor, but differs slightly from the description in Section 3.5 of</span>
<span class="sd">    &quot;Attention Is All You Need&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">half_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="o">-</span><span class="n">emb</span><span class="p">)</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">ops</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">emb</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">emb</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">embedding_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># zero pad</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">emb</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">emb</span><span class="p">[</span><span class="n">padding_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">return</span> <span class="n">emb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_default_dtype</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoder" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoder</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel" href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2PreTrainedModel">SeamlessM4Tv2PreTrainedModel</a></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2TextToUnitDecoder</span><span class="p">(</span><span class="n">SeamlessM4Tv2PreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4Tv2Config</span><span class="p">,</span>
        <span class="n">embed_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">decoder_layerdrop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_target_positions</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_embedding</span> <span class="k">else</span> <span class="mf">1.0</span>

        <span class="k">if</span> <span class="n">embed_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if embed_tokens defined, use its shape instead</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">embed_tokens</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embed_tokens</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">embed_tokens</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_char</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">char_vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_char_positions</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2SinusoidalPositionalEmbedding</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_target_positions</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb_alpha_char</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb_alpha</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">duration_predictor</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2VariancePredictor</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">variance_predictor_embed_dim</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">variance_predictor_hidden_dim</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">variance_predictor_kernel_size</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">variance_pred_dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2SinusoidalPositionalEmbedding</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_target_positions</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">SeamlessM4Tv2TextToUnitDecoderLayer</span><span class="p">(</span>
                    <span class="n">config</span><span class="p">,</span>
                    <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
                    <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_ffn_dim</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">char_input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">char_count_per_id</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">SeamlessM4Tv2TextToUnitDecoderOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            char_input_ids (`mindspore.Tensor` of shape `(batch_size, char_sequence_length)`):</span>
<span class="sd">                Character indices. The correspondence between characters and indices can be found in `char_to_id`, a</span>
<span class="sd">                dictionary in the generation configuration.</span>
<span class="sd">            char_count_per_id (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length)`):</span>
<span class="sd">                Number of characters per text input id.</span>
<span class="sd">            encoder_hidden_states (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`):</span>
<span class="sd">                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention</span>
<span class="sd">                of the decoder.</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">                returned tensors for more detail.</span>
<span class="sd">            output_hidden_states (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class="sd">                for more detail.</span>
<span class="sd">            return_dict (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="c1"># create padding mask for character lengths</span>
        <span class="n">char_padding_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">char_input_ids</span><span class="p">,</span> <span class="n">char_count_per_id</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># upsample hidden states according to characters sequence lengths</span>
        <span class="n">char_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hard_upsample</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">char_count_per_id</span><span class="p">)</span>
        <span class="c1"># embed char positions</span>
        <span class="n">char_positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb_alpha_char</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_char_positions</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">char_hidden_states</span><span class="p">)</span>
        <span class="c1"># update char hidden states with positions and char embeddings</span>
        <span class="n">char_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_char</span><span class="p">(</span><span class="n">char_input_ids</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span> <span class="o">+</span> <span class="n">char_positions</span> <span class="o">+</span> <span class="n">char_hidden_states</span>

        <span class="c1"># predict duration</span>
        <span class="n">log_dur_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">duration_predictor</span><span class="p">(</span><span class="n">char_hidden_states</span><span class="p">,</span> <span class="n">padding_mask</span><span class="o">=</span><span class="n">char_padding_mask</span><span class="p">)</span>
        <span class="n">dur_out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_dur_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dur_out</span> <span class="o">=</span> <span class="n">dur_out</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">char_padding_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="mf">0.0</span><span class="p">)</span>

        <span class="c1"># upsample char hidden states according to predicted duration</span>
        <span class="n">char_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hard_upsample</span><span class="p">(</span><span class="n">char_hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">)</span>

        <span class="n">positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb_alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">char_hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">char_hidden_states</span> <span class="o">+</span> <span class="n">positions</span>

        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_attention_mask</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># decoder layers</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">decoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">([])</span>
                <span class="k">if</span> <span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">:</span>
                    <span class="k">continue</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
                    <span class="n">decoder_layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">padding_mask</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">decoder_layer</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># add hidden states from the last decoder layer</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attns</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">SeamlessM4Tv2TextToUnitDecoderOutput</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attns</span><span class="p">,</span>
            <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2TextToUnitDecoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">char_input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">char_count_per_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoder.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>char_input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Character indices. The correspondence between characters and indices can be found in <code>char_to_id</code>, a
dictionary in the generation configuration.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, char_sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>char_count_per_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of characters per text input id.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
of the decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return a [<code>~utils.ModelOutput</code>] instead of a plain tuple.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">char_input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">char_count_per_id</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">SeamlessM4Tv2TextToUnitDecoderOutput</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        char_input_ids (`mindspore.Tensor` of shape `(batch_size, char_sequence_length)`):</span>
<span class="sd">            Character indices. The correspondence between characters and indices can be found in `char_to_id`, a</span>
<span class="sd">            dictionary in the generation configuration.</span>
<span class="sd">        char_count_per_id (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length)`):</span>
<span class="sd">            Number of characters per text input id.</span>
<span class="sd">        encoder_hidden_states (`mindspore.Tensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`):</span>
<span class="sd">            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention</span>
<span class="sd">            of the decoder.</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">            returned tensors for more detail.</span>
<span class="sd">        output_hidden_states (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class="sd">            for more detail.</span>
<span class="sd">        return_dict (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="c1"># create padding mask for character lengths</span>
    <span class="n">char_padding_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">char_input_ids</span><span class="p">,</span> <span class="n">char_count_per_id</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># upsample hidden states according to characters sequence lengths</span>
    <span class="n">char_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hard_upsample</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">char_count_per_id</span><span class="p">)</span>
    <span class="c1"># embed char positions</span>
    <span class="n">char_positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb_alpha_char</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_char_positions</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">char_hidden_states</span><span class="p">)</span>
    <span class="c1"># update char hidden states with positions and char embeddings</span>
    <span class="n">char_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_char</span><span class="p">(</span><span class="n">char_input_ids</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span> <span class="o">+</span> <span class="n">char_positions</span> <span class="o">+</span> <span class="n">char_hidden_states</span>

    <span class="c1"># predict duration</span>
    <span class="n">log_dur_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">duration_predictor</span><span class="p">(</span><span class="n">char_hidden_states</span><span class="p">,</span> <span class="n">padding_mask</span><span class="o">=</span><span class="n">char_padding_mask</span><span class="p">)</span>
    <span class="n">dur_out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_dur_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dur_out</span> <span class="o">=</span> <span class="n">dur_out</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">char_padding_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="mf">0.0</span><span class="p">)</span>

    <span class="c1"># upsample char hidden states according to predicted duration</span>
    <span class="n">char_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hard_upsample</span><span class="p">(</span><span class="n">char_hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="p">)</span>

    <span class="n">positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb_alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">char_hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">char_hidden_states</span> <span class="o">+</span> <span class="n">positions</span>

    <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">_compute_new_attention_mask</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dur_out</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">_prepare_4d_attention_mask</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

    <span class="c1"># decoder layers</span>
    <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">decoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
        <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">([])</span>
            <span class="k">if</span> <span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">:</span>
                <span class="k">continue</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
                <span class="n">decoder_layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">decoder_layer</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="c1"># add hidden states from the last decoder layer</span>
    <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
        <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attns</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">SeamlessM4Tv2TextToUnitDecoderOutput</span><span class="p">(</span>
        <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
        <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attns</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderLayer" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderLayer</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderLayer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2TextToUnitDecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SeamlessM4Tv2Config</span><span class="p">,</span> <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">decoder_ffn_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">decoder_ffn_dim</span> <span class="k">if</span> <span class="n">decoder_ffn_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">decoder_ffn_dim</span>
        <span class="n">decoder_attention_heads</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">decoder_attention_heads</span> <span class="k">if</span> <span class="n">decoder_attention_heads</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">decoder_attention_heads</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Attention</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
            <span class="n">is_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">activation_function</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor`):</span>
<span class="sd">                input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">            attention_mask (`mindspore.Tensor`):</span>
<span class="sd">                attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very</span>
<span class="sd">                large negative values.</span>
<span class="sd">            padding_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Indicates which inputs are to be ignored due to padding, where elements are either 1 for *not masked*</span>
<span class="sd">                or 0 for *masked*</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">                returned tensors for more detail.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="c1"># Self Attention</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># Conv</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="c1"># Apply padding mask to avoid leaking padded positions in the convolution layer</span>
        <span class="k">if</span> <span class="n">padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">padding_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">padding_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_value</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="n">self_attn_weights</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderLayer.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2TextToUnitDecoderLayer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderLayer.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>input to the layer of shape <code>(batch, seq_len, embed_dim)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>attention mask of size <code>(batch, 1, tgt_len, src_len)</code> where padding elements are indicated by very
large negative values.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indicates which inputs are to be ignored due to padding, where elements are either 1 for <em>not masked</em>
or 0 for <em>masked</em></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more detail.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor`):</span>
<span class="sd">            input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class="sd">        attention_mask (`mindspore.Tensor`):</span>
<span class="sd">            attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very</span>
<span class="sd">            large negative values.</span>
<span class="sd">        padding_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Indicates which inputs are to be ignored due to padding, where elements are either 1 for *not masked*</span>
<span class="sd">            or 0 for *masked*</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class="sd">            returned tensors for more detail.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

    <span class="c1"># Self Attention</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="c1"># Conv</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

    <span class="c1"># Apply padding mask to avoid leaking padded positions in the convolution layer</span>
    <span class="k">if</span> <span class="n">padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">padding_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">padding_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_value</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="n">self_attn_weights</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderOutput" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitDecoderOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.utils.ModelOutput">ModelOutput</span></code></p>


        <p>Class defining the outputs from [<code>SeamlessM4Tv2TextToUnitDecoder</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>last_hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Sequence of hidden-states at the output of the last layer of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length,
sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indicates which inputs are to be ignored due to padding, where elements are either 1 for <em>not masked</em> or 0
for <em>masked</em></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SeamlessM4Tv2TextToUnitDecoderOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class defining the outputs from [`SeamlessM4Tv2TextToUnitDecoder`].</span>

<span class="sd">    Args:</span>
<span class="sd">        last_hidden_state (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):</span>
<span class="sd">            Sequence of hidden-states at the output of the last layer of the model.</span>
<span class="sd">        hidden_states (`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for the output of the embeddings, if the model has an embedding layer, +</span>
<span class="sd">            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</span>
<span class="sd">        attentions (`tuple(mindspore.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,</span>
<span class="sd">            sequence_length)`.</span>

<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention</span>
<span class="sd">            heads.</span>
<span class="sd">        padding_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Indicates which inputs are to be ignored due to padding, where elements are either 1 for *not masked* or 0</span>
<span class="sd">            for *masked*</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">last_hidden_state</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitOutput" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.SeamlessM4Tv2TextToUnitOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.utils.ModelOutput">ModelOutput</span></code></p>


        <div class="highlight"><pre><span></span><code>Class defining the outputs from [`SeamlessM4Tv2TextToUnitForConditionalGeneration`] and
[`SeamlessM4Tv2TextToUnitModel`].
</code></pre></div>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>last_hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1,
hidden_size)</code> is output.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indicates which inputs are to be ignored due to padding, where elements are either 1 for <em>not masked</em> or 0
for <em>masked</em></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length,
sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_last_hidden_state</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length,
sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>loss</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Language modeling loss.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SeamlessM4Tv2TextToUnitOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Class defining the outputs from [`SeamlessM4Tv2TextToUnitForConditionalGeneration`] and</span>
<span class="sd">        [`SeamlessM4Tv2TextToUnitModel`].</span>

<span class="sd">    Args:</span>
<span class="sd">        last_hidden_state (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):</span>
<span class="sd">            Sequence of hidden-states at the output of the last layer of the decoder of the model.</span>

<span class="sd">            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,</span>
<span class="sd">            hidden_size)` is output.</span>
<span class="sd">        padding_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Indicates which inputs are to be ignored due to padding, where elements are either 1 for *not masked* or 0</span>
<span class="sd">            for *masked*</span>
<span class="sd">        decoder_hidden_states (`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for the output of the embeddings, if the model has an embedding layer, +</span>
<span class="sd">            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.</span>
<span class="sd">        decoder_attentions (`tuple(mindspore.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,</span>
<span class="sd">            sequence_length)`.</span>

<span class="sd">            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the</span>
<span class="sd">            self-attention heads.</span>
<span class="sd">        encoder_last_hidden_state (`mindspore.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class="sd">            Sequence of hidden-states at the output of the last layer of the encoder of the model.</span>
<span class="sd">        encoder_hidden_states (`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for the output of the embeddings, if the model has an embedding layer, +</span>
<span class="sd">            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.</span>
<span class="sd">        encoder_attentions (`tuple(mindspore.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,</span>
<span class="sd">            sequence_length)`.</span>

<span class="sd">            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the</span>
<span class="sd">            self-attention heads.</span>
<span class="sd">        loss (`mindspore.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided):</span>
<span class="sd">            Language modeling loss.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">last_hidden_state</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_last_hidden_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.create_position_ids_from_input_ids" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">create_position_ids_from_input_ids</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.create_position_ids_from_input_ids" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols
are ignored. This is modified from fairseq's <code>utils.make_positions</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor x:</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_position_ids_from_input_ids</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols</span>
<span class="sd">    are ignored. This is modified from fairseq&#39;s `utils.make_positions`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: mindspore.Tensor x:</span>

<span class="sd">    Returns: mindspore.Tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">padding_idx</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="n">incremental_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span> <span class="o">+</span> <span class="n">past_key_values_length</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="n">incremental_indices</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="o">+</span> <span class="n">padding_idx</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.format_speech_generation_kwargs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.format_speech_generation_kwargs" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Format kwargs for SeamlessM4Tv2 models that generate speech, attribute kwargs to either the text generation or the
speech generation models.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Keyword arguments are of two types:</p>
<div class="highlight"><pre><span></span><code>- Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,
except for `decoder_input_ids` which will only be passed through the text components.
- With a *text_* or *speech_* prefix, they will be input for the `generate` method of the
text model and speech model respectively. It has the priority over the keywords without a prefix.

This means you can, for example, specify a generation strategy for one generation but not for the
other.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`)`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">format_speech_generation_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Format kwargs for SeamlessM4Tv2 models that generate speech, attribute kwargs to either the text generation or the</span>
<span class="sd">    speech generation models.</span>

<span class="sd">    Args:</span>
<span class="sd">        kwargs (`dict`)`:</span>
<span class="sd">             Keyword arguments are of two types:</span>

<span class="sd">                - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,</span>
<span class="sd">                except for `decoder_input_ids` which will only be passed through the text components.</span>
<span class="sd">                - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the</span>
<span class="sd">                text model and speech model respectively. It has the priority over the keywords without a prefix.</span>

<span class="sd">                This means you can, for example, specify a generation strategy for one generation but not for the</span>
<span class="sd">                other.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># attribute kwargs to models</span>
    <span class="n">kwargs_text</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">kwargs_speech</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;text_&quot;</span><span class="p">):</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;text_&quot;</span><span class="p">)</span> <span class="p">:]</span>
            <span class="n">kwargs_text</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">elif</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;speech_&quot;</span><span class="p">):</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;speech_&quot;</span><span class="p">)</span> <span class="p">:]</span>
            <span class="n">kwargs_speech</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If the key is already in a specific config, then it&#39;s been set with a</span>
            <span class="c1"># submodules specific value and we don&#39;t override</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs_text</span><span class="p">:</span>
                <span class="n">kwargs_text</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs_speech</span><span class="p">:</span>
                <span class="n">kwargs_speech</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">kwargs_text</span><span class="p">,</span> <span class="n">kwargs_speech</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.pad_sequence" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.pad_sequence" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Pad a list of sequences to the same length.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sequences</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The list of sequences to be padded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[<span title="mindnlp.transformers.generation.List">List</span>[float]]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>batch_first</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If True, the output tensor will have shape (batch_size, max_len, features).
If False, the shape will be (max_len, batch_size, features). Default is False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value used for padding. Default is 0.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor: A tensor containing the padded sequences.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">pad_sequence</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pad a list of sequences to the same length.</span>

<span class="sd">    Args:</span>
<span class="sd">        sequences (List[List[float]]): The list of sequences to be padded.</span>
<span class="sd">        batch_first (bool, optional): If True, the output tensor will have shape (batch_size, max_len, features).</span>
<span class="sd">            If False, the shape will be (max_len, batch_size, features). Default is False.</span>
<span class="sd">        padding_value (float, optional): The value used for padding. Default is 0.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        mindspore.Tensor: A tensor containing the padded sequences.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Determine the maximum sequence length</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">)</span>

    <span class="c1"># Pad each sequence using cp.pad</span>
    <span class="n">padded_sequences</span> <span class="o">=</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)),</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">padding_value</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">]</span>
    <span class="c1"># Stack the padded sequences along the appropriate axis</span>
    <span class="k">if</span> <span class="n">batch_first</span><span class="p">:</span>
        <span class="n">padded_sequence</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">padded_sequences</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">padded_sequence</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">padded_sequences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padded_sequence</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.shift_tokens_right" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">modeling_seamless_m4t_v2</span><span class="o">.</span><span class="n">shift_tokens_right</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">,</span> <span class="n">decoder_start_token_id</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2.shift_tokens_right" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Shift input ids one token to the right.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\modeling_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">shift_tokens_right</span><span class="p">(</span><span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">decoder_start_token_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shift input ids one token to the right.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shifted_input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">shifted_input_ids</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">shifted_input_ids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoder_start_token_id</span>

    <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;self.model.config.pad_token_id has to be defined.&quot;</span><span class="p">)</span>
    <span class="c1"># replace possible -100 values in labels by `pad_token_id`</span>
    <span class="n">shifted_input_ids</span> <span class="o">=</span> <span class="n">shifted_input_ids</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">shifted_input_ids</span> <span class="o">==</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">shifted_input_ids</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>SeamlessM4Tv2 model configuration</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2.SeamlessM4Tv2Config" class="doc doc-heading">
            <code>mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2.SeamlessM4Tv2Config</code>


<a href="#mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2.SeamlessM4Tv2Config" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.configuration_utils.PretrainedConfig" href="../../configuration_utils/#mindnlp.transformers.configuration_utils.PretrainedConfig">PretrainedConfig</a></code></p>


        <p>This is the configuration class to store the configuration of a [<code>~SeamlessM4Tv2Model</code>]. It is used to instantiate
an SeamlessM4Tv2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the SeamlessM4Tv2
<a href="https://hf-mirror.com/" title="">""</a> architecture.</p>
<p>Configuration objects inherit from [<code>PretrainedConfig</code>] and can be used to control the model outputs. Read the
documentation from [<code>PretrainedConfig</code>] for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vocabulary size of the text modality of the SeamlessM4Tv2 model. Defines the number of different tokens
that can be represented by the <code>inputs_ids</code> passed when calling [<code>~SeamlessM4Tv2Model</code>],
[<code>~SeamlessM4Tv2ForTextToSpeech</code>] or [<code>~SeamlessM4Tv2ForTextToText</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 256102</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256102</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Unit vocabulary size of the SeamlessM4Tv2 model. Defines the number of different "unit tokens" that can be
represented by the <code>inputs_ids</code> passed when calling the Text-To-Units sub-model of [<code>~SeamlessM4Tv2Model</code>],
[<code>~SeamlessM4Tv2ForSpeechToSpeech</code>] or [<code>~SeamlessM4Tv2ForTextToSpeech</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 10082</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10082</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>char_vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Character vocabulary size of the SeamlessM4Tv2 model. Defines the number of different character tokens that
can be represented by the <code>char_inputs_ids</code> passed when calling the Text-To-Units sub-model of
[<code>~SeamlessM4Tv2Model</code>], [<code>~SeamlessM4Tv2ForSpeechToSpeech</code>] or [<code>~SeamlessM4Tv2ForTextToSpeech</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 10943</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10943</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>Parameters</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>param below are Parameters shared across sub-models</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>shared across sub-models</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimensionality of the "intermediate" layers in the architecture.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1024</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1024</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>initializer_range</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.02</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.02</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_norm_eps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The epsilon used by the layer normalization layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 1e-05</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not the model should return the last key/values attentions (not used by all models).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum sequence length that this model text encoder and decoder might ever be used with. Typically set
this to something large just in case (e.g., 512 or 1024 or 2048).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 4096</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_encoder_decoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the model is used as an encoder/decoder or not.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The LayerDrop probability for the encoders. See the <a href="see https://arxiv.org/abs/1909.11556">LayerDrop paper</a>
for more details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.05</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The LayerDrop probability for the decoders. See the <a href="see https://arxiv.org/abs/1909.11556">LayerDrop paper</a>
for more details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.05</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>activation_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The non-linear activation function (function or string) in the decoder and feed-forward layers. If string,
<code>"gelu"</code>, <code>"relu"</code>, <code>"selu"</code>, <code>"swish"</code> and <code>"gelu_new"</code> are supported.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `function`, *optional*, defaults to `&#34;relu&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;relu&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for all fully connected layers in the embeddings, encoder, decoder, and pooler.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for all attention layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>activation_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for all activation layers in the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scale_embedding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Scale embeddings by diving by sqrt(d_model).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>Text</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>param below are Text encoder and text decoder specific parameters</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>encoder and text decoder specific parameters</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the Transformer text encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 24</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer text encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8192</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in the Transformer text encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the Transformer text decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 24</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer text decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8192</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in the Transformer text decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_start_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token. Only
applied in the text decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 3</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_new_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum numbers of text tokens to generate, ignoring the number of tokens in the prompt.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 256</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>padding</em> text token. Only applied to the text-decoder model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>beginning-of-stream</em> text token. Only applied to the text-decoder model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 2</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>end-of-stream</em> text token. Only applied to the text-decoder model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 3</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>Speech</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>param below are Speech encoder specific parameters</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>encoder specific parameters</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the Transformer speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 24</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in the Transformer speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_intermediate_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 4096</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_hidden_act</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The non-linear activation function (function or string) in the speech encoder. If string, <code>"gelu"</code>,
<code>"relu"</code>, <code>"selu"</code>, <code>"swish"</code> and <code>"gelu_new"</code> are supported.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `function`, *optional*, defaults to `&#34;swish&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;swish&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for all layers in the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_adapter</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Add an adapter layer on top of the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The LayerDrop probability for the speech encoder. See the <a href="see
https://arxiv.org/abs/1909.11556">LayerDrop paper</a> for more details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>feature_projection_input_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input dimension of the input feature projection of the speech encoder, i.e the dimension after processing
input audios with [<code>SeamlessM4TFeatureExtractor</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 160</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>160</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Kernel size of the convolutional layers in the adapter network. Only relevant if <code>add_adapter is True</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Stride of the convolutional layers in the adapter network. Only relevant if <code>add_adapter is True</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for all layers in the speech adapter.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_adapter_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of convolutional layers that should be used in the adapter network. Only relevant if <code>add_adapter is
True</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_embeddings_type</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be specified to <code>relative_key</code>. If left to <code>None</code>, no relative position embedding is applied. Only
applied to the speech encoder. For more information on <code>"relative_key"</code>, please refer to <a href="https://arxiv.org/abs/1803.02155">Self-Attention
with Relative Position Representations (Shaw et al.)</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;relative_key&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;relative_key&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>conv_depthwise_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Kernel size of convolutional depthwise 1D layer in Conformer blocks. Only applied to the speech encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 31</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>31</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>left_max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The left clipping value for relative positions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 64</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>64</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>right_max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The right clipping value for relative positions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_chunk_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of each attention chunk.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 20000</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>20000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_left_chunk_num</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of chunks on the left up to which lookahead is allowed.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 128</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>128</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>Text-To-Unit</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>param below are Text-To-Unit (t2u) model specific parameters</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>t2u) model specific parameters</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>beginning-of-stream</em> unit token. Only applied to the text-to-unit seq2seq model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>padding</em> unit token. Only applied to the text-to-unit seq2seq model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The id of the <em>end-of-stream</em> unit token. Only applied to the text-to-unit seq2seq model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 2</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the Transformer text-to-unit encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 6</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>6</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer text-to-unit encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8192</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in the Transformer text-to-unit encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of hidden layers in the Transformer text-to-unit decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 6</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>6</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer text-to-unit decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8192</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads for each attention layer in the Transformer text-to-unit decoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum sequence length that this model text-to-unit component might ever be used with. Typically set
this to something large just in case (e.g., 512 or 1024 or 2048).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 4096</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_variance_predictor_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The projection dimension of the text-to-unit's duration predictor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1024</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1024</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_variance_predictor_hidden_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Internal dimension of the text-to-unit's duration predictor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 256</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_variance_predictor_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Kernel size of the convolutional layers of the text-to-unit's duration predictor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 3</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_variance_pred_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probabilitiy of the text-to-unit's duration predictor.</p>
<p>Hifi-Gan Vocoder specific parameters: param below are Hifi-Gan Vocoder specific parameters</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.5</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.5</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sampling_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sampling rate at which the output audio will be generated, expressed in hertz (Hz).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 16000</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_initial_channel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of input channels into the hifi-gan upsampling network. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 512</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>512</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_rates</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tuple of integers defining the stride of each 1D convolutional layer in the vocoder upsampling network.
The length of <em>upsample_rates</em> defines the number of convolutional layers and has to match the length of
<em>upsample_kernel_sizes</em>. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[int]` or `List[int]`, *optional*, defaults to `[5, 4, 4, 2, 2]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[5, 4, 4, 2, 2]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_kernel_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tuple of integers defining the kernel size of each 1D convolutional layer in the vocoder upsampling
network. The length of <em>upsample_kernel_sizes</em> defines the number of convolutional layers and has to match
the length of <em>upsample_rates</em>. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[int]` or `List[int]`, *optional*, defaults to `[11, 8, 8, 4, 4]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[11, 8, 8, 4, 4]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resblock_kernel_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tuple of integers defining the kernel sizes of the vocoder 1D convolutional layers in the multi-receptive
field fusion (MRF) module. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[int]` or `List[int]`, *optional*, defaults to `[3, 7, 11]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[3, 7, 11]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resblock_dilation_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A nested tuple of integers defining the dilation rates of the vocoder dilated 1D convolutional layers in
the multi-receptive field fusion (MRF) module. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[Tuple[int]]` or `List[List[int]]`, *optional*, defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[[1, 3, 5], [1, 3, 5], [1, 3, 5]]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>leaky_relu_slope</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The angle of the negative slope used by the leaky ReLU activation in the vocoder. Applies to the vocoder
only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unit_hifi_gan_vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vocabulary size of the SeamlessM4Tv2 vocoder. Defines the number of different unit tokens that can be
represented by the <code>inputs_ids</code> passed when calling the vocoder of [<code>~SeamlessM4Tv2Model</code>],
[<code>~SeamlessM4Tv2ForSpeechToSpeech</code>] or [<code>~SeamlessM4Tv2ForTextToSpeech</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 10000</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unit_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The projection dimension of the input ids given to the hifi-gan vocoder. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1280</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1280</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lang_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The projection dimension of the target language given to the hifi-gan vocoder. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 256</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>spkr_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The projection dimension of the speaker id given to the hifi-gan vocoder. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 256</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_num_langs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of langs supported by the vocoder. Might be different from <code>t2u_num_langs</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 36</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>36</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_num_spkrs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of speakers supported by the vocoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 200</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>200</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>variance_predictor_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Kernel size of the duration predictor. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 3</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>var_pred_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probabilitiy of the duration predictor. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.5</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.5</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_offset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Offset the unit token ids by this number to account for symbol tokens. Applies to the vocoder only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 4</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">SeamlessM4Tv2Model</span><span class="p">,</span> <span class="n">SeamlessM4Tv2Config</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a SeamlessM4Tv2 &quot;&quot; style configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Config</span><span class="p">()</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a model from the &quot;&quot; style configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">SeamlessM4Tv2Model</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Accessing the model configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</code></pre></div>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\configuration_seamless_m4t_v2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SeamlessM4Tv2Config</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the configuration class to store the configuration of a [`~SeamlessM4Tv2Model`]. It is used to instantiate</span>
<span class="sd">    an SeamlessM4Tv2 model according to the specified arguments, defining the model architecture. Instantiating a</span>
<span class="sd">    configuration with the defaults will yield a similar configuration to that of the SeamlessM4Tv2</span>
<span class="sd">    [&quot;&quot;](https://hf-mirror.com/&quot;&quot;) architecture.</span>

<span class="sd">    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the</span>
<span class="sd">    documentation from [`PretrainedConfig`] for more information.</span>


<span class="sd">    Args:</span>
<span class="sd">        vocab_size (`int`, *optional*, defaults to 256102):</span>
<span class="sd">            Vocabulary size of the text modality of the SeamlessM4Tv2 model. Defines the number of different tokens</span>
<span class="sd">            that can be represented by the `inputs_ids` passed when calling [`~SeamlessM4Tv2Model`],</span>
<span class="sd">            [`~SeamlessM4Tv2ForTextToSpeech`] or [`~SeamlessM4Tv2ForTextToText`].</span>
<span class="sd">        t2u_vocab_size (`int`, *optional*, defaults to 10082):</span>
<span class="sd">            Unit vocabulary size of the SeamlessM4Tv2 model. Defines the number of different &quot;unit tokens&quot; that can be</span>
<span class="sd">            represented by the `inputs_ids` passed when calling the Text-To-Units sub-model of [`~SeamlessM4Tv2Model`],</span>
<span class="sd">            [`~SeamlessM4Tv2ForSpeechToSpeech`] or [`~SeamlessM4Tv2ForTextToSpeech`].</span>
<span class="sd">        char_vocab_size (`int`, *optional*, defaults to 10943):</span>
<span class="sd">            Character vocabulary size of the SeamlessM4Tv2 model. Defines the number of different character tokens that</span>
<span class="sd">            can be represented by the `char_inputs_ids` passed when calling the Text-To-Units sub-model of</span>
<span class="sd">            [`~SeamlessM4Tv2Model`], [`~SeamlessM4Tv2ForSpeechToSpeech`] or [`~SeamlessM4Tv2ForTextToSpeech`].</span>

<span class="sd">        Parameters shared across sub-models: param below are Parameters shared across sub-models</span>

<span class="sd">        hidden_size (`int`, *optional*, defaults to 1024):</span>
<span class="sd">            Dimensionality of the &quot;intermediate&quot; layers in the architecture.</span>
<span class="sd">        initializer_range (`float`, *optional*, defaults to 0.02):</span>
<span class="sd">            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</span>
<span class="sd">        layer_norm_eps (`float`, *optional*, defaults to 1e-05):</span>
<span class="sd">            The epsilon used by the layer normalization layers.</span>
<span class="sd">        use_cache (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether or not the model should return the last key/values attentions (not used by all models).</span>
<span class="sd">        max_position_embeddings (`int`, *optional*, defaults to 4096):</span>
<span class="sd">            The maximum sequence length that this model text encoder and decoder might ever be used with. Typically set</span>
<span class="sd">            this to something large just in case (e.g., 512 or 1024 or 2048).</span>
<span class="sd">        is_encoder_decoder (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the model is used as an encoder/decoder or not.</span>
<span class="sd">        encoder_layerdrop (`float`, *optional*, defaults to 0.05):</span>
<span class="sd">            The LayerDrop probability for the encoders. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)</span>
<span class="sd">            for more details.</span>
<span class="sd">        decoder_layerdrop (`float`, *optional*, defaults to 0.05):</span>
<span class="sd">            The LayerDrop probability for the decoders. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)</span>
<span class="sd">            for more details.</span>
<span class="sd">        activation_function (`str` or `function`, *optional*, defaults to `&quot;relu&quot;`):</span>
<span class="sd">            The non-linear activation function (function or string) in the decoder and feed-forward layers. If string,</span>
<span class="sd">            `&quot;gelu&quot;`, `&quot;relu&quot;`, `&quot;selu&quot;`, `&quot;swish&quot;` and `&quot;gelu_new&quot;` are supported.</span>
<span class="sd">        dropout (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The dropout probability for all fully connected layers in the embeddings, encoder, decoder, and pooler.</span>
<span class="sd">        attention_dropout (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The dropout probability for all attention layers.</span>
<span class="sd">        activation_dropout (`float`, *optional*, defaults to 0.0):</span>
<span class="sd">            The dropout probability for all activation layers in the model.</span>
<span class="sd">        scale_embedding (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Scale embeddings by diving by sqrt(d_model).</span>

<span class="sd">        Text encoder and text decoder specific parameters: param below are Text encoder and text decoder specific parameters</span>

<span class="sd">        encoder_layers (`int`, *optional*, defaults to 24):</span>
<span class="sd">            Number of hidden layers in the Transformer text encoder.</span>
<span class="sd">        encoder_ffn_dim (`int`, *optional*, defaults to 8192):</span>
<span class="sd">            Dimension of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer text encoder.</span>
<span class="sd">        encoder_attention_heads (`int`, *optional*, defaults to 16):</span>
<span class="sd">            Number of attention heads for each attention layer in the Transformer text encoder.</span>
<span class="sd">        decoder_layers (`int`, *optional*, defaults to 24):</span>
<span class="sd">            Number of hidden layers in the Transformer text decoder.</span>
<span class="sd">        decoder_ffn_dim (`int`, *optional*, defaults to 8192):</span>
<span class="sd">            Dimension of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer text decoder.</span>
<span class="sd">        decoder_attention_heads (`int`, *optional*, defaults to 16):</span>
<span class="sd">            Number of attention heads for each attention layer in the Transformer text decoder.</span>
<span class="sd">        decoder_start_token_id (`int`, *optional*, defaults to 3):</span>
<span class="sd">            If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token. Only</span>
<span class="sd">            applied in the text decoder.</span>
<span class="sd">        max_new_tokens (`int`, *optional*, defaults to 256):</span>
<span class="sd">            The maximum numbers of text tokens to generate, ignoring the number of tokens in the prompt.</span>
<span class="sd">        pad_token_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the _padding_ text token. Only applied to the text-decoder model.</span>
<span class="sd">        bos_token_id (`int`, *optional*, defaults to 2):</span>
<span class="sd">            The id of the _beginning-of-stream_ text token. Only applied to the text-decoder model.</span>
<span class="sd">        eos_token_id (`int`, *optional*, defaults to 3):</span>
<span class="sd">            The id of the _end-of-stream_ text token. Only applied to the text-decoder model.</span>

<span class="sd">        Speech encoder specific parameters: param below are Speech encoder specific parameters</span>

<span class="sd">        speech_encoder_layers (`int`, *optional*, defaults to 24):</span>
<span class="sd">            Number of hidden layers in the Transformer speech encoder.</span>
<span class="sd">        speech_encoder_attention_heads (`int`, *optional*, defaults to 16):</span>
<span class="sd">            Number of attention heads for each attention layer in the Transformer speech encoder.</span>
<span class="sd">        speech_encoder_intermediate_size (`int`, *optional*, defaults to 4096):</span>
<span class="sd">            Dimension of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer speech encoder.</span>
<span class="sd">        speech_encoder_hidden_act (`str` or `function`, *optional*, defaults to `&quot;swish&quot;`):</span>
<span class="sd">            The non-linear activation function (function or string) in the speech encoder. If string, `&quot;gelu&quot;`,</span>
<span class="sd">            `&quot;relu&quot;`, `&quot;selu&quot;`, `&quot;swish&quot;` and `&quot;gelu_new&quot;` are supported.</span>
<span class="sd">        speech_encoder_dropout (`float`, *optional*, defaults to 0.0):</span>
<span class="sd">            The dropout probability for all layers in the speech encoder.</span>
<span class="sd">        add_adapter (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Add an adapter layer on top of the speech encoder.</span>
<span class="sd">        speech_encoder_layerdrop (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The LayerDrop probability for the speech encoder. See the [LayerDrop paper](see</span>
<span class="sd">            https://arxiv.org/abs/1909.11556) for more details.</span>
<span class="sd">        feature_projection_input_dim (`int`, *optional*, defaults to 160):</span>
<span class="sd">            Input dimension of the input feature projection of the speech encoder, i.e the dimension after processing</span>
<span class="sd">            input audios with [`SeamlessM4TFeatureExtractor`].</span>
<span class="sd">        adaptor_kernel_size (`int`, *optional*, defaults to 8):</span>
<span class="sd">            Kernel size of the convolutional layers in the adapter network. Only relevant if `add_adapter is True`.</span>
<span class="sd">        adaptor_stride (`int`, *optional*, defaults to 8):</span>
<span class="sd">            Stride of the convolutional layers in the adapter network. Only relevant if `add_adapter is True`.</span>
<span class="sd">        adaptor_dropout (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The dropout probability for all layers in the speech adapter.</span>
<span class="sd">        num_adapter_layers (`int`, *optional*, defaults to 1):</span>
<span class="sd">            Number of convolutional layers that should be used in the adapter network. Only relevant if `add_adapter is</span>
<span class="sd">            True`.</span>
<span class="sd">        position_embeddings_type (`str`, *optional*, defaults to `&quot;relative_key&quot;`):</span>
<span class="sd">            Can be specified to `relative_key`. If left to `None`, no relative position embedding is applied. Only</span>
<span class="sd">            applied to the speech encoder. For more information on `&quot;relative_key&quot;`, please refer to [Self-Attention</span>
<span class="sd">            with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).</span>
<span class="sd">        conv_depthwise_kernel_size (`int`, *optional*, defaults to 31):</span>
<span class="sd">            Kernel size of convolutional depthwise 1D layer in Conformer blocks. Only applied to the speech encoder.</span>
<span class="sd">        left_max_position_embeddings (`int`, *optional*, defaults to 64):</span>
<span class="sd">            The left clipping value for relative positions.</span>
<span class="sd">        right_max_position_embeddings (`int`, *optional*, defaults to 8):</span>
<span class="sd">            The right clipping value for relative positions.</span>
<span class="sd">        speech_encoder_chunk_size (`int`, *optional*, defaults to 20000): The size of each attention chunk.</span>
<span class="sd">        speech_encoder_left_chunk_num (`int`, *optional*, defaults to 128):</span>
<span class="sd">            Number of chunks on the left up to which lookahead is allowed.</span>

<span class="sd">        Text-To-Unit (t2u) model specific parameters:  param below are Text-To-Unit (t2u) model specific parameters</span>

<span class="sd">        t2u_bos_token_id (`int`, *optional*, defaults to 0):</span>
<span class="sd">            The id of the _beginning-of-stream_ unit token. Only applied to the text-to-unit seq2seq model.</span>
<span class="sd">        t2u_pad_token_id (`int`, *optional*, defaults to 1):</span>
<span class="sd">            The id of the _padding_ unit token. Only applied to the text-to-unit seq2seq model.</span>
<span class="sd">        t2u_eos_token_id (`int`, *optional*, defaults to 2):</span>
<span class="sd">            The id of the _end-of-stream_ unit token. Only applied to the text-to-unit seq2seq model.</span>
<span class="sd">        t2u_encoder_layers (`int`, *optional*, defaults to 6):</span>
<span class="sd">            Number of hidden layers in the Transformer text-to-unit encoder.</span>
<span class="sd">        t2u_encoder_ffn_dim (`int`, *optional*, defaults to 8192):</span>
<span class="sd">            Dimension of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer text-to-unit encoder.</span>
<span class="sd">        t2u_encoder_attention_heads (`int`, *optional*, defaults to 16):</span>
<span class="sd">            Number of attention heads for each attention layer in the Transformer text-to-unit encoder.</span>
<span class="sd">        t2u_decoder_layers (`int`, *optional*, defaults to 6):</span>
<span class="sd">            Number of hidden layers in the Transformer text-to-unit decoder.</span>
<span class="sd">        t2u_decoder_ffn_dim (`int`, *optional*, defaults to 8192):</span>
<span class="sd">            Dimension of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer text-to-unit decoder.</span>
<span class="sd">        t2u_decoder_attention_heads (`int`, *optional*, defaults to 16):</span>
<span class="sd">            Number of attention heads for each attention layer in the Transformer text-to-unit decoder.</span>
<span class="sd">        t2u_max_position_embeddings (`int`, *optional*, defaults to 4096):</span>
<span class="sd">            The maximum sequence length that this model text-to-unit component might ever be used with. Typically set</span>
<span class="sd">            this to something large just in case (e.g., 512 or 1024 or 2048).</span>
<span class="sd">        t2u_variance_predictor_embed_dim (`int`, *optional*, defaults to 1024):</span>
<span class="sd">            The projection dimension of the text-to-unit&#39;s duration predictor.</span>
<span class="sd">        t2u_variance_predictor_hidden_dim (`int`, *optional*, defaults to 256):</span>
<span class="sd">            Internal dimension of the text-to-unit&#39;s duration predictor.</span>
<span class="sd">        t2u_variance_predictor_kernel_size (`int`, *optional*, defaults to 3):</span>
<span class="sd">            Kernel size of the convolutional layers of the text-to-unit&#39;s duration predictor.</span>
<span class="sd">        t2u_variance_pred_dropout (`float`, *optional*, defaults to 0.5):</span>
<span class="sd">            The dropout probabilitiy of the text-to-unit&#39;s duration predictor.</span>

<span class="sd">         Hifi-Gan Vocoder specific parameters: param below are Hifi-Gan Vocoder specific parameters</span>

<span class="sd">        sampling_rate (`int`, *optional*, defaults to 16000):</span>
<span class="sd">            The sampling rate at which the output audio will be generated, expressed in hertz (Hz).</span>
<span class="sd">        upsample_initial_channel (`int`, *optional*, defaults to 512):</span>
<span class="sd">            The number of input channels into the hifi-gan upsampling network. Applies to the vocoder only.</span>
<span class="sd">        upsample_rates (`Tuple[int]` or `List[int]`, *optional*, defaults to `[5, 4, 4, 2, 2]`):</span>
<span class="sd">            A tuple of integers defining the stride of each 1D convolutional layer in the vocoder upsampling network.</span>
<span class="sd">            The length of *upsample_rates* defines the number of convolutional layers and has to match the length of</span>
<span class="sd">            *upsample_kernel_sizes*. Applies to the vocoder only.</span>
<span class="sd">        upsample_kernel_sizes (`Tuple[int]` or `List[int]`, *optional*, defaults to `[11, 8, 8, 4, 4]`):</span>
<span class="sd">            A tuple of integers defining the kernel size of each 1D convolutional layer in the vocoder upsampling</span>
<span class="sd">            network. The length of *upsample_kernel_sizes* defines the number of convolutional layers and has to match</span>
<span class="sd">            the length of *upsample_rates*. Applies to the vocoder only.</span>
<span class="sd">        resblock_kernel_sizes (`Tuple[int]` or `List[int]`, *optional*, defaults to `[3, 7, 11]`):</span>
<span class="sd">            A tuple of integers defining the kernel sizes of the vocoder 1D convolutional layers in the multi-receptive</span>
<span class="sd">            field fusion (MRF) module. Applies to the vocoder only.</span>
<span class="sd">        resblock_dilation_sizes (`Tuple[Tuple[int]]` or `List[List[int]]`, *optional*, defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`):</span>
<span class="sd">            A nested tuple of integers defining the dilation rates of the vocoder dilated 1D convolutional layers in</span>
<span class="sd">            the multi-receptive field fusion (MRF) module. Applies to the vocoder only.</span>
<span class="sd">        leaky_relu_slope (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The angle of the negative slope used by the leaky ReLU activation in the vocoder. Applies to the vocoder</span>
<span class="sd">            only.</span>
<span class="sd">        unit_hifi_gan_vocab_size (`int`, *optional*, defaults to 10000):</span>
<span class="sd">            Vocabulary size of the SeamlessM4Tv2 vocoder. Defines the number of different unit tokens that can be</span>
<span class="sd">            represented by the `inputs_ids` passed when calling the vocoder of [`~SeamlessM4Tv2Model`],</span>
<span class="sd">            [`~SeamlessM4Tv2ForSpeechToSpeech`] or [`~SeamlessM4Tv2ForTextToSpeech`].</span>
<span class="sd">        unit_embed_dim (`int`, *optional*, defaults to 1280):</span>
<span class="sd">            The projection dimension of the input ids given to the hifi-gan vocoder. Applies to the vocoder only.</span>
<span class="sd">        lang_embed_dim (`int`, *optional*, defaults to 256):</span>
<span class="sd">            The projection dimension of the target language given to the hifi-gan vocoder. Applies to the vocoder only.</span>
<span class="sd">        spkr_embed_dim (`int`, *optional*, defaults to 256):</span>
<span class="sd">            The projection dimension of the speaker id given to the hifi-gan vocoder. Applies to the vocoder only.</span>
<span class="sd">        vocoder_num_langs (`int`, *optional*, defaults to 36):</span>
<span class="sd">            Number of langs supported by the vocoder. Might be different from `t2u_num_langs`.</span>
<span class="sd">        vocoder_num_spkrs (`int`, *optional*, defaults to 200):</span>
<span class="sd">            Number of speakers supported by the vocoder.</span>
<span class="sd">        variance_predictor_kernel_size (`int`, *optional*, defaults to 3):</span>
<span class="sd">            Kernel size of the duration predictor. Applies to the vocoder only.</span>
<span class="sd">        var_pred_dropout (`float`, *optional*, defaults to 0.5):</span>
<span class="sd">            The dropout probabilitiy of the duration predictor. Applies to the vocoder only.</span>
<span class="sd">        vocoder_offset (`int`, *optional*, defaults to 4):</span>
<span class="sd">            Offset the unit token ids by this number to account for symbol tokens. Applies to the vocoder only.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import SeamlessM4Tv2Model, SeamlessM4Tv2Config</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a SeamlessM4Tv2 &quot;&quot; style configuration</span>
<span class="sd">        &gt;&gt;&gt; configuration = SeamlessM4Tv2Config()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a model from the &quot;&quot; style configuration</span>
<span class="sd">        &gt;&gt;&gt; model = SeamlessM4Tv2Model(configuration)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Accessing the model configuration</span>
<span class="sd">        &gt;&gt;&gt; configuration = model.config</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;seamless_m4t_v2&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="mi">256102</span><span class="p">,</span>
        <span class="n">t2u_vocab_size</span><span class="o">=</span><span class="mi">10082</span><span class="p">,</span>
        <span class="n">char_vocab_size</span><span class="o">=</span><span class="mi">10943</span><span class="p">,</span>
        <span class="c1"># shared config</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
        <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
        <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">encoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
        <span class="n">decoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
        <span class="n">activation_function</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">activation_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">scale_embedding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="c1"># text encoder|decoder</span>
        <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
        <span class="n">encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
        <span class="n">encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">decoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
        <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
        <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="c1"># speech_encoder</span>
        <span class="n">speech_encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
        <span class="n">speech_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">speech_encoder_intermediate_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
        <span class="n">speech_encoder_hidden_act</span><span class="o">=</span><span class="s2">&quot;swish&quot;</span><span class="p">,</span>
        <span class="n">speech_encoder_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">add_adapter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">speech_encoder_layerdrop</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">feature_projection_input_dim</span><span class="o">=</span><span class="mi">160</span><span class="p">,</span>
        <span class="n">adaptor_kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">adaptor_stride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">adaptor_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">num_adapter_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">position_embeddings_type</span><span class="o">=</span><span class="s2">&quot;relative_key&quot;</span><span class="p">,</span>
        <span class="n">conv_depthwise_kernel_size</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span>
        <span class="n">left_max_position_embeddings</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
        <span class="n">right_max_position_embeddings</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">speech_encoder_chunk_size</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>
        <span class="n">speech_encoder_left_chunk_num</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="c1"># t2u config</span>
        <span class="n">t2u_bos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">t2u_pad_token_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">t2u_eos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">t2u_encoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
        <span class="n">t2u_encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
        <span class="n">t2u_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">t2u_decoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
        <span class="n">t2u_decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
        <span class="n">t2u_decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">t2u_max_position_embeddings</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
        <span class="n">t2u_variance_predictor_embed_dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">t2u_variance_predictor_hidden_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">t2u_variance_predictor_kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">t2u_variance_pred_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="c1"># hifi-gan vocoder config</span>
        <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span>
        <span class="n">upsample_initial_channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="n">upsample_rates</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
        <span class="n">upsample_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
        <span class="n">resblock_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>
        <span class="n">resblock_dilation_sizes</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span>
        <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="c1"># specific to Code Hifi-Gan</span>
        <span class="n">unit_hifi_gan_vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
        <span class="n">unit_embed_dim</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span>
        <span class="n">lang_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">spkr_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">vocoder_num_langs</span><span class="o">=</span><span class="mi">36</span><span class="p">,</span>
        <span class="n">vocoder_num_spkrs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">variance_predictor_kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">var_pred_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">vocoder_offset</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes a new instance of the SeamlessM4Tv2Config class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            vocab_size (int, optional): The size of the vocabulary. Defaults to 256102.</span>
<span class="sd">            t2u_vocab_size (int, optional): The size of the text-to-unit vocabulary. Defaults to 10082.</span>
<span class="sd">            char_vocab_size (int, optional): The size of the character vocabulary. Defaults to 10943.</span>
<span class="sd">            hidden_size (int, optional): The size of the hidden layers. Defaults to 1024.</span>
<span class="sd">            initializer_range (float, optional): The range for weight initialization. Defaults to 0.02.</span>
<span class="sd">            layer_norm_eps (float, optional): The epsilon value for layer normalization. Defaults to 1e-05.</span>
<span class="sd">            use_cache (bool, optional): Whether to use cache. Defaults to True.</span>
<span class="sd">            max_position_embeddings (int, optional): The maximum number of position embeddings. Defaults to 4096.</span>
<span class="sd">            is_encoder_decoder (bool, optional): Whether it is an encoder-decoder model. Defaults to True.</span>
<span class="sd">            encoder_layerdrop (float, optional): The layerdrop probability for the encoder. Defaults to 0.05.</span>
<span class="sd">            decoder_layerdrop (float, optional): The layerdrop probability for the decoder. Defaults to 0.05.</span>
<span class="sd">            activation_function (str, optional): The activation function to use. Defaults to &#39;relu&#39;.</span>
<span class="sd">            dropout (float, optional): The dropout probability. Defaults to 0.1.</span>
<span class="sd">            attention_dropout (float, optional): The dropout probability for attention layers. Defaults to 0.1.</span>
<span class="sd">            activation_dropout (float, optional): The dropout probability for activation layers. Defaults to 0.0.</span>
<span class="sd">            scale_embedding (bool, optional): Whether to scale the embeddings. Defaults to True.</span>
<span class="sd">            encoder_layers (int, optional): The number of encoder layers. Defaults to 24.</span>
<span class="sd">            encoder_ffn_dim (int, optional): The dimension of the encoder feed-forward network. Defaults to 8192.</span>
<span class="sd">            encoder_attention_heads (int, optional): The number of attention heads in the encoder. Defaults to 16.</span>
<span class="sd">            decoder_layers (int, optional): The number of decoder layers. Defaults to 24.</span>
<span class="sd">            decoder_ffn_dim (int, optional): The dimension of the decoder feed-forward network. Defaults to 8192.</span>
<span class="sd">            decoder_attention_heads (int, optional): The number of attention heads in the decoder. Defaults to 16.</span>
<span class="sd">            decoder_start_token_id (int, optional): The token ID for the start of decoding. Defaults to 3.</span>
<span class="sd">            max_new_tokens (int, optional): The maximum number of new tokens. Defaults to 256.</span>
<span class="sd">            pad_token_id (int, optional): The token ID for padding. Defaults to 0.</span>
<span class="sd">            bos_token_id (int, optional): The token ID for the beginning of sequence. Defaults to 2.</span>
<span class="sd">            eos_token_id (int, optional): The token ID for the end of sequence. Defaults to 3.</span>
<span class="sd">            speech_encoder_layers (int, optional): The number of speech encoder layers. Defaults to 24.</span>
<span class="sd">            speech_encoder_attention_heads (int, optional): The number of attention heads in the speech encoder. Defaults to 16.</span>
<span class="sd">            speech_encoder_intermediate_size (int, optional): The intermediate size of the speech encoder. Defaults to 4096.</span>
<span class="sd">            speech_encoder_hidden_act (str, optional): The activation function for the speech encoder. Defaults to &#39;swish&#39;.</span>
<span class="sd">            speech_encoder_dropout (float, optional): The dropout probability for the speech encoder. Defaults to 0.0.</span>
<span class="sd">            add_adapter (bool, optional): Whether to add an adapter. Defaults to True.</span>
<span class="sd">            speech_encoder_layerdrop (float, optional): The layerdrop probability for the speech encoder. Defaults to 0.1.</span>
<span class="sd">            feature_projection_input_dim (int, optional): The input dimension for feature projection. Defaults to 160.</span>
<span class="sd">            adaptor_kernel_size (int, optional): The kernel size for the adaptor. Defaults to 8.</span>
<span class="sd">            adaptor_stride (int, optional): The stride for the adaptor. Defaults to 8.</span>
<span class="sd">            adaptor_dropout (float, optional): The dropout probability for the adaptor. Defaults to 0.1.</span>
<span class="sd">            num_adapter_layers (int, optional): The number of adapter layers. Defaults to 1.</span>
<span class="sd">            position_embeddings_type (str, optional): The type of position embeddings. Defaults to &#39;relative_key&#39;.</span>
<span class="sd">            conv_depthwise_kernel_size (int, optional): The kernel size for depthwise convolution. Defaults to 31.</span>
<span class="sd">            left_max_position_embeddings (int, optional): The maximum number of left position embeddings. Defaults to 64.</span>
<span class="sd">            right_max_position_embeddings (int, optional): The maximum number of right position embeddings. Defaults to 8.</span>
<span class="sd">            speech_encoder_chunk_size (int, optional): The chunk size for the speech encoder. Defaults to 20000.</span>
<span class="sd">            speech_encoder_left_chunk_num (int, optional): The number of left chunks for the speech encoder. Defaults to 128.</span>
<span class="sd">            t2u_bos_token_id (int, optional): The token ID for the beginning of text-to-unit conversion. Defaults to 0.</span>
<span class="sd">            t2u_pad_token_id (int, optional): The token ID for padding in text-to-unit conversion. Defaults to 1.</span>
<span class="sd">            t2u_eos_token_id (int, optional): The token ID for the end of text-to-unit conversion. Defaults to 2.</span>
<span class="sd">            t2u_encoder_layers (int, optional): The number of text-to-unit encoder layers. Defaults to 6.</span>
<span class="sd">            t2u_encoder_ffn_dim (int, optional): The dimension of the text-to-unit encoder feed-forward network. Defaults to 8192.</span>
<span class="sd">            t2u_encoder_attention_heads (int, optional): The number of attention heads in the text-to-unit encoder. Defaults to 16.</span>
<span class="sd">            t2u_decoder_layers (int, optional): The number of text-to-unit decoder layers. Defaults to 6.</span>
<span class="sd">            t2u_decoder_ffn_dim (int, optional): The dimension of the text-to-unit decoder feed-forward network. Defaults to 8192.</span>
<span class="sd">            t2u_decoder_attention_heads (int, optional): The number of attention heads in the text-to-unit decoder. Defaults to 16.</span>
<span class="sd">            t2u_max_position_embeddings (int, optional): The maximum number of position embeddings for text-to-unit conversion. Defaults to 4096.</span>
<span class="sd">            t2u_variance_predictor_embed_dim (int, optional): The embedding dimension for the variance predictor in text-to-unit conversion. Defaults to 1024.</span>
<span class="sd">            t2u_variance_predictor_hidden_dim (int, optional): The hidden dimension for the variance predictor in text-to-unit conversion. Defaults to 256.</span>
<span class="sd">            t2u_variance_predictor_kernel_size (int, optional): The kernel size for the variance predictor in text-to-unit conversion. Defaults to 3.</span>
<span class="sd">            t2u_variance_pred_dropout (float, optional): The dropout probability for the variance predictor in text-to-unit conversion. Defaults to 0.5.</span>
<span class="sd">            sampling_rate (int, optional): The sampling rate of audio data. Defaults to 16000.</span>
<span class="sd">            upsample_initial_channel (int, optional): The initial number of channels for upsampling. Defaults to 512.</span>
<span class="sd">            upsample_rates (List[int], optional): The rates for upsampling. Defaults to [5, 4, 4, 2, 2].</span>
<span class="sd">            upsample_kernel_sizes (List[int], optional): The kernel sizes for upsampling. Defaults to [11, 8, 8, 4, 4].</span>
<span class="sd">            resblock_kernel_sizes (List[int], optional): The kernel sizes for residual blocks. Defaults to [3, 7, 11].</span>
<span class="sd">            resblock_dilation_sizes (List[List[int]], optional): The dilation sizes for residual blocks. Defaults to [[1, 3, 5], [1, 3, 5], [1, 3, 5]].</span>
<span class="sd">            leaky_relu_slope (float, optional): The slope for LeakyReLU activation. Defaults to 0.1.</span>
<span class="sd">            unit_hifi_gan_vocab_size (int, optional): The vocabulary size for the unit HiFi-GAN. Defaults to 10000.</span>
<span class="sd">            unit_embed_dim (int, optional): The embedding dimension for the unit HiFi-GAN. Defaults to 1280.</span>
<span class="sd">            lang_embed_dim (int, optional): The embedding dimension for language. Defaults to 256.</span>
<span class="sd">            spkr_embed_dim (int, optional): The embedding dimension for speaker. Defaults to 256.</span>
<span class="sd">            vocoder_num_langs (int, optional): The number of languages for the vocoder. Defaults to 36.</span>
<span class="sd">            vocoder_num_spkrs (int, optional): The number of speakers for the vocoder. Defaults to 200.</span>
<span class="sd">            variance_predictor_kernel_size (int, optional): The kernel size for the variance predictor. Defaults to 3.</span>
<span class="sd">            var_pred_dropout (float, optional): The dropout probability for the variance predictor. Defaults to 0.5.</span>
<span class="sd">            vocoder_offset (int, optional): The offset for the vocoder. Defaults to 4.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="c1"># overall_config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_vocab_size</span> <span class="o">=</span> <span class="n">t2u_vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">char_vocab_size</span> <span class="o">=</span> <span class="n">char_vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_eps</span> <span class="o">=</span> <span class="n">layer_norm_eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="o">=</span> <span class="n">max_new_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layerdrop</span> <span class="o">=</span> <span class="n">encoder_layerdrop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layerdrop</span> <span class="o">=</span> <span class="n">decoder_layerdrop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">activation_function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_dropout</span> <span class="o">=</span> <span class="n">activation_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_embedding</span> <span class="o">=</span> <span class="n">scale_embedding</span>
        <span class="c1"># for proper config init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">decoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">decoder_layers</span>

        <span class="c1"># text|unit encoder|decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">encoder_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_ffn_dim</span> <span class="o">=</span> <span class="n">encoder_ffn_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attention_heads</span> <span class="o">=</span> <span class="n">encoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">decoder_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_ffn_dim</span> <span class="o">=</span> <span class="n">decoder_ffn_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_attention_heads</span> <span class="o">=</span> <span class="n">decoder_attention_heads</span>

        <span class="c1"># speech_encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_layers</span> <span class="o">=</span> <span class="n">speech_encoder_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_hidden_act</span> <span class="o">=</span> <span class="n">speech_encoder_hidden_act</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_dropout</span> <span class="o">=</span> <span class="n">speech_encoder_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_attention_heads</span> <span class="o">=</span> <span class="n">speech_encoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_layerdrop</span> <span class="o">=</span> <span class="n">speech_encoder_layerdrop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_intermediate_size</span> <span class="o">=</span> <span class="n">speech_encoder_intermediate_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_projection_input_dim</span> <span class="o">=</span> <span class="n">feature_projection_input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_kernel_size</span> <span class="o">=</span> <span class="n">adaptor_kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_stride</span> <span class="o">=</span> <span class="n">adaptor_stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_dropout</span> <span class="o">=</span> <span class="n">adaptor_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_adapter_layers</span> <span class="o">=</span> <span class="n">num_adapter_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="o">=</span> <span class="n">position_embeddings_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_depthwise_kernel_size</span> <span class="o">=</span> <span class="n">conv_depthwise_kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_adapter</span> <span class="o">=</span> <span class="n">add_adapter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">left_max_position_embeddings</span> <span class="o">=</span> <span class="n">left_max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">right_max_position_embeddings</span> <span class="o">=</span> <span class="n">right_max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_chunk_size</span> <span class="o">=</span> <span class="n">speech_encoder_chunk_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_left_chunk_num</span> <span class="o">=</span> <span class="n">speech_encoder_left_chunk_num</span>

        <span class="c1"># t2u config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_bos_token_id</span> <span class="o">=</span> <span class="n">t2u_bos_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_pad_token_id</span> <span class="o">=</span> <span class="n">t2u_pad_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_eos_token_id</span> <span class="o">=</span> <span class="n">t2u_eos_token_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_layers</span> <span class="o">=</span> <span class="n">t2u_encoder_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_ffn_dim</span> <span class="o">=</span> <span class="n">t2u_encoder_ffn_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_attention_heads</span> <span class="o">=</span> <span class="n">t2u_encoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_layers</span> <span class="o">=</span> <span class="n">t2u_decoder_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_ffn_dim</span> <span class="o">=</span> <span class="n">t2u_decoder_ffn_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_attention_heads</span> <span class="o">=</span> <span class="n">t2u_decoder_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_max_position_embeddings</span> <span class="o">=</span> <span class="n">t2u_max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_variance_predictor_embed_dim</span> <span class="o">=</span> <span class="n">t2u_variance_predictor_embed_dim</span>  <span class="c1"># TODO: add to docstrings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_variance_predictor_hidden_dim</span> <span class="o">=</span> <span class="n">t2u_variance_predictor_hidden_dim</span>  <span class="c1"># TODO: add to docstrings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_variance_predictor_kernel_size</span> <span class="o">=</span> <span class="n">t2u_variance_predictor_kernel_size</span>  <span class="c1"># TODO: add to docstrings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2u_variance_pred_dropout</span> <span class="o">=</span> <span class="n">t2u_variance_pred_dropout</span>  <span class="c1"># TODO: add to docstrings</span>

        <span class="c1"># hifi-gan vocoder config</span>
        <span class="c1"># original parameters specific to Hifi-Gan</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span> <span class="o">=</span> <span class="n">sampling_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upsample_initial_channel</span> <span class="o">=</span> <span class="n">upsample_initial_channel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upsample_rates</span> <span class="o">=</span> <span class="n">upsample_rates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upsample_kernel_sizes</span> <span class="o">=</span> <span class="n">upsample_kernel_sizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resblock_kernel_sizes</span> <span class="o">=</span> <span class="n">resblock_kernel_sizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resblock_dilation_sizes</span> <span class="o">=</span> <span class="n">resblock_dilation_sizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu_slope</span> <span class="o">=</span> <span class="n">leaky_relu_slope</span>

        <span class="c1"># specific to Code Hifi-Gan</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unit_hifi_gan_vocab_size</span> <span class="o">=</span> <span class="n">unit_hifi_gan_vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unit_embed_dim</span> <span class="o">=</span> <span class="n">unit_embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lang_embed_dim</span> <span class="o">=</span> <span class="n">lang_embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spkr_embed_dim</span> <span class="o">=</span> <span class="n">spkr_embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_num_langs</span> <span class="o">=</span> <span class="n">vocoder_num_langs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_num_spkrs</span> <span class="o">=</span> <span class="n">vocoder_num_spkrs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">variance_predictor_kernel_size</span> <span class="o">=</span> <span class="n">variance_predictor_kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_pred_dropout</span> <span class="o">=</span> <span class="n">var_pred_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_offset</span> <span class="o">=</span> <span class="n">vocoder_offset</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
            <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
            <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
            <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="n">decoder_start_token_id</span><span class="p">,</span>
            <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="n">max_position_embeddings</span><span class="o">=</span><span class="n">max_position_embeddings</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2.SeamlessM4Tv2Config.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seamless_m4t_v2</span><span class="o">.</span><span class="n">configuration_seamless_m4t_v2</span><span class="o">.</span><span class="n">SeamlessM4Tv2Config</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">256102</span><span class="p">,</span> <span class="n">t2u_vocab_size</span><span class="o">=</span><span class="mi">10082</span><span class="p">,</span> <span class="n">char_vocab_size</span><span class="o">=</span><span class="mi">10943</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">encoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">decoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">activation_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale_embedding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span> <span class="n">encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">decoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span> <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">speech_encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">speech_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">speech_encoder_intermediate_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">speech_encoder_hidden_act</span><span class="o">=</span><span class="s1">&#39;swish&#39;</span><span class="p">,</span> <span class="n">speech_encoder_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">add_adapter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">speech_encoder_layerdrop</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">feature_projection_input_dim</span><span class="o">=</span><span class="mi">160</span><span class="p">,</span> <span class="n">adaptor_kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">adaptor_stride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">adaptor_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_adapter_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">position_embeddings_type</span><span class="o">=</span><span class="s1">&#39;relative_key&#39;</span><span class="p">,</span> <span class="n">conv_depthwise_kernel_size</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span> <span class="n">left_max_position_embeddings</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">right_max_position_embeddings</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">speech_encoder_chunk_size</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">speech_encoder_left_chunk_num</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">t2u_bos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">t2u_pad_token_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">t2u_eos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">t2u_encoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">t2u_encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span> <span class="n">t2u_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">t2u_decoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">t2u_decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span> <span class="n">t2u_decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">t2u_max_position_embeddings</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">t2u_variance_predictor_embed_dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">t2u_variance_predictor_hidden_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">t2u_variance_predictor_kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">t2u_variance_pred_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span> <span class="n">upsample_initial_channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">upsample_rates</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">upsample_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">resblock_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span> <span class="n">resblock_dilation_sizes</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span> <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">unit_hifi_gan_vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">unit_embed_dim</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span> <span class="n">lang_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">spkr_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">vocoder_num_langs</span><span class="o">=</span><span class="mi">36</span><span class="p">,</span> <span class="n">vocoder_num_spkrs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">variance_predictor_kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">var_pred_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">vocoder_offset</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.seamless_m4t_v2.configuration_seamless_m4t_v2.SeamlessM4Tv2Config.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a new instance of the SeamlessM4Tv2Config class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the vocabulary. Defaults to 256102.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256102</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the text-to-unit vocabulary. Defaults to 10082.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10082</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>char_vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the character vocabulary. Defaults to 10943.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10943</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the hidden layers. Defaults to 1024.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1024</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>initializer_range</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The range for weight initialization. Defaults to 0.02.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.02</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_norm_eps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The epsilon value for layer normalization. Defaults to 1e-05.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to use cache. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum number of position embeddings. Defaults to 4096.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_encoder_decoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether it is an encoder-decoder model. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The layerdrop probability for the encoder. Defaults to 0.05.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The layerdrop probability for the decoder. Defaults to 0.05.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>activation_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The activation function to use. Defaults to 'relu'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;relu&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability. Defaults to 0.1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for attention layers. Defaults to 0.1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>activation_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for activation layers. Defaults to 0.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scale_embedding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to scale the embeddings. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of encoder layers. Defaults to 24.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the encoder feed-forward network. Defaults to 8192.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads in the encoder. Defaults to 16.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of decoder layers. Defaults to 24.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the decoder feed-forward network. Defaults to 8192.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads in the decoder. Defaults to 16.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_start_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for the start of decoding. Defaults to 3.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_new_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum number of new tokens. Defaults to 256.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for padding. Defaults to 0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for the beginning of sequence. Defaults to 2.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for the end of sequence. Defaults to 3.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of speech encoder layers. Defaults to 24.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>24</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads in the speech encoder. Defaults to 16.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_intermediate_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The intermediate size of the speech encoder. Defaults to 4096.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_hidden_act</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The activation function for the speech encoder. Defaults to 'swish'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;swish&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for the speech encoder. Defaults to 0.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_adapter</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to add an adapter. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_layerdrop</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The layerdrop probability for the speech encoder. Defaults to 0.1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>feature_projection_input_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input dimension for feature projection. Defaults to 160.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>160</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size for the adaptor. Defaults to 8.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_stride</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The stride for the adaptor. Defaults to 8.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adaptor_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for the adaptor. Defaults to 0.1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_adapter_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of adapter layers. Defaults to 1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_embeddings_type</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The type of position embeddings. Defaults to 'relative_key'.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;relative_key&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>conv_depthwise_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size for depthwise convolution. Defaults to 31.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>31</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>left_max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum number of left position embeddings. Defaults to 64.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>64</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>right_max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum number of right position embeddings. Defaults to 8.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_chunk_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The chunk size for the speech encoder. Defaults to 20000.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>20000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>speech_encoder_left_chunk_num</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of left chunks for the speech encoder. Defaults to 128.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>128</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for the beginning of text-to-unit conversion. Defaults to 0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for padding in text-to-unit conversion. Defaults to 1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for the end of text-to-unit conversion. Defaults to 2.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of text-to-unit encoder layers. Defaults to 6.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>6</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the text-to-unit encoder feed-forward network. Defaults to 8192.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_encoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads in the text-to-unit encoder. Defaults to 16.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of text-to-unit decoder layers. Defaults to 6.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>6</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_ffn_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the text-to-unit decoder feed-forward network. Defaults to 8192.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8192</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_decoder_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads in the text-to-unit decoder. Defaults to 16.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_max_position_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum number of position embeddings for text-to-unit conversion. Defaults to 4096.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_variance_predictor_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The embedding dimension for the variance predictor in text-to-unit conversion. Defaults to 1024.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1024</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_variance_predictor_hidden_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden dimension for the variance predictor in text-to-unit conversion. Defaults to 256.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_variance_predictor_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size for the variance predictor in text-to-unit conversion. Defaults to 3.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>t2u_variance_pred_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for the variance predictor in text-to-unit conversion. Defaults to 0.5.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.5</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sampling_rate</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sampling rate of audio data. Defaults to 16000.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_initial_channel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The initial number of channels for upsampling. Defaults to 512.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>512</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_rates</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The rates for upsampling. Defaults to [5, 4, 4, 2, 2].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[5, 4, 4, 2, 2]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>upsample_kernel_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel sizes for upsampling. Defaults to [11, 8, 8, 4, 4].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[11, 8, 8, 4, 4]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resblock_kernel_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel sizes for residual blocks. Defaults to [3, 7, 11].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[3, 7, 11]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resblock_dilation_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dilation sizes for residual blocks. Defaults to [[1, 3, 5], [1, 3, 5], [1, 3, 5]].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.transformers.generation.List">List</span>[<span title="mindnlp.transformers.generation.List">List</span>[int]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[[1, 3, 5], [1, 3, 5], [1, 3, 5]]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>leaky_relu_slope</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The slope for LeakyReLU activation. Defaults to 0.1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unit_hifi_gan_vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The vocabulary size for the unit HiFi-GAN. Defaults to 10000.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10000</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unit_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The embedding dimension for the unit HiFi-GAN. Defaults to 1280.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1280</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lang_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The embedding dimension for language. Defaults to 256.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>spkr_embed_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The embedding dimension for speaker. Defaults to 256.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_num_langs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of languages for the vocoder. Defaults to 36.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>36</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_num_spkrs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of speakers for the vocoder. Defaults to 200.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>200</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>variance_predictor_kernel_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size for the variance predictor. Defaults to 3.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>var_pred_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for the variance predictor. Defaults to 0.5.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.5</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder_offset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The offset for the vocoder. Defaults to 4.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\seamless_m4t_v2\configuration_seamless_m4t_v2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">256102</span><span class="p">,</span>
    <span class="n">t2u_vocab_size</span><span class="o">=</span><span class="mi">10082</span><span class="p">,</span>
    <span class="n">char_vocab_size</span><span class="o">=</span><span class="mi">10943</span><span class="p">,</span>
    <span class="c1"># shared config</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
    <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">encoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">decoder_layerdrop</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">activation_function</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">activation_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">scale_embedding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># text encoder|decoder</span>
    <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
    <span class="n">encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
    <span class="n">encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">decoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
    <span class="n">decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
    <span class="n">decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">bos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">eos_token_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="c1"># speech_encoder</span>
    <span class="n">speech_encoder_layers</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
    <span class="n">speech_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">speech_encoder_intermediate_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
    <span class="n">speech_encoder_hidden_act</span><span class="o">=</span><span class="s2">&quot;swish&quot;</span><span class="p">,</span>
    <span class="n">speech_encoder_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">add_adapter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">speech_encoder_layerdrop</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">feature_projection_input_dim</span><span class="o">=</span><span class="mi">160</span><span class="p">,</span>
    <span class="n">adaptor_kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">adaptor_stride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">adaptor_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">num_adapter_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">position_embeddings_type</span><span class="o">=</span><span class="s2">&quot;relative_key&quot;</span><span class="p">,</span>
    <span class="n">conv_depthwise_kernel_size</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span>
    <span class="n">left_max_position_embeddings</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">right_max_position_embeddings</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">speech_encoder_chunk_size</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>
    <span class="n">speech_encoder_left_chunk_num</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="c1"># t2u config</span>
    <span class="n">t2u_bos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">t2u_pad_token_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">t2u_eos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">t2u_encoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">t2u_encoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
    <span class="n">t2u_encoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">t2u_decoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">t2u_decoder_ffn_dim</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
    <span class="n">t2u_decoder_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">t2u_max_position_embeddings</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
    <span class="n">t2u_variance_predictor_embed_dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">t2u_variance_predictor_hidden_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">t2u_variance_predictor_kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">t2u_variance_pred_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="c1"># hifi-gan vocoder config</span>
    <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span>
    <span class="n">upsample_initial_channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">upsample_rates</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="n">upsample_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="n">resblock_kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>
    <span class="n">resblock_dilation_sizes</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span>
    <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="c1"># specific to Code Hifi-Gan</span>
    <span class="n">unit_hifi_gan_vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">unit_embed_dim</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span>
    <span class="n">lang_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">spkr_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">vocoder_num_langs</span><span class="o">=</span><span class="mi">36</span><span class="p">,</span>
    <span class="n">vocoder_num_spkrs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">variance_predictor_kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">var_pred_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">vocoder_offset</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Initializes a new instance of the SeamlessM4Tv2Config class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        vocab_size (int, optional): The size of the vocabulary. Defaults to 256102.</span>
<span class="sd">        t2u_vocab_size (int, optional): The size of the text-to-unit vocabulary. Defaults to 10082.</span>
<span class="sd">        char_vocab_size (int, optional): The size of the character vocabulary. Defaults to 10943.</span>
<span class="sd">        hidden_size (int, optional): The size of the hidden layers. Defaults to 1024.</span>
<span class="sd">        initializer_range (float, optional): The range for weight initialization. Defaults to 0.02.</span>
<span class="sd">        layer_norm_eps (float, optional): The epsilon value for layer normalization. Defaults to 1e-05.</span>
<span class="sd">        use_cache (bool, optional): Whether to use cache. Defaults to True.</span>
<span class="sd">        max_position_embeddings (int, optional): The maximum number of position embeddings. Defaults to 4096.</span>
<span class="sd">        is_encoder_decoder (bool, optional): Whether it is an encoder-decoder model. Defaults to True.</span>
<span class="sd">        encoder_layerdrop (float, optional): The layerdrop probability for the encoder. Defaults to 0.05.</span>
<span class="sd">        decoder_layerdrop (float, optional): The layerdrop probability for the decoder. Defaults to 0.05.</span>
<span class="sd">        activation_function (str, optional): The activation function to use. Defaults to &#39;relu&#39;.</span>
<span class="sd">        dropout (float, optional): The dropout probability. Defaults to 0.1.</span>
<span class="sd">        attention_dropout (float, optional): The dropout probability for attention layers. Defaults to 0.1.</span>
<span class="sd">        activation_dropout (float, optional): The dropout probability for activation layers. Defaults to 0.0.</span>
<span class="sd">        scale_embedding (bool, optional): Whether to scale the embeddings. Defaults to True.</span>
<span class="sd">        encoder_layers (int, optional): The number of encoder layers. Defaults to 24.</span>
<span class="sd">        encoder_ffn_dim (int, optional): The dimension of the encoder feed-forward network. Defaults to 8192.</span>
<span class="sd">        encoder_attention_heads (int, optional): The number of attention heads in the encoder. Defaults to 16.</span>
<span class="sd">        decoder_layers (int, optional): The number of decoder layers. Defaults to 24.</span>
<span class="sd">        decoder_ffn_dim (int, optional): The dimension of the decoder feed-forward network. Defaults to 8192.</span>
<span class="sd">        decoder_attention_heads (int, optional): The number of attention heads in the decoder. Defaults to 16.</span>
<span class="sd">        decoder_start_token_id (int, optional): The token ID for the start of decoding. Defaults to 3.</span>
<span class="sd">        max_new_tokens (int, optional): The maximum number of new tokens. Defaults to 256.</span>
<span class="sd">        pad_token_id (int, optional): The token ID for padding. Defaults to 0.</span>
<span class="sd">        bos_token_id (int, optional): The token ID for the beginning of sequence. Defaults to 2.</span>
<span class="sd">        eos_token_id (int, optional): The token ID for the end of sequence. Defaults to 3.</span>
<span class="sd">        speech_encoder_layers (int, optional): The number of speech encoder layers. Defaults to 24.</span>
<span class="sd">        speech_encoder_attention_heads (int, optional): The number of attention heads in the speech encoder. Defaults to 16.</span>
<span class="sd">        speech_encoder_intermediate_size (int, optional): The intermediate size of the speech encoder. Defaults to 4096.</span>
<span class="sd">        speech_encoder_hidden_act (str, optional): The activation function for the speech encoder. Defaults to &#39;swish&#39;.</span>
<span class="sd">        speech_encoder_dropout (float, optional): The dropout probability for the speech encoder. Defaults to 0.0.</span>
<span class="sd">        add_adapter (bool, optional): Whether to add an adapter. Defaults to True.</span>
<span class="sd">        speech_encoder_layerdrop (float, optional): The layerdrop probability for the speech encoder. Defaults to 0.1.</span>
<span class="sd">        feature_projection_input_dim (int, optional): The input dimension for feature projection. Defaults to 160.</span>
<span class="sd">        adaptor_kernel_size (int, optional): The kernel size for the adaptor. Defaults to 8.</span>
<span class="sd">        adaptor_stride (int, optional): The stride for the adaptor. Defaults to 8.</span>
<span class="sd">        adaptor_dropout (float, optional): The dropout probability for the adaptor. Defaults to 0.1.</span>
<span class="sd">        num_adapter_layers (int, optional): The number of adapter layers. Defaults to 1.</span>
<span class="sd">        position_embeddings_type (str, optional): The type of position embeddings. Defaults to &#39;relative_key&#39;.</span>
<span class="sd">        conv_depthwise_kernel_size (int, optional): The kernel size for depthwise convolution. Defaults to 31.</span>
<span class="sd">        left_max_position_embeddings (int, optional): The maximum number of left position embeddings. Defaults to 64.</span>
<span class="sd">        right_max_position_embeddings (int, optional): The maximum number of right position embeddings. Defaults to 8.</span>
<span class="sd">        speech_encoder_chunk_size (int, optional): The chunk size for the speech encoder. Defaults to 20000.</span>
<span class="sd">        speech_encoder_left_chunk_num (int, optional): The number of left chunks for the speech encoder. Defaults to 128.</span>
<span class="sd">        t2u_bos_token_id (int, optional): The token ID for the beginning of text-to-unit conversion. Defaults to 0.</span>
<span class="sd">        t2u_pad_token_id (int, optional): The token ID for padding in text-to-unit conversion. Defaults to 1.</span>
<span class="sd">        t2u_eos_token_id (int, optional): The token ID for the end of text-to-unit conversion. Defaults to 2.</span>
<span class="sd">        t2u_encoder_layers (int, optional): The number of text-to-unit encoder layers. Defaults to 6.</span>
<span class="sd">        t2u_encoder_ffn_dim (int, optional): The dimension of the text-to-unit encoder feed-forward network. Defaults to 8192.</span>
<span class="sd">        t2u_encoder_attention_heads (int, optional): The number of attention heads in the text-to-unit encoder. Defaults to 16.</span>
<span class="sd">        t2u_decoder_layers (int, optional): The number of text-to-unit decoder layers. Defaults to 6.</span>
<span class="sd">        t2u_decoder_ffn_dim (int, optional): The dimension of the text-to-unit decoder feed-forward network. Defaults to 8192.</span>
<span class="sd">        t2u_decoder_attention_heads (int, optional): The number of attention heads in the text-to-unit decoder. Defaults to 16.</span>
<span class="sd">        t2u_max_position_embeddings (int, optional): The maximum number of position embeddings for text-to-unit conversion. Defaults to 4096.</span>
<span class="sd">        t2u_variance_predictor_embed_dim (int, optional): The embedding dimension for the variance predictor in text-to-unit conversion. Defaults to 1024.</span>
<span class="sd">        t2u_variance_predictor_hidden_dim (int, optional): The hidden dimension for the variance predictor in text-to-unit conversion. Defaults to 256.</span>
<span class="sd">        t2u_variance_predictor_kernel_size (int, optional): The kernel size for the variance predictor in text-to-unit conversion. Defaults to 3.</span>
<span class="sd">        t2u_variance_pred_dropout (float, optional): The dropout probability for the variance predictor in text-to-unit conversion. Defaults to 0.5.</span>
<span class="sd">        sampling_rate (int, optional): The sampling rate of audio data. Defaults to 16000.</span>
<span class="sd">        upsample_initial_channel (int, optional): The initial number of channels for upsampling. Defaults to 512.</span>
<span class="sd">        upsample_rates (List[int], optional): The rates for upsampling. Defaults to [5, 4, 4, 2, 2].</span>
<span class="sd">        upsample_kernel_sizes (List[int], optional): The kernel sizes for upsampling. Defaults to [11, 8, 8, 4, 4].</span>
<span class="sd">        resblock_kernel_sizes (List[int], optional): The kernel sizes for residual blocks. Defaults to [3, 7, 11].</span>
<span class="sd">        resblock_dilation_sizes (List[List[int]], optional): The dilation sizes for residual blocks. Defaults to [[1, 3, 5], [1, 3, 5], [1, 3, 5]].</span>
<span class="sd">        leaky_relu_slope (float, optional): The slope for LeakyReLU activation. Defaults to 0.1.</span>
<span class="sd">        unit_hifi_gan_vocab_size (int, optional): The vocabulary size for the unit HiFi-GAN. Defaults to 10000.</span>
<span class="sd">        unit_embed_dim (int, optional): The embedding dimension for the unit HiFi-GAN. Defaults to 1280.</span>
<span class="sd">        lang_embed_dim (int, optional): The embedding dimension for language. Defaults to 256.</span>
<span class="sd">        spkr_embed_dim (int, optional): The embedding dimension for speaker. Defaults to 256.</span>
<span class="sd">        vocoder_num_langs (int, optional): The number of languages for the vocoder. Defaults to 36.</span>
<span class="sd">        vocoder_num_spkrs (int, optional): The number of speakers for the vocoder. Defaults to 200.</span>
<span class="sd">        variance_predictor_kernel_size (int, optional): The kernel size for the variance predictor. Defaults to 3.</span>
<span class="sd">        var_pred_dropout (float, optional): The dropout probability for the variance predictor. Defaults to 0.5.</span>
<span class="sd">        vocoder_offset (int, optional): The offset for the vocoder. Defaults to 4.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># overall_config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_vocab_size</span> <span class="o">=</span> <span class="n">t2u_vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">char_vocab_size</span> <span class="o">=</span> <span class="n">char_vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_eps</span> <span class="o">=</span> <span class="n">layer_norm_eps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">max_position_embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="o">=</span> <span class="n">max_new_tokens</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layerdrop</span> <span class="o">=</span> <span class="n">encoder_layerdrop</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layerdrop</span> <span class="o">=</span> <span class="n">decoder_layerdrop</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">activation_function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation_dropout</span> <span class="o">=</span> <span class="n">activation_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scale_embedding</span> <span class="o">=</span> <span class="n">scale_embedding</span>
    <span class="c1"># for proper config init</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">decoder_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">decoder_layers</span>

    <span class="c1"># text|unit encoder|decoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">encoder_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_ffn_dim</span> <span class="o">=</span> <span class="n">encoder_ffn_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attention_heads</span> <span class="o">=</span> <span class="n">encoder_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">decoder_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_ffn_dim</span> <span class="o">=</span> <span class="n">decoder_ffn_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder_attention_heads</span> <span class="o">=</span> <span class="n">decoder_attention_heads</span>

    <span class="c1"># speech_encoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_layers</span> <span class="o">=</span> <span class="n">speech_encoder_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_hidden_act</span> <span class="o">=</span> <span class="n">speech_encoder_hidden_act</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_dropout</span> <span class="o">=</span> <span class="n">speech_encoder_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_attention_heads</span> <span class="o">=</span> <span class="n">speech_encoder_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_layerdrop</span> <span class="o">=</span> <span class="n">speech_encoder_layerdrop</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_intermediate_size</span> <span class="o">=</span> <span class="n">speech_encoder_intermediate_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">feature_projection_input_dim</span> <span class="o">=</span> <span class="n">feature_projection_input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_kernel_size</span> <span class="o">=</span> <span class="n">adaptor_kernel_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_stride</span> <span class="o">=</span> <span class="n">adaptor_stride</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">adaptor_dropout</span> <span class="o">=</span> <span class="n">adaptor_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_adapter_layers</span> <span class="o">=</span> <span class="n">num_adapter_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings_type</span> <span class="o">=</span> <span class="n">position_embeddings_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv_depthwise_kernel_size</span> <span class="o">=</span> <span class="n">conv_depthwise_kernel_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_adapter</span> <span class="o">=</span> <span class="n">add_adapter</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">left_max_position_embeddings</span> <span class="o">=</span> <span class="n">left_max_position_embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">right_max_position_embeddings</span> <span class="o">=</span> <span class="n">right_max_position_embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_chunk_size</span> <span class="o">=</span> <span class="n">speech_encoder_chunk_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speech_encoder_left_chunk_num</span> <span class="o">=</span> <span class="n">speech_encoder_left_chunk_num</span>

    <span class="c1"># t2u config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_bos_token_id</span> <span class="o">=</span> <span class="n">t2u_bos_token_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_pad_token_id</span> <span class="o">=</span> <span class="n">t2u_pad_token_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_eos_token_id</span> <span class="o">=</span> <span class="n">t2u_eos_token_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_layers</span> <span class="o">=</span> <span class="n">t2u_encoder_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_ffn_dim</span> <span class="o">=</span> <span class="n">t2u_encoder_ffn_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_encoder_attention_heads</span> <span class="o">=</span> <span class="n">t2u_encoder_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_layers</span> <span class="o">=</span> <span class="n">t2u_decoder_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_ffn_dim</span> <span class="o">=</span> <span class="n">t2u_decoder_ffn_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_decoder_attention_heads</span> <span class="o">=</span> <span class="n">t2u_decoder_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_max_position_embeddings</span> <span class="o">=</span> <span class="n">t2u_max_position_embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_variance_predictor_embed_dim</span> <span class="o">=</span> <span class="n">t2u_variance_predictor_embed_dim</span>  <span class="c1"># TODO: add to docstrings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_variance_predictor_hidden_dim</span> <span class="o">=</span> <span class="n">t2u_variance_predictor_hidden_dim</span>  <span class="c1"># TODO: add to docstrings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_variance_predictor_kernel_size</span> <span class="o">=</span> <span class="n">t2u_variance_predictor_kernel_size</span>  <span class="c1"># TODO: add to docstrings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">t2u_variance_pred_dropout</span> <span class="o">=</span> <span class="n">t2u_variance_pred_dropout</span>  <span class="c1"># TODO: add to docstrings</span>

    <span class="c1"># hifi-gan vocoder config</span>
    <span class="c1"># original parameters specific to Hifi-Gan</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sampling_rate</span> <span class="o">=</span> <span class="n">sampling_rate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">upsample_initial_channel</span> <span class="o">=</span> <span class="n">upsample_initial_channel</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">upsample_rates</span> <span class="o">=</span> <span class="n">upsample_rates</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">upsample_kernel_sizes</span> <span class="o">=</span> <span class="n">upsample_kernel_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">resblock_kernel_sizes</span> <span class="o">=</span> <span class="n">resblock_kernel_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">resblock_dilation_sizes</span> <span class="o">=</span> <span class="n">resblock_dilation_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu_slope</span> <span class="o">=</span> <span class="n">leaky_relu_slope</span>

    <span class="c1"># specific to Code Hifi-Gan</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unit_hifi_gan_vocab_size</span> <span class="o">=</span> <span class="n">unit_hifi_gan_vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unit_embed_dim</span> <span class="o">=</span> <span class="n">unit_embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lang_embed_dim</span> <span class="o">=</span> <span class="n">lang_embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">spkr_embed_dim</span> <span class="o">=</span> <span class="n">spkr_embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_num_langs</span> <span class="o">=</span> <span class="n">vocoder_num_langs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_num_spkrs</span> <span class="o">=</span> <span class="n">vocoder_num_spkrs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">variance_predictor_kernel_size</span> <span class="o">=</span> <span class="n">variance_predictor_kernel_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">var_pred_dropout</span> <span class="o">=</span> <span class="n">var_pred_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocoder_offset</span> <span class="o">=</span> <span class="n">vocoder_offset</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="n">decoder_start_token_id</span><span class="p">,</span>
        <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
        <span class="n">max_position_embeddings</span><span class="o">=</span><span class="n">max_position_embeddings</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../seamless_m4t/" class="md-footer__link md-footer__link--prev" aria-label="Previous: seamless_m4t">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                seamless_m4t
              </div>
            </div>
          </a>
        
        
          
          <a href="../segformer/" class="md-footer__link md-footer__link--next" aria-label="Next: segformer">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                segformer
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 MindSpore Lab and CQU NLP Team.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:lvyufeng@cqu.edu.cn" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindnlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/lu-yu-feng-46-1" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>