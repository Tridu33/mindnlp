
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../train_args/seq2seq/">
      
      
        <link rel="next" href="../default_func/">
      
      
      <link rel="icon" href="../../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>base - MindNLP Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindnlp.engine.trainer.base.Trainer" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../../../" title="MindNLP Docs" class="md-header__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindNLP Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              base
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="选择当前语言">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="../../../../../api/engine/trainer/base/" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="./" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindnlp" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../" class="md-tabs__link">
        
  
    
  
  主页

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../tutorials/quick_start/" class="md-tabs__link">
          
  
    
  
  教程

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../supported_models/" class="md-tabs__link">
        
  
    
  
  模型列表

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../contribute/" class="md-tabs__link">
        
  
    
  
  代码贡献

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../accelerate/" class="md-tabs__link">
          
  
    
  
  API文档

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../notes/changelog/" class="md-tabs__link">
          
  
    
  
  说明

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../" title="MindNLP Docs" class="md-nav__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    MindNLP Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindnlp" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    主页
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/data_preprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocess
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_mirror/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Mirror
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../supported_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    模型列表
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    代码贡献
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API文档
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API文档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../accelerate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/load_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    load_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/BaseMapFunction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BaseMapFunction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transforms
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" checked>
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engine
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Engine
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_1" >
        
          
          <label class="md-nav__link" for="__nav_5_4_1" id="__nav_5_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    train_args
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_1">
            <span class="md-nav__icon md-icon"></span>
            train_args
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../train_args/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../train_args/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seq2seq
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_2" checked>
        
          
          <label class="md-nav__link" for="__nav_5_4_2" id="__nav_5_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_4_2">
            <span class="md-nav__icon md-icon"></span>
            trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    base
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer" class="md-nav__link">
    <span class="md-ellipsis">
      Trainer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Trainer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.add_callback" class="md-nav__link">
    <span class="md-ellipsis">
      add_callback
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.call_model_init" class="md-nav__link">
    <span class="md-ellipsis">
      call_model_init
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.compute_loss" class="md-nav__link">
    <span class="md-ellipsis">
      compute_loss
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.create_optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      create_optimizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.create_optimizer_and_scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      create_optimizer_and_scheduler
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.create_scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      create_scheduler
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.evaluate" class="md-nav__link">
    <span class="md-ellipsis">
      evaluate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.evaluation_loop" class="md-nav__link">
    <span class="md-ellipsis">
      evaluation_loop
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.floating_point_ops" class="md-nav__link">
    <span class="md-ellipsis">
      floating_point_ops
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.get_decay_parameter_names" class="md-nav__link">
    <span class="md-ellipsis">
      get_decay_parameter_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.get_eval_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      get_eval_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.get_optimizer_cls_and_kwargs" class="md-nav__link">
    <span class="md-ellipsis">
      get_optimizer_cls_and_kwargs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.get_test_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      get_test_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.get_train_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      get_train_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.is_local_process_zero" class="md-nav__link">
    <span class="md-ellipsis">
      is_local_process_zero
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.is_world_process_zero" class="md-nav__link">
    <span class="md-ellipsis">
      is_world_process_zero
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.log" class="md-nav__link">
    <span class="md-ellipsis">
      log
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.num_examples" class="md-nav__link">
    <span class="md-ellipsis">
      num_examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.num_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      num_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.pop_callback" class="md-nav__link">
    <span class="md-ellipsis">
      pop_callback
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.predict" class="md-nav__link">
    <span class="md-ellipsis">
      predict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.prediction_step" class="md-nav__link">
    <span class="md-ellipsis">
      prediction_step
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.remove_callback" class="md-nav__link">
    <span class="md-ellipsis">
      remove_callback
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.save_model" class="md-nav__link">
    <span class="md-ellipsis">
      save_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.store_flos" class="md-nav__link">
    <span class="md-ellipsis">
      store_flos
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.train" class="md-nav__link">
    <span class="md-ellipsis">
      train
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.training_step" class="md-nav__link">
    <span class="md-ellipsis">
      training_step
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../default_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    default_func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../peft/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_2" >
        
          
          <label class="md-nav__link" for="__nav_5_7_2" id="__nav_5_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tuners
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_2">
            <span class="md-nav__icon md-icon"></span>
            tuners
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adalora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AdaLoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adaption_prompt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adaption_Prompt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/ia3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lokr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoKr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/prompt_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt tuning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_3" >
        
          
          <label class="md-nav__link" for="__nav_5_7_3" id="__nav_5_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/utils/merge_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    merge_utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/peft_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    peft_model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sentence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../transformers/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_2" >
        
          
          <label class="md-nav__link" for="__nav_5_9_2" id="__nav_5_9_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_2">
            <span class="md-nav__icon md-icon"></span>
            generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../api/transforemrs/generation/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/beam_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/logits_process/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/stopping_criteria/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stopping_criteria
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/streamers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    streamers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../transformers/models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9_3" id="__nav_5_9_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_3">
            <span class="md-nav__icon md-icon"></span>
            models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/albert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    albert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    align
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/altclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    altclip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/audio_spectrogram_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    audio_spectrogram_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/auto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    auto
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    autoformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/baichuan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    baichuan
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/barthez/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    barthez
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bartpho/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bartpho
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/beit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bert_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bert_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bertweet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bertweet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bge_m3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bge_m3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/big_bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    big_bird
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bigbird_pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bigbird_pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/biogpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    biogpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/blenderbot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/blenderbot_small/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot_small
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/blip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/blip_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip_2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bloom
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bridgetower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bros/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bros
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/byt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    byt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/camembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    camembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/canine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    canine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/chatglm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/chatglm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/chatglm3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/codegen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    codegen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cogvlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cogvlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cohere/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/convbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/convnext/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convnext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cpmant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cpmbee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/ctrl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctrl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cvt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cvt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/data2vec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data2vec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/deberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/deberta_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/decision_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decision_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/distilbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    distilbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/efficientformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/efficientnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/electra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    electra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/encodec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encodec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/ernie/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/ernie_m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie_m
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/esm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    esm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/flava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    flava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/funnel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    funnel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gemma
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt_bigcode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_bigcode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt_neo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt_neox/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt_neox_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt_pangu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_pangu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gptj/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gptj
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/graphormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    graphormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/groupvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    groupvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/hubert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hubert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/imagegpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    imagegpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/internlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    internlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/jamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/jetmoe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jetmoe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/layoutlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/layoutlmv2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlmv2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/led/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    led
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/llava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/llava_next/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava_next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/longformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/longt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/luke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    luke
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/marian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    marian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/maskformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maskformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mbart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mbart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/megatron_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/megatron_gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/minicpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minicpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/minigpt4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minigpt4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mixtral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mobilebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mobilevit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilevit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/moss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    moss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mpnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/musicgen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/musicgen_melody/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen_melody
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mvp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mvp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/nezha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nezha
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/nystromformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nystromformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/olmo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    olmo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/openelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    openelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    opt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/owlvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    owlvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/phi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/phi3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/poolformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    poolformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/pop2piano/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pop2piano
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/qwen2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2_moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/regnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/rembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    resnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/roc_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roc_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rwkv
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/sam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/seamless_m4t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/seamless_m4t_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/segformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    segformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/seggpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seggpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/speech_encoder_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_encoder_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/speech_to_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_to_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/squeezebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    squeezebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/stablelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stablelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    starcoder2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/swiftformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    swiftformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/switch_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    switch_transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/t5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    t5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/table_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    table_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/timesformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    timesformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/tinybert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tinybert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/van/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    van
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/vipllava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vipllava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/vision_text_dual_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision_text_dual_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/visual_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visual_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/wav2vec2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/wav2vec2_conformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_conformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/wav2vec2_with_lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_with_lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/wavlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    whisper
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/x_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    x_clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/xlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/xlm_roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/xlm_roberta_xl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta_xl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/xlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlnet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_4" >
        
          
          <label class="md-nav__link" for="__nav_5_9_4" id="__nav_5_9_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_4">
            <span class="md-nav__icon md-icon"></span>
            pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../api/transforemrs/pipeline/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/automatic_speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    automatic_speech_recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/document_question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    document_question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/fill_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fill_mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/text2text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text2text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/zero_shot_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_shot_classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/configuration_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    configuration_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/modeling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/tokenization_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/tokenization_utils_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/tokenization_utils_fast/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_fast
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TRL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    说明
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            说明
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    更新日志
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    行为准则
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    常见问题
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer" class="md-nav__link">
    <span class="md-ellipsis">
      Trainer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Trainer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.add_callback" class="md-nav__link">
    <span class="md-ellipsis">
      add_callback
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.call_model_init" class="md-nav__link">
    <span class="md-ellipsis">
      call_model_init
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.compute_loss" class="md-nav__link">
    <span class="md-ellipsis">
      compute_loss
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.create_optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      create_optimizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.create_optimizer_and_scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      create_optimizer_and_scheduler
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.create_scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      create_scheduler
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.evaluate" class="md-nav__link">
    <span class="md-ellipsis">
      evaluate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.evaluation_loop" class="md-nav__link">
    <span class="md-ellipsis">
      evaluation_loop
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.floating_point_ops" class="md-nav__link">
    <span class="md-ellipsis">
      floating_point_ops
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.get_decay_parameter_names" class="md-nav__link">
    <span class="md-ellipsis">
      get_decay_parameter_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.get_eval_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      get_eval_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.get_optimizer_cls_and_kwargs" class="md-nav__link">
    <span class="md-ellipsis">
      get_optimizer_cls_and_kwargs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.get_test_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      get_test_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.get_train_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      get_train_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.is_local_process_zero" class="md-nav__link">
    <span class="md-ellipsis">
      is_local_process_zero
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.is_world_process_zero" class="md-nav__link">
    <span class="md-ellipsis">
      is_world_process_zero
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.log" class="md-nav__link">
    <span class="md-ellipsis">
      log
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.num_examples" class="md-nav__link">
    <span class="md-ellipsis">
      num_examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.num_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      num_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.pop_callback" class="md-nav__link">
    <span class="md-ellipsis">
      pop_callback
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.predict" class="md-nav__link">
    <span class="md-ellipsis">
      predict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.prediction_step" class="md-nav__link">
    <span class="md-ellipsis">
      prediction_step
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.remove_callback" class="md-nav__link">
    <span class="md-ellipsis">
      remove_callback
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.save_model" class="md-nav__link">
    <span class="md-ellipsis">
      save_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.store_flos" class="md-nav__link">
    <span class="md-ellipsis">
      store_flos
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.train" class="md-nav__link">
    <span class="md-ellipsis">
      train
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.engine.trainer.base.Trainer.training_step" class="md-nav__link">
    <span class="md-ellipsis">
      training_step
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindnlp/edit/master/docs/en/api/engine/trainer/base.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindnlp/raw/master/docs/en/api/engine/trainer/base.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>base</h1>

<div class="doc doc-object doc-class">



<h2 id="mindnlp.engine.trainer.base.Trainer" class="doc doc-heading">
            <code>mindnlp.engine.trainer.base.Trainer</code>


<a href="#mindnlp.engine.trainer.base.Trainer" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Trainer is a simple but feature-complete training and eval loop for MindSpore, optimized for 🤗 Transformers.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 129</span>
<span class="normal"> 130</span>
<span class="normal"> 131</span>
<span class="normal"> 132</span>
<span class="normal"> 133</span>
<span class="normal"> 134</span>
<span class="normal"> 135</span>
<span class="normal"> 136</span>
<span class="normal"> 137</span>
<span class="normal"> 138</span>
<span class="normal"> 139</span>
<span class="normal"> 140</span>
<span class="normal"> 141</span>
<span class="normal"> 142</span>
<span class="normal"> 143</span>
<span class="normal"> 144</span>
<span class="normal"> 145</span>
<span class="normal"> 146</span>
<span class="normal"> 147</span>
<span class="normal"> 148</span>
<span class="normal"> 149</span>
<span class="normal"> 150</span>
<span class="normal"> 151</span>
<span class="normal"> 152</span>
<span class="normal"> 153</span>
<span class="normal"> 154</span>
<span class="normal"> 155</span>
<span class="normal"> 156</span>
<span class="normal"> 157</span>
<span class="normal"> 158</span>
<span class="normal"> 159</span>
<span class="normal"> 160</span>
<span class="normal"> 161</span>
<span class="normal"> 162</span>
<span class="normal"> 163</span>
<span class="normal"> 164</span>
<span class="normal"> 165</span>
<span class="normal"> 166</span>
<span class="normal"> 167</span>
<span class="normal"> 168</span>
<span class="normal"> 169</span>
<span class="normal"> 170</span>
<span class="normal"> 171</span>
<span class="normal"> 172</span>
<span class="normal"> 173</span>
<span class="normal"> 174</span>
<span class="normal"> 175</span>
<span class="normal"> 176</span>
<span class="normal"> 177</span>
<span class="normal"> 178</span>
<span class="normal"> 179</span>
<span class="normal"> 180</span>
<span class="normal"> 181</span>
<span class="normal"> 182</span>
<span class="normal"> 183</span>
<span class="normal"> 184</span>
<span class="normal"> 185</span>
<span class="normal"> 186</span>
<span class="normal"> 187</span>
<span class="normal"> 188</span>
<span class="normal"> 189</span>
<span class="normal"> 190</span>
<span class="normal"> 191</span>
<span class="normal"> 192</span>
<span class="normal"> 193</span>
<span class="normal"> 194</span>
<span class="normal"> 195</span>
<span class="normal"> 196</span>
<span class="normal"> 197</span>
<span class="normal"> 198</span>
<span class="normal"> 199</span>
<span class="normal"> 200</span>
<span class="normal"> 201</span>
<span class="normal"> 202</span>
<span class="normal"> 203</span>
<span class="normal"> 204</span>
<span class="normal"> 205</span>
<span class="normal"> 206</span>
<span class="normal"> 207</span>
<span class="normal"> 208</span>
<span class="normal"> 209</span>
<span class="normal"> 210</span>
<span class="normal"> 211</span>
<span class="normal"> 212</span>
<span class="normal"> 213</span>
<span class="normal"> 214</span>
<span class="normal"> 215</span>
<span class="normal"> 216</span>
<span class="normal"> 217</span>
<span class="normal"> 218</span>
<span class="normal"> 219</span>
<span class="normal"> 220</span>
<span class="normal"> 221</span>
<span class="normal"> 222</span>
<span class="normal"> 223</span>
<span class="normal"> 224</span>
<span class="normal"> 225</span>
<span class="normal"> 226</span>
<span class="normal"> 227</span>
<span class="normal"> 228</span>
<span class="normal"> 229</span>
<span class="normal"> 230</span>
<span class="normal"> 231</span>
<span class="normal"> 232</span>
<span class="normal"> 233</span>
<span class="normal"> 234</span>
<span class="normal"> 235</span>
<span class="normal"> 236</span>
<span class="normal"> 237</span>
<span class="normal"> 238</span>
<span class="normal"> 239</span>
<span class="normal"> 240</span>
<span class="normal"> 241</span>
<span class="normal"> 242</span>
<span class="normal"> 243</span>
<span class="normal"> 244</span>
<span class="normal"> 245</span>
<span class="normal"> 246</span>
<span class="normal"> 247</span>
<span class="normal"> 248</span>
<span class="normal"> 249</span>
<span class="normal"> 250</span>
<span class="normal"> 251</span>
<span class="normal"> 252</span>
<span class="normal"> 253</span>
<span class="normal"> 254</span>
<span class="normal"> 255</span>
<span class="normal"> 256</span>
<span class="normal"> 257</span>
<span class="normal"> 258</span>
<span class="normal"> 259</span>
<span class="normal"> 260</span>
<span class="normal"> 261</span>
<span class="normal"> 262</span>
<span class="normal"> 263</span>
<span class="normal"> 264</span>
<span class="normal"> 265</span>
<span class="normal"> 266</span>
<span class="normal"> 267</span>
<span class="normal"> 268</span>
<span class="normal"> 269</span>
<span class="normal"> 270</span>
<span class="normal"> 271</span>
<span class="normal"> 272</span>
<span class="normal"> 273</span>
<span class="normal"> 274</span>
<span class="normal"> 275</span>
<span class="normal"> 276</span>
<span class="normal"> 277</span>
<span class="normal"> 278</span>
<span class="normal"> 279</span>
<span class="normal"> 280</span>
<span class="normal"> 281</span>
<span class="normal"> 282</span>
<span class="normal"> 283</span>
<span class="normal"> 284</span>
<span class="normal"> 285</span>
<span class="normal"> 286</span>
<span class="normal"> 287</span>
<span class="normal"> 288</span>
<span class="normal"> 289</span>
<span class="normal"> 290</span>
<span class="normal"> 291</span>
<span class="normal"> 292</span>
<span class="normal"> 293</span>
<span class="normal"> 294</span>
<span class="normal"> 295</span>
<span class="normal"> 296</span>
<span class="normal"> 297</span>
<span class="normal"> 298</span>
<span class="normal"> 299</span>
<span class="normal"> 300</span>
<span class="normal"> 301</span>
<span class="normal"> 302</span>
<span class="normal"> 303</span>
<span class="normal"> 304</span>
<span class="normal"> 305</span>
<span class="normal"> 306</span>
<span class="normal"> 307</span>
<span class="normal"> 308</span>
<span class="normal"> 309</span>
<span class="normal"> 310</span>
<span class="normal"> 311</span>
<span class="normal"> 312</span>
<span class="normal"> 313</span>
<span class="normal"> 314</span>
<span class="normal"> 315</span>
<span class="normal"> 316</span>
<span class="normal"> 317</span>
<span class="normal"> 318</span>
<span class="normal"> 319</span>
<span class="normal"> 320</span>
<span class="normal"> 321</span>
<span class="normal"> 322</span>
<span class="normal"> 323</span>
<span class="normal"> 324</span>
<span class="normal"> 325</span>
<span class="normal"> 326</span>
<span class="normal"> 327</span>
<span class="normal"> 328</span>
<span class="normal"> 329</span>
<span class="normal"> 330</span>
<span class="normal"> 331</span>
<span class="normal"> 332</span>
<span class="normal"> 333</span>
<span class="normal"> 334</span>
<span class="normal"> 335</span>
<span class="normal"> 336</span>
<span class="normal"> 337</span>
<span class="normal"> 338</span>
<span class="normal"> 339</span>
<span class="normal"> 340</span>
<span class="normal"> 341</span>
<span class="normal"> 342</span>
<span class="normal"> 343</span>
<span class="normal"> 344</span>
<span class="normal"> 345</span>
<span class="normal"> 346</span>
<span class="normal"> 347</span>
<span class="normal"> 348</span>
<span class="normal"> 349</span>
<span class="normal"> 350</span>
<span class="normal"> 351</span>
<span class="normal"> 352</span>
<span class="normal"> 353</span>
<span class="normal"> 354</span>
<span class="normal"> 355</span>
<span class="normal"> 356</span>
<span class="normal"> 357</span>
<span class="normal"> 358</span>
<span class="normal"> 359</span>
<span class="normal"> 360</span>
<span class="normal"> 361</span>
<span class="normal"> 362</span>
<span class="normal"> 363</span>
<span class="normal"> 364</span>
<span class="normal"> 365</span>
<span class="normal"> 366</span>
<span class="normal"> 367</span>
<span class="normal"> 368</span>
<span class="normal"> 369</span>
<span class="normal"> 370</span>
<span class="normal"> 371</span>
<span class="normal"> 372</span>
<span class="normal"> 373</span>
<span class="normal"> 374</span>
<span class="normal"> 375</span>
<span class="normal"> 376</span>
<span class="normal"> 377</span>
<span class="normal"> 378</span>
<span class="normal"> 379</span>
<span class="normal"> 380</span>
<span class="normal"> 381</span>
<span class="normal"> 382</span>
<span class="normal"> 383</span>
<span class="normal"> 384</span>
<span class="normal"> 385</span>
<span class="normal"> 386</span>
<span class="normal"> 387</span>
<span class="normal"> 388</span>
<span class="normal"> 389</span>
<span class="normal"> 390</span>
<span class="normal"> 391</span>
<span class="normal"> 392</span>
<span class="normal"> 393</span>
<span class="normal"> 394</span>
<span class="normal"> 395</span>
<span class="normal"> 396</span>
<span class="normal"> 397</span>
<span class="normal"> 398</span>
<span class="normal"> 399</span>
<span class="normal"> 400</span>
<span class="normal"> 401</span>
<span class="normal"> 402</span>
<span class="normal"> 403</span>
<span class="normal"> 404</span>
<span class="normal"> 405</span>
<span class="normal"> 406</span>
<span class="normal"> 407</span>
<span class="normal"> 408</span>
<span class="normal"> 409</span>
<span class="normal"> 410</span>
<span class="normal"> 411</span>
<span class="normal"> 412</span>
<span class="normal"> 413</span>
<span class="normal"> 414</span>
<span class="normal"> 415</span>
<span class="normal"> 416</span>
<span class="normal"> 417</span>
<span class="normal"> 418</span>
<span class="normal"> 419</span>
<span class="normal"> 420</span>
<span class="normal"> 421</span>
<span class="normal"> 422</span>
<span class="normal"> 423</span>
<span class="normal"> 424</span>
<span class="normal"> 425</span>
<span class="normal"> 426</span>
<span class="normal"> 427</span>
<span class="normal"> 428</span>
<span class="normal"> 429</span>
<span class="normal"> 430</span>
<span class="normal"> 431</span>
<span class="normal"> 432</span>
<span class="normal"> 433</span>
<span class="normal"> 434</span>
<span class="normal"> 435</span>
<span class="normal"> 436</span>
<span class="normal"> 437</span>
<span class="normal"> 438</span>
<span class="normal"> 439</span>
<span class="normal"> 440</span>
<span class="normal"> 441</span>
<span class="normal"> 442</span>
<span class="normal"> 443</span>
<span class="normal"> 444</span>
<span class="normal"> 445</span>
<span class="normal"> 446</span>
<span class="normal"> 447</span>
<span class="normal"> 448</span>
<span class="normal"> 449</span>
<span class="normal"> 450</span>
<span class="normal"> 451</span>
<span class="normal"> 452</span>
<span class="normal"> 453</span>
<span class="normal"> 454</span>
<span class="normal"> 455</span>
<span class="normal"> 456</span>
<span class="normal"> 457</span>
<span class="normal"> 458</span>
<span class="normal"> 459</span>
<span class="normal"> 460</span>
<span class="normal"> 461</span>
<span class="normal"> 462</span>
<span class="normal"> 463</span>
<span class="normal"> 464</span>
<span class="normal"> 465</span>
<span class="normal"> 466</span>
<span class="normal"> 467</span>
<span class="normal"> 468</span>
<span class="normal"> 469</span>
<span class="normal"> 470</span>
<span class="normal"> 471</span>
<span class="normal"> 472</span>
<span class="normal"> 473</span>
<span class="normal"> 474</span>
<span class="normal"> 475</span>
<span class="normal"> 476</span>
<span class="normal"> 477</span>
<span class="normal"> 478</span>
<span class="normal"> 479</span>
<span class="normal"> 480</span>
<span class="normal"> 481</span>
<span class="normal"> 482</span>
<span class="normal"> 483</span>
<span class="normal"> 484</span>
<span class="normal"> 485</span>
<span class="normal"> 486</span>
<span class="normal"> 487</span>
<span class="normal"> 488</span>
<span class="normal"> 489</span>
<span class="normal"> 490</span>
<span class="normal"> 491</span>
<span class="normal"> 492</span>
<span class="normal"> 493</span>
<span class="normal"> 494</span>
<span class="normal"> 495</span>
<span class="normal"> 496</span>
<span class="normal"> 497</span>
<span class="normal"> 498</span>
<span class="normal"> 499</span>
<span class="normal"> 500</span>
<span class="normal"> 501</span>
<span class="normal"> 502</span>
<span class="normal"> 503</span>
<span class="normal"> 504</span>
<span class="normal"> 505</span>
<span class="normal"> 506</span>
<span class="normal"> 507</span>
<span class="normal"> 508</span>
<span class="normal"> 509</span>
<span class="normal"> 510</span>
<span class="normal"> 511</span>
<span class="normal"> 512</span>
<span class="normal"> 513</span>
<span class="normal"> 514</span>
<span class="normal"> 515</span>
<span class="normal"> 516</span>
<span class="normal"> 517</span>
<span class="normal"> 518</span>
<span class="normal"> 519</span>
<span class="normal"> 520</span>
<span class="normal"> 521</span>
<span class="normal"> 522</span>
<span class="normal"> 523</span>
<span class="normal"> 524</span>
<span class="normal"> 525</span>
<span class="normal"> 526</span>
<span class="normal"> 527</span>
<span class="normal"> 528</span>
<span class="normal"> 529</span>
<span class="normal"> 530</span>
<span class="normal"> 531</span>
<span class="normal"> 532</span>
<span class="normal"> 533</span>
<span class="normal"> 534</span>
<span class="normal"> 535</span>
<span class="normal"> 536</span>
<span class="normal"> 537</span>
<span class="normal"> 538</span>
<span class="normal"> 539</span>
<span class="normal"> 540</span>
<span class="normal"> 541</span>
<span class="normal"> 542</span>
<span class="normal"> 543</span>
<span class="normal"> 544</span>
<span class="normal"> 545</span>
<span class="normal"> 546</span>
<span class="normal"> 547</span>
<span class="normal"> 548</span>
<span class="normal"> 549</span>
<span class="normal"> 550</span>
<span class="normal"> 551</span>
<span class="normal"> 552</span>
<span class="normal"> 553</span>
<span class="normal"> 554</span>
<span class="normal"> 555</span>
<span class="normal"> 556</span>
<span class="normal"> 557</span>
<span class="normal"> 558</span>
<span class="normal"> 559</span>
<span class="normal"> 560</span>
<span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trainer is a simple but feature-complete training and eval loop for MindSpore, optimized for 🤗 Transformers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">_get_learning_rate</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedModel</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">TrainingArguments</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">map_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">BaseMapFunction</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dataset</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Dataset</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PreTrainedTokenizerBase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">PreTrainedModel</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compute_metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">EvalPrediction</span><span class="p">],</span> <span class="n">Dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TrainerCallback</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optimizers</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">preprocess_logits_for_metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the Trainer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (Trainer): The Trainer object itself.</span>
<span class="sd">            model (Union[PreTrainedModel, nn.Module]): The pre-trained model or neural network cell to be trained.</span>
<span class="sd">            args (TrainingArguments): The training arguments including hyperparameters and output directory.</span>
<span class="sd">            map_fn (Optional[Union[Callable, BaseMapFunction]]): Optional map function for data preprocessing.</span>
<span class="sd">            train_dataset (Optional[Dataset]): The training dataset.</span>
<span class="sd">            eval_dataset (Optional[Union[Dataset, Dict[str, Dataset]]]): The evaluation dataset.</span>
<span class="sd">            tokenizer (Optional[PreTrainedTokenizerBase]): The pre-trained tokenizer for tokenizing inputs.</span>
<span class="sd">            model_init (Optional[Callable[[], PreTrainedModel]]): Optional model initialization function.</span>
<span class="sd">            compute_metrics (Optional[Callable[[EvalPrediction], Dict]]): Optional function to compute evaluation metrics.</span>
<span class="sd">            callbacks (Optional[List[TrainerCallback]]): Optional list of trainer callbacks.</span>
<span class="sd">            optimizers (Tuple[nn.Optimizer, LearningRateSchedule]): Tuple of optimizer and learning rate scheduler.</span>
<span class="sd">            preprocess_logits_for_metrics (Optional[Callable[[mindspore.Tensor, mindspore.Tensor], mindspore.Tensor]]): Optional function to preprocess logits for metrics.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: If `model` or `model_init` is not provided.</span>
<span class="sd">            ValueError: If the provided model cannot be used for training, or if there is an issue with the map function.</span>
<span class="sd">            ValueError: If `train_dataset` does not implement __len__ and `max_steps` is not specified.</span>
<span class="sd">            RuntimeError: If there is a conflict between `model_init` and `optimizers` arguments.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;tmp_trainer&quot;</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No `TrainingArguments` passed, using `output_dir=</span><span class="si">{</span><span class="n">output_dir</span><span class="si">}</span><span class="s2">`.&quot;</span><span class="p">)</span>
            <span class="n">args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="c1"># Seed must be set before instantiating the model when using model</span>
        <span class="c1"># mindspore do not support full determinisim on 2.2</span>
        <span class="n">enable_full_determinism</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">full_determinism</span> <span class="k">else</span> <span class="n">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_in_train</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">create_accelerator_and_postprocess</span><span class="p">()</span>

        <span class="c1"># set the correct log level depending on the node</span>
        <span class="n">log_level</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">get_process_log_level</span><span class="p">()</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">log_level</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">model_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_init</span> <span class="o">=</span> <span class="n">model_init</span>
                <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_model_init</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;`Trainer` requires either a `model` or `model_init` argument&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">model_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;`Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will&quot;</span>
                    <span class="s2">&quot; overwrite your model when calling the `train` method&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_init</span> <span class="o">=</span> <span class="n">model_init</span>

        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="n">MODEL_MAPPING_NAMES</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The model you have picked (</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">) cannot be used as is for training: it only &quot;</span>
                <span class="s2">&quot;computes hidden states and does not accept any labels. You should choose a model with a head &quot;</span>
                <span class="s2">&quot;suitable for your task like any of the `AutoModelForXxx` listed at &quot;</span>
                <span class="s2">&quot;https://huggingface.co/docs/transformers/model_doc/auto&quot;</span>
            <span class="p">)</span>

        <span class="c1"># if hasattr(model, &quot;is_parallelizable&quot;) and model.is_parallelizable and model.model_parallel:</span>
        <span class="c1">#     self.is_model_parallel = True</span>
        <span class="c1"># else:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_model_parallel</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># TODO: support quantized model</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">data_map_fn</span> <span class="o">=</span> <span class="n">map_fn</span>
        <span class="k">if</span> <span class="n">map_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">map_fn</span><span class="p">,</span> <span class="s1">&#39;input_columns&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">map_fn</span><span class="p">,</span> <span class="s1">&#39;output_columns&#39;</span><span class="p">))</span> <span class="ow">and</span> \
            <span class="ow">not</span> <span class="n">check_input_output_count</span><span class="p">(</span><span class="n">map_fn</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`map_fn` must have same number of inputs and outputs when it is callable function&#39;</span>
                             <span class="s1">&#39; without attributes `input_columns` and `output_columns`&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>

        <span class="c1"># later use `self.model is self.model_wrapped` to check if it&#39;s wrapped or not</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">neftune_noise_alpha</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">neftune_noise_alpha</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span> <span class="o">=</span> <span class="n">compute_metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_logits_for_metrics</span> <span class="o">=</span> <span class="n">preprocess_logits_for_metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">optimizers</span>
        <span class="k">if</span> <span class="n">model_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Passing a `model_init` is incompatible with providing the `optimizers` argument. &quot;</span>
                <span class="s2">&quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;</span>
            <span class="p">)</span>

        <span class="n">default_callbacks</span> <span class="o">=</span> <span class="n">DEFAULT_CALLBACKS</span>
        <span class="n">callbacks</span> <span class="o">=</span> <span class="n">default_callbacks</span> <span class="k">if</span> <span class="n">callbacks</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">default_callbacks</span> <span class="o">+</span> <span class="n">callbacks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span> <span class="o">=</span> <span class="n">CallbackHandler</span><span class="p">(</span>
            <span class="n">callbacks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span><span class="n">PrinterCallback</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">disable_tqdm</span> <span class="k">else</span> <span class="n">DEFAULT_PROGRESS_CALLBACK</span><span class="p">)</span>

        <span class="c1"># Will be set to True by `self._setup_loggers()` on first call to `self.log()`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loggers_initialized</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">should_save</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;max_steps is given, it will override any value given in num_train_epochs&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">train_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">has_length</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The train_dataset does not implement __len__, max_steps has to be specified. &quot;</span>
                <span class="s2">&quot;The number of steps needs to be known in advance for the learning rate scheduler.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_signature_columns</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Mixed precision setup</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_amp</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Label smoothing</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">label_smoothing_factor</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">label_smoother</span> <span class="o">=</span> <span class="n">LabelSmoother</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">label_smoothing_factor</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">label_smoother</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">TrainerState</span><span class="p">(</span>
            <span class="n">is_local_process_zero</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_local_process_zero</span><span class="p">(),</span>
            <span class="n">is_world_process_zero</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="n">TrainerControl</span><span class="p">()</span>
        <span class="c1"># Internal variable to count flos in each process, will be accumulated in `self.state.total_flos` then</span>
        <span class="c1"># returned to 0 every time flos need to be logged</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">current_flos</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hp_search_backend</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">default_label_names</span> <span class="o">=</span> <span class="n">find_labels</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_names</span> <span class="o">=</span> <span class="n">default_label_names</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">label_names</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">label_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">can_return_loss</span> <span class="o">=</span> <span class="n">can_return_loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_init_end</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>
        <span class="c1"># Internal variables to help with automatic batch size reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">train_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_created_lr_scheduler</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">create_accelerator_and_postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># We explicitly don&#39;t rely on the `Accelerator` to do gradient accumulation</span>
        <span class="n">grad_acc_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">is_accelerate_available</span><span class="p">(</span><span class="s2">&quot;0.28.0&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">accelerator_config</span><span class="o">.</span><span class="n">gradient_accumulation_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">grad_acc_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">accelerator_config</span><span class="o">.</span><span class="n">gradient_accumulation_kwargs</span>

        <span class="c1"># check if num_steps is attempted to be passed in gradient_accumulation_kwargs</span>
        <span class="k">if</span> <span class="s2">&quot;num_steps&quot;</span> <span class="ow">in</span> <span class="n">grad_acc_kwargs</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># raise because we do not know which setting is intended.</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The `AcceleratorConfig`&#39;s `num_steps` is set but `gradient_accumulation_steps` is greater than 1 in the passed `TrainingArguments`&quot;</span>
                    <span class="s2">&quot;If using the passed `AcceleratorConfig` is desired, do not set the `TrainingArguments` `gradient_accumulation_steps`.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="n">grad_acc_kwargs</span><span class="p">[</span><span class="s2">&quot;num_steps&quot;</span><span class="p">]</span>

        <span class="n">accelerator_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">accelerator_config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">is_accelerate_available</span><span class="p">(</span><span class="s2">&quot;0.28.0&quot;</span><span class="p">):</span>
            <span class="kn">from</span> <span class="nn">accelerate.utils</span> <span class="kn">import</span> <span class="n">DataLoaderConfiguration</span>
            <span class="n">dataloader_config</span> <span class="o">=</span> <span class="n">DataLoaderConfiguration</span><span class="p">(</span>
                <span class="n">split_batches</span><span class="o">=</span><span class="n">accelerator_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;split_batches&quot;</span><span class="p">),</span>
                <span class="n">dispatch_batches</span><span class="o">=</span><span class="n">accelerator_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;dispatch_batches&quot;</span><span class="p">),</span>
                <span class="n">even_batches</span><span class="o">=</span><span class="n">accelerator_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;even_batches&quot;</span><span class="p">),</span>
                <span class="n">use_seedable_sampler</span><span class="o">=</span><span class="n">accelerator_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_seedable_sampler&quot;</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">is_accelerate_available</span><span class="p">(</span><span class="s2">&quot;1.1.0&quot;</span><span class="p">):</span>
                <span class="n">dataloader_config</span><span class="o">.</span><span class="n">data_seed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">data_seed</span>

        <span class="n">non_blocking</span> <span class="o">=</span> <span class="n">accelerator_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;non_blocking&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_accelerate_available</span><span class="p">(</span><span class="s2">&quot;0.30.0&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">non_blocking</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                    <span class="s2">&quot;`non_blocking` is only supported in accelerate v0.30.0 and above. Please upgrade accelerate to use this feature.&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">non_blocking</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataloader_pin_memory</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;`non_blocking` is enabled but `dataloader_pin_memory` is not. For the best performance, it&#39;s recommended to enable both.&quot;</span>
                <span class="p">)</span>
            <span class="n">dataloader_config</span><span class="o">.</span><span class="n">non_blocking</span> <span class="o">=</span> <span class="n">non_blocking</span>
        <span class="c1"># this would have been updated above, no need for it anymore</span>
        <span class="n">accelerator_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;gradient_accumulation_kwargs&quot;</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;deepspeed_plugin&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">deepspeed_plugin</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">is_accelerate_available</span><span class="p">(</span><span class="s2">&quot;0.28.0&quot;</span><span class="p">):</span>
            <span class="n">args</span><span class="p">[</span><span class="s2">&quot;dataloader_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataloader_config</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">accelerator_config</span><span class="p">)</span>

        <span class="c1"># create accelerator object</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span>
        <span class="c1"># some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gather_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">gather_for_metrics</span>

        <span class="k">if</span> <span class="s2">&quot;use_gather_object&quot;</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gather_function</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gather_function</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gather_function</span><span class="p">,</span> <span class="n">use_gather_object</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_use_gather_object</span>
            <span class="p">)</span>

        <span class="c1"># deepspeed and accelerate flags covering both trainer args and accelerate launcher</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_deepspeed_enabled</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="s2">&quot;deepspeed_plugin&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_fsdp_enabled</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="s2">&quot;fsdp_plugin&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="c1"># post accelerator creation setup</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_fsdp_enabled</span><span class="p">:</span>
            <span class="n">fsdp_plugin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">fsdp_plugin</span>
            <span class="n">fsdp_plugin</span><span class="o">.</span><span class="n">limit_all_gathers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">fsdp_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="s2">&quot;limit_all_gathers&quot;</span><span class="p">,</span> <span class="n">fsdp_plugin</span><span class="o">.</span><span class="n">limit_all_gathers</span>
            <span class="p">)</span>
            <span class="n">fsdp_plugin</span><span class="o">.</span><span class="n">activation_checkpointing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">fsdp_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="s2">&quot;activation_checkpointing&quot;</span><span class="p">,</span> <span class="n">fsdp_plugin</span><span class="o">.</span><span class="n">activation_checkpointing</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">fsdp_plugin</span><span class="o">.</span><span class="n">activation_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">gradient_checkpointing</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The activation_checkpointing in FSDP config and the gradient_checkpointing in training arg &quot;</span>
                    <span class="s2">&quot;can&#39;t be set to True simultaneously. Please use FSDP&#39;s activation_checkpointing logic &quot;</span>
                    <span class="s2">&quot;when using FSDP.&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_deepspeed_enabled</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="s2">&quot;hf_deepspeed_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">propagate_args_to_deepspeed</span><span class="p">()</span>

        <span class="c1"># `save_only_model` can&#39;t be used with DeepSpeed/FSDP along with `load_best_model_at_end`</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_only_model</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_deepspeed_enabled</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_fsdp_enabled</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">load_best_model_at_end</span>
        <span class="p">):</span>
            <span class="n">wrapper</span> <span class="o">=</span> <span class="s2">&quot;DeepSpeed&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_deepspeed_enabled</span> <span class="k">else</span> <span class="s2">&quot;FSDP&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">wrapper</span><span class="si">}</span><span class="s2"> can&#39;t be used with `save_only_model` along with `load_best_model_at_end`.&quot;</span><span class="p">)</span>

        <span class="c1"># `auto_find_batch_size` isn&#39;t supported yet with DeepSpeed Zero-3</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_deepspeed_enabled</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">deepspeed_plugin</span><span class="o">.</span><span class="n">zero_stage</span> <span class="o">==</span> <span class="mi">3</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">auto_find_batch_size</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`auto_find_batch_size` isn&#39;t supported yet with DeepSpeed Zero-3. Please consider using Zero-2, Zero-1, or FSDP&quot;</span>
            <span class="p">)</span>


    <span class="k">def</span> <span class="nf">_activate_neftune</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Activates the neftune as presented in this code: https://github.com/neelsjain/NEFTune and paper:</span>
<span class="sd">        https://arxiv.org/abs/2310.05914</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: support neftune</span>
        <span class="n">unwrapped_model</span> <span class="o">=</span> <span class="n">model</span>

        <span class="k">if</span> <span class="n">_is_peft_model</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="p">):</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">unwrapped_model</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">unwrapped_model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>

        <span class="k">del</span> <span class="n">unwrapped_model</span>

        <span class="n">embeddings</span><span class="o">.</span><span class="n">neftune_noise_alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neftune_noise_alpha</span>
        <span class="n">hook_handle</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">neftune_post_forward_hook</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neftune_hook_handle</span> <span class="o">=</span> <span class="n">hook_handle</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">_deactivate_neftune</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deactivates the neftune method. Make sure to call `_activate_neftune` first.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: support neftune</span>
        <span class="c1"># if not hasattr(self, &quot;neftune_hook_handle&quot;):</span>
        <span class="c1">#     raise ValueError(&quot;Neftune is not activated make sure to call `trainer._activate_neftune()` first&quot;)</span>

        <span class="n">unwrapped_model</span> <span class="o">=</span> <span class="n">model</span>

        <span class="k">if</span> <span class="n">_is_peft_model</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="p">):</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">unwrapped_model</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">unwrapped_model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">neftune_hook_handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
        <span class="k">del</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">neftune_noise_alpha</span><span class="p">,</span> <span class="n">unwrapped_model</span>

    <span class="k">def</span> <span class="nf">add_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callback</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a callback to the current list of [`~transformers.TrainerCallback`].</span>

<span class="sd">        Args:</span>
<span class="sd">           callback (`type` or [`~transformers.TrainerCallback`]):</span>
<span class="sd">               A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the</span>
<span class="sd">               first case, will instantiate a member of that class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pop_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callback</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Remove a callback from the current list of [`~transformers.TrainerCallback`] and returns it.</span>

<span class="sd">        If the callback is not found, returns `None` (and no error is raised).</span>

<span class="sd">        Args:</span>
<span class="sd">           callback (`type` or [`~transformers.TrainerCallback`]):</span>
<span class="sd">               A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the</span>
<span class="sd">               first case, will pop the first member of that class found in the list of callbacks.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`~transformers.TrainerCallback`]: The callback removed, if found.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">pop_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">remove_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callback</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Remove a callback from the current list of [`~transformers.TrainerCallback`].</span>

<span class="sd">        Args:</span>
<span class="sd">           callback (`type` or [`~transformers.TrainerCallback`]):</span>
<span class="sd">               A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the</span>
<span class="sd">               first case, will remove the first member of that class found in the list of callbacks.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">remove_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_signature_columns_if_needed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to set signature columns if they are not already set.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (Trainer): The instance of the Trainer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: If the model does not have a &#39;forward&#39; method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_signature_columns</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Inspect model forward signature to keep only the arguments it accepts.</span>
            <span class="n">model_to_inspect</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>
            <span class="k">if</span> <span class="n">_is_peft_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;get_base_model&quot;</span><span class="p">):</span>
                    <span class="n">model_to_inspect</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_base_model</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># PeftMixedModel do not provide a `get_base_model` method</span>
                    <span class="n">model_to_inspect</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">model</span>
            <span class="n">signature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">model_to_inspect</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_signature_columns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">signature</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

            <span class="c1"># Labels may be named label or label_ids, the default data collator handles that.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_signature_columns</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">([</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;label_ids&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_names</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_remove_unused_columns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="s2">&quot;mindspore.dataset.Dataset&quot;</span><span class="p">,</span> <span class="n">description</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; </span>
<span class="sd">        Method _remove_unused_columns in the class Trainer removes unused columns from the input dataset if the corresponding argument is set to True.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the Trainer class.</span>
<span class="sd">            dataset (mindspore.dataset.Dataset): The input dataset from which the unused columns need to be removed.</span>
<span class="sd">            description (Optional[str]): An optional description of the dataset. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">remove_unused_columns</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_signature_columns_if_needed</span><span class="p">()</span>
        <span class="n">signature_columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_signature_columns</span>

        <span class="n">ignored_columns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">get_col_names</span><span class="p">())</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">signature_columns</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ignored_columns</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">dset_description</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">description</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;in the </span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="s2"> set&quot;</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The following columns </span><span class="si">{</span><span class="n">dset_description</span><span class="si">}</span><span class="s2"> don&#39;t have a corresponding argument in &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.forward` and have been ignored: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ignored_columns</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; If </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ignored_columns</span><span class="p">)</span><span class="si">}</span><span class="s2"> are not expected by `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.forward`, &quot;</span>
                <span class="s2">&quot; you can safely ignore this message.&quot;</span>
            <span class="p">)</span>

        <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_col_names</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ignored_columns</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">dataset</span><span class="o">.</span><span class="n">project</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">create_optimizer_and_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setup the optimizer and the learning rate scheduler.</span>

<span class="sd">        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the</span>
<span class="sd">        Trainer&#39;s init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or</span>
<span class="sd">        `create_scheduler`) in a subclass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">create_optimizer</span><span class="p">()</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">create_scheduler</span><span class="p">(</span><span class="n">num_training_steps</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_decay_parameter_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get all parameter names that weight decay will be applied to</span>

<span class="sd">        Note that some models implement their own layernorm instead of calling nn.LayerNorm, weight decay could still</span>
<span class="sd">        apply to those modules since this function only filter out instance of nn.LayerNorm</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">decay_parameters</span> <span class="o">=</span> <span class="n">get_parameter_names</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ALL_LAYERNORM_LAYERS</span><span class="p">)</span>
        <span class="n">decay_parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">decay_parameters</span>

    <span class="k">def</span> <span class="nf">create_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setup the optimizer.</span>

<span class="sd">        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the</span>
<span class="sd">        Trainer&#39;s init through `optimizers`, or subclass and override this method in a subclass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">opt_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decay_parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_decay_parameter_names</span><span class="p">(</span><span class="n">opt_model</span><span class="p">)</span>
            <span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">opt_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
                    <span class="p">],</span>
                    <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">opt_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
                    <span class="p">],</span>
                    <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">]</span>

            <span class="n">optimizer_cls</span><span class="p">,</span> <span class="n">optimizer_kwargs</span> <span class="o">=</span> <span class="n">Trainer</span><span class="o">.</span><span class="n">get_optimizer_cls_and_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">opt_model</span><span class="p">)</span>

            <span class="c1"># Overwrite `params` in case it&#39;s created by `get_optimizer_cls_and_kwargs`</span>
            <span class="c1"># e.g. for GaLore optimizer.</span>
            <span class="k">if</span> <span class="s2">&quot;params&quot;</span> <span class="ow">in</span> <span class="n">optimizer_kwargs</span><span class="p">:</span>
                <span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="n">optimizer_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;params&quot;</span><span class="p">)</span>

            <span class="c1"># For layer-wise dummy optimizers we overwrite optimizer_grouped_parameters with `optimizer_dict`</span>
            <span class="c1"># to avoid arguments conflicts.</span>
            <span class="k">if</span> <span class="s2">&quot;optimizer_dict&quot;</span> <span class="ow">in</span> <span class="n">optimizer_kwargs</span><span class="p">:</span>
                <span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="n">optimizer_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;optimizer_dict&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="p">(</span><span class="n">optimizer_grouped_parameters</span><span class="p">,</span> <span class="o">**</span><span class="n">optimizer_kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_optimizer_cls_and_kwargs</span><span class="p">(</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PreTrainedModel</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the optimizer class and optimizer parameters based on the training arguments.</span>

<span class="sd">        Args:</span>
<span class="sd">            args (`transformers.training_args.TrainingArguments`):</span>
<span class="sd">                The training arguments for the training session.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># parse args.optim_args</span>
        <span class="n">optim_args</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">optim_args</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">mapping</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">optim_args</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">):</span>
                <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">mapping</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="p">)</span>
                <span class="n">optim_args</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="n">optimizer_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">}</span>

        <span class="n">adam_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;betas&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">adam_beta1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">adam_beta2</span><span class="p">),</span>
            <span class="s2">&quot;eps&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">adam_epsilon</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="c1"># TODO: support Adafactor</span>
        <span class="c1"># if args.optim == OptimizerNames.ADAFACTOR:</span>
        <span class="c1">#     optimizer_cls = Adafactor</span>
        <span class="c1">#     optimizer_kwargs.update({&quot;scale_parameter&quot;: False, &quot;relative_step&quot;: False})</span>
        <span class="c1"># TODO: support AdamW huggingface version</span>
        <span class="c1"># elif args.optim == OptimizerNames.ADAMW_HF:</span>
        <span class="c1">#     from .optimization import AdamW</span>

        <span class="c1">#     optimizer_cls = AdamW</span>
        <span class="c1">#     optimizer_kwargs.update(adam_kwargs)</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="n">OptimizerNames</span><span class="o">.</span><span class="n">ADAMW</span><span class="p">:</span>
            <span class="n">optimizer_cls</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span>
            <span class="n">optimizer_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adam_kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="n">OptimizerNames</span><span class="o">.</span><span class="n">SGD</span><span class="p">:</span>
            <span class="n">optimizer_cls</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span>
        <span class="c1"># TODO: support Adagrad and Rmsporp</span>
        <span class="c1"># elif args.optim == OptimizerNames.ADAGRAD:</span>
        <span class="c1">#     optimizer_cls = mindspore.nn.Adagrad</span>
        <span class="c1"># elif args.optim == OptimizerNames.RMSPROP:</span>
        <span class="c1">#     optimizer_cls = mindspore.nn.RMSprop</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Trainer cannot instantiate unsupported optimizer: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">optim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optimizer_cls</span><span class="p">,</span> <span class="n">optimizer_kwargs</span>

    <span class="k">def</span> <span class="nf">create_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or</span>
<span class="sd">        passed as an argument.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_training_steps (int): The number of training steps to do.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">...transformers.optimization</span> <span class="kn">import</span> <span class="n">get_scheduler</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">get_scheduler</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">lr_scheduler_type</span><span class="p">,</span>
                <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">optimizer</span><span class="p">,</span>
                <span class="n">num_warmup_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">get_warmup_steps</span><span class="p">(</span><span class="n">num_training_steps</span><span class="p">),</span>
                <span class="n">num_training_steps</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">,</span>
                <span class="n">scheduler_specific_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">lr_scheduler_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_created_lr_scheduler</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span>

    <span class="k">def</span> <span class="nf">num_examples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="s1">&#39;mindspore.dataset.Dataset&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper to get number of samples in a [`~mindspore.dataset.GeneratorDataset`] by accessing its dataset. When</span>
<span class="sd">        dataloader.dataset does not exist or has no length, estimates as best it can</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">()</span> <span class="o">*</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_batch_size</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">num_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_ds</span><span class="p">:</span> <span class="s1">&#39;mindspore.dataset.Dataset&#39;</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper to get number of tokens in a [`~mindspore.dataset.GeneratorDataset`] by enumerating dataloader.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">train_tokens</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">max_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">tokens</span> <span class="o">*</span> <span class="n">max_steps</span>
                <span class="n">train_tokens</span> <span class="o">+=</span> <span class="n">tokens</span>
            <span class="k">return</span> <span class="n">train_tokens</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Cannot get num_tokens from dataloader&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">train_tokens</span>

    <span class="k">def</span> <span class="nf">call_model_init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to call the model initialization function and validate its output.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (Trainer): Instance of the Trainer class.</span>
<span class="sd">                This parameter represents the current instance of the Trainer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: If the model_init method does not have 0 or 1 arguments.</span>
<span class="sd">                This exception is raised when the number of arguments in the model_init method is not 0 or 1.</span>
<span class="sd">            RuntimeError: If the model_init method returns None.</span>
<span class="sd">                This exception is raised when the model_init method returns None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_init_argcount</span> <span class="o">=</span> <span class="n">number_of_arguments</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_init</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model_init_argcount</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_init</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;model_init should have 0 or 1 argument.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;model_init should not return None.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span>

    <span class="nd">@lru_cache</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_train_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dataset</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the training [`~mindspore.dataset.GeneratorDataset`].</span>

<span class="sd">        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed</span>
<span class="sd">        training if necessary) otherwise.</span>

<span class="sd">        Subclass and override this method if you want to inject some custom behavior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trainer: training requires a train_dataset.&quot;</span><span class="p">)</span>

        <span class="n">train_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span>
        <span class="n">map_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_map_fn</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="p">(</span><span class="n">BatchDataset</span><span class="p">,</span> <span class="n">PaddedBatchDataset</span><span class="p">)):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_map_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The trainer has been passed a `map_fn` and found `BatchDataset` at same time, &quot;</span>
                               <span class="s2">&quot;the `map_fn` will be ignored.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">train_dataset</span>

        <span class="k">if</span> <span class="n">map_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">mismatch_dataset_col_names</span><span class="p">(</span><span class="n">get_function_args</span><span class="p">(</span><span class="n">map_fn</span><span class="p">),</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">get_col_names</span><span class="p">()):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The arguments of `map_fn` must be subset of useful dataset columns, &#39;</span>
                                <span class="sa">f</span><span class="s1">&#39;but found </span><span class="si">{</span><span class="n">args_only_in_map_fn</span><span class="p">(</span><span class="n">get_function_args</span><span class="p">(</span><span class="n">map_fn</span><span class="p">),</span><span class="w"> </span><span class="n">train_dataset</span><span class="o">.</span><span class="n">get_col_names</span><span class="p">())</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">map_fn</span><span class="p">,</span> <span class="n">map_fn</span><span class="o">.</span><span class="n">input_columns</span><span class="p">,</span> <span class="n">map_fn</span><span class="o">.</span><span class="n">output_columns</span><span class="p">)</span>

        <span class="n">train_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_remove_unused_columns</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>

        <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_drop_last</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_num_workers</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">train_dataset</span>

    <span class="nd">@lru_cache</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_test_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dataset</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the test [`~mindspore.dataset.GeneratorDataset`].</span>

<span class="sd">        Subclass and override this method if you want to inject some custom behavior.</span>

<span class="sd">        Args:</span>
<span class="sd">            test_dataset (`mindspore.dataset`, *optional*):</span>
<span class="sd">                The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the</span>
<span class="sd">                `model.forward()` method are automatically removed. It must implement `__len__`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># data_collator = self.data_collator</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="p">(</span><span class="n">BatchDataset</span><span class="p">,</span> <span class="n">PaddedBatchDataset</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">test_dataset</span>
        <span class="n">test_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_remove_unused_columns</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
        <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_drop_last</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_num_workers</span><span class="p">)</span>

        <span class="c1"># We use the same batch_size as for eval.</span>
        <span class="k">return</span> <span class="n">test_dataset</span>

    <span class="nd">@lru_cache</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_eval_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Dataset</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dataset</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the test [`~mindspore.dataset.GeneratorDataset`].</span>

<span class="sd">        Subclass and override this method if you want to inject some custom behavior.</span>

<span class="sd">        Args:</span>
<span class="sd">            test_dataset (`mindspore.dataset`, *optional*):</span>
<span class="sd">                The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the</span>
<span class="sd">                `model.forward()` method are automatically removed. It must implement `__len__`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># data_collator = self.data_collator</span>
        <span class="k">if</span> <span class="n">eval_dataset</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_dataset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trainer: evaluation requires an eval_dataset.&quot;</span><span class="p">)</span>

        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">eval_dataset</span> <span class="k">if</span> <span class="n">eval_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_dataset</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="p">(</span><span class="n">BatchDataset</span><span class="p">,</span> <span class="n">PaddedBatchDataset</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">eval_dataset</span>

        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_remove_unused_columns</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">eval_dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_drop_last</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_num_workers</span><span class="p">)</span>

        <span class="c1"># We use the same batch_size as for eval.</span>
        <span class="k">return</span> <span class="n">eval_dataset</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">resume_from_checkpoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ignore_keys_for_eval</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Main training entry point.</span>

<span class="sd">        Args:</span>
<span class="sd">            resume_from_checkpoint (`str` or `bool`, *optional*):</span>
<span class="sd">                If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a</span>
<span class="sd">                `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance</span>
<span class="sd">                of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.</span>
<span class="sd">            ignore_keys_for_eval (`List[str]`, *optional*)</span>
<span class="sd">                A list of keys in the output of your model (if it is a dictionary) that should be ignored when</span>
<span class="sd">                gathering predictions for evaluation during the training.</span>
<span class="sd">            kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">                Additional keyword arguments used to hide deprecated arguments</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">resume_from_checkpoint</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">resume_from_checkpoint</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_in_train</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Attach NEFTune hooks if necessary</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">neftune_noise_alpha</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activate_neftune</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;train() received got unexpected keyword arguments: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="c1"># This might change the seed so needs to run first.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">train_batch_size</span>
        <span class="c1"># Model re-init</span>
        <span class="n">model_reloaded</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Seed must be set before instantiating the model when using model_init.</span>
            <span class="n">enable_full_determinism</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">full_determinism</span> <span class="k">else</span> <span class="n">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_model_init</span><span class="p">()</span>
            <span class="n">model_reloaded</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># Reinitializes optimizer and scheduler</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model_reload</span> <span class="o">=</span> <span class="n">model_reloaded</span>

        <span class="c1"># Load potential model checkpoint</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span> <span class="ow">and</span> <span class="n">resume_from_checkpoint</span><span class="p">:</span>
            <span class="n">resume_from_checkpoint</span> <span class="o">=</span> <span class="n">get_last_checkpoint</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">resume_from_checkpoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No valid checkpoint found in output directory (</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">resume_from_checkpoint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_load_from_checkpoint</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">)</span>
            <span class="c1"># In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">TrainerState</span><span class="o">.</span><span class="n">load_from_json</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">TRAINER_STATE_NAME</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">train_batch_size</span>

        <span class="k">if</span> <span class="n">model_reloaded</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_wrapped</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>

        <span class="n">inner_training_loop</span> <span class="o">=</span> <span class="n">find_executable_batch_size</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_inner_training_loop</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">auto_find_batch_size</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">inner_training_loop</span><span class="p">(</span>
            <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
            <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="n">resume_from_checkpoint</span><span class="p">,</span>
            <span class="n">ignore_keys_for_eval</span><span class="o">=</span><span class="n">ignore_keys_for_eval</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_load_best_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Load the best model checkpoint.</span>

<span class="sd">        This method loads the best model checkpoint based on the provided state. The best model checkpoint is determined by the highest score achieved during training.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the Trainer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. The method only loads the best model checkpoint.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        Note:</span>
<span class="sd">            This method assumes that the best model checkpoint is saved in the specified checkpoint directory. If the best model checkpoint is not found, a warning message will be logged.</span>

<span class="sd">            If the model is a PEFT model, the method checks if the active adapter is available and loads the adapter model if it exists. Otherwise, a warning message is logged.</span>

<span class="sd">            If `save_safetensors` flag is enabled and the best safe model checkpoint is available, the method loads the safe model state dictionary. Otherwise, it loads the model state dictionary using</span>
<span class="sd">MindSpore&#39;s `load_checkpoint` function.</span>

<span class="sd">            If the best model checkpoint is not found, but the weights index file exists, the method attempts to load the sharded checkpoint using the `load_sharded_checkpoint` function.</span>

<span class="sd">            If the best model checkpoint is not found and `save_on_each_node` is not activated during distributed training, a warning message is logged.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading best model from </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span><span class="si">}</span><span class="s2"> (score: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_metric</span><span class="si">}</span><span class="s2">).&quot;</span><span class="p">)</span>
        <span class="n">best_model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">)</span>
        <span class="n">best_safe_model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span><span class="p">,</span> <span class="n">SAFE_WEIGHTS_NAME</span><span class="p">)</span>
        <span class="n">best_adapter_model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span><span class="p">,</span> <span class="n">ADAPTER_WEIGHTS_NAME</span><span class="p">)</span>
        <span class="n">best_safe_adapter_model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span><span class="p">,</span> <span class="n">ADAPTER_SAFE_WEIGHTS_NAME</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">best_model_path</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">best_safe_model_path</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">best_adapter_model_path</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">best_safe_adapter_model_path</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">has_been_loaded</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="k">if</span> <span class="n">_is_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
                <span class="c1"># If train a model using PEFT &amp; LoRA, assume that adapter have been saved properly.</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;active_adapter&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;load_adapter&quot;</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">best_adapter_model_path</span><span class="p">)</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">best_safe_adapter_model_path</span><span class="p">):</span>
                        <span class="n">model</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">)</span>
                        <span class="c1"># Load_adapter has no return value present, modify it when appropriate.</span>
                        <span class="n">load_result</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="s2">&quot;The intermediate checkpoints of PEFT may not be saved correctly, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;consider using a custom callback to save </span><span class="si">{</span><span class="n">ADAPTER_WEIGHTS_NAME</span><span class="si">}</span><span class="s2"> in corresponding saving folders. &quot;</span>
                            <span class="s2">&quot;Check some examples here: https://github.com/huggingface/peft/issues/96&quot;</span>
                        <span class="p">)</span>
                        <span class="n">has_been_loaded</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Could not load adapter model, make sure to have `peft&gt;=0.3.0` installed&quot;</span><span class="p">)</span>
                    <span class="n">has_been_loaded</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># We load the model state dict on the CPU to avoid an OOM error.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_safetensors</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">best_safe_model_path</span><span class="p">):</span>
                    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">safe_load_file</span><span class="p">(</span><span class="n">best_safe_model_path</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span>
                        <span class="n">best_model_path</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="c1"># If the model is on the GPU, it still works!</span>
                <span class="c1"># which takes *args instead of **kwargs</span>
                <span class="n">load_result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">has_been_loaded</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_issue_warnings_after_load</span><span class="p">(</span><span class="n">load_result</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span><span class="p">,</span> <span class="n">WEIGHTS_INDEX_NAME</span><span class="p">)):</span>
            <span class="n">load_result</span> <span class="o">=</span> <span class="n">load_sharded_checkpoint</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_issue_warnings_after_load</span><span class="p">(</span><span class="n">load_result</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Could not locate the best model at </span><span class="si">{</span><span class="n">best_model_path</span><span class="si">}</span><span class="s2">, if you are running a distributed training &quot;</span>
                <span class="s2">&quot;on multiple nodes, you should activate `--save_on_each_node`.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_output_dir</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method retrieves the output directory from the Trainer object.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: Trainer object instance.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. Returns the output directory path specified in the Trainer object.</span>

<span class="sd">        Raises:</span>
<span class="sd">            This method does not raise any exceptions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">run_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span>
        <span class="k">return</span> <span class="n">run_dir</span>

    <span class="k">def</span> <span class="nf">_inner_training_loop</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_keys_for_eval</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method _inner_training_loop in the class Trainer.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (Trainer): The Trainer object.</span>
<span class="sd">            batch_size (int): The batch size for training.</span>
<span class="sd">            args (dict): Additional arguments for training configuration.</span>
<span class="sd">            resume_from_checkpoint (str): Path to a checkpoint to resume training from.</span>
<span class="sd">            ignore_keys_for_eval (list): List of keys to ignore during evaluation.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If args.max_steps must be set to a positive value if dataloader does not have a length.</span>
<span class="sd">            Warning: If there are no samples in the epoch_iterator during training.</span>
<span class="sd">            Exception: Any unexpected exceptions raised during the training process.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">auto_find_batch_size</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Currently training with a batch size of: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># Dataset and number of training steps</span>
        <span class="n">train_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_train_dataset</span><span class="p">()</span>

        <span class="c1"># Setting up training control variables:</span>
        <span class="c1"># number of training epochs: num_train_epochs</span>
        <span class="c1"># number of training steps per epoch: num_update_steps_per_epoch</span>
        <span class="c1"># total number of training steps to execute: max_steps</span>
        <span class="n">total_train_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">world_size</span>

        <span class="n">len_dataset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">num_train_tokens</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">has_length</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">):</span>
            <span class="n">len_dataset</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
            <span class="n">num_update_steps_per_epoch</span> <span class="o">=</span> <span class="n">len_dataset</span> <span class="o">//</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>
            <span class="n">num_update_steps_per_epoch</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">num_update_steps_per_epoch</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">num_examples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_examples</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">max_steps</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span>
                <span class="n">num_train_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">//</span> <span class="n">num_update_steps_per_epoch</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span>
                    <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">%</span> <span class="n">num_update_steps_per_epoch</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="p">)</span>
                <span class="c1"># May be slightly incorrect if the last batch in the training dataloader has a smaller size but it&#39;s</span>
                <span class="c1"># the best we can do.</span>
                <span class="n">num_train_samples</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">*</span> <span class="n">total_train_batch_size</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">include_tokens_per_second</span><span class="p">:</span>
                    <span class="n">num_train_tokens</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">num_tokens</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span><span class="p">)</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">max_steps</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_train_epochs</span> <span class="o">*</span> <span class="n">num_update_steps_per_epoch</span><span class="p">)</span>
                <span class="n">num_train_epochs</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_train_epochs</span><span class="p">)</span>
                <span class="n">num_train_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_examples</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">num_train_epochs</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">include_tokens_per_second</span><span class="p">:</span>
                    <span class="n">num_train_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_tokens</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">num_train_epochs</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Rely on max_steps when dataloader does not have a working size</span>
            <span class="n">max_steps</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span>
            <span class="c1"># Setting a very large number of epochs so we go as many times as necessary over the iterator.</span>
            <span class="n">num_train_epochs</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
            <span class="n">num_update_steps_per_epoch</span> <span class="o">=</span> <span class="n">max_steps</span>
            <span class="n">num_examples</span> <span class="o">=</span> <span class="n">total_train_batch_size</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span>
            <span class="n">num_train_samples</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">*</span> <span class="n">total_train_batch_size</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">include_tokens_per_second</span><span class="p">:</span>
                <span class="n">num_train_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_tokens</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span><span class="p">)</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;args.max_steps must be set to a positive value if dataloader does not have a length, was&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">max_steps</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">delay_optimizer_creation</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># do not delay now</span>

        <span class="c1"># We need to reset the scheduler, as its parameters may be different on subsequent calls</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_created_lr_scheduler</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_created_lr_scheduler</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">delay_optimizer_creation</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">create_optimizer_and_scheduler</span><span class="p">(</span><span class="n">num_training_steps</span><span class="o">=</span><span class="n">max_steps</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">TrainerState</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span>

        <span class="c1"># Compute absolute values for logging, eval, and save if given as ratio</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">logging_steps</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">max_steps</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">logging_steps</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">eval_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">eval_steps</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">eval_steps</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">max_steps</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">eval_steps</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">eval_steps</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">eval_steps</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">save_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">save_steps</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">save_steps</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">max_steps</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">save_steps</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">save_steps</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">save_steps</span>

        <span class="c1"># Activate gradient checkpointing if needed</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">recompute</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">recompute_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">recompute_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">recompute_kwargs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">recompute_kwargs</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">recompute_enable</span><span class="p">(</span><span class="n">recompute_kwargs</span><span class="o">=</span><span class="n">recompute_kwargs</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>

        <span class="c1"># as the model is wrapped, don&#39;t use `accelerator.prepare`</span>
        <span class="c1"># this is for unhandled cases such as</span>
        <span class="c1"># DataParallel</span>
        <span class="n">use_accelerator_prepare</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="k">else</span> <span class="kc">False</span>

        <span class="c1"># prepare using `accelerator` prepare</span>
        <span class="k">if</span> <span class="n">use_accelerator_prepare</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="p">,</span> <span class="s2">&quot;step&quot;</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_apex</span><span class="p">:</span>
                    <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># to handle cases wherein we pass &quot;DummyScheduler&quot; such as when it is specified in DeepSpeed config.</span>
                <span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span>
                <span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">optim</span> <span class="ow">in</span> <span class="p">[</span><span class="n">OptimizerNames</span><span class="o">.</span><span class="n">LOMO</span><span class="p">,</span> <span class="n">OptimizerNames</span><span class="o">.</span><span class="n">ADALOMO</span><span class="p">]:</span>
            <span class="c1"># In this case we are in DDP + LOMO, which should be supported</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">delay_optimizer_creation</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">create_optimizer_and_scheduler</span><span class="p">(</span><span class="n">num_training_steps</span><span class="o">=</span><span class="n">max_steps</span><span class="p">)</span>

        <span class="c1"># # Check if saved optimizer or scheduler states exist</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_load_optimizer_and_scheduler</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">)</span>

        <span class="c1"># important: at this point:</span>
        <span class="c1"># self.model         is the Transformers Model</span>

        <span class="c1"># Train!</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;***** Running training *****&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Num examples = </span><span class="si">{</span><span class="n">num_examples</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Num Epochs = </span><span class="si">{</span><span class="n">num_train_epochs</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Instantaneous batch size per device = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">per_device_train_batch_size</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">per_device_train_batch_size</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Training with DataParallel so batch size has been adjusted to: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Total train batch size (w. parallel, distributed &amp; accumulation) = </span><span class="si">{</span><span class="n">total_train_batch_size</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Gradient Accumulation steps = </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Total optimization steps = </span><span class="si">{</span><span class="n">max_steps</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Number of trainable parameters = </span><span class="si">{</span><span class="n">get_model_param_count</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">trainable_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">epochs_trained</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">steps_trained_in_current_epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">steps_trained_progress_bar</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Check if continuing training from a checkpoint</span>
        <span class="k">if</span> <span class="n">resume_from_checkpoint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">TRAINER_STATE_NAME</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">TrainerState</span><span class="o">.</span><span class="n">load_from_json</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">TRAINER_STATE_NAME</span><span class="p">))</span>
            <span class="n">epochs_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">//</span> <span class="n">num_update_steps_per_epoch</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">ignore_data_skip</span><span class="p">:</span>
                <span class="n">steps_trained_in_current_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_update_steps_per_epoch</span><span class="p">)</span>
                <span class="n">steps_trained_in_current_epoch</span> <span class="o">*=</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">steps_trained_in_current_epoch</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Continuing training from checkpoint, will skip to saved global_step&quot;</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Continuing training from epoch </span><span class="si">{</span><span class="n">epochs_trained</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Continuing training from global step </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">ignore_data_skip</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;  Will skip the first </span><span class="si">{</span><span class="n">epochs_trained</span><span class="si">}</span><span class="s2"> epochs then the first&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">steps_trained_in_current_epoch</span><span class="si">}</span><span class="s2"> batches in the first epoch.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Update the references</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span>
        <span class="c1"># This should be the same if the state has been saved but in case the training arguments changed, it&#39;s safer</span>
        <span class="c1"># to set this after the load.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">=</span> <span class="n">max_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">num_train_epochs</span> <span class="o">=</span> <span class="n">num_train_epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_local_process_zero</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_local_process_zero</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">()</span>

        <span class="c1"># tr_loss is a tensor to avoid synchronization of TPUs through .item()</span>
        <span class="n">tr_loss</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="c1"># _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_total_loss_scalar</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_globalstep_last_logged</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span>
        <span class="n">grad_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_train_begin</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>

        <span class="n">total_batched_samples</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs_trained</span><span class="p">,</span> <span class="n">num_train_epochs</span><span class="p">):</span>
            <span class="n">epoch_iterator</span> <span class="o">=</span> <span class="n">train_dataset</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">epoch_iterator</span><span class="p">,</span> <span class="s2">&quot;set_epoch&quot;</span><span class="p">):</span>
                <span class="n">epoch_iterator</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

            <span class="c1"># Reset the past mems state at the beginning of each epoch if necessary.</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_past</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">steps_in_epoch</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">epoch_iterator</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">len_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_epoch_begin</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="n">epochs_trained</span> <span class="ow">and</span> <span class="n">resume_from_checkpoint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">steps_trained_in_current_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># self._load_rng_state(resume_from_checkpoint)</span>
                <span class="k">pass</span>

            <span class="n">rng_to_sync</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">steps_skipped</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">steps_trained_in_current_epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># epoch_iterator = skip_first_batches(epoch_iterator, steps_trained_in_current_epoch)</span>
                <span class="n">steps_skipped</span> <span class="o">=</span> <span class="n">steps_trained_in_current_epoch</span>
                <span class="n">steps_trained_in_current_epoch</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">rng_to_sync</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">step</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">inputs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">epoch_iterator</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">()):</span>
                <span class="n">total_batched_samples</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">include_num_input_tokens_seen</span><span class="p">:</span>
                    <span class="n">main_input_name</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;main_input_name&quot;</span><span class="p">,</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">main_input_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="s2">&quot;Tried to track the number of tokens seen, however the current model is &quot;</span>
                            <span class="s2">&quot;not configured properly to know what item is the input. To fix this, add &quot;</span>
                            <span class="s2">&quot;a `main_input_name` attribute to the model class you are using.&quot;</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">num_input_tokens_seen</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">main_input_name</span><span class="p">])</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">rng_to_sync</span><span class="p">:</span>
                    <span class="c1"># self._load_rng_state(resume_from_checkpoint)</span>
                    <span class="n">rng_to_sync</span> <span class="o">=</span> <span class="kc">False</span>

                <span class="c1"># Skip past any already trained steps if resuming training</span>
                <span class="k">if</span> <span class="n">steps_trained_in_current_epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">steps_trained_in_current_epoch</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="k">if</span> <span class="n">steps_trained_progress_bar</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">steps_trained_progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">steps_trained_in_current_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_load_rng_state</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">steps_trained_progress_bar</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">steps_trained_progress_bar</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
                    <span class="n">steps_trained_progress_bar</span> <span class="o">=</span> <span class="kc">None</span>

                <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_step_begin</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>

                <span class="n">tr_loss_step</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">args</span><span class="o">.</span><span class="n">logging_nan_inf_filter</span>
                    <span class="ow">and</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">tr_loss_step</span><span class="p">)</span> <span class="ow">or</span> <span class="n">ops</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">tr_loss_step</span><span class="p">))</span>
                <span class="p">):</span>
                    <span class="c1"># if loss is nan or inf simply add the average of previous logged losses</span>
                    <span class="n">tr_loss</span> <span class="o">+=</span> <span class="n">tr_loss</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_globalstep_last_logged</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tr_loss</span> <span class="o">+=</span> <span class="n">tr_loss_step</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">current_flos</span> <span class="o">+=</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">floating_point_ops</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>

                <span class="n">is_last_step_and_steps_less_than_grad_acc</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">steps_in_epoch</span> <span class="o">&lt;=</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="ow">and</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">steps_in_epoch</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">total_batched_samples</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span>
                    <span class="ow">or</span>
                    <span class="c1"># last step in epoch but step is always smaller than gradient_accumulation_steps</span>
                    <span class="n">is_last_step_and_steps_less_than_grad_acc</span>
                <span class="p">):</span>
                    <span class="c1"># Gradient clipping</span>
                    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">max_grad_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">max_grad_norm</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="c1"># deepspeed does its own clipping</span>
                        <span class="n">_grad_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span>
                            <span class="n">grads</span><span class="p">,</span>
                            <span class="n">args</span><span class="o">.</span><span class="n">max_grad_norm</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="c1"># Optimizer step</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>


                    <span class="n">optimizer_was_run</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">if</span> <span class="n">optimizer_was_run</span><span class="p">:</span>
                        <span class="c1"># Delay optimizer scheduling until metrics are generated</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="p">,</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">):</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">+</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">steps_skipped</span><span class="p">)</span> <span class="o">/</span> <span class="n">steps_in_epoch</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_step_end</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_log_save_evaluate</span><span class="p">(</span><span class="n">tr_loss</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">ignore_keys_for_eval</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_substep_end</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="o">.</span><span class="n">should_epoch_stop</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="o">.</span><span class="n">should_training_stop</span><span class="p">:</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;There seems to be not a single sample in your epoch_iterator, stopping training at step&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span><span class="si">}</span><span class="s2">! This is expected if you&#39;re using an IterableDataset and set&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; num_steps (</span><span class="si">{</span><span class="n">max_steps</span><span class="si">}</span><span class="s2">) higher than the number of available samples.&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="o">.</span><span class="n">should_training_stop</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_epoch_end</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_log_save_evaluate</span><span class="p">(</span><span class="n">tr_loss</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">ignore_keys_for_eval</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="o">.</span><span class="n">should_training_stop</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_past&quot;</span><span class="p">):</span>
            <span class="c1"># Clean the state at the end of training</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_past&quot;</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Training completed. Do not forget to share your model on huggingface.co/models =)</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">load_best_model_at_end</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Wait for everyone to get here so we are sure the model has been saved by process 0.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_load_best_model</span><span class="p">()</span>

        <span class="c1"># add remaining tr_loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_total_loss_scalar</span> <span class="o">+=</span> <span class="n">tr_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_loss_scalar</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span>

        <span class="n">metrics</span> <span class="o">=</span> <span class="n">speed_metrics</span><span class="p">(</span>
            <span class="s2">&quot;train&quot;</span><span class="p">,</span>
            <span class="n">start_time</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_train_samples</span><span class="p">,</span>
            <span class="n">num_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">max_steps</span><span class="p">,</span>
            <span class="n">num_tokens</span><span class="o">=</span><span class="n">num_train_tokens</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store_flos</span><span class="p">()</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;total_flos&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">total_flos</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_loss</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_in_train</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>

        <span class="n">checkpoints_sorted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sorted_checkpoints</span><span class="p">(</span><span class="n">use_mtime</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>

        <span class="c1"># Delete the last checkpoint when save_total_limit=1 if it&#39;s different from the best checkpoint and process allowed to save.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">should_save</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_total_limit</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">checkpoint</span> <span class="ow">in</span> <span class="n">checkpoints_sorted</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">samefile</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span><span class="p">):</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Deleting older checkpoint [</span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">] due to args.save_total_limit&quot;</span><span class="p">)</span>
                    <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>

        <span class="c1"># After training we make sure to retrieve back the original forward pass method</span>
        <span class="c1"># for the embedding layer by removing the forward post hook.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">neftune_noise_alpha</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_deactivate_neftune</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">TrainOutput</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">metrics</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_load_from_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads the model from a checkpoint directory.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (Trainer): The Trainer instance.</span>
<span class="sd">            resume_from_checkpoint (str): The path to the checkpoint directory.</span>
<span class="sd">            model (Optional[object]): The model to load. If not provided, the model specified in the Trainer instance will be used.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If a valid checkpoint cannot be found at the given directory.</span>

<span class="sd">        &#39;&#39;&#39;</span>

<span class="sd">        The method `_load_from_checkpoint` is responsible for loading the model from a checkpoint directory. It takes three parameters: `self`, `resume_from_checkpoint`, and an optional parameter `model`. The</span>
<span class="sd">method does not return any value (`None`).</span>

<span class="sd">        - `self` (Trainer): The `Trainer` instance on which the method is called.</span>
<span class="sd">        - `resume_from_checkpoint` (str): The path to the checkpoint directory from which the model will be loaded.</span>
<span class="sd">        - `model` (Optional[object]): An optional parameter that specifies the model to be loaded. If not provided, the model specified in the `Trainer` instance will be used.</span>

<span class="sd">        The method first checks if a valid checkpoint can be found at the given `resume_from_checkpoint` directory. If no valid checkpoint is found, a `ValueError` is raised.</span>

<span class="sd">        If a valid checkpoint is found, the method proceeds to load the model. It first checks if a configuration file (`CONFIG_NAME`) is present in the checkpoint directory. If a configuration file is found,</span>
<span class="sd">it loads the configuration using the `PretrainedConfig.from_json_file` method.</span>

<span class="sd">        Next, the method checks if either the weights file (`weights_file`) or the safe weights file (`safe_weights_file`) is present in the checkpoint directory. If either of these files is found, the method</span>
<span class="sd">checks if the `save_safetensors` flag is enabled. If the flag is enabled and the safe weights file is present, it loads the model&#39;s state dictionary using the `safe_load_file` method. Otherwise, it uses the</span>
<span class="sd">`mindspore.load_checkpoint` method to load the model&#39;s state dictionary. The method then loads the state dictionary into the model using the `model.load_state_dict` method, with the `False` argument indicating</span>
<span class="sd">that strict loading should be disabled. After loading the state dictionary, any temporary variables are deleted and any warnings are issued using the `_issue_warnings_after_load` method.</span>

<span class="sd">        If neither the weights file nor the safe weights file is found, the method calls the `load_sharded_checkpoint` method to load the model from the checkpoint directory, with the `prefer_safe` parameter</span>
<span class="sd">indicating whether to prefer safe tensors.</span>

<span class="sd">        Note: The method assumes that the necessary imports and variables are already defined.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>

        <span class="n">config_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">CONFIG_NAME</span><span class="p">)</span>
        <span class="n">adapter_weights_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">ADAPTER_WEIGHTS_NAME</span><span class="p">)</span>
        <span class="n">adapter_safe_weights_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">ADAPTER_SAFE_WEIGHTS_NAME</span><span class="p">)</span>
        <span class="n">weights_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">)</span>
        <span class="n">weights_index_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">WEIGHTS_INDEX_NAME</span><span class="p">)</span>
        <span class="n">safe_weights_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">SAFE_WEIGHTS_NAME</span><span class="p">)</span>
        <span class="n">safe_weights_index_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">SAFE_WEIGHTS_INDEX_NAME</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="nb">any</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="n">weights_file</span><span class="p">,</span>
                    <span class="n">safe_weights_file</span><span class="p">,</span>
                    <span class="n">weights_index_file</span><span class="p">,</span>
                    <span class="n">safe_weights_index_file</span><span class="p">,</span>
                    <span class="n">adapter_weights_file</span><span class="p">,</span>
                    <span class="n">adapter_safe_weights_file</span><span class="p">,</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Can&#39;t find a valid checkpoint at </span><span class="si">{</span><span class="n">resume_from_checkpoint</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading model from </span><span class="si">{</span><span class="n">resume_from_checkpoint</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">config_file</span><span class="p">):</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">PretrainedConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">weights_file</span><span class="p">)</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">safe_weights_file</span><span class="p">):</span>
            <span class="c1"># If the model is on the GPU, it still works!</span>
            <span class="c1"># We load the model state dict on the CPU to avoid an OOM error.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_safetensors</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">safe_weights_file</span><span class="p">):</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">safe_load_file</span><span class="p">(</span><span class="n">safe_weights_file</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span>
                    <span class="n">weights_file</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># workaround for FSDP bug</span>
            <span class="c1"># which takes *args instead of **kwargs</span>
            <span class="n">load_result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># release memory</span>
            <span class="k">del</span> <span class="n">state_dict</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_issue_warnings_after_load</span><span class="p">(</span><span class="n">load_result</span><span class="p">)</span>

        <span class="c1"># Load adapters following PR # 24096</span>
        <span class="c1"># elif _is_peft_model(model):</span>
        <span class="c1">#     # If train a model using PEFT &amp; LoRA, assume that adapter have been saved properly.</span>
        <span class="c1">#     if hasattr(model, &quot;active_adapter&quot;) and hasattr(model, &quot;load_adapter&quot;):</span>
        <span class="c1">#         if os.path.exists(resume_from_checkpoint):</span>
        <span class="c1">#             model.load_adapter(resume_from_checkpoint, model.active_adapter, is_trainable=True)</span>
        <span class="c1">#         else:</span>
        <span class="c1">#             logger.warning(</span>
        <span class="c1">#                 &quot;The intermediate checkpoints of PEFT may not be saved correctly, &quot;</span>
        <span class="c1">#                 f&quot;consider using a custom callback to save {ADAPTER_WEIGHTS_NAME} in corresponding saving folders. &quot;</span>
        <span class="c1">#                 &quot;Check some examples here: https://github.com/huggingface/peft/issues/96&quot;</span>
        <span class="c1">#             )</span>
        <span class="c1">#     else:</span>
        <span class="c1">#         logger.warning(&quot;Could not load adapter model, make sure to have `peft&gt;=0.3.0` installed&quot;)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># We load the sharded checkpoint</span>
            <span class="n">load_result</span> <span class="o">=</span> <span class="n">load_sharded_checkpoint</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">prefer_safe</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_safetensors</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_issue_warnings_after_load</span><span class="p">(</span><span class="n">load_result</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_load_optimizer_and_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;If optimizer and scheduler states exist, load them.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">checkpoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">checkpoint_file_exists</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">OPTIMIZER_NAME</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">checkpoint_file_exists</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">SCHEDULER_NAME</span><span class="p">)):</span>
            <span class="c1"># Load in optimizer and scheduler states</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">OPTIMIZER_NAME</span><span class="p">)))</span>
            <span class="c1"># with warnings.catch_warnings(record=True) as caught_warnings:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">SCHEDULER_NAME</span><span class="p">)))</span>

            <span class="c1"># reissue_pt_warnings(caught_warnings)</span>

    <span class="k">def</span> <span class="nf">_prepare_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and</span>
<span class="sd">        handling potential state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The batch received was empty, your model won&#39;t be able to train on it. Double-check that your &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;training dataset contains keys expected by the model: </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_signature_columns</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_past</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;mems&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_past</span>

        <span class="k">return</span> <span class="n">inputs</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform a training step on a batch of inputs.</span>

<span class="sd">        Subclass and override to inject custom behavior.</span>

<span class="sd">        Args:</span>
<span class="sd">            model (`nn.Module`):</span>
<span class="sd">                The model to train.</span>
<span class="sd">            inputs (`Dict[str, Union[mindspore.Tensor, Any]]`):</span>
<span class="sd">                The inputs and targets of the model.</span>

<span class="sd">                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the</span>
<span class="sd">                argument `labels`. Check your model&#39;s documentation for all accepted arguments.</span>

<span class="sd">        Return:</span>
<span class="sd">            `mindspore.Tensor`: The tensor with training loss on this batch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;grad_fn&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_reload</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="p">()</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="n">weights</span> <span class="o">+=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

        <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span><span class="p">,</span> <span class="n">grads</span>

    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">return_outputs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        How the loss is computed by Trainer. By default, all models return the loss in the first element.</span>

<span class="sd">        Subclass and override for custom behavior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_smoother</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;labels&quot;</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="c1"># Save past state if it exists</span>
        <span class="c1"># TODO: this needs to be fixed and made cleaner later.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_past</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">past_index</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># unwrapped_model = self.accelerator.unwrap_model(model)</span>
            <span class="n">unwrapped_model</span> <span class="o">=</span> <span class="n">model</span>
            <span class="k">if</span> <span class="n">_is_peft_model</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="p">):</span>
                <span class="n">model_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="o">.</span><span class="n">get_base_model</span><span class="p">())</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="k">if</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">MODEL_FOR_CAUSAL_LM_MAPPING_NAMES</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_smoother</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">shift_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_smoother</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;loss&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The model did not return a loss from the inputs, only the following keys: &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. For reference, the inputs it received are </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="c1"># We don&#39;t use .loss here since the model may return tuples instead of ModelOutput.</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_outputs</span> <span class="k">else</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">is_local_process_zero</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several</span>
<span class="sd">        machines) main process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">local_process_index</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">is_world_process_zero</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Whether or not this process is the global main process (when training in a distributed fashion on several</span>
<span class="sd">        machines, this is only going to be `True` for one process).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Special case for SageMaker ModelParallel since there process_index is dp_process_index, not the global</span>
        <span class="c1"># process index.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">process_index</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">floating_point_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point</span>
<span class="sd">        operations for every backward + forward pass. If using another model, either implement such a method in the</span>
<span class="sd">        model or subclass and override this method.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs (`Dict[str, Union[mindspore.Tensor, Any]]`):</span>
<span class="sd">                The inputs and targets of the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `int`: The number of floating-point operations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;floating_point_ops&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">floating_point_ops</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">_issue_warnings_after_load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">load_result</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Issues warnings after loading a checkpoint model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (Trainer): The current instance of the Trainer class.</span>
<span class="sd">            load_result (tuple): A tuple containing two lists. The first list represents the missing keys in the loaded checkpoint model, </span>
<span class="sd">                while the second list represents the unexpected keys in the loaded checkpoint model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None. This method does not raise any exceptions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">load_result</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_keys_to_ignore_on_save</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">set</span><span class="p">(</span><span class="n">load_result</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="nb">set</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_keys_to_ignore_on_save</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There were missing keys in the checkpoint model loaded: </span><span class="si">{</span><span class="n">load_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">load_result</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;There were unexpected keys in the checkpoint model loaded: </span><span class="si">{</span><span class="n">load_result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_maybe_log_save_evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tr_loss</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">ignore_keys_for_eval</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; </span>
<span class="sd">        This method &#39;_maybe_log_save_evaluate&#39; is a part of the &#39;Trainer&#39; class. It takes 6 parameters:</span>

<span class="sd">        Args:</span>
<span class="sd">        - self (object): The instance of the Trainer class.</span>
<span class="sd">        - tr_loss (Tensor): The training loss value.</span>
<span class="sd">        - grad_norm (float or None): The gradient norm value, or None if not available.</span>
<span class="sd">        - model (object): The model being trained.</span>
<span class="sd">        - epoch (int): The current epoch number.</span>
<span class="sd">        - ignore_keys_for_eval (list or None): A list of keys to ignore during evaluation, or None if not applicable.</span>

<span class="sd">        Returns:</span>
<span class="sd">        None: This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">        - ValueError: If an invalid input is provided.</span>
<span class="sd">        - RuntimeError: If any runtime error occurs during execution.</span>
<span class="sd">        - KeyError: If a key error occurs while accessing dictionaries.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="o">.</span><span class="n">should_log</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_globalstep_last_logged</span><span class="p">:</span>
            <span class="n">logs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="c1"># # all_gather + mean() to get average loss over all processes</span>
            <span class="c1"># tr_loss_scalar = self._nested_gather(tr_loss).mean().item()</span>
            <span class="n">tr_loss_scalar</span> <span class="o">=</span> <span class="n">tr_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># reset tr_loss to zero</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">tr_loss</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

            <span class="n">logs</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">tr_loss_scalar</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_globalstep_last_logged</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">grad_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logs</span><span class="p">[</span><span class="s2">&quot;grad_norm&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_norm</span>
            <span class="n">logs</span><span class="p">[</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_learning_rate</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_total_loss_scalar</span> <span class="o">+=</span> <span class="n">tr_loss_scalar</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_globalstep_last_logged</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">store_flos</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">logs</span><span class="p">)</span>

        <span class="n">metrics</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="o">.</span><span class="n">should_evaluate</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">ignore_keys</span><span class="o">=</span><span class="n">ignore_keys_for_eval</span><span class="p">)</span>
            <span class="c1"># self._report_to_hp_search(trial, self.state.global_step, metrics)</span>

            <span class="c1"># Run delayed LR scheduler now that metrics are populated</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="p">,</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">):</span>
                <span class="n">metric_to_check</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">metric_for_best_model</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">metric_to_check</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;eval_&quot;</span><span class="p">):</span>
                    <span class="n">metric_to_check</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;eval_</span><span class="si">{</span><span class="n">metric_to_check</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="n">metric_to_check</span><span class="p">])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="o">.</span><span class="n">should_save</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">_internal_call</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Will save the model, so you can reload it using `from_pretrained()`.</span>

<span class="sd">        Will only save from the main process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">output_dir</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">should_save</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the model checkpoint to the specified output directory.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (Trainer): The instance of the Trainer class.</span>
<span class="sd">            output_dir (Optional[str], optional): The directory path where the model checkpoint will be saved. If not provided, it defaults to self.args.output_dir. Defaults to None.</span>
<span class="sd">            state_dict (optional): The state dictionary of the model. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the model is not an instance of PreTrainedModel.</span>
<span class="sd">            RuntimeError: If an error occurs during the saving process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If we are executing this function, we are the process zero, so we don&#39;t check for that.</span>
        <span class="n">output_dir</span> <span class="o">=</span> <span class="n">output_dir</span> <span class="k">if</span> <span class="n">output_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saving model checkpoint to </span><span class="si">{</span><span class="n">output_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">supported_classes</span> <span class="o">=</span> <span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">,)</span> <span class="c1">#if not is_peft_available() else (PreTrainedModel, PeftModel)</span>
        <span class="c1"># Save a trained model and configuration using `save_pretrained()`.</span>
        <span class="c1"># They can then be reloaded using `from_pretrained()`</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">supported_classes</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">state_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters_dict</span><span class="p">()</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">supported_classes</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span>
                    <span class="n">output_dir</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_safetensors</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Trainer.model is not a `PreTrainedModel`, only saving its state dict.&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_safetensors</span><span class="p">:</span>
                    <span class="n">safe_save_file</span><span class="p">(</span>
                        <span class="n">state_dict</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">SAFE_WEIGHTS_NAME</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;np&quot;</span><span class="p">}</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span>
                <span class="n">output_dir</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_safetensors</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>

        <span class="c1"># # Good practice: save your training arguments together with the trained model</span>
        <span class="c1"># mindspore.save_checkpoint(save_obj, ckpt_file_name, integrated_save=True, async_save=False, append_dict=None, enc_key=None, enc_mode=&#39;AES-GCM&#39;, choice_func=None, **kwargs)</span>

    <span class="k">def</span> <span class="nf">_save_optimizer_and_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the optimizer and scheduler states to the specified output directory.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (Trainer): The Trainer object instance.</span>
<span class="sd">            output_dir (str): The directory path where the optimizer and scheduler states will be saved.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            N/A</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">should_save</span><span class="p">:</span>
            <span class="c1"># deepspeed.save_checkpoint above saves model/optim/sched</span>
            <span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">OPTIMIZER_NAME</span><span class="p">))</span>

        <span class="c1"># Save SCHEDULER &amp; SCALER</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">should_save</span><span class="p">:</span>
            <span class="c1"># with warnings.catch_warnings(record=True) as caught_warnings:</span>
            <span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">SCHEDULER_NAME</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the model checkpoint along with relevant metrics and state information.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (Trainer): The Trainer instance.</span>
<span class="sd">            model: The model to be saved.</span>
<span class="sd">            metrics (dict): A dictionary containing evaluation metrics. Defaults to None if metrics are not available.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the metric for the best model is not specified correctly.</span>
<span class="sd">            FileNotFoundError: If the specified output directory does not exist.</span>
<span class="sd">            OSError: If there are any issues with file operations while saving the checkpoint.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># In all cases, including ddp/dp/deepspeed, self.model is always a reference to the model we</span>
        <span class="c1"># want to save except FullyShardedDDP.</span>
        <span class="c1"># assert unwrap_model(model) is self.model, &quot;internal model should be a reference to self.model&quot;</span>

        <span class="c1"># Save model checkpoint</span>
        <span class="n">checkpoint_folder</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">PREFIX_CHECKPOINT_DIR</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># self.store_flos()</span>

        <span class="n">run_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_output_dir</span><span class="p">()</span>
        <span class="n">output_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">run_dir</span><span class="p">,</span> <span class="n">checkpoint_folder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">_internal_call</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_only_model</span><span class="p">:</span>
            <span class="c1"># Save optimizer and scheduler</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_optimizer_and_scheduler</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
            <span class="c1"># Save RNG state</span>
            <span class="c1"># self._save_rng_state(output_dir)</span>

        <span class="c1"># Determine the new best metric / best model checkpoint</span>
        <span class="k">if</span> <span class="n">metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">metric_for_best_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">metric_to_check</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">metric_for_best_model</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">metric_to_check</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;eval_&quot;</span><span class="p">):</span>
                <span class="n">metric_to_check</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;eval_</span><span class="si">{</span><span class="n">metric_to_check</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">metric_value</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="n">metric_to_check</span><span class="p">]</span>

            <span class="n">operator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">greater</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">greater_is_better</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">less</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_metric</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">or</span> <span class="n">operator</span><span class="p">(</span><span class="n">metric_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_metric</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_metric</span> <span class="o">=</span> <span class="n">metric_value</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span> <span class="o">=</span> <span class="n">output_dir</span>

        <span class="c1"># Save the Trainer state</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">should_save</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">save_to_json</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">TRAINER_STATE_NAME</span><span class="p">))</span>

        <span class="c1"># Maybe delete some older checkpoints.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">should_save</span><span class="p">:</span>
            <span class="c1"># Solely rely on numerical checkpoint id for rotation.</span>
            <span class="c1"># mtime is not reliable especially on some fuse fs in cloud environments.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_rotate_checkpoints</span><span class="p">(</span><span class="n">use_mtime</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_dir</span><span class="o">=</span><span class="n">run_dir</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_rotate_checkpoints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_mtime</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Rotate the checkpoints to limit the total number of saved checkpoints.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (Trainer): The instance of the Trainer class.</span>
<span class="sd">            use_mtime (bool, optional): If True, sorts the checkpoints based on modification time. Defaults to False.</span>
<span class="sd">            output_dir (str, optional): The directory where the checkpoints are stored. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            - OSError: If an error occurs while deleting the older checkpoints using shutil.rmtree.</span>
<span class="sd">            - TypeError: If the input parameters are of incorrect types.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_total_limit</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_total_limit</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Check if we should delete older checkpoint(s)</span>
        <span class="n">checkpoints_sorted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sorted_checkpoints</span><span class="p">(</span><span class="n">use_mtime</span><span class="o">=</span><span class="n">use_mtime</span><span class="p">,</span> <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">checkpoints_sorted</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_total_limit</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># If save_total_limit=1 with load_best_model_at_end=True, we could end up deleting the last checkpoint, which</span>
        <span class="c1"># we don&#39;t do to allow resuming.</span>
        <span class="n">save_total_limit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_total_limit</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_total_limit</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="ow">and</span> <span class="n">checkpoints_sorted</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span>
        <span class="p">):</span>
            <span class="n">save_total_limit</span> <span class="o">=</span> <span class="mi">2</span>

        <span class="n">number_of_checkpoints_to_delete</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">checkpoints_sorted</span><span class="p">)</span> <span class="o">-</span> <span class="n">save_total_limit</span><span class="p">)</span>
        <span class="n">checkpoints_to_be_deleted</span> <span class="o">=</span> <span class="n">checkpoints_sorted</span><span class="p">[:</span><span class="n">number_of_checkpoints_to_delete</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">checkpoint</span> <span class="ow">in</span> <span class="n">checkpoints_to_be_deleted</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Deleting older checkpoint [</span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">] due to args.save_total_limit&quot;</span><span class="p">)</span>
            <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">ignore_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">store_flos</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Stores the current number of floating point operations (FLOs) and updates the total FLOs count.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (Trainer): The Trainer object itself.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            N/A</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Storing the number of floating-point operations that went into the model</span>
        <span class="c1"># if self.args.parallel_mode == ParallelMode.DISTRIBUTED:</span>
        <span class="c1">#     self.state.total_flos += (</span>
        <span class="c1">#         distributed_broadcast_scalars([self.current_flos], device=self.args.device).sum().item()</span>
        <span class="c1">#     )</span>
        <span class="c1">#     self.current_flos = 0</span>
        <span class="c1"># else:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">total_flos</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_flos</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_flos</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Dataset</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metric_key_prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;eval&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Run evaluation and returns metrics.</span>

<span class="sd">        The calling script will be responsible for providing a method to compute metrics, as they are task-dependent</span>
<span class="sd">        (pass it to the init `compute_metrics` argument).</span>

<span class="sd">        You can also subclass and override this method to inject custom behavior.</span>

<span class="sd">        Args:</span>
<span class="sd">            eval_dataset (Union[`Dataset`, Dict[str, `Dataset`]), *optional*):</span>
<span class="sd">                Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns</span>
<span class="sd">                not accepted by the `model.forward()` method are automatically removed. If it is a dictionary, it will</span>
<span class="sd">                evaluate on each dataset, prepending the dictionary key to the metric name. Datasets must implement the</span>
<span class="sd">                `__len__` method.</span>

<span class="sd">                &lt;Tip&gt;</span>

<span class="sd">                If you pass a dictionary with names of datasets as keys and datasets as values, evaluate will run</span>
<span class="sd">                separate evaluations on each dataset. This can be useful to monitor how training affects other</span>
<span class="sd">                datasets or simply to get a more fine-grained evaluation.</span>
<span class="sd">                When used with `load_best_model_at_end`, make sure `metric_for_best_model` references exactly one</span>
<span class="sd">                of the datasets. If you, for example, pass in `{&quot;data1&quot;: data1, &quot;data2&quot;: data2}` for two datasets</span>
<span class="sd">                `data1` and `data2`, you could specify `metric_for_best_model=&quot;eval_data1_loss&quot;` for using the</span>
<span class="sd">                loss on `data1` and `metric_for_best_model=&quot;eval_data1_loss&quot;` for the loss on `data2`.</span>

<span class="sd">                &lt;/Tip&gt;</span>

<span class="sd">            ignore_keys (`List[str]`, *optional*):</span>
<span class="sd">                A list of keys in the output of your model (if it is a dictionary) that should be ignored when</span>
<span class="sd">                gathering predictions.</span>
<span class="sd">            metric_key_prefix (`str`, *optional*, defaults to `&quot;eval&quot;`):</span>
<span class="sd">                An optional prefix to be used as the metrics key prefix. For example the metrics &quot;bleu&quot; will be named</span>
<span class="sd">                &quot;eval_bleu&quot; if the prefix is &quot;eval&quot; (default)</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The</span>
<span class="sd">            dictionary also contains the epoch number which comes from the training state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># handle multipe eval datasets</span>
        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">eval_dataset</span> <span class="k">if</span> <span class="n">eval_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_dataset</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">eval_dataset_name</span><span class="p">,</span> <span class="n">_eval_dataset</span> <span class="ow">in</span> <span class="n">eval_dataset</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">dataset_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
                    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">_eval_dataset</span><span class="p">,</span>
                    <span class="n">ignore_keys</span><span class="o">=</span><span class="n">ignore_keys</span><span class="p">,</span>
                    <span class="n">metric_key_prefix</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">eval_dataset_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">dataset_metrics</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">metrics</span>

        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_eval_dataset</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span>

        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">eval_loop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_loop</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">eval_loop</span><span class="p">(</span>
            <span class="n">eval_dataset</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Evaluation&quot;</span><span class="p">,</span>
            <span class="c1"># No point gathering the predictions if there are no metrics, otherwise we defer to</span>
            <span class="c1"># self.args.prediction_loss_only</span>
            <span class="n">prediction_loss_only</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">ignore_keys</span><span class="o">=</span><span class="n">ignore_keys</span><span class="p">,</span>
            <span class="n">metric_key_prefix</span><span class="o">=</span><span class="n">metric_key_prefix</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">total_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span>
        <span class="k">if</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_jit_compilation_time&quot;</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">:</span>
            <span class="n">start_time</span> <span class="o">+=</span> <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_jit_compilation_time&quot;</span><span class="p">]</span>
        <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">speed_metrics</span><span class="p">(</span>
                <span class="n">metric_key_prefix</span><span class="p">,</span>
                <span class="n">start_time</span><span class="p">,</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span>
                <span class="n">num_steps</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">/</span> <span class="n">total_batch_size</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_evaluate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">metrics</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">metric_key_prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;test&quot;</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Run prediction and returns predictions and potential metrics.</span>

<span class="sd">        Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method</span>
<span class="sd">        will also return metrics, like in `evaluate()`.</span>

<span class="sd">        Args:</span>
<span class="sd">            test_dataset (`Dataset`):</span>
<span class="sd">                Dataset to run the predictions on. If it is an `datasets.Dataset`, columns not accepted by the</span>
<span class="sd">                `model.forward()` method are automatically removed. Has to implement the method `__len__`</span>
<span class="sd">            ignore_keys (`List[str]`, *optional*):</span>
<span class="sd">                A list of keys in the output of your model (if it is a dictionary) that should be ignored when</span>
<span class="sd">                gathering predictions.</span>
<span class="sd">            metric_key_prefix (`str`, *optional*, defaults to `&quot;test&quot;`):</span>
<span class="sd">                An optional prefix to be used as the metrics key prefix. For example the metrics &quot;bleu&quot; will be named</span>
<span class="sd">                &quot;test_bleu&quot; if the prefix is &quot;test&quot; (default)</span>

<span class="sd">        &lt;Tip&gt;</span>

<span class="sd">        If your predictions or labels have different sequence length (for instance because you&#39;re doing dynamic padding</span>
<span class="sd">        in a token classification task) the predictions will be padded (on the right) to allow for concatenation into</span>
<span class="sd">        one array. The padding index is -100.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Returns: *NamedTuple* A namedtuple with the following keys:</span>

<span class="sd">            - predictions (`np.ndarray`): The predictions on `test_dataset`.</span>
<span class="sd">            - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).</span>
<span class="sd">            - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained</span>
<span class="sd">              labels).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># memory metrics - must set up as early as possible</span>
        <span class="c1"># self._memory_tracker.start()</span>

        <span class="n">test_dataloader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_test_dataset</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">eval_loop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_loop</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">eval_loop</span><span class="p">(</span>
            <span class="n">test_dataloader</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Prediction&quot;</span><span class="p">,</span> <span class="n">ignore_keys</span><span class="o">=</span><span class="n">ignore_keys</span><span class="p">,</span> <span class="n">metric_key_prefix</span><span class="o">=</span><span class="n">metric_key_prefix</span>
        <span class="p">)</span>
        <span class="n">total_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span>
        <span class="c1"># if f&quot;{metric_key_prefix}_jit_compilation_time&quot; in output.metrics:</span>
        <span class="c1">#     start_time += output.metrics[f&quot;{metric_key_prefix}_jit_compilation_time&quot;]</span>
        <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">speed_metrics</span><span class="p">(</span>
                <span class="n">metric_key_prefix</span><span class="p">,</span>
                <span class="n">start_time</span><span class="p">,</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span>
                <span class="n">num_steps</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">/</span> <span class="n">total_batch_size</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">PredictionOutput</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">predictions</span><span class="p">,</span> <span class="n">label_ids</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">label_ids</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">evaluation_loop</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataset</span><span class="p">,</span>
        <span class="n">description</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">prediction_loss_only</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metric_key_prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;eval&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EvalLoopOutput</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.</span>

<span class="sd">        Works both with or without labels.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span>

        <span class="n">prediction_loss_only</span> <span class="o">=</span> <span class="n">prediction_loss_only</span> <span class="k">if</span> <span class="n">prediction_loss_only</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">prediction_loss_only</span>

        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>
        <span class="c1"># if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn&#39;t called</span>
        <span class="c1"># while ``train`` is running, cast it to the right dtype first and then put on device</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_in_train</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16_full_eval</span><span class="p">:</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">bf16_full_eval</span><span class="p">:</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;***** Running </span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="s2"> *****&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">has_length</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Num examples = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_examples</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Num examples: Unknown&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Batch size = </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">model</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span>

        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_past</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Initialize containers</span>
        <span class="c1"># losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)</span>
        <span class="n">losses_host</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">preds_host</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">labels_host</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">inputs_host</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># losses/preds/labels on CPU (final containers)</span>
        <span class="n">all_losses</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">all_preds</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">all_labels</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">all_inputs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Will be useful when we have an iterable dataset so don&#39;t know its length.</span>

        <span class="n">observed_num_examples</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Main evaluation loop</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">inputs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">()):</span>
            <span class="c1"># Update the observed num examples</span>
            <span class="n">observed_batch_size</span> <span class="o">=</span> <span class="n">find_batch_size</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">observed_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">observed_num_examples</span> <span class="o">+=</span> <span class="n">observed_batch_size</span>
                <span class="c1"># For batch samplers, batch_size is not known by the dataloader in advance.</span>
                <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">observed_batch_size</span>

            <span class="c1"># Prediction step</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prediction_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">prediction_loss_only</span><span class="p">,</span> <span class="n">ignore_keys</span><span class="o">=</span><span class="n">ignore_keys</span><span class="p">)</span>
            <span class="n">main_input_name</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;main_input_name&quot;</span><span class="p">,</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">)</span>
            <span class="n">inputs_decode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">main_input_name</span><span class="p">])</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">include_inputs_for_metrics</span> <span class="k">else</span> <span class="kc">None</span>

            <span class="c1"># Update containers on host</span>
            <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">losses</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">observed_batch_size</span><span class="p">)</span>
                <span class="c1"># losses = loss.repeat(batch_size)</span>
                <span class="n">losses_host</span> <span class="o">=</span> <span class="n">losses</span> <span class="k">if</span> <span class="n">losses_host</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">losses_host</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
            <span class="c1"># if labels is not None:</span>
            <span class="c1">#     labels = self.accelerator.pad_across_processes(labels, dim=1, pad_index=-100)</span>
            <span class="k">if</span> <span class="n">inputs_decode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># inputs_decode = self.accelerator.pad_across_processes(inputs_decode, dim=1, pad_index=-100)</span>
                <span class="c1"># inputs_decode = self.gather_function((inputs_decode))</span>
                <span class="n">inputs_host</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">inputs_decode</span>
                    <span class="k">if</span> <span class="n">inputs_host</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">inputs_host</span><span class="p">,</span> <span class="n">inputs_decode</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">logits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># logits = self.accelerator.pad_across_processes(logits, dim=1, pad_index=-100)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_logits_for_metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_logits_for_metrics</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                <span class="c1"># logits = self.gather_function((logits))</span>
                <span class="n">preds_host</span> <span class="o">=</span> <span class="n">logits</span> <span class="k">if</span> <span class="n">preds_host</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">preds_host</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># labels = self.gather_function((labels))</span>
                <span class="n">labels_host</span> <span class="o">=</span> <span class="n">labels</span> <span class="k">if</span> <span class="n">labels_host</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">labels_host</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_prediction_step</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>

            <span class="c1"># Gather all tensors and put them back on the CPU if we have done enough accumulation steps.</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">eval_accumulation_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">eval_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">losses_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">losses</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">losses_host</span><span class="p">)</span>
                    <span class="n">all_losses</span> <span class="o">=</span> <span class="n">losses</span> <span class="k">if</span> <span class="n">all_losses</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">all_losses</span><span class="p">,</span> <span class="n">losses</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">preds_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">logits</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">preds_host</span><span class="p">)</span>
                    <span class="n">all_preds</span> <span class="o">=</span> <span class="n">logits</span> <span class="k">if</span> <span class="n">all_preds</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">inputs_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">inputs_decode</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">inputs_host</span><span class="p">)</span>
                    <span class="n">all_inputs</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">inputs_decode</span>
                        <span class="k">if</span> <span class="n">all_inputs</span> <span class="ow">is</span> <span class="kc">None</span>
                        <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_inputs</span><span class="p">,</span> <span class="n">inputs_decode</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="n">labels_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">labels</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">labels_host</span><span class="p">)</span>
                    <span class="n">all_labels</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">labels</span> <span class="k">if</span> <span class="n">all_labels</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
                    <span class="p">)</span>

                <span class="c1"># Set back to None to begin a new accumulation</span>
                <span class="n">losses_host</span><span class="p">,</span> <span class="n">preds_host</span><span class="p">,</span> <span class="n">inputs_host</span><span class="p">,</span> <span class="n">labels_host</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_past&quot;</span><span class="p">):</span>
            <span class="c1"># Clean the state at the end of the evaluation loop</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_past&quot;</span><span class="p">)</span>

        <span class="c1"># Gather all remaining tensors and put them back on the CPU</span>
        <span class="k">if</span> <span class="n">losses_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">losses_host</span><span class="p">)</span>
            <span class="n">all_losses</span> <span class="o">=</span> <span class="n">losses</span> <span class="k">if</span> <span class="n">all_losses</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">all_losses</span><span class="p">,</span> <span class="n">losses</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">preds_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">preds_host</span><span class="p">)</span>
            <span class="n">all_preds</span> <span class="o">=</span> <span class="n">logits</span> <span class="k">if</span> <span class="n">all_preds</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inputs_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs_decode</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">inputs_host</span><span class="p">)</span>
            <span class="n">all_inputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">inputs_decode</span> <span class="k">if</span> <span class="n">all_inputs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_inputs</span><span class="p">,</span> <span class="n">inputs_decode</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">labels_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">labels_host</span><span class="p">)</span>
            <span class="n">all_labels</span> <span class="o">=</span> <span class="n">labels</span> <span class="k">if</span> <span class="n">all_labels</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>

        <span class="c1"># Number of samples</span>
        <span class="k">if</span> <span class="n">has_length</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
            <span class="n">num_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_samples</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">observed_num_examples</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">num_samples</span> <span class="o">=</span> <span class="n">observed_num_examples</span>

        <span class="c1"># Metrics!</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">all_preds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">all_labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">include_inputs_for_metrics</span><span class="p">:</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span><span class="p">(</span>
                    <span class="n">EvalPrediction</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">label_ids</span><span class="o">=</span><span class="n">all_labels</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">all_inputs</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span><span class="p">(</span><span class="n">EvalPrediction</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">label_ids</span><span class="o">=</span><span class="n">all_labels</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># To be JSON-serializable, we need to remove numpy types or zero-d tensors</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="n">denumpify_detensorize</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">all_losses</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">all_losses</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Prefix all keys with metric_key_prefix + &#39;_&#39;</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_&quot;</span><span class="p">):</span>
                <span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">EvalLoopOutput</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">label_ids</span><span class="o">=</span><span class="n">all_labels</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_nested_gather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before</span>
<span class="sd">        concatenating them to `gathered`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">tensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">return</span> <span class="n">tensors</span>

    <span class="k">def</span> <span class="nf">prediction_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
        <span class="n">prediction_loss_only</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform an evaluation step on `model` using `inputs`.</span>

<span class="sd">        Subclass and override to inject custom behavior.</span>

<span class="sd">        Args:</span>
<span class="sd">            model (`nn.cell`):</span>
<span class="sd">                The model to evaluate.</span>
<span class="sd">            inputs (`Dict[str, Union[mindspore.Tensor, Any]]`):</span>
<span class="sd">                The inputs and targets of the model.</span>

<span class="sd">                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the</span>
<span class="sd">                argument `labels`. Check your model&#39;s documentation for all accepted arguments.</span>
<span class="sd">            prediction_loss_only (`bool`):</span>
<span class="sd">                Whether or not to return the loss only.</span>
<span class="sd">            ignore_keys (`List[str]`, *optional*):</span>
<span class="sd">                A list of keys in the output of your model (if it is a dictionary) that should be ignored when</span>
<span class="sd">                gathering predictions.</span>

<span class="sd">        Return:</span>
<span class="sd">            Tuple[Optional[mindspore.Tensor], Optional[mindspore.Tensor], Optional[mindspore.Tensor]]: A tuple with the loss,</span>
<span class="sd">            logits and labels (each being optional).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">has_labels</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="nb">all</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_names</span><span class="p">)</span>
        <span class="c1"># For CLIP-like models capable of returning loss values.</span>
        <span class="c1"># If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`</span>
        <span class="c1"># is `True` in `model.forward`.</span>
        <span class="n">return_loss</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;return_loss&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_loss</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">can_return_loss</span>
        <span class="n">loss_without_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">return_loss</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ignore_keys</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">):</span>
                <span class="n">ignore_keys</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;keys_to_ignore_at_inference&quot;</span><span class="p">,</span> <span class="p">[])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ignore_keys</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># labels may be popped when computing the loss (label smoothing for instance) so we grab them first.</span>
        <span class="k">if</span> <span class="n">has_labels</span> <span class="ow">or</span> <span class="n">loss_without_labels</span><span class="p">:</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_names</span><span class="p">)</span>
            <span class="c1"># labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">has_labels</span> <span class="ow">or</span> <span class="n">loss_without_labels</span><span class="p">:</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">return_outputs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ignore_keys</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ignore_keys</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span>
            <span class="c1"># TODO: this needs to be fixed and made cleaner later.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_past</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">prediction_loss_only</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># logits = nested_detach(logits)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Log `logs` on the various objects watching training.</span>

<span class="sd">        Subclass and override this method to inject custom behavior.</span>

<span class="sd">        Args:</span>
<span class="sd">            logs (`Dict[str, float]`):</span>
<span class="sd">                The values to log.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logs</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">include_num_input_tokens_seen</span><span class="p">:</span>
            <span class="n">logs</span><span class="p">[</span><span class="s2">&quot;num_input_tokens_seen&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">num_input_tokens_seen</span>

        <span class="n">output</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">logs</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span><span class="p">}}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">log_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">,</span> <span class="n">logs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sorted_checkpoints</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">output_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">checkpoint_prefix</span><span class="o">=</span><span class="n">PREFIX_CHECKPOINT_DIR</span><span class="p">,</span> <span class="n">use_mtime</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to retrieve and sort checkpoints based on specific criteria.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: Trainer object, the instance invoking the method.</span>
<span class="sd">            output_dir (str, optional): Directory path to search for checkpoints. Defaults to None.</span>
<span class="sd">            checkpoint_prefix (str): Prefix for identifying checkpoint files.</span>
<span class="sd">            use_mtime (bool): Flag to indicate whether to use modification time for sorting.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List[str]: A list of sorted checkpoint file paths.</span>

<span class="sd">        Raises:</span>
<span class="sd">            N/A</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ordering_and_checkpoint_path</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">glob_checkpoints</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Path</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">checkpoint_prefix</span><span class="si">}</span><span class="s2">-*&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>

        <span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">glob_checkpoints</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_mtime</span><span class="p">:</span>
                <span class="n">ordering_and_checkpoint_path</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">getmtime</span><span class="p">(</span><span class="n">path</span><span class="p">),</span> <span class="n">path</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">regex_match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;.*</span><span class="si">{</span><span class="n">checkpoint_prefix</span><span class="si">}</span><span class="s2">-([0-9]+)&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">regex_match</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">regex_match</span><span class="o">.</span><span class="n">groups</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">ordering_and_checkpoint_path</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">regex_match</span><span class="o">.</span><span class="n">groups</span><span class="p">()[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">path</span><span class="p">))</span>

        <span class="n">checkpoints_sorted</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">ordering_and_checkpoint_path</span><span class="p">)</span>
        <span class="n">checkpoints_sorted</span> <span class="o">=</span> <span class="p">[</span><span class="n">checkpoint</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">checkpoint</span> <span class="ow">in</span> <span class="n">checkpoints_sorted</span><span class="p">]</span>
        <span class="c1"># Make sure we don&#39;t delete the best model.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span><span class="p">))</span> <span class="ow">in</span> <span class="n">checkpoints_sorted</span>
        <span class="p">):</span>
            <span class="n">best_model_index</span> <span class="o">=</span> <span class="n">checkpoints_sorted</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_model_checkpoint</span><span class="p">)))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">best_model_index</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">checkpoints_sorted</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
                <span class="n">checkpoints_sorted</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">checkpoints_sorted</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">checkpoints_sorted</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">checkpoints_sorted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">checkpoints_sorted</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">map_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">train_dataset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eval_dataset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">model_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">compute_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimizers</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">preprocess_logits_for_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Initializes the Trainer class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Trainer object itself.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.engine.trainer.base.Trainer" href="../../../../../api/engine/trainer/base/#mindnlp.engine.trainer.base.Trainer">Trainer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The pre-trained model or neural network cell to be trained.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="mindnlp.transformers.modeling_utils.PreTrainedModel" href="../../../../../api/transformers/modeling_utils/#mindnlp.transformers.modeling_utils.PreTrainedModel">PreTrainedModel</a>, <span title="mindnlp.core.nn.Module">Module</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>args</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The training arguments including hyperparameters and output directory.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.engine.train_args.TrainingArguments" href="../../../../../api/engine/train_args/base/#mindnlp.engine.train_args.TrainingArguments">TrainingArguments</a></code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>map_fn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional map function for data preprocessing.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, <span title="mindnlp.dataset.BaseMapFunction">BaseMapFunction</span>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>train_dataset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The training dataset.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.dataset.Dataset">Dataset</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eval_dataset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The evaluation dataset.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="mindspore.dataset.Dataset">Dataset</span>, <span title="typing.Dict">Dict</span>[str, <span title="mindspore.dataset.Dataset">Dataset</span>]]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The pre-trained tokenizer for tokenizing inputs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="mindnlp.transformers.tokenization_utils_base.PreTrainedTokenizerBase" href="../../../../../api/transformers/tokenization_utils_base/#mindnlp.transformers.tokenization_utils_base.PreTrainedTokenizerBase">PreTrainedTokenizerBase</a>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model_init</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional model initialization function.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>[[], <a class="autorefs autorefs-internal" title="mindnlp.transformers.modeling_utils.PreTrainedModel" href="../../../../../api/transformers/modeling_utils/#mindnlp.transformers.modeling_utils.PreTrainedModel">PreTrainedModel</a>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>compute_metrics</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional function to compute evaluation metrics.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>[[<a class="autorefs autorefs-internal" title="mindnlp.engine.utils.EvalPrediction" href="../../../../../api/engine/utils/#mindnlp.engine.utils.EvalPrediction">EvalPrediction</a>], <span title="typing.Dict">Dict</span>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>callbacks</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional list of trainer callbacks.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<a class="autorefs autorefs-internal" title="mindnlp.engine.callbacks.TrainerCallback" href="../../../../../api/engine/callbacks/#mindnlp.engine.callbacks.TrainerCallback">TrainerCallback</a>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>optimizers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of optimizer and learning rate scheduler.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Tuple">Tuple</span>[<span title="mindnlp.core.nn.Optimizer">Optimizer</span>, LearningRateSchedule]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>(None, None)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>preprocess_logits_for_metrics</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional function to preprocess logits for metrics.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>[[<span title="mindspore.Tensor">Tensor</span>, <span title="mindspore.Tensor">Tensor</span>], <span title="mindspore.Tensor">Tensor</span>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method does not return any value.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If <code>model</code> or <code>model_init</code> is not provided.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the provided model cannot be used for training, or if there is an issue with the map function.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If <code>train_dataset</code> does not implement <strong>len</strong> and <code>max_steps</code> is not specified.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If there is a conflict between <code>model_init</code> and <code>optimizers</code> arguments.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedModel</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">TrainingArguments</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">map_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">BaseMapFunction</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dataset</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Dataset</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PreTrainedTokenizerBase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">PreTrainedModel</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">EvalPrediction</span><span class="p">],</span> <span class="n">Dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TrainerCallback</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">optimizers</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="n">preprocess_logits_for_metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the Trainer class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (Trainer): The Trainer object itself.</span>
<span class="sd">        model (Union[PreTrainedModel, nn.Module]): The pre-trained model or neural network cell to be trained.</span>
<span class="sd">        args (TrainingArguments): The training arguments including hyperparameters and output directory.</span>
<span class="sd">        map_fn (Optional[Union[Callable, BaseMapFunction]]): Optional map function for data preprocessing.</span>
<span class="sd">        train_dataset (Optional[Dataset]): The training dataset.</span>
<span class="sd">        eval_dataset (Optional[Union[Dataset, Dict[str, Dataset]]]): The evaluation dataset.</span>
<span class="sd">        tokenizer (Optional[PreTrainedTokenizerBase]): The pre-trained tokenizer for tokenizing inputs.</span>
<span class="sd">        model_init (Optional[Callable[[], PreTrainedModel]]): Optional model initialization function.</span>
<span class="sd">        compute_metrics (Optional[Callable[[EvalPrediction], Dict]]): Optional function to compute evaluation metrics.</span>
<span class="sd">        callbacks (Optional[List[TrainerCallback]]): Optional list of trainer callbacks.</span>
<span class="sd">        optimizers (Tuple[nn.Optimizer, LearningRateSchedule]): Tuple of optimizer and learning rate scheduler.</span>
<span class="sd">        preprocess_logits_for_metrics (Optional[Callable[[mindspore.Tensor, mindspore.Tensor], mindspore.Tensor]]): Optional function to preprocess logits for metrics.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: This method does not return any value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: If `model` or `model_init` is not provided.</span>
<span class="sd">        ValueError: If the provided model cannot be used for training, or if there is an issue with the map function.</span>
<span class="sd">        ValueError: If `train_dataset` does not implement __len__ and `max_steps` is not specified.</span>
<span class="sd">        RuntimeError: If there is a conflict between `model_init` and `optimizers` arguments.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;tmp_trainer&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No `TrainingArguments` passed, using `output_dir=</span><span class="si">{</span><span class="n">output_dir</span><span class="si">}</span><span class="s2">`.&quot;</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span>
    <span class="c1"># Seed must be set before instantiating the model when using model</span>
    <span class="c1"># mindspore do not support full determinisim on 2.2</span>
    <span class="n">enable_full_determinism</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">full_determinism</span> <span class="k">else</span> <span class="n">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">is_in_train</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">create_accelerator_and_postprocess</span><span class="p">()</span>

    <span class="c1"># set the correct log level depending on the node</span>
    <span class="n">log_level</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">get_process_log_level</span><span class="p">()</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">log_level</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">model_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_init</span> <span class="o">=</span> <span class="n">model_init</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_model_init</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;`Trainer` requires either a `model` or `model_init` argument&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">model_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;`Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will&quot;</span>
                <span class="s2">&quot; overwrite your model when calling the `train` method&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_init</span> <span class="o">=</span> <span class="n">model_init</span>

    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="n">MODEL_MAPPING_NAMES</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The model you have picked (</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">) cannot be used as is for training: it only &quot;</span>
            <span class="s2">&quot;computes hidden states and does not accept any labels. You should choose a model with a head &quot;</span>
            <span class="s2">&quot;suitable for your task like any of the `AutoModelForXxx` listed at &quot;</span>
            <span class="s2">&quot;https://huggingface.co/docs/transformers/model_doc/auto&quot;</span>
        <span class="p">)</span>

    <span class="c1"># if hasattr(model, &quot;is_parallelizable&quot;) and model.is_parallelizable and model.model_parallel:</span>
    <span class="c1">#     self.is_model_parallel = True</span>
    <span class="c1"># else:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">is_model_parallel</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># TODO: support quantized model</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">data_map_fn</span> <span class="o">=</span> <span class="n">map_fn</span>
    <span class="k">if</span> <span class="n">map_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">map_fn</span><span class="p">,</span> <span class="s1">&#39;input_columns&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">map_fn</span><span class="p">,</span> <span class="s1">&#39;output_columns&#39;</span><span class="p">))</span> <span class="ow">and</span> \
        <span class="ow">not</span> <span class="n">check_input_output_count</span><span class="p">(</span><span class="n">map_fn</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`map_fn` must have same number of inputs and outputs when it is callable function&#39;</span>
                         <span class="s1">&#39; without attributes `input_columns` and `output_columns`&#39;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>

    <span class="c1"># later use `self.model is self.model_wrapped` to check if it&#39;s wrapped or not</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">neftune_noise_alpha</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">neftune_noise_alpha</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span> <span class="o">=</span> <span class="n">compute_metrics</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_logits_for_metrics</span> <span class="o">=</span> <span class="n">preprocess_logits_for_metrics</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">optimizers</span>
    <span class="k">if</span> <span class="n">model_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Passing a `model_init` is incompatible with providing the `optimizers` argument. &quot;</span>
            <span class="s2">&quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;</span>
        <span class="p">)</span>

    <span class="n">default_callbacks</span> <span class="o">=</span> <span class="n">DEFAULT_CALLBACKS</span>
    <span class="n">callbacks</span> <span class="o">=</span> <span class="n">default_callbacks</span> <span class="k">if</span> <span class="n">callbacks</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">default_callbacks</span> <span class="o">+</span> <span class="n">callbacks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span> <span class="o">=</span> <span class="n">CallbackHandler</span><span class="p">(</span>
        <span class="n">callbacks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span><span class="n">PrinterCallback</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">disable_tqdm</span> <span class="k">else</span> <span class="n">DEFAULT_PROGRESS_CALLBACK</span><span class="p">)</span>

    <span class="c1"># Will be set to True by `self._setup_loggers()` on first call to `self.log()`.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loggers_initialized</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">should_save</span><span class="p">:</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;max_steps is given, it will override any value given in num_train_epochs&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">train_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">has_length</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The train_dataset does not implement __len__, max_steps has to be specified. &quot;</span>
            <span class="s2">&quot;The number of steps needs to be known in advance for the learning rate scheduler.&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_signature_columns</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Mixed precision setup</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_amp</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Label smoothing</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">label_smoothing_factor</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_smoother</span> <span class="o">=</span> <span class="n">LabelSmoother</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">label_smoothing_factor</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_smoother</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">TrainerState</span><span class="p">(</span>
        <span class="n">is_local_process_zero</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_local_process_zero</span><span class="p">(),</span>
        <span class="n">is_world_process_zero</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">(),</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="n">TrainerControl</span><span class="p">()</span>
    <span class="c1"># Internal variable to count flos in each process, will be accumulated in `self.state.total_flos` then</span>
    <span class="c1"># returned to 0 every time flos need to be logged</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">current_flos</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hp_search_backend</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">default_label_names</span> <span class="o">=</span> <span class="n">find_labels</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_names</span> <span class="o">=</span> <span class="n">default_label_names</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">label_names</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">label_names</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">can_return_loss</span> <span class="o">=</span> <span class="n">can_return_loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_init_end</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>
    <span class="c1"># Internal variables to help with automatic batch size reduction</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">train_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_created_lr_scheduler</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.add_callback" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.add_callback" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Add a callback to the current list of [<code>~transformers.TrainerCallback</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>callback</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A [<code>~transformers.TrainerCallback</code>] class or an instance of a [<code>~transformers.TrainerCallback</code>]. In the
 first case, will instantiate a member of that class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`type` or [`~transformers.TrainerCallback`]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">add_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callback</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add a callback to the current list of [`~transformers.TrainerCallback`].</span>

<span class="sd">    Args:</span>
<span class="sd">       callback (`type` or [`~transformers.TrainerCallback`]):</span>
<span class="sd">           A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the</span>
<span class="sd">           first case, will instantiate a member of that class.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.call_model_init" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">call_model_init</span><span class="p">()</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.call_model_init" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Method to call the model initialization function and validate its output.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Instance of the Trainer class.
This parameter represents the current instance of the Trainer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.engine.trainer.base.Trainer" href="../../../../../api/engine/trainer/base/#mindnlp.engine.trainer.base.Trainer">Trainer</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method does not return any value.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the model_init method does not have 0 or 1 arguments.
This exception is raised when the number of arguments in the model_init method is not 0 or 1.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the model_init method returns None.
This exception is raised when the model_init method returns None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">call_model_init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method to call the model initialization function and validate its output.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (Trainer): Instance of the Trainer class.</span>
<span class="sd">            This parameter represents the current instance of the Trainer class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: This method does not return any value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: If the model_init method does not have 0 or 1 arguments.</span>
<span class="sd">            This exception is raised when the number of arguments in the model_init method is not 0 or 1.</span>
<span class="sd">        RuntimeError: If the model_init method returns None.</span>
<span class="sd">            This exception is raised when the model_init method returns None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_init_argcount</span> <span class="o">=</span> <span class="n">number_of_arguments</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_init</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">model_init_argcount</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_init</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;model_init should have 0 or 1 argument.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;model_init should not return None.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.compute_loss" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">return_outputs</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.compute_loss" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>How the loss is computed by Trainer. By default, all models return the loss in the first element.</p>
<p>Subclass and override for custom behavior.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">return_outputs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    How the loss is computed by Trainer. By default, all models return the loss in the first element.</span>

<span class="sd">    Subclass and override for custom behavior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_smoother</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;labels&quot;</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
    <span class="c1"># Save past state if it exists</span>
    <span class="c1"># TODO: this needs to be fixed and made cleaner later.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_past</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">past_index</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># unwrapped_model = self.accelerator.unwrap_model(model)</span>
        <span class="n">unwrapped_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="k">if</span> <span class="n">_is_peft_model</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="p">):</span>
            <span class="n">model_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="o">.</span><span class="n">get_base_model</span><span class="p">())</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">MODEL_FOR_CAUSAL_LM_MAPPING_NAMES</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_smoother</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">shift_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_smoother</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;loss&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The model did not return a loss from the inputs, only the following keys: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. For reference, the inputs it received are </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># We don&#39;t use .loss here since the model may return tuples instead of ModelOutput.</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_outputs</span> <span class="k">else</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.create_optimizer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">create_optimizer</span><span class="p">()</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.create_optimizer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Setup the optimizer.</p>
<p>We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
Trainer's init through <code>optimizers</code>, or subclass and override this method in a subclass.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Setup the optimizer.</span>

<span class="sd">    We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the</span>
<span class="sd">    Trainer&#39;s init through `optimizers`, or subclass and override this method in a subclass.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">opt_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">decay_parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_decay_parameter_names</span><span class="p">(</span><span class="n">opt_model</span><span class="p">)</span>
        <span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">opt_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
                <span class="p">],</span>
                <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="p">{</span>
                <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">opt_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
                <span class="p">],</span>
                <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">]</span>

        <span class="n">optimizer_cls</span><span class="p">,</span> <span class="n">optimizer_kwargs</span> <span class="o">=</span> <span class="n">Trainer</span><span class="o">.</span><span class="n">get_optimizer_cls_and_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">opt_model</span><span class="p">)</span>

        <span class="c1"># Overwrite `params` in case it&#39;s created by `get_optimizer_cls_and_kwargs`</span>
        <span class="c1"># e.g. for GaLore optimizer.</span>
        <span class="k">if</span> <span class="s2">&quot;params&quot;</span> <span class="ow">in</span> <span class="n">optimizer_kwargs</span><span class="p">:</span>
            <span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="n">optimizer_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;params&quot;</span><span class="p">)</span>

        <span class="c1"># For layer-wise dummy optimizers we overwrite optimizer_grouped_parameters with `optimizer_dict`</span>
        <span class="c1"># to avoid arguments conflicts.</span>
        <span class="k">if</span> <span class="s2">&quot;optimizer_dict&quot;</span> <span class="ow">in</span> <span class="n">optimizer_kwargs</span><span class="p">:</span>
            <span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="n">optimizer_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;optimizer_dict&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="p">(</span><span class="n">optimizer_grouped_parameters</span><span class="p">,</span> <span class="o">**</span><span class="n">optimizer_kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.create_optimizer_and_scheduler" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">create_optimizer_and_scheduler</span><span class="p">(</span><span class="n">num_training_steps</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.create_optimizer_and_scheduler" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Setup the optimizer and the learning rate scheduler.</p>
<p>We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
Trainer's init through <code>optimizers</code>, or subclass and override this method (or <code>create_optimizer</code> and/or
<code>create_scheduler</code>) in a subclass.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_optimizer_and_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Setup the optimizer and the learning rate scheduler.</span>

<span class="sd">    We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the</span>
<span class="sd">    Trainer&#39;s init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or</span>
<span class="sd">    `create_scheduler`) in a subclass.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">create_optimizer</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">create_scheduler</span><span class="p">(</span><span class="n">num_training_steps</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.create_scheduler" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">create_scheduler</span><span class="p">(</span><span class="n">num_training_steps</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.create_scheduler" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or
passed as an argument.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>num_training_steps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of training steps to do.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or</span>
<span class="sd">    passed as an argument.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_training_steps (int): The number of training steps to do.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">...transformers.optimization</span> <span class="kn">import</span> <span class="n">get_scheduler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">get_scheduler</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">lr_scheduler_type</span><span class="p">,</span>
            <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">optimizer</span><span class="p">,</span>
            <span class="n">num_warmup_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">get_warmup_steps</span><span class="p">(</span><span class="n">num_training_steps</span><span class="p">),</span>
            <span class="n">num_training_steps</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">,</span>
            <span class="n">scheduler_specific_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">lr_scheduler_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_created_lr_scheduler</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.evaluate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metric_key_prefix</span><span class="o">=</span><span class="s1">&#39;eval&#39;</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.evaluate" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Run evaluation and returns metrics.</p>
<p>The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
(pass it to the init <code>compute_metrics</code> argument).</p>
<p>You can also subclass and override this method to inject custom behavior.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>eval_dataset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pass a dataset if you wish to override <code>self.eval_dataset</code>. If it is a [<code>~datasets.Dataset</code>], columns
not accepted by the <code>model.forward()</code> method are automatically removed. If it is a dictionary, it will
evaluate on each dataset, prepending the dictionary key to the metric name. Datasets must implement the
<code>__len__</code> method.</p>
<p><Tip></p>
<p>If you pass a dictionary with names of datasets as keys and datasets as values, evaluate will run
separate evaluations on each dataset. This can be useful to monitor how training affects other
datasets or simply to get a more fine-grained evaluation.
When used with <code>load_best_model_at_end</code>, make sure <code>metric_for_best_model</code> references exactly one
of the datasets. If you, for example, pass in <code>{"data1": data1, "data2": data2}</code> for two datasets
<code>data1</code> and <code>data2</code>, you could specify <code>metric_for_best_model="eval_data1_loss"</code> for using the
loss on <code>data1</code> and <code>metric_for_best_model="eval_data1_loss"</code> for the loss on <code>data2</code>.</p>
<p></Tip></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Union[`Dataset`, Dict[str, `Dataset`]), *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ignore_keys</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metric_key_prefix</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An optional prefix to be used as the metrics key prefix. For example the metrics "bleu" will be named
"eval_bleu" if the prefix is "eval" (default)</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;eval&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;eval&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Dict">Dict</span>[str, float]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Dict">Dict</span>[str, float]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>dictionary also contains the epoch number which comes from the training state.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Dataset</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_key_prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;eval&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run evaluation and returns metrics.</span>

<span class="sd">    The calling script will be responsible for providing a method to compute metrics, as they are task-dependent</span>
<span class="sd">    (pass it to the init `compute_metrics` argument).</span>

<span class="sd">    You can also subclass and override this method to inject custom behavior.</span>

<span class="sd">    Args:</span>
<span class="sd">        eval_dataset (Union[`Dataset`, Dict[str, `Dataset`]), *optional*):</span>
<span class="sd">            Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns</span>
<span class="sd">            not accepted by the `model.forward()` method are automatically removed. If it is a dictionary, it will</span>
<span class="sd">            evaluate on each dataset, prepending the dictionary key to the metric name. Datasets must implement the</span>
<span class="sd">            `__len__` method.</span>

<span class="sd">            &lt;Tip&gt;</span>

<span class="sd">            If you pass a dictionary with names of datasets as keys and datasets as values, evaluate will run</span>
<span class="sd">            separate evaluations on each dataset. This can be useful to monitor how training affects other</span>
<span class="sd">            datasets or simply to get a more fine-grained evaluation.</span>
<span class="sd">            When used with `load_best_model_at_end`, make sure `metric_for_best_model` references exactly one</span>
<span class="sd">            of the datasets. If you, for example, pass in `{&quot;data1&quot;: data1, &quot;data2&quot;: data2}` for two datasets</span>
<span class="sd">            `data1` and `data2`, you could specify `metric_for_best_model=&quot;eval_data1_loss&quot;` for using the</span>
<span class="sd">            loss on `data1` and `metric_for_best_model=&quot;eval_data1_loss&quot;` for the loss on `data2`.</span>

<span class="sd">            &lt;/Tip&gt;</span>

<span class="sd">        ignore_keys (`List[str]`, *optional*):</span>
<span class="sd">            A list of keys in the output of your model (if it is a dictionary) that should be ignored when</span>
<span class="sd">            gathering predictions.</span>
<span class="sd">        metric_key_prefix (`str`, *optional*, defaults to `&quot;eval&quot;`):</span>
<span class="sd">            An optional prefix to be used as the metrics key prefix. For example the metrics &quot;bleu&quot; will be named</span>
<span class="sd">            &quot;eval_bleu&quot; if the prefix is &quot;eval&quot; (default)</span>

<span class="sd">    Returns:</span>
<span class="sd">        A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The</span>
<span class="sd">        dictionary also contains the epoch number which comes from the training state.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># handle multipe eval datasets</span>
    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">eval_dataset</span> <span class="k">if</span> <span class="n">eval_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_dataset</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">eval_dataset_name</span><span class="p">,</span> <span class="n">_eval_dataset</span> <span class="ow">in</span> <span class="n">eval_dataset</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">dataset_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
                <span class="n">eval_dataset</span><span class="o">=</span><span class="n">_eval_dataset</span><span class="p">,</span>
                <span class="n">ignore_keys</span><span class="o">=</span><span class="n">ignore_keys</span><span class="p">,</span>
                <span class="n">metric_key_prefix</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">eval_dataset_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">dataset_metrics</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">metrics</span>

    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_eval_dataset</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span>

    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="n">eval_loop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_loop</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">eval_loop</span><span class="p">(</span>
        <span class="n">eval_dataset</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Evaluation&quot;</span><span class="p">,</span>
        <span class="c1"># No point gathering the predictions if there are no metrics, otherwise we defer to</span>
        <span class="c1"># self.args.prediction_loss_only</span>
        <span class="n">prediction_loss_only</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ignore_keys</span><span class="o">=</span><span class="n">ignore_keys</span><span class="p">,</span>
        <span class="n">metric_key_prefix</span><span class="o">=</span><span class="n">metric_key_prefix</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">total_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span>
    <span class="k">if</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_jit_compilation_time&quot;</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">:</span>
        <span class="n">start_time</span> <span class="o">+=</span> <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_jit_compilation_time&quot;</span><span class="p">]</span>
    <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="n">speed_metrics</span><span class="p">(</span>
            <span class="n">metric_key_prefix</span><span class="p">,</span>
            <span class="n">start_time</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">num_steps</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">/</span> <span class="n">total_batch_size</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_evaluate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">metrics</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.evaluation_loop" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">evaluation_loop</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">description</span><span class="p">,</span> <span class="n">prediction_loss_only</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metric_key_prefix</span><span class="o">=</span><span class="s1">&#39;eval&#39;</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.evaluation_loop" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Prediction/evaluation loop, shared by <code>Trainer.evaluate()</code> and <code>Trainer.predict()</code>.</p>
<p>Works both with or without labels.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">evaluation_loop</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">description</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">prediction_loss_only</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_key_prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;eval&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EvalLoopOutput</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.</span>

<span class="sd">    Works both with or without labels.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span>

    <span class="n">prediction_loss_only</span> <span class="o">=</span> <span class="n">prediction_loss_only</span> <span class="k">if</span> <span class="n">prediction_loss_only</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">prediction_loss_only</span>

    <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>
    <span class="c1"># if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn&#39;t called</span>
    <span class="c1"># while ``train`` is running, cast it to the right dtype first and then put on device</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_in_train</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16_full_eval</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">bf16_full_eval</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;***** Running </span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="s2"> *****&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">has_length</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Num examples = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_examples</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Num examples: Unknown&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Batch size = </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_past</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Initialize containers</span>
    <span class="c1"># losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)</span>
    <span class="n">losses_host</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">preds_host</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">labels_host</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">inputs_host</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># losses/preds/labels on CPU (final containers)</span>
    <span class="n">all_losses</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">all_preds</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">all_labels</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">all_inputs</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Will be useful when we have an iterable dataset so don&#39;t know its length.</span>

    <span class="n">observed_num_examples</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Main evaluation loop</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">inputs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">()):</span>
        <span class="c1"># Update the observed num examples</span>
        <span class="n">observed_batch_size</span> <span class="o">=</span> <span class="n">find_batch_size</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">observed_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">observed_num_examples</span> <span class="o">+=</span> <span class="n">observed_batch_size</span>
            <span class="c1"># For batch samplers, batch_size is not known by the dataloader in advance.</span>
            <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">batch_size</span> <span class="o">=</span> <span class="n">observed_batch_size</span>

        <span class="c1"># Prediction step</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prediction_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">prediction_loss_only</span><span class="p">,</span> <span class="n">ignore_keys</span><span class="o">=</span><span class="n">ignore_keys</span><span class="p">)</span>
        <span class="n">main_input_name</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;main_input_name&quot;</span><span class="p">,</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">)</span>
        <span class="n">inputs_decode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">main_input_name</span><span class="p">])</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">include_inputs_for_metrics</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># Update containers on host</span>
        <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">observed_batch_size</span><span class="p">)</span>
            <span class="c1"># losses = loss.repeat(batch_size)</span>
            <span class="n">losses_host</span> <span class="o">=</span> <span class="n">losses</span> <span class="k">if</span> <span class="n">losses_host</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">losses_host</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
        <span class="c1"># if labels is not None:</span>
        <span class="c1">#     labels = self.accelerator.pad_across_processes(labels, dim=1, pad_index=-100)</span>
        <span class="k">if</span> <span class="n">inputs_decode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># inputs_decode = self.accelerator.pad_across_processes(inputs_decode, dim=1, pad_index=-100)</span>
            <span class="c1"># inputs_decode = self.gather_function((inputs_decode))</span>
            <span class="n">inputs_host</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">inputs_decode</span>
                <span class="k">if</span> <span class="n">inputs_host</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">inputs_host</span><span class="p">,</span> <span class="n">inputs_decode</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">logits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># logits = self.accelerator.pad_across_processes(logits, dim=1, pad_index=-100)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_logits_for_metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_logits_for_metrics</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="c1"># logits = self.gather_function((logits))</span>
            <span class="n">preds_host</span> <span class="o">=</span> <span class="n">logits</span> <span class="k">if</span> <span class="n">preds_host</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">preds_host</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># labels = self.gather_function((labels))</span>
            <span class="n">labels_host</span> <span class="o">=</span> <span class="n">labels</span> <span class="k">if</span> <span class="n">labels_host</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">labels_host</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_prediction_step</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">)</span>

        <span class="c1"># Gather all tensors and put them back on the CPU if we have done enough accumulation steps.</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">eval_accumulation_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">eval_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">losses_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">losses</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">losses_host</span><span class="p">)</span>
                <span class="n">all_losses</span> <span class="o">=</span> <span class="n">losses</span> <span class="k">if</span> <span class="n">all_losses</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">all_losses</span><span class="p">,</span> <span class="n">losses</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">preds_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">preds_host</span><span class="p">)</span>
                <span class="n">all_preds</span> <span class="o">=</span> <span class="n">logits</span> <span class="k">if</span> <span class="n">all_preds</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inputs_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">inputs_decode</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">inputs_host</span><span class="p">)</span>
                <span class="n">all_inputs</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">inputs_decode</span>
                    <span class="k">if</span> <span class="n">all_inputs</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_inputs</span><span class="p">,</span> <span class="n">inputs_decode</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">labels_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">labels_host</span><span class="p">)</span>
                <span class="n">all_labels</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">labels</span> <span class="k">if</span> <span class="n">all_labels</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># Set back to None to begin a new accumulation</span>
            <span class="n">losses_host</span><span class="p">,</span> <span class="n">preds_host</span><span class="p">,</span> <span class="n">inputs_host</span><span class="p">,</span> <span class="n">labels_host</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_past&quot;</span><span class="p">):</span>
        <span class="c1"># Clean the state at the end of the evaluation loop</span>
        <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_past&quot;</span><span class="p">)</span>

    <span class="c1"># Gather all remaining tensors and put them back on the CPU</span>
    <span class="k">if</span> <span class="n">losses_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">losses_host</span><span class="p">)</span>
        <span class="n">all_losses</span> <span class="o">=</span> <span class="n">losses</span> <span class="k">if</span> <span class="n">all_losses</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">all_losses</span><span class="p">,</span> <span class="n">losses</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">preds_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">preds_host</span><span class="p">)</span>
        <span class="n">all_preds</span> <span class="o">=</span> <span class="n">logits</span> <span class="k">if</span> <span class="n">all_preds</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inputs_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs_decode</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">inputs_host</span><span class="p">)</span>
        <span class="n">all_inputs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">inputs_decode</span> <span class="k">if</span> <span class="n">all_inputs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_inputs</span><span class="p">,</span> <span class="n">inputs_decode</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">labels_host</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">nested_numpify</span><span class="p">(</span><span class="n">labels_host</span><span class="p">)</span>
        <span class="n">all_labels</span> <span class="o">=</span> <span class="n">labels</span> <span class="k">if</span> <span class="n">all_labels</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nested_concat</span><span class="p">(</span><span class="n">all_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">padding_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>

    <span class="c1"># Number of samples</span>
    <span class="k">if</span> <span class="n">has_length</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_samples</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">observed_num_examples</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">observed_num_examples</span>

    <span class="c1"># Metrics!</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">all_preds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">all_labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">include_inputs_for_metrics</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span><span class="p">(</span>
                <span class="n">EvalPrediction</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">label_ids</span><span class="o">=</span><span class="n">all_labels</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">all_inputs</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span><span class="p">(</span><span class="n">EvalPrediction</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">label_ids</span><span class="o">=</span><span class="n">all_labels</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># To be JSON-serializable, we need to remove numpy types or zero-d tensors</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">denumpify_detensorize</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">all_losses</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">all_losses</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># Prefix all keys with metric_key_prefix + &#39;_&#39;</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_&quot;</span><span class="p">):</span>
            <span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric_key_prefix</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">EvalLoopOutput</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">label_ids</span><span class="o">=</span><span class="n">all_labels</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.floating_point_ops" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">floating_point_ops</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.floating_point_ops" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>For models that inherit from [<code>PreTrainedModel</code>], uses that method to compute the number of floating point
operations for every backward + forward pass. If using another model, either implement such a method in the
model or subclass and override this method.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>inputs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The inputs and targets of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Union[mindspore.Tensor, Any]]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>int</code>: The number of floating-point operations.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">floating_point_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point</span>
<span class="sd">    operations for every backward + forward pass. If using another model, either implement such a method in the</span>
<span class="sd">    model or subclass and override this method.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (`Dict[str, Union[mindspore.Tensor, Any]]`):</span>
<span class="sd">            The inputs and targets of the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `int`: The number of floating-point operations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;floating_point_ops&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">floating_point_ops</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.get_decay_parameter_names" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">get_decay_parameter_names</span><span class="p">(</span><span class="n">model</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.get_decay_parameter_names" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Get all parameter names that weight decay will be applied to</p>
<p>Note that some models implement their own layernorm instead of calling nn.LayerNorm, weight decay could still
apply to those modules since this function only filter out instance of nn.LayerNorm</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_decay_parameter_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get all parameter names that weight decay will be applied to</span>

<span class="sd">    Note that some models implement their own layernorm instead of calling nn.LayerNorm, weight decay could still</span>
<span class="sd">    apply to those modules since this function only filter out instance of nn.LayerNorm</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">decay_parameters</span> <span class="o">=</span> <span class="n">get_parameter_names</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ALL_LAYERNORM_LAYERS</span><span class="p">)</span>
    <span class="n">decay_parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">decay_parameters</span> <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">decay_parameters</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.get_eval_dataset" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">get_eval_dataset</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-cached"><code>cached</code></small>
  </span>

<a href="#mindnlp.engine.trainer.base.Trainer.get_eval_dataset" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the test [<code>~mindspore.dataset.GeneratorDataset</code>].</p>
<p>Subclass and override this method if you want to inject some custom behavior.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>test_dataset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The test dataset to use. If it is a [<code>~datasets.Dataset</code>], columns not accepted by the
<code>model.forward()</code> method are automatically removed. It must implement <code>__len__</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.dataset`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@lru_cache</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_eval_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Dataset</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dataset</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the test [`~mindspore.dataset.GeneratorDataset`].</span>

<span class="sd">    Subclass and override this method if you want to inject some custom behavior.</span>

<span class="sd">    Args:</span>
<span class="sd">        test_dataset (`mindspore.dataset`, *optional*):</span>
<span class="sd">            The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the</span>
<span class="sd">            `model.forward()` method are automatically removed. It must implement `__len__`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># data_collator = self.data_collator</span>
    <span class="k">if</span> <span class="n">eval_dataset</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_dataset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trainer: evaluation requires an eval_dataset.&quot;</span><span class="p">)</span>

    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">eval_dataset</span> <span class="k">if</span> <span class="n">eval_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_dataset</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="p">(</span><span class="n">BatchDataset</span><span class="p">,</span> <span class="n">PaddedBatchDataset</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">eval_dataset</span>

    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_remove_unused_columns</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">eval_dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_drop_last</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_num_workers</span><span class="p">)</span>

    <span class="c1"># We use the same batch_size as for eval.</span>
    <span class="k">return</span> <span class="n">eval_dataset</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.get_optimizer_cls_and_kwargs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">get_optimizer_cls_and_kwargs</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#mindnlp.engine.trainer.base.Trainer.get_optimizer_cls_and_kwargs" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the optimizer class and optimizer parameters based on the training arguments.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>args</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The training arguments for the training session.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`transformers.training_args.TrainingArguments`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">get_optimizer_cls_and_kwargs</span><span class="p">(</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PreTrainedModel</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the optimizer class and optimizer parameters based on the training arguments.</span>

<span class="sd">    Args:</span>
<span class="sd">        args (`transformers.training_args.TrainingArguments`):</span>
<span class="sd">            The training arguments for the training session.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># parse args.optim_args</span>
    <span class="n">optim_args</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">optim_args</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">mapping</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">optim_args</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">):</span>
            <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">mapping</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="p">)</span>
            <span class="n">optim_args</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="n">optimizer_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">}</span>

    <span class="n">adam_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;betas&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">adam_beta1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">adam_beta2</span><span class="p">),</span>
        <span class="s2">&quot;eps&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">adam_epsilon</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="c1"># TODO: support Adafactor</span>
    <span class="c1"># if args.optim == OptimizerNames.ADAFACTOR:</span>
    <span class="c1">#     optimizer_cls = Adafactor</span>
    <span class="c1">#     optimizer_kwargs.update({&quot;scale_parameter&quot;: False, &quot;relative_step&quot;: False})</span>
    <span class="c1"># TODO: support AdamW huggingface version</span>
    <span class="c1"># elif args.optim == OptimizerNames.ADAMW_HF:</span>
    <span class="c1">#     from .optimization import AdamW</span>

    <span class="c1">#     optimizer_cls = AdamW</span>
    <span class="c1">#     optimizer_kwargs.update(adam_kwargs)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="n">OptimizerNames</span><span class="o">.</span><span class="n">ADAMW</span><span class="p">:</span>
        <span class="n">optimizer_cls</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span>
        <span class="n">optimizer_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">adam_kwargs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="n">OptimizerNames</span><span class="o">.</span><span class="n">SGD</span><span class="p">:</span>
        <span class="n">optimizer_cls</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span>
    <span class="c1"># TODO: support Adagrad and Rmsporp</span>
    <span class="c1"># elif args.optim == OptimizerNames.ADAGRAD:</span>
    <span class="c1">#     optimizer_cls = mindspore.nn.Adagrad</span>
    <span class="c1"># elif args.optim == OptimizerNames.RMSPROP:</span>
    <span class="c1">#     optimizer_cls = mindspore.nn.RMSprop</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Trainer cannot instantiate unsupported optimizer: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">optim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">optimizer_cls</span><span class="p">,</span> <span class="n">optimizer_kwargs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.get_test_dataset" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">get_test_dataset</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-cached"><code>cached</code></small>
  </span>

<a href="#mindnlp.engine.trainer.base.Trainer.get_test_dataset" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the test [<code>~mindspore.dataset.GeneratorDataset</code>].</p>
<p>Subclass and override this method if you want to inject some custom behavior.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>test_dataset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The test dataset to use. If it is a [<code>~datasets.Dataset</code>], columns not accepted by the
<code>model.forward()</code> method are automatically removed. It must implement <code>__len__</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.dataset`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@lru_cache</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_test_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dataset</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the test [`~mindspore.dataset.GeneratorDataset`].</span>

<span class="sd">    Subclass and override this method if you want to inject some custom behavior.</span>

<span class="sd">    Args:</span>
<span class="sd">        test_dataset (`mindspore.dataset`, *optional*):</span>
<span class="sd">            The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the</span>
<span class="sd">            `model.forward()` method are automatically removed. It must implement `__len__`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># data_collator = self.data_collator</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="p">(</span><span class="n">BatchDataset</span><span class="p">,</span> <span class="n">PaddedBatchDataset</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">test_dataset</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_remove_unused_columns</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_drop_last</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_num_workers</span><span class="p">)</span>

    <span class="c1"># We use the same batch_size as for eval.</span>
    <span class="k">return</span> <span class="n">test_dataset</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.get_train_dataset" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">get_train_dataset</span><span class="p">()</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-cached"><code>cached</code></small>
  </span>

<a href="#mindnlp.engine.trainer.base.Trainer.get_train_dataset" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the training [<code>~mindspore.dataset.GeneratorDataset</code>].</p>
<p>Will use no sampler if <code>train_dataset</code> does not implement <code>__len__</code>, a random sampler (adapted to distributed
training if necessary) otherwise.</p>
<p>Subclass and override this method if you want to inject some custom behavior.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@lru_cache</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_train_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dataset</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the training [`~mindspore.dataset.GeneratorDataset`].</span>

<span class="sd">    Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed</span>
<span class="sd">    training if necessary) otherwise.</span>

<span class="sd">    Subclass and override this method if you want to inject some custom behavior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trainer: training requires a train_dataset.&quot;</span><span class="p">)</span>

    <span class="n">train_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span>
    <span class="n">map_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_map_fn</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="p">(</span><span class="n">BatchDataset</span><span class="p">,</span> <span class="n">PaddedBatchDataset</span><span class="p">)):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_map_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The trainer has been passed a `map_fn` and found `BatchDataset` at same time, &quot;</span>
                           <span class="s2">&quot;the `map_fn` will be ignored.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">train_dataset</span>

    <span class="k">if</span> <span class="n">map_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">mismatch_dataset_col_names</span><span class="p">(</span><span class="n">get_function_args</span><span class="p">(</span><span class="n">map_fn</span><span class="p">),</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">get_col_names</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The arguments of `map_fn` must be subset of useful dataset columns, &#39;</span>
                            <span class="sa">f</span><span class="s1">&#39;but found </span><span class="si">{</span><span class="n">args_only_in_map_fn</span><span class="p">(</span><span class="n">get_function_args</span><span class="p">(</span><span class="n">map_fn</span><span class="p">),</span><span class="w"> </span><span class="n">train_dataset</span><span class="o">.</span><span class="n">get_col_names</span><span class="p">())</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">map_fn</span><span class="p">,</span> <span class="n">map_fn</span><span class="o">.</span><span class="n">input_columns</span><span class="p">,</span> <span class="n">map_fn</span><span class="o">.</span><span class="n">output_columns</span><span class="p">)</span>

    <span class="n">train_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_remove_unused_columns</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>

    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_drop_last</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_num_workers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_dataset</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.is_local_process_zero" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">is_local_process_zero</span><span class="p">()</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.is_local_process_zero" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several
machines) main process.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">is_local_process_zero</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several</span>
<span class="sd">    machines) main process.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">local_process_index</span> <span class="o">==</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.is_world_process_zero" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">()</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.is_world_process_zero" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Whether or not this process is the global main process (when training in a distributed fashion on several
machines, this is only going to be <code>True</code> for one process).</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">is_world_process_zero</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Whether or not this process is the global main process (when training in a distributed fashion on several</span>
<span class="sd">    machines, this is only going to be `True` for one process).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Special case for SageMaker ModelParallel since there process_index is dp_process_index, not the global</span>
    <span class="c1"># process index.</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">process_index</span> <span class="o">==</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.log" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">logs</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.log" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Log <code>logs</code> on the various objects watching training.</p>
<p>Subclass and override this method to inject custom behavior.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>logs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The values to log.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log `logs` on the various objects watching training.</span>

<span class="sd">    Subclass and override this method to inject custom behavior.</span>

<span class="sd">    Args:</span>
<span class="sd">        logs (`Dict[str, float]`):</span>
<span class="sd">            The values to log.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logs</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">include_num_input_tokens_seen</span><span class="p">:</span>
        <span class="n">logs</span><span class="p">[</span><span class="s2">&quot;num_input_tokens_seen&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">num_input_tokens_seen</span>

    <span class="n">output</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">logs</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span><span class="p">}}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">log_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">,</span> <span class="n">logs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.num_examples" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">num_examples</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.num_examples" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Helper to get number of samples in a [<code>~mindspore.dataset.GeneratorDataset</code>] by accessing its dataset. When
dataloader.dataset does not exist or has no length, estimates as best it can</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">num_examples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="s1">&#39;mindspore.dataset.Dataset&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper to get number of samples in a [`~mindspore.dataset.GeneratorDataset`] by accessing its dataset. When</span>
<span class="sd">    dataloader.dataset does not exist or has no length, estimates as best it can</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">()</span> <span class="o">*</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_batch_size</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.num_tokens" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">num_tokens</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">max_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.num_tokens" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Helper to get number of tokens in a [<code>~mindspore.dataset.GeneratorDataset</code>] by enumerating dataloader.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">num_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_ds</span><span class="p">:</span> <span class="s1">&#39;mindspore.dataset.Dataset&#39;</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper to get number of tokens in a [`~mindspore.dataset.GeneratorDataset`] by enumerating dataloader.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">train_tokens</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">max_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">tokens</span> <span class="o">*</span> <span class="n">max_steps</span>
            <span class="n">train_tokens</span> <span class="o">+=</span> <span class="n">tokens</span>
        <span class="k">return</span> <span class="n">train_tokens</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Cannot get num_tokens from dataloader&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">train_tokens</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.pop_callback" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">pop_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.pop_callback" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Remove a callback from the current list of [<code>~transformers.TrainerCallback</code>] and returns it.</p>
<p>If the callback is not found, returns <code>None</code> (and no error is raised).</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>callback</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A [<code>~transformers.TrainerCallback</code>] class or an instance of a [<code>~transformers.TrainerCallback</code>]. In the
 first case, will pop the first member of that class found in the list of callbacks.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`type` or [`~transformers.TrainerCallback`]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>[<code>~transformers.TrainerCallback</code>]: The callback removed, if found.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">pop_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callback</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Remove a callback from the current list of [`~transformers.TrainerCallback`] and returns it.</span>

<span class="sd">    If the callback is not found, returns `None` (and no error is raised).</span>

<span class="sd">    Args:</span>
<span class="sd">       callback (`type` or [`~transformers.TrainerCallback`]):</span>
<span class="sd">           A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the</span>
<span class="sd">           first case, will pop the first member of that class found in the list of callbacks.</span>

<span class="sd">    Returns:</span>
<span class="sd">        [`~transformers.TrainerCallback`]: The callback removed, if found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">pop_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.predict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">ignore_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metric_key_prefix</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.predict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Run prediction and returns predictions and potential metrics.</p>
<p>Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in <code>evaluate()</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>test_dataset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dataset to run the predictions on. If it is an <code>datasets.Dataset</code>, columns not accepted by the
<code>model.forward()</code> method are automatically removed. Has to implement the method <code>__len__</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dataset`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ignore_keys</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metric_key_prefix</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An optional prefix to be used as the metrics key prefix. For example the metrics "bleu" will be named
"test_bleu" if the prefix is "test" (default)</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;test&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;test&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <p><Tip></p>
<p>If your predictions or labels have different sequence length (for instance because you're doing dynamic padding
in a token classification task) the predictions will be padded (on the right) to allow for concatenation into
one array. The padding index is -100.</p>
<p></Tip></p>
<p>Returns: <em>NamedTuple</em> A namedtuple with the following keys:</p>
<div class="highlight"><pre><span></span><code>- predictions (`np.ndarray`): The predictions on `test_dataset`.
- label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).
- metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained
  labels).
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">metric_key_prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;test&quot;</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run prediction and returns predictions and potential metrics.</span>

<span class="sd">    Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method</span>
<span class="sd">    will also return metrics, like in `evaluate()`.</span>

<span class="sd">    Args:</span>
<span class="sd">        test_dataset (`Dataset`):</span>
<span class="sd">            Dataset to run the predictions on. If it is an `datasets.Dataset`, columns not accepted by the</span>
<span class="sd">            `model.forward()` method are automatically removed. Has to implement the method `__len__`</span>
<span class="sd">        ignore_keys (`List[str]`, *optional*):</span>
<span class="sd">            A list of keys in the output of your model (if it is a dictionary) that should be ignored when</span>
<span class="sd">            gathering predictions.</span>
<span class="sd">        metric_key_prefix (`str`, *optional*, defaults to `&quot;test&quot;`):</span>
<span class="sd">            An optional prefix to be used as the metrics key prefix. For example the metrics &quot;bleu&quot; will be named</span>
<span class="sd">            &quot;test_bleu&quot; if the prefix is &quot;test&quot; (default)</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    If your predictions or labels have different sequence length (for instance because you&#39;re doing dynamic padding</span>
<span class="sd">    in a token classification task) the predictions will be padded (on the right) to allow for concatenation into</span>
<span class="sd">    one array. The padding index is -100.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Returns: *NamedTuple* A namedtuple with the following keys:</span>

<span class="sd">        - predictions (`np.ndarray`): The predictions on `test_dataset`.</span>
<span class="sd">        - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).</span>
<span class="sd">        - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained</span>
<span class="sd">          labels).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># memory metrics - must set up as early as possible</span>
    <span class="c1"># self._memory_tracker.start()</span>

    <span class="n">test_dataloader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_test_dataset</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="n">eval_loop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_loop</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">eval_loop</span><span class="p">(</span>
        <span class="n">test_dataloader</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Prediction&quot;</span><span class="p">,</span> <span class="n">ignore_keys</span><span class="o">=</span><span class="n">ignore_keys</span><span class="p">,</span> <span class="n">metric_key_prefix</span><span class="o">=</span><span class="n">metric_key_prefix</span>
    <span class="p">)</span>
    <span class="n">total_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span>
    <span class="c1"># if f&quot;{metric_key_prefix}_jit_compilation_time&quot; in output.metrics:</span>
    <span class="c1">#     start_time += output.metrics[f&quot;{metric_key_prefix}_jit_compilation_time&quot;]</span>
    <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="n">speed_metrics</span><span class="p">(</span>
            <span class="n">metric_key_prefix</span><span class="p">,</span>
            <span class="n">start_time</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">num_steps</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">/</span> <span class="n">total_batch_size</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">control</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">on_predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">control</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">PredictionOutput</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">predictions</span><span class="p">,</span> <span class="n">label_ids</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">label_ids</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.prediction_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">prediction_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">prediction_loss_only</span><span class="p">,</span> <span class="n">ignore_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.prediction_step" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Perform an evaluation step on <code>model</code> using <code>inputs</code>.</p>
<p>Subclass and override to inject custom behavior.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The model to evaluate.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`nn.cell`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inputs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The inputs and targets of the model.</p>
<p>The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
argument <code>labels</code>. Check your model's documentation for all accepted arguments.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Union[mindspore.Tensor, Any]]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prediction_loss_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the loss only.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ignore_keys</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p>Tuple[Optional[mindspore.Tensor], Optional[mindspore.Tensor], Optional[mindspore.Tensor]]: A tuple with the loss,
logits and labels (each being optional).</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prediction_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="n">prediction_loss_only</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform an evaluation step on `model` using `inputs`.</span>

<span class="sd">    Subclass and override to inject custom behavior.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (`nn.cell`):</span>
<span class="sd">            The model to evaluate.</span>
<span class="sd">        inputs (`Dict[str, Union[mindspore.Tensor, Any]]`):</span>
<span class="sd">            The inputs and targets of the model.</span>

<span class="sd">            The dictionary will be unpacked before being fed to the model. Most models expect the targets under the</span>
<span class="sd">            argument `labels`. Check your model&#39;s documentation for all accepted arguments.</span>
<span class="sd">        prediction_loss_only (`bool`):</span>
<span class="sd">            Whether or not to return the loss only.</span>
<span class="sd">        ignore_keys (`List[str]`, *optional*):</span>
<span class="sd">            A list of keys in the output of your model (if it is a dictionary) that should be ignored when</span>
<span class="sd">            gathering predictions.</span>

<span class="sd">    Return:</span>
<span class="sd">        Tuple[Optional[mindspore.Tensor], Optional[mindspore.Tensor], Optional[mindspore.Tensor]]: A tuple with the loss,</span>
<span class="sd">        logits and labels (each being optional).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">has_labels</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="nb">all</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_names</span><span class="p">)</span>
    <span class="c1"># For CLIP-like models capable of returning loss values.</span>
    <span class="c1"># If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`</span>
    <span class="c1"># is `True` in `model.forward`.</span>
    <span class="n">return_loss</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;return_loss&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_loss</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">return_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">can_return_loss</span>
    <span class="n">loss_without_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">return_loss</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ignore_keys</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">):</span>
            <span class="n">ignore_keys</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;keys_to_ignore_at_inference&quot;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ignore_keys</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># labels may be popped when computing the loss (label smoothing for instance) so we grab them first.</span>
    <span class="k">if</span> <span class="n">has_labels</span> <span class="ow">or</span> <span class="n">loss_without_labels</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_names</span><span class="p">)</span>
        <span class="c1"># labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">has_labels</span> <span class="ow">or</span> <span class="n">loss_without_labels</span><span class="p">:</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">return_outputs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ignore_keys</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ignore_keys</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span>
        <span class="c1"># TODO: this needs to be fixed and made cleaner later.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_past</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">past_index</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">prediction_loss_only</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="c1"># logits = nested_detach(logits)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.remove_callback" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">remove_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.remove_callback" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Remove a callback from the current list of [<code>~transformers.TrainerCallback</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>callback</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A [<code>~transformers.TrainerCallback</code>] class or an instance of a [<code>~transformers.TrainerCallback</code>]. In the
 first case, will remove the first member of that class found in the list of callbacks.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`type` or [`~transformers.TrainerCallback`]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">remove_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callback</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Remove a callback from the current list of [`~transformers.TrainerCallback`].</span>

<span class="sd">    Args:</span>
<span class="sd">       callback (`type` or [`~transformers.TrainerCallback`]):</span>
<span class="sd">           A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the</span>
<span class="sd">           first case, will remove the first member of that class found in the list of callbacks.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">callback_handler</span><span class="o">.</span><span class="n">remove_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.save_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">output_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_internal_call</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.save_model" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Will save the model, so you can reload it using <code>from_pretrained()</code>.</p>
<p>Will only save from the main process.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">_internal_call</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Will save the model, so you can reload it using `from_pretrained()`.</span>

<span class="sd">    Will only save from the main process.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">output_dir</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">should_save</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_save</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.store_flos" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">store_flos</span><span class="p">()</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.store_flos" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Stores the current number of floating point operations (FLOs) and updates the total FLOs count.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Trainer object itself.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.engine.trainer.base.Trainer" href="../../../../../api/engine/trainer/base/#mindnlp.engine.trainer.base.Trainer">Trainer</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method does not return any value.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">store_flos</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stores the current number of floating point operations (FLOs) and updates the total FLOs count.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (Trainer): The Trainer object itself.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: This method does not return any value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        N/A</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Storing the number of floating-point operations that went into the model</span>
    <span class="c1"># if self.args.parallel_mode == ParallelMode.DISTRIBUTED:</span>
    <span class="c1">#     self.state.total_flos += (</span>
    <span class="c1">#         distributed_broadcast_scalars([self.current_flos], device=self.args.device).sum().item()</span>
    <span class="c1">#     )</span>
    <span class="c1">#     self.current_flos = 0</span>
    <span class="c1"># else:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">total_flos</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_flos</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">current_flos</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.train" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_keys_for_eval</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.train" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Main training entry point.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>resume_from_checkpoint</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If a <code>str</code>, local path to a saved checkpoint as saved by a previous instance of [<code>Trainer</code>]. If a
<code>bool</code> and equals <code>True</code>, load the last checkpoint in <em>args.output_dir</em> as saved by a previous instance
of [<code>Trainer</code>]. If present, training will resume from the model/optimizer/scheduler states loaded here.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional keyword arguments used to hide deprecated arguments</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Any]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">resume_from_checkpoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ignore_keys_for_eval</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Main training entry point.</span>

<span class="sd">    Args:</span>
<span class="sd">        resume_from_checkpoint (`str` or `bool`, *optional*):</span>
<span class="sd">            If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a</span>
<span class="sd">            `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance</span>
<span class="sd">            of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.</span>
<span class="sd">        ignore_keys_for_eval (`List[str]`, *optional*)</span>
<span class="sd">            A list of keys in the output of your model (if it is a dictionary) that should be ignored when</span>
<span class="sd">            gathering predictions for evaluation during the training.</span>
<span class="sd">        kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">            Additional keyword arguments used to hide deprecated arguments</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">resume_from_checkpoint</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="n">resume_from_checkpoint</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_in_train</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Attach NEFTune hooks if necessary</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">neftune_noise_alpha</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activate_neftune</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;train() received got unexpected keyword arguments: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="c1"># This might change the seed so needs to run first.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">train_batch_size</span>
    <span class="c1"># Model re-init</span>
    <span class="n">model_reloaded</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Seed must be set before instantiating the model when using model_init.</span>
        <span class="n">enable_full_determinism</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">full_determinism</span> <span class="k">else</span> <span class="n">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_model_init</span><span class="p">()</span>
        <span class="n">model_reloaded</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># Reinitializes optimizer and scheduler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">model_reload</span> <span class="o">=</span> <span class="n">model_reloaded</span>

    <span class="c1"># Load potential model checkpoint</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span> <span class="ow">and</span> <span class="n">resume_from_checkpoint</span><span class="p">:</span>
        <span class="n">resume_from_checkpoint</span> <span class="o">=</span> <span class="n">get_last_checkpoint</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">resume_from_checkpoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No valid checkpoint found in output directory (</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">resume_from_checkpoint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_load_from_checkpoint</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">)</span>
        <span class="c1"># In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">TrainerState</span><span class="o">.</span><span class="n">load_from_json</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">TRAINER_STATE_NAME</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">train_batch_size</span>

    <span class="k">if</span> <span class="n">model_reloaded</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_wrapped</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>

    <span class="n">inner_training_loop</span> <span class="o">=</span> <span class="n">find_executable_batch_size</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_inner_training_loop</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">auto_find_batch_size</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">inner_training_loop</span><span class="p">(</span>
        <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
        <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="n">resume_from_checkpoint</span><span class="p">,</span>
        <span class="n">ignore_keys_for_eval</span><span class="o">=</span><span class="n">ignore_keys_for_eval</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.engine.trainer.base.Trainer.training_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">Trainer</span><span class="o">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span></code>

<a href="#mindnlp.engine.trainer.base.Trainer.training_step" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Perform a training step on a batch of inputs.</p>
<p>Subclass and override to inject custom behavior.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The model to train.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`nn.Module`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inputs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The inputs and targets of the model.</p>
<p>The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
argument <code>labels</code>. Check your model's documentation for all accepted arguments.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Union[mindspore.Tensor, Any]]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p><code>mindspore.Tensor</code>: The tensor with training loss on this batch.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\engine\trainer\base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform a training step on a batch of inputs.</span>

<span class="sd">    Subclass and override to inject custom behavior.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (`nn.Module`):</span>
<span class="sd">            The model to train.</span>
<span class="sd">        inputs (`Dict[str, Union[mindspore.Tensor, Any]]`):</span>
<span class="sd">            The inputs and targets of the model.</span>

<span class="sd">            The dictionary will be unpacked before being fed to the model. Most models expect the targets under the</span>
<span class="sd">            argument `labels`. Check your model&#39;s documentation for all accepted arguments.</span>

<span class="sd">    Return:</span>
<span class="sd">        `mindspore.Tensor`: The tensor with training loss on this batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;grad_fn&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_reload</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">+=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span><span class="p">,</span> <span class="n">grads</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../../train_args/seq2seq/" class="md-footer__link md-footer__link--prev" aria-label="上一页: seq2seq">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                seq2seq
              </div>
            </div>
          </a>
        
        
          
          <a href="../default_func/" class="md-footer__link md-footer__link--next" aria-label="下一页: default_func">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                default_func
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 MindSpore Lab and CQU NLP Team.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:lvyufeng@cqu.edu.cn" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindnlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/lu-yu-feng-46-1" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>