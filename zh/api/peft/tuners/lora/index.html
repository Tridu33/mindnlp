
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../lokr/">
      
      
        <link rel="next" href="../prompt_tuning/">
      
      
      <link rel="icon" href="../../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>LoRA - MindNLP Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindnlp.peft.tuners.lora.config" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../../../" title="MindNLP Docs" class="md-header__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindNLP Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LoRA
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="选择当前语言">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="../../../../../api/peft/tuners/lora/" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="./" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindnlp" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../" class="md-tabs__link">
        
  
    
  
  主页

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../tutorials/quick_start/" class="md-tabs__link">
          
  
    
  
  教程

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../supported_models/" class="md-tabs__link">
        
  
    
  
  模型列表

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../contribute/" class="md-tabs__link">
        
  
    
  
  代码贡献

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../accelerate/" class="md-tabs__link">
          
  
    
  
  API文档

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../notes/changelog/" class="md-tabs__link">
          
  
    
  
  说明

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../" title="MindNLP Docs" class="md-nav__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    MindNLP Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindnlp" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    主页
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/data_preprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocess
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_mirror/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Mirror
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../supported_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    模型列表
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    代码贡献
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API文档
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API文档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../accelerate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/load_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    load_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/BaseMapFunction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BaseMapFunction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transforms
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engine
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Engine
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_1" >
        
          
          <label class="md-nav__link" for="__nav_5_4_1" id="__nav_5_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    train_args
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_1">
            <span class="md-nav__icon md-icon"></span>
            train_args
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seq2seq
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_2" >
        
          
          <label class="md-nav__link" for="__nav_5_4_2" id="__nav_5_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_2">
            <span class="md-nav__icon md-icon"></span>
            trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/default_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    default_func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_2" checked>
        
          
          <label class="md-nav__link" for="__nav_5_7_2" id="__nav_5_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tuners
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_7_2">
            <span class="md-nav__icon md-icon"></span>
            tuners
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../adalora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AdaLoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../adaption_prompt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adaption_Prompt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ia3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lokr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoKr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.config" class="md-nav__link">
    <span class="md-ellipsis">
      config
    </span>
  </a>
  
    <nav class="md-nav" aria-label="config">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.config.LoftQConfig" class="md-nav__link">
    <span class="md-ellipsis">
      LoftQConfig
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.config.LoraConfig" class="md-nav__link">
    <span class="md-ellipsis">
      LoraConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoraConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.config.LoraConfig.to_dict" class="md-nav__link">
    <span class="md-ellipsis">
      to_dict
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model" class="md-nav__link">
    <span class="md-ellipsis">
      model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel" class="md-nav__link">
    <span class="md-ellipsis">
      LoraModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoraModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.__getattr__" class="md-nav__link">
    <span class="md-ellipsis">
      __getattr__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.add_weighted_adapter" class="md-nav__link">
    <span class="md-ellipsis">
      add_weighted_adapter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.delete_adapter" class="md-nav__link">
    <span class="md-ellipsis">
      delete_adapter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.disable_adapter_layers" class="md-nav__link">
    <span class="md-ellipsis">
      disable_adapter_layers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.enable_adapter_layers" class="md-nav__link">
    <span class="md-ellipsis">
      enable_adapter_layers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.get_peft_config_as_dict" class="md-nav__link">
    <span class="md-ellipsis">
      get_peft_config_as_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.merge_and_unload" class="md-nav__link">
    <span class="md-ellipsis">
      merge_and_unload
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.set_adapter" class="md-nav__link">
    <span class="md-ellipsis">
      set_adapter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.unload" class="md-nav__link">
    <span class="md-ellipsis">
      unload
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../prompt_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt tuning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_3" >
        
          
          <label class="md-nav__link" for="__nav_5_7_3" id="__nav_5_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/merge_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    merge_utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../peft_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    peft_model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sentence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../transformers/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_2" >
        
          
          <label class="md-nav__link" for="__nav_5_9_2" id="__nav_5_9_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_2">
            <span class="md-nav__icon md-icon"></span>
            generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../api/transforemrs/generation/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/beam_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/logits_process/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/stopping_criteria/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stopping_criteria
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/streamers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    streamers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/generation/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../transformers/models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9_3" id="__nav_5_9_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_3">
            <span class="md-nav__icon md-icon"></span>
            models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/albert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    albert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    align
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/altclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    altclip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/audio_spectrogram_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    audio_spectrogram_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/auto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    auto
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    autoformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/baichuan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    baichuan
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/barthez/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    barthez
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bartpho/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bartpho
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/beit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bert_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bert_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bertweet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bertweet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bge_m3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bge_m3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/big_bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    big_bird
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bigbird_pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bigbird_pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/biogpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    biogpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/blenderbot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/blenderbot_small/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot_small
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/blip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/blip_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip_2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bloom
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bridgetower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/bros/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bros
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/byt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    byt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/camembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    camembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/canine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    canine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/chatglm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/chatglm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/chatglm3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/codegen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    codegen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cogvlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cogvlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cohere/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/convbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/convnext/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convnext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cpmant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cpmbee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/ctrl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctrl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/cvt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cvt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/data2vec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data2vec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/deberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/deberta_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/decision_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decision_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/distilbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    distilbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/efficientformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/efficientnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/electra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    electra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/encodec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encodec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/ernie/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/ernie_m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie_m
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/esm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    esm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/flava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    flava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/funnel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    funnel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gemma
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt_bigcode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_bigcode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt_neo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt_neox/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt_neox_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gpt_pangu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_pangu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/gptj/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gptj
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/graphormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    graphormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/groupvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    groupvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/hubert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hubert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/imagegpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    imagegpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/internlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    internlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/jamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/jetmoe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jetmoe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/layoutlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/layoutlmv2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlmv2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/led/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    led
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/llava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/llava_next/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava_next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/longformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/longt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/luke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    luke
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/marian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    marian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/maskformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maskformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mbart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mbart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/megatron_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/megatron_gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/minicpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minicpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/minigpt4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minigpt4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mixtral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mobilebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mobilevit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilevit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/moss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    moss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mpnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/musicgen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/musicgen_melody/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen_melody
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/mvp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mvp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/nezha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nezha
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/nystromformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nystromformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/olmo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    olmo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/openelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    openelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    opt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/owlvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    owlvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/phi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/phi3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/poolformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    poolformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/pop2piano/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pop2piano
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/qwen2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2_moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/regnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/rembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    resnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/roc_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roc_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rwkv
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/sam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/seamless_m4t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/seamless_m4t_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/segformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    segformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/seggpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seggpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/speech_encoder_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_encoder_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/speech_to_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_to_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/squeezebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    squeezebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/stablelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stablelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    starcoder2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/swiftformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    swiftformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/switch_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    switch_transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/t5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    t5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/table_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    table_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/timesformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    timesformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/tinybert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tinybert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/van/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    van
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/vipllava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vipllava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/vision_text_dual_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision_text_dual_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/visual_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visual_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/wav2vec2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/wav2vec2_conformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_conformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/wav2vec2_with_lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_with_lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/wavlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    whisper
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/x_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    x_clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/xlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/xlm_roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/xlm_roberta_xl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta_xl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/models/xlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlnet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_4" >
        
          
          <label class="md-nav__link" for="__nav_5_9_4" id="__nav_5_9_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_4">
            <span class="md-nav__icon md-icon"></span>
            pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../api/transforemrs/pipeline/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/automatic_speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    automatic_speech_recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/document_question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    document_question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/fill_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fill_mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/text2text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text2text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/pipeline/zero_shot_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_shot_classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/configuration_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    configuration_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/modeling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/tokenization_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/tokenization_utils_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/tokenization_utils_fast/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_fast
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TRL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    说明
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            说明
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    更新日志
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    行为准则
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    常见问题
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.config" class="md-nav__link">
    <span class="md-ellipsis">
      config
    </span>
  </a>
  
    <nav class="md-nav" aria-label="config">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.config.LoftQConfig" class="md-nav__link">
    <span class="md-ellipsis">
      LoftQConfig
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.config.LoraConfig" class="md-nav__link">
    <span class="md-ellipsis">
      LoraConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoraConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.config.LoraConfig.to_dict" class="md-nav__link">
    <span class="md-ellipsis">
      to_dict
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model" class="md-nav__link">
    <span class="md-ellipsis">
      model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel" class="md-nav__link">
    <span class="md-ellipsis">
      LoraModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoraModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.__getattr__" class="md-nav__link">
    <span class="md-ellipsis">
      __getattr__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.add_weighted_adapter" class="md-nav__link">
    <span class="md-ellipsis">
      add_weighted_adapter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.delete_adapter" class="md-nav__link">
    <span class="md-ellipsis">
      delete_adapter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.disable_adapter_layers" class="md-nav__link">
    <span class="md-ellipsis">
      disable_adapter_layers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.enable_adapter_layers" class="md-nav__link">
    <span class="md-ellipsis">
      enable_adapter_layers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.get_peft_config_as_dict" class="md-nav__link">
    <span class="md-ellipsis">
      get_peft_config_as_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.merge_and_unload" class="md-nav__link">
    <span class="md-ellipsis">
      merge_and_unload
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.set_adapter" class="md-nav__link">
    <span class="md-ellipsis">
      set_adapter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.peft.tuners.lora.model.LoraModel.unload" class="md-nav__link">
    <span class="md-ellipsis">
      unload
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindnlp/edit/master/docs/en/api/peft/tuners/lora.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindnlp/raw/master/docs/en/api/peft/tuners/lora.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>LoRA</h1>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.peft.tuners.lora.config" class="doc doc-heading">
            <code>mindnlp.peft.tuners.lora.config</code>


<a href="#mindnlp.peft.tuners.lora.config" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>lora config</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.peft.tuners.lora.config.LoftQConfig" class="doc doc-heading">
            <code>mindnlp.peft.tuners.lora.config.LoftQConfig</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.peft.tuners.lora.config.LoftQConfig" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>This is the sub-configuration class to store the configuration of a [<code>LoraModel</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>bits_pattern</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The mapping from layer names or regexp expression to bits which are different from the
default bits specified by <code>bits</code>. For example, <code>{model.decoder.layers.0.encoder_attn.k_proj: 2</code>}.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Quantization bits for LoftQ.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>iter</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Alternating iterations for LoftQ.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>fake</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>models. weights can't be saved. Recommend to set to True, save the weights and load the saved weights in 4
bits.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\peft\tuners\lora\config.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">LoftQConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the sub-configuration class to store the configuration of a [`LoraModel`].</span>

<span class="sd">    Args:</span>
<span class="sd">        bits_pattern (`dict`): The mapping from layer names or regexp expression to bits which are different from the</span>
<span class="sd">            default bits specified by `bits`. For example, `{model.decoder.layers.0.encoder_attn.k_proj: 2`}.</span>
<span class="sd">        bits (`int`): Quantization bits for LoftQ.</span>
<span class="sd">        iter (`int`): Alternating iterations for LoftQ.</span>
<span class="sd">        fake (`bool`): True: use fp16/fp32; used for first time to save weights. False: use bitsandbytes 4bit linear</span>
<span class="sd">            models. weights can&#39;t be saved. Recommend to set to True, save the weights and load the saved weights in 4</span>
<span class="sd">            bits.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">loftq_bits</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Quantization bits for LoftQ&quot;</span><span class="p">})</span>
    <span class="n">loftq_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Alternating iterations for LoftQ&quot;</span><span class="p">})</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.peft.tuners.lora.config.LoraConfig" class="doc doc-heading">
            <code>mindnlp.peft.tuners.lora.config.LoraConfig</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.peft.tuners.lora.config.LoraConfig" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.peft.config.PeftConfig" href="../../../../../api/peft/config/#mindnlp.peft.config.PeftConfig">PeftConfig</a></code></p>


        <p>This is the configuration class to store the configuration of a [<code>LoraModel</code>].</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>r</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Lora attention dimension (the "rank").</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>target_modules</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The names of the modules to apply the adapter to. If this is specified, only the modules with the specified
names will be replaced. When passing a string, a regex match will be performed. When passing a list of
strings, either an exact match will be performed or it is checked if the name of the module ends with any
of the passed strings. If this is specified as 'all-linear', then all linear/Conv1D modules are chosen,
excluding the output layer. If this is not specified, modules will be chosen according to the model
architecture. If the architecture is not known, an error will be raised -- in this case, you should specify
the target modules manually.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Optional[Union[List[str], str]]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_alpha</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The alpha parameter for Lora scaling.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability for Lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>fan_in_fan_out</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Set this to True if the layer to replace stores weight like (fan_in, fan_out). For example, gpt-2 uses
<code>Conv1D</code> which stores weights like (fan_in, fan_out) and hence this should be set to <code>True</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Bias type for LoRA. Can be 'none', 'all' or 'lora_only'. If 'all' or 'lora_only', the corresponding biases
will be updated during training. Be aware that this means that, even when disabling the adapters, the model
will not produce the same output as the base model would have without adaptation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;none&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_rslora</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When set to True, uses <a href='https://doi.org/10.48550/arXiv.2312.03732'>Rank-Stabilized LoRA</a> which
sets the adapter scaling factor to <code>lora_alpha/math.sqrt(r)</code>, since it was proven to work better.
Otherwise, it will use the original default value of <code>lora_alpha/r</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>modules_to_save</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of modules apart from adapter layers to be set as trainable and saved in the final checkpoint.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_lora_weights</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>How to initialize the weights of the adapter layers. Passing True (default) results in the default
initialization from the reference implementation from Microsoft. Passing 'gaussian' results in Gaussian
initialization scaled by the LoRA rank for linear and layers. Setting the initialization to False leads to
completely random initialization and is discouraged. Pass <code>'loftq'</code> to use LoftQ initialization. Pass
<code>'olora'</code> to use OLoRA initialization. Passing <code>'pissa'</code> results in the initialization of <a
href='https://arxiv.org/abs/2404.02948'>Principal Singular values and Singular vectors Adaptation
(PiSSA)</a>, which converges more rapidly than LoRA and ultimately achieves superior performance. Moreover,
PiSSA reduces the quantization error compared to QLoRA, leading to further enhancements. Passing
<code>'pissa_niter_[number of iters]'</code> initiates Fast-SVD-based PiSSA initialization, where <code>[number of iters]</code>
indicates the number of subspace iterations to perform FSVD, and must be a nonnegative integer. When
<code>[number of iters]</code> is set to 16, it can complete the initialization of a 7B model within seconds, and the
training effect is approximately equivalent to using SVD.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool` | `Literal[&#34;gaussian&#34;, &#34;olora&#34;, &#34;pissa&#34;, &#34;pissa_niter_[number of iters]&#34;, &#34;loftq&#34;]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layers_to_transform</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The layer indices to transform. If a list of ints is passed, it will apply the adapter to the layer indices
that are specified in this list. If a single integer is passed, it will apply the transformations on the
layer at this index.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[List[int], int]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layers_pattern</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The layer pattern name, used only if <code>layers_to_transform</code> is different from <code>None</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rank_pattern</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The mapping from layer names or regexp expression to ranks which are different from the default rank
specified by <code>r</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>dict()</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>alpha_pattern</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The mapping from layer names or regexp expression to alphas which are different from the default alpha
specified by <code>lora_alpha</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>dict()</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>megatron_config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The TransformerConfig arguments for Megatron. It is used to create LoRA's parallel linear layer. You can
get it like this, <code>core_transformer_config_from_args(get_args())</code>, these two functions being from Megatron.
The arguments will be used to initialize the TransformerConfig of Megatron. You need to specify this
parameter when you want to apply LoRA to the ColumnParallelLinear and RowParallelLinear layers of megatron.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Optional[dict]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>megatron_core</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The core module from Megatron to use, defaults to <code>"megatron.core"</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Optional[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;megatron.core&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>loftq_config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration of LoftQ. If this is not None, then LoftQ will be used to quantize the backbone weights
and initialize Lora layers. Also pass <code>init_lora_weights='loftq'</code>. Note that you should not pass a
quantized model in this case, as LoftQ will quantize the model itself.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Optional[LoftQConfig]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>dict()</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_dora</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Enable 'Weight-Decomposed Low-Rank Adaptation' (DoRA). This technique decomposes the updates of the weights
into two parts, magnitude and direction. Direction is handled by normal LoRA, whereas the magnitude is
handled by a separate learnable parameter. This can improve the performance of LoRA especially at low
ranks. Right now, DoRA only supports linear and Conv2D layers. DoRA introduces a bigger overhead than pure
LoRA, so it is recommended to merge weights for inference. For more information, see
https://arxiv.org/abs/2402.09353.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer_replication</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Build a new stack of layers by stacking the original model layers according to the ranges specified. This
allows expanding (or shrinking) the model without duplicating the base model weights. The new layers will
all have separate LoRA adapters attached to them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[Tuple[int, int]]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\peft\tuners\lora\config.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">LoraConfig</span><span class="p">(</span><span class="n">PeftConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the configuration class to store the configuration of a [`LoraModel`].</span>

<span class="sd">    Args:</span>
<span class="sd">        r (`int`):</span>
<span class="sd">            Lora attention dimension (the &quot;rank&quot;).</span>
<span class="sd">        target_modules (`Optional[Union[List[str], str]]`):</span>
<span class="sd">            The names of the modules to apply the adapter to. If this is specified, only the modules with the specified</span>
<span class="sd">            names will be replaced. When passing a string, a regex match will be performed. When passing a list of</span>
<span class="sd">            strings, either an exact match will be performed or it is checked if the name of the module ends with any</span>
<span class="sd">            of the passed strings. If this is specified as &#39;all-linear&#39;, then all linear/Conv1D modules are chosen,</span>
<span class="sd">            excluding the output layer. If this is not specified, modules will be chosen according to the model</span>
<span class="sd">            architecture. If the architecture is not known, an error will be raised -- in this case, you should specify</span>
<span class="sd">            the target modules manually.</span>
<span class="sd">        lora_alpha (`int`):</span>
<span class="sd">            The alpha parameter for Lora scaling.</span>
<span class="sd">        lora_dropout (`float`):</span>
<span class="sd">            The dropout probability for Lora layers.</span>
<span class="sd">        fan_in_fan_out (`bool`):</span>
<span class="sd">            Set this to True if the layer to replace stores weight like (fan_in, fan_out). For example, gpt-2 uses</span>
<span class="sd">            `Conv1D` which stores weights like (fan_in, fan_out) and hence this should be set to `True`.</span>
<span class="sd">        bias (`str`):</span>
<span class="sd">            Bias type for LoRA. Can be &#39;none&#39;, &#39;all&#39; or &#39;lora_only&#39;. If &#39;all&#39; or &#39;lora_only&#39;, the corresponding biases</span>
<span class="sd">            will be updated during training. Be aware that this means that, even when disabling the adapters, the model</span>
<span class="sd">            will not produce the same output as the base model would have without adaptation.</span>
<span class="sd">        use_rslora (`bool`):</span>
<span class="sd">            When set to True, uses &lt;a href=&#39;https://doi.org/10.48550/arXiv.2312.03732&#39;&gt;Rank-Stabilized LoRA&lt;/a&gt; which</span>
<span class="sd">            sets the adapter scaling factor to `lora_alpha/math.sqrt(r)`, since it was proven to work better.</span>
<span class="sd">            Otherwise, it will use the original default value of `lora_alpha/r`.</span>
<span class="sd">        modules_to_save (`List[str]`):</span>
<span class="sd">            List of modules apart from adapter layers to be set as trainable and saved in the final checkpoint.</span>
<span class="sd">        init_lora_weights (`bool` | `Literal[&quot;gaussian&quot;, &quot;olora&quot;, &quot;pissa&quot;, &quot;pissa_niter_[number of iters]&quot;, &quot;loftq&quot;]`):</span>
<span class="sd">            How to initialize the weights of the adapter layers. Passing True (default) results in the default</span>
<span class="sd">            initialization from the reference implementation from Microsoft. Passing &#39;gaussian&#39; results in Gaussian</span>
<span class="sd">            initialization scaled by the LoRA rank for linear and layers. Setting the initialization to False leads to</span>
<span class="sd">            completely random initialization and is discouraged. Pass `&#39;loftq&#39;` to use LoftQ initialization. Pass</span>
<span class="sd">            `&#39;olora&#39;` to use OLoRA initialization. Passing `&#39;pissa&#39;` results in the initialization of &lt;a</span>
<span class="sd">            href=&#39;https://arxiv.org/abs/2404.02948&#39;&gt;Principal Singular values and Singular vectors Adaptation</span>
<span class="sd">            (PiSSA)&lt;/a&gt;, which converges more rapidly than LoRA and ultimately achieves superior performance. Moreover,</span>
<span class="sd">            PiSSA reduces the quantization error compared to QLoRA, leading to further enhancements. Passing</span>
<span class="sd">            `&#39;pissa_niter_[number of iters]&#39;` initiates Fast-SVD-based PiSSA initialization, where `[number of iters]`</span>
<span class="sd">            indicates the number of subspace iterations to perform FSVD, and must be a nonnegative integer. When</span>
<span class="sd">            `[number of iters]` is set to 16, it can complete the initialization of a 7B model within seconds, and the</span>
<span class="sd">            training effect is approximately equivalent to using SVD.</span>
<span class="sd">        layers_to_transform (`Union[List[int], int]`):</span>
<span class="sd">            The layer indices to transform. If a list of ints is passed, it will apply the adapter to the layer indices</span>
<span class="sd">            that are specified in this list. If a single integer is passed, it will apply the transformations on the</span>
<span class="sd">            layer at this index.</span>
<span class="sd">        layers_pattern (`str`):</span>
<span class="sd">            The layer pattern name, used only if `layers_to_transform` is different from `None`.</span>
<span class="sd">        rank_pattern (`dict`):</span>
<span class="sd">            The mapping from layer names or regexp expression to ranks which are different from the default rank</span>
<span class="sd">            specified by `r`.</span>
<span class="sd">        alpha_pattern (`dict`):</span>
<span class="sd">            The mapping from layer names or regexp expression to alphas which are different from the default alpha</span>
<span class="sd">            specified by `lora_alpha`.</span>
<span class="sd">        megatron_config (`Optional[dict]`):</span>
<span class="sd">            The TransformerConfig arguments for Megatron. It is used to create LoRA&#39;s parallel linear layer. You can</span>
<span class="sd">            get it like this, `core_transformer_config_from_args(get_args())`, these two functions being from Megatron.</span>
<span class="sd">            The arguments will be used to initialize the TransformerConfig of Megatron. You need to specify this</span>
<span class="sd">            parameter when you want to apply LoRA to the ColumnParallelLinear and RowParallelLinear layers of megatron.</span>
<span class="sd">        megatron_core (`Optional[str]`):</span>
<span class="sd">            The core module from Megatron to use, defaults to `&quot;megatron.core&quot;`.</span>
<span class="sd">        loftq_config (`Optional[LoftQConfig]`):</span>
<span class="sd">            The configuration of LoftQ. If this is not None, then LoftQ will be used to quantize the backbone weights</span>
<span class="sd">            and initialize Lora layers. Also pass `init_lora_weights=&#39;loftq&#39;`. Note that you should not pass a</span>
<span class="sd">            quantized model in this case, as LoftQ will quantize the model itself.</span>
<span class="sd">        use_dora (`bool`):</span>
<span class="sd">            Enable &#39;Weight-Decomposed Low-Rank Adaptation&#39; (DoRA). This technique decomposes the updates of the weights</span>
<span class="sd">            into two parts, magnitude and direction. Direction is handled by normal LoRA, whereas the magnitude is</span>
<span class="sd">            handled by a separate learnable parameter. This can improve the performance of LoRA especially at low</span>
<span class="sd">            ranks. Right now, DoRA only supports linear and Conv2D layers. DoRA introduces a bigger overhead than pure</span>
<span class="sd">            LoRA, so it is recommended to merge weights for inference. For more information, see</span>
<span class="sd">            https://arxiv.org/abs/2402.09353.</span>
<span class="sd">        layer_replication (`List[Tuple[int, int]]`):</span>
<span class="sd">            Build a new stack of layers by stacking the original model layers according to the ranges specified. This</span>
<span class="sd">            allows expanding (or shrinking) the model without duplicating the base model weights. The new layers will</span>
<span class="sd">            all have separate LoRA adapters attached to them.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">r</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Lora attention dimension&quot;</span><span class="p">})</span>
    <span class="n">target_modules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;List of module names or regex expression of the module names to replace with LoRA.&quot;</span>
                <span class="s2">&quot;For example, [&#39;q&#39;, &#39;v&#39;] or &#39;.*decoder.*(SelfAttention|EncDecAttention).*(q|v)$&#39;.&quot;</span>
                <span class="s2">&quot;This can also be a wildcard &#39;all-linear&#39; which matches all linear/Conv1D layers except the output layer.&quot;</span>
                <span class="s2">&quot;If not specified, modules will be chosen according to the model architecture, If the architecture is &quot;</span>
                <span class="s2">&quot;not known, an error will be raised -- in this case, you should specify the target modules manually.&quot;</span>
            <span class="p">),</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">lora_alpha</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Lora alpha&quot;</span><span class="p">})</span>
    <span class="n">lora_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Lora dropout&quot;</span><span class="p">})</span>
    <span class="n">fan_in_fan_out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Set this to True if the layer to replace stores weight like (fan_in, fan_out)&quot;</span><span class="p">},</span>
    <span class="p">)</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;lora_only&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Bias type for Lora. Can be &#39;none&#39;, &#39;all&#39; or &#39;lora_only&#39;&quot;</span><span class="p">}</span>
    <span class="p">)</span>
    <span class="n">use_rslora</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;When set to True, uses &lt;a href=&#39;https://doi.org/10.48550/arXiv.2312.03732&#39;&gt;Rank-Stabilized LoRA&lt;/a&gt;&quot;</span>
                <span class="s2">&quot; which sets the adapter scaling factor to `lora_alpha/math.sqrt(r)`, since it&quot;</span>
                <span class="s2">&quot; was proven to work better. Otherwise, it will use the original default&quot;</span>
                <span class="s2">&quot; value of `lora_alpha/r`.&quot;</span>
            <span class="p">)</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">modules_to_save</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint. &quot;</span>
            <span class="s2">&quot;For example, in Sequence Classification or Token Classification tasks, &quot;</span>
            <span class="s2">&quot;the final layer `classifier/score` are randomly initialized and as such need to be trainable and saved.&quot;</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">init_lora_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="s2">&quot;olora&quot;</span><span class="p">,</span> <span class="s2">&quot;pissa&quot;</span><span class="p">,</span> <span class="s2">&quot;pissa_niter_[number of iters]&quot;</span><span class="p">,</span> <span class="s2">&quot;loftq&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;How to initialize the weights of the LoRA layers. Passing `&#39;True&#39;` (default) results in the default &quot;</span>
                <span class="s2">&quot;initialization from the reference implementation from Microsoft. Passing `&#39;gaussian&#39;` results &quot;</span>
                <span class="s2">&quot;in Gaussian initialization scaled by the LoRA rank for linear and layers. Setting the initialization &quot;</span>
                <span class="s2">&quot;to `&#39;False&#39;` leads to completely random initialization and *is discouraged.*&quot;</span>
                <span class="s2">&quot;Passing `&#39;olora&#39;` results in OLoRA initialization.&quot;</span>
                <span class="s2">&quot;Passing `&#39;pissa&#39;` results in PiSSA initialization.&quot;</span>
                <span class="s2">&quot;Passing `&#39;pissa_niter_[number of iters]&#39;` initiates Fast-SVD-based PiSSA initialization, &quot;</span>
                <span class="s2">&quot;where [number of iters] indicates the number of subspace iterations to perform fsvd, and must be a nonnegative integer.&quot;</span>
                <span class="s2">&quot;Pass `&#39;loftq&#39;` to use LoftQ initialization&quot;</span>
            <span class="p">),</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">layers_to_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;The layer indexes to transform, is this argument is specified, PEFT will transform only the layers indexes that are specified inside this list. &quot;</span> \
                    <span class="s2">&quot;If a single integer is passed, PEFT will transform only the layer at this index. &quot;</span>
            <span class="s2">&quot;This only works when target_modules is a list of str.&quot;</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">layers_pattern</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;The layer pattern name, used only if `layers_to_transform` is different to None and if the layer pattern is not in the common layers pattern.&quot;</span>
            <span class="s2">&quot;This only works when target_modules is a list of str.&quot;</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">rank_pattern</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;The mapping from layer names or regexp expression to ranks which are different from the default rank specified by `r`. &quot;</span>
                <span class="s2">&quot;For example, `{model.decoder.layers.0.encoder_attn.k_proj: 8`}&quot;</span>
            <span class="p">)</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">alpha_pattern</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;The mapping from layer names or regexp expression to alphas which are different from the default alpha specified by `lora_alpha`. &quot;</span>
                <span class="s2">&quot;For example, `{model.decoder.layers.0.encoder_attn.k_proj: 32`}&quot;</span>
            <span class="p">)</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">megatron_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;The TransformerConfig from Megatron. It is used to create LoRA&#39;s parallel linear layer.&quot;</span>
                <span class="s2">&quot;You can get it like this, `core_transformer_config_from_args(get_args())`, &quot;</span>
                <span class="s2">&quot;these two functions being from Megatron.&quot;</span>
                <span class="s2">&quot;You need to specify this parameter when you want to apply LoRA to the ColumnParallelLinear and &quot;</span>
                <span class="s2">&quot;RowParallelLinear layers of megatron.&quot;</span>
                <span class="s2">&quot;It should be noted that we may not be able to use the `save_pretrained` and `from_pretrained` &quot;</span>
                <span class="s2">&quot;functions, because TransformerConfig may not necessarily be serialized.&quot;</span>
                <span class="s2">&quot;But when using megatron, we can use `get_peft_model_state_dict` function and &quot;</span>
                <span class="s2">&quot;megatron&#39;s framework, they can also save and load models and configurations.&quot;</span>
            <span class="p">)</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">megatron_core</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;megatron.core&quot;</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;The core module from Megatron, it is used to create LoRA&#39;s parallel linear layer. &quot;</span>
                <span class="s2">&quot;It only needs to be passed in when you need to use your own modified megatron core module. &quot;</span>
                <span class="s2">&quot;Otherwise, it will use the default value `megatron.core`. &quot;</span>
            <span class="p">)</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="c1"># dict type is used when loading config.json</span>
    <span class="n">loftq_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">LoftQConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;The configuration of LoftQ. If this is passed, then LoftQ will be used to quantize the backbone &quot;</span>
                <span class="s2">&quot;weights and initialize Lora layers. Also set `init_lora_weights=&#39;loftq&#39;` in this case.&quot;</span>
            <span class="p">)</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">use_dora</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;Enable &lt;a href=&#39;https://arxiv.org/abs/2402.09353&#39;&gt;&#39;Weight-Decomposed Low-Rank Adaptation&#39; (DoRA)&lt;/a&gt;. This technique decomposes the updates of the &quot;</span>
                <span class="s2">&quot;weights into two parts, magnitude and direction. Direction is handled by normal LoRA, whereas the &quot;</span>
                <span class="s2">&quot;magnitude is handled by a separate learnable parameter. This can improve the performance of LoRA, &quot;</span>
                <span class="s2">&quot;especially at low ranks. Right now, DoRA only supports linear and Conv2D layers. DoRA introduces a bigger&quot;</span>
                <span class="s2">&quot;overhead than pure LoRA, so it is recommended to merge weights for inference.&quot;</span>
            <span class="p">)</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="c1"># Enables replicating layers in a model to expand it to a larger model.</span>
    <span class="n">layer_replication</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;This enables using LoRA to effectively expand a transformer model to a larger size by repeating some layers. &quot;</span>
                <span class="s2">&quot;The transformation handles models (currently Llama, Bert or Falcon compatible architectures) with &quot;</span>
                <span class="s2">&quot;a module list in the model which it modifies to expand the number of modules. &quot;</span>
                <span class="s2">&quot;Base weights are shared so the memory usage is close to the original model. The intended use is these base weights &quot;</span>
                <span class="s2">&quot;remain fixed during finetuning but each layer has a separate LoRA adapter so the layers can be specialed via &quot;</span>
                <span class="s2">&quot;the adapter layers fit during fine tuning.&quot;</span>
                <span class="s2">&quot;The format is a list of [start, end) pairs which specify the layer ranges to stack. For example:</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;   Original model has 5 layers labelled by their position in the model: `[0, 1, 2, 3, 4]`</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;   layer_replication: `[[0, 4], [2, 5]]`</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;   Final model will have this arrangement of original layers: `[0, 1, 2, 3, 2, 3, 4]`</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;This format is based on what is used for pass-through merges in mergekit. It makes it simple to select sequential &quot;</span>
                <span class="s2">&quot;ranges of a model and stack them while reusing layers at either end of each sequence.&quot;</span>
            <span class="p">)</span>
        <span class="p">},</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the configuration for your adapter model as a dictionary. Removes runtime configurations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rv</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="n">rv</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;runtime_config&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rv</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">peft_type</span> <span class="o">=</span> <span class="n">PeftType</span><span class="o">.</span><span class="n">LORA</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_modules</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_modules</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_modules</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_modules</span>
        <span class="p">)</span>
        <span class="c1"># if target_modules is a regex expression, then layers_to_transform should be None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_modules</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_to_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`layers_to_transform` cannot be used when `target_modules` is a str.&quot;</span><span class="p">)</span>

        <span class="c1"># if target_modules is a regex expression, then layers_pattern should be None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_modules</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers_pattern</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`layers_pattern` cannot be used when `target_modules` is a str.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dora</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">megatron_config</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;DoRA does not support megatron_core, please set `use_dora=False`.&quot;</span><span class="p">)</span>

        <span class="c1"># handle init_lora_weights and loftq_config</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_lora_weights</span> <span class="o">==</span> <span class="s2">&quot;loftq&quot;</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">importlib</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;scipy&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;The required package &#39;scipy&#39; is not installed. Please install it to continue.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loftq_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`loftq_config` must be specified when `init_lora_weights` is &#39;loftq&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Using post training conversion of modified base weights to restore their initial values (PiSSA, OLoRA) cannot</span>
        <span class="c1"># be correctly done when using rslora + rank_pattern/alpha_pattern. We can&#39;t really know if the user intends</span>
        <span class="c1"># this when they&#39;ll eventually call save_pretrained (i.e. if they&#39;ll pass</span>
        <span class="c1"># path_initial_model_for_weight_conversionl). Therefore, we only warn but don&#39;t raise an error here.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_rslora</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank_pattern</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_pattern</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_lora_weights</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_lora_weights</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;pissa&quot;</span><span class="p">)))</span>
                <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_lora_weights</span> <span class="o">==</span> <span class="s2">&quot;olora&quot;</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;Using Rank-Stabilized LoRA with rank_pattern/alpha_pattern and post-training conversion of modified &quot;</span>
                <span class="s2">&quot;base weights (PiSSA, OLoRA) means that you won&#39;t be able to pass &quot;</span>
                <span class="s2">&quot;`path_initial_model_for_weight_conversion` to `save_pretrained` to restore the initial values of the &quot;</span>
                <span class="s2">&quot;base weights; if you intend to do this, please ensure not to use rslora or rank_pattern/alpha_pattern.&quot;</span>
            <span class="p">)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="c1"># convert loftq_config to dict</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loftq_config</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loftq_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loftq_config</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loftq_config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_custom_modules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Mmodule</span><span class="p">],</span> <span class="nb">type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_register_custom_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mapping</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Mmodule</span><span class="p">],</span> <span class="nb">type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Experimental API to support providing custom LoRA layers.</span>

<span class="sd">        This API is subject to change, you should carefully read the docs before deciding to use it:</span>

<span class="sd">        https://huggingface.co/docs/peft/developer_guides/custom_models</span>

<span class="sd">        To register custom LoRA module types, call this method with a `mapping` argument that is a dict that maps from</span>
<span class="sd">        the target layer type to the custom LoRA layer type. The dict can contain multiple items if you wish to target</span>
<span class="sd">        multiple layer types. The target layer type can be any nn.Module that we currently don&#39;t support in PEFT,</span>
<span class="sd">        whether that is an official PyTorch layer type or a custom layer type. The custom LoRA module class has to be</span>
<span class="sd">        implemented by the user and follow the PEFT conventions for LoRA layers.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_custom_modules</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_custom_modules</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_custom_modules</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">mapping</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.peft.tuners.lora.config.LoraConfig.to_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">peft</span><span class="o">.</span><span class="n">tuners</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">LoraConfig</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span></code>

<a href="#mindnlp.peft.tuners.lora.config.LoraConfig.to_dict" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the configuration for your adapter model as a dictionary. Removes runtime configurations.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\peft\tuners\lora\config.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the configuration for your adapter model as a dictionary. Removes runtime configurations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rv</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
    <span class="n">rv</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;runtime_config&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rv</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.peft.tuners.lora.model" class="doc doc-heading">
            <code>mindnlp.peft.tuners.lora.model</code>


<a href="#mindnlp.peft.tuners.lora.model" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>lora model</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.peft.tuners.lora.model.LoraModel" class="doc doc-heading">
            <code>mindnlp.peft.tuners.lora.model.LoraModel</code>


<a href="#mindnlp.peft.tuners.lora.model.LoraModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.peft.tuners.tuners_utils.BaseTuner">BaseTuner</span></code></p>


        <p>Creates Low Rank Adapter (LoRA) model from a pretrained transformers model.</p>
<p>The method is described in detail in https://arxiv.org/abs/2106.09685.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The model to be adapted.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`nn.Module`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration of the Lora model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`LoraConfig`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The name of the adapter, defaults to <code>"default"</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>LoraModel</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The Lora model.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>[`mindspore.nn.Module`]</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code>```py
&gt;&gt;&gt; from transformers import AutoModelForSeq2SeqLM
&gt;&gt;&gt; from peft import LoraModel, LoraConfig

&gt;&gt;&gt; config = LoraConfig(
...     task_type=&quot;SEQ_2_SEQ_LM&quot;,
...     r=8,
...     lora_alpha=32,
...     target_modules=[&quot;q&quot;, &quot;v&quot;],
...     lora_dropout=0.01,
... )

&gt;&gt;&gt; model = AutoModelForSeq2SeqLM.from_pretrained(&quot;t5-base&quot;)
&gt;&gt;&gt; lora_model = LoraModel(model, config, &quot;default&quot;)
```

```py
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import transformers
&gt;&gt;&gt; from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training

&gt;&gt;&gt; rank = ...
&gt;&gt;&gt; target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;out_proj&quot;, &quot;fc_in&quot;, &quot;fc_out&quot;, &quot;wte&quot;]
&gt;&gt;&gt; config = LoraConfig(
...     r=4, lora_alpha=16, target_modules=target_modules, lora_dropout=0.1, bias=&quot;none&quot;, task_type=&quot;CAUSAL_LM&quot;
... )
&gt;&gt;&gt; quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)

&gt;&gt;&gt; tokenizer = transformers.AutoTokenizer.from_pretrained(
...     &quot;kakaobrain/kogpt&quot;,
...     revision=&quot;KoGPT6B-ryan1.5b-float16&quot;,  # or float32 version: revision=KoGPT6B-ryan1.5b
...     bos_token=&quot;[BOS]&quot;,
...     eos_token=&quot;[EOS]&quot;,
...     unk_token=&quot;[UNK]&quot;,
...     pad_token=&quot;[PAD]&quot;,
...     mask_token=&quot;[MASK]&quot;,
... )
&gt;&gt;&gt; model = transformers.GPTJForCausalLM.from_pretrained(
...     &quot;kakaobrain/kogpt&quot;,
...     revision=&quot;KoGPT6B-ryan1.5b-float16&quot;,  # or float32 version: revision=KoGPT6B-ryan1.5b
...     pad_token_id=tokenizer.eos_token_id,
...     use_cache=False,
...     device_map={&quot;&quot;: rank},
...     ms_dtype=torch.float16,
...     quantization_config=quantization_config,
... )
&gt;&gt;&gt; model = prepare_model_for_kbit_training(model)
&gt;&gt;&gt; lora_model = get_peft_model(model, config)
```
</code></pre></div>
<blockquote>
<p><strong>Attributes</strong>:  </p>
<ul>
<li>
<p><strong>model</strong> ([<code>transformers.PreTrainedModel</code>])— The model to be adapted. </p>
</li>
<li>
<p><strong>peft_config</strong> ([<code>LoraConfig</code>]): The configuration of the Lora model.</p>
</li>
</ul>
</blockquote>






              <details class="quote">
                <summary>Source code in <code>mindnlp\peft\tuners\lora\model.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  74</span>
<span class="normal">  75</span>
<span class="normal">  76</span>
<span class="normal">  77</span>
<span class="normal">  78</span>
<span class="normal">  79</span>
<span class="normal">  80</span>
<span class="normal">  81</span>
<span class="normal">  82</span>
<span class="normal">  83</span>
<span class="normal">  84</span>
<span class="normal">  85</span>
<span class="normal">  86</span>
<span class="normal">  87</span>
<span class="normal">  88</span>
<span class="normal">  89</span>
<span class="normal">  90</span>
<span class="normal">  91</span>
<span class="normal">  92</span>
<span class="normal">  93</span>
<span class="normal">  94</span>
<span class="normal">  95</span>
<span class="normal">  96</span>
<span class="normal">  97</span>
<span class="normal">  98</span>
<span class="normal">  99</span>
<span class="normal"> 100</span>
<span class="normal"> 101</span>
<span class="normal"> 102</span>
<span class="normal"> 103</span>
<span class="normal"> 104</span>
<span class="normal"> 105</span>
<span class="normal"> 106</span>
<span class="normal"> 107</span>
<span class="normal"> 108</span>
<span class="normal"> 109</span>
<span class="normal"> 110</span>
<span class="normal"> 111</span>
<span class="normal"> 112</span>
<span class="normal"> 113</span>
<span class="normal"> 114</span>
<span class="normal"> 115</span>
<span class="normal"> 116</span>
<span class="normal"> 117</span>
<span class="normal"> 118</span>
<span class="normal"> 119</span>
<span class="normal"> 120</span>
<span class="normal"> 121</span>
<span class="normal"> 122</span>
<span class="normal"> 123</span>
<span class="normal"> 124</span>
<span class="normal"> 125</span>
<span class="normal"> 126</span>
<span class="normal"> 127</span>
<span class="normal"> 128</span>
<span class="normal"> 129</span>
<span class="normal"> 130</span>
<span class="normal"> 131</span>
<span class="normal"> 132</span>
<span class="normal"> 133</span>
<span class="normal"> 134</span>
<span class="normal"> 135</span>
<span class="normal"> 136</span>
<span class="normal"> 137</span>
<span class="normal"> 138</span>
<span class="normal"> 139</span>
<span class="normal"> 140</span>
<span class="normal"> 141</span>
<span class="normal"> 142</span>
<span class="normal"> 143</span>
<span class="normal"> 144</span>
<span class="normal"> 145</span>
<span class="normal"> 146</span>
<span class="normal"> 147</span>
<span class="normal"> 148</span>
<span class="normal"> 149</span>
<span class="normal"> 150</span>
<span class="normal"> 151</span>
<span class="normal"> 152</span>
<span class="normal"> 153</span>
<span class="normal"> 154</span>
<span class="normal"> 155</span>
<span class="normal"> 156</span>
<span class="normal"> 157</span>
<span class="normal"> 158</span>
<span class="normal"> 159</span>
<span class="normal"> 160</span>
<span class="normal"> 161</span>
<span class="normal"> 162</span>
<span class="normal"> 163</span>
<span class="normal"> 164</span>
<span class="normal"> 165</span>
<span class="normal"> 166</span>
<span class="normal"> 167</span>
<span class="normal"> 168</span>
<span class="normal"> 169</span>
<span class="normal"> 170</span>
<span class="normal"> 171</span>
<span class="normal"> 172</span>
<span class="normal"> 173</span>
<span class="normal"> 174</span>
<span class="normal"> 175</span>
<span class="normal"> 176</span>
<span class="normal"> 177</span>
<span class="normal"> 178</span>
<span class="normal"> 179</span>
<span class="normal"> 180</span>
<span class="normal"> 181</span>
<span class="normal"> 182</span>
<span class="normal"> 183</span>
<span class="normal"> 184</span>
<span class="normal"> 185</span>
<span class="normal"> 186</span>
<span class="normal"> 187</span>
<span class="normal"> 188</span>
<span class="normal"> 189</span>
<span class="normal"> 190</span>
<span class="normal"> 191</span>
<span class="normal"> 192</span>
<span class="normal"> 193</span>
<span class="normal"> 194</span>
<span class="normal"> 195</span>
<span class="normal"> 196</span>
<span class="normal"> 197</span>
<span class="normal"> 198</span>
<span class="normal"> 199</span>
<span class="normal"> 200</span>
<span class="normal"> 201</span>
<span class="normal"> 202</span>
<span class="normal"> 203</span>
<span class="normal"> 204</span>
<span class="normal"> 205</span>
<span class="normal"> 206</span>
<span class="normal"> 207</span>
<span class="normal"> 208</span>
<span class="normal"> 209</span>
<span class="normal"> 210</span>
<span class="normal"> 211</span>
<span class="normal"> 212</span>
<span class="normal"> 213</span>
<span class="normal"> 214</span>
<span class="normal"> 215</span>
<span class="normal"> 216</span>
<span class="normal"> 217</span>
<span class="normal"> 218</span>
<span class="normal"> 219</span>
<span class="normal"> 220</span>
<span class="normal"> 221</span>
<span class="normal"> 222</span>
<span class="normal"> 223</span>
<span class="normal"> 224</span>
<span class="normal"> 225</span>
<span class="normal"> 226</span>
<span class="normal"> 227</span>
<span class="normal"> 228</span>
<span class="normal"> 229</span>
<span class="normal"> 230</span>
<span class="normal"> 231</span>
<span class="normal"> 232</span>
<span class="normal"> 233</span>
<span class="normal"> 234</span>
<span class="normal"> 235</span>
<span class="normal"> 236</span>
<span class="normal"> 237</span>
<span class="normal"> 238</span>
<span class="normal"> 239</span>
<span class="normal"> 240</span>
<span class="normal"> 241</span>
<span class="normal"> 242</span>
<span class="normal"> 243</span>
<span class="normal"> 244</span>
<span class="normal"> 245</span>
<span class="normal"> 246</span>
<span class="normal"> 247</span>
<span class="normal"> 248</span>
<span class="normal"> 249</span>
<span class="normal"> 250</span>
<span class="normal"> 251</span>
<span class="normal"> 252</span>
<span class="normal"> 253</span>
<span class="normal"> 254</span>
<span class="normal"> 255</span>
<span class="normal"> 256</span>
<span class="normal"> 257</span>
<span class="normal"> 258</span>
<span class="normal"> 259</span>
<span class="normal"> 260</span>
<span class="normal"> 261</span>
<span class="normal"> 262</span>
<span class="normal"> 263</span>
<span class="normal"> 264</span>
<span class="normal"> 265</span>
<span class="normal"> 266</span>
<span class="normal"> 267</span>
<span class="normal"> 268</span>
<span class="normal"> 269</span>
<span class="normal"> 270</span>
<span class="normal"> 271</span>
<span class="normal"> 272</span>
<span class="normal"> 273</span>
<span class="normal"> 274</span>
<span class="normal"> 275</span>
<span class="normal"> 276</span>
<span class="normal"> 277</span>
<span class="normal"> 278</span>
<span class="normal"> 279</span>
<span class="normal"> 280</span>
<span class="normal"> 281</span>
<span class="normal"> 282</span>
<span class="normal"> 283</span>
<span class="normal"> 284</span>
<span class="normal"> 285</span>
<span class="normal"> 286</span>
<span class="normal"> 287</span>
<span class="normal"> 288</span>
<span class="normal"> 289</span>
<span class="normal"> 290</span>
<span class="normal"> 291</span>
<span class="normal"> 292</span>
<span class="normal"> 293</span>
<span class="normal"> 294</span>
<span class="normal"> 295</span>
<span class="normal"> 296</span>
<span class="normal"> 297</span>
<span class="normal"> 298</span>
<span class="normal"> 299</span>
<span class="normal"> 300</span>
<span class="normal"> 301</span>
<span class="normal"> 302</span>
<span class="normal"> 303</span>
<span class="normal"> 304</span>
<span class="normal"> 305</span>
<span class="normal"> 306</span>
<span class="normal"> 307</span>
<span class="normal"> 308</span>
<span class="normal"> 309</span>
<span class="normal"> 310</span>
<span class="normal"> 311</span>
<span class="normal"> 312</span>
<span class="normal"> 313</span>
<span class="normal"> 314</span>
<span class="normal"> 315</span>
<span class="normal"> 316</span>
<span class="normal"> 317</span>
<span class="normal"> 318</span>
<span class="normal"> 319</span>
<span class="normal"> 320</span>
<span class="normal"> 321</span>
<span class="normal"> 322</span>
<span class="normal"> 323</span>
<span class="normal"> 324</span>
<span class="normal"> 325</span>
<span class="normal"> 326</span>
<span class="normal"> 327</span>
<span class="normal"> 328</span>
<span class="normal"> 329</span>
<span class="normal"> 330</span>
<span class="normal"> 331</span>
<span class="normal"> 332</span>
<span class="normal"> 333</span>
<span class="normal"> 334</span>
<span class="normal"> 335</span>
<span class="normal"> 336</span>
<span class="normal"> 337</span>
<span class="normal"> 338</span>
<span class="normal"> 339</span>
<span class="normal"> 340</span>
<span class="normal"> 341</span>
<span class="normal"> 342</span>
<span class="normal"> 343</span>
<span class="normal"> 344</span>
<span class="normal"> 345</span>
<span class="normal"> 346</span>
<span class="normal"> 347</span>
<span class="normal"> 348</span>
<span class="normal"> 349</span>
<span class="normal"> 350</span>
<span class="normal"> 351</span>
<span class="normal"> 352</span>
<span class="normal"> 353</span>
<span class="normal"> 354</span>
<span class="normal"> 355</span>
<span class="normal"> 356</span>
<span class="normal"> 357</span>
<span class="normal"> 358</span>
<span class="normal"> 359</span>
<span class="normal"> 360</span>
<span class="normal"> 361</span>
<span class="normal"> 362</span>
<span class="normal"> 363</span>
<span class="normal"> 364</span>
<span class="normal"> 365</span>
<span class="normal"> 366</span>
<span class="normal"> 367</span>
<span class="normal"> 368</span>
<span class="normal"> 369</span>
<span class="normal"> 370</span>
<span class="normal"> 371</span>
<span class="normal"> 372</span>
<span class="normal"> 373</span>
<span class="normal"> 374</span>
<span class="normal"> 375</span>
<span class="normal"> 376</span>
<span class="normal"> 377</span>
<span class="normal"> 378</span>
<span class="normal"> 379</span>
<span class="normal"> 380</span>
<span class="normal"> 381</span>
<span class="normal"> 382</span>
<span class="normal"> 383</span>
<span class="normal"> 384</span>
<span class="normal"> 385</span>
<span class="normal"> 386</span>
<span class="normal"> 387</span>
<span class="normal"> 388</span>
<span class="normal"> 389</span>
<span class="normal"> 390</span>
<span class="normal"> 391</span>
<span class="normal"> 392</span>
<span class="normal"> 393</span>
<span class="normal"> 394</span>
<span class="normal"> 395</span>
<span class="normal"> 396</span>
<span class="normal"> 397</span>
<span class="normal"> 398</span>
<span class="normal"> 399</span>
<span class="normal"> 400</span>
<span class="normal"> 401</span>
<span class="normal"> 402</span>
<span class="normal"> 403</span>
<span class="normal"> 404</span>
<span class="normal"> 405</span>
<span class="normal"> 406</span>
<span class="normal"> 407</span>
<span class="normal"> 408</span>
<span class="normal"> 409</span>
<span class="normal"> 410</span>
<span class="normal"> 411</span>
<span class="normal"> 412</span>
<span class="normal"> 413</span>
<span class="normal"> 414</span>
<span class="normal"> 415</span>
<span class="normal"> 416</span>
<span class="normal"> 417</span>
<span class="normal"> 418</span>
<span class="normal"> 419</span>
<span class="normal"> 420</span>
<span class="normal"> 421</span>
<span class="normal"> 422</span>
<span class="normal"> 423</span>
<span class="normal"> 424</span>
<span class="normal"> 425</span>
<span class="normal"> 426</span>
<span class="normal"> 427</span>
<span class="normal"> 428</span>
<span class="normal"> 429</span>
<span class="normal"> 430</span>
<span class="normal"> 431</span>
<span class="normal"> 432</span>
<span class="normal"> 433</span>
<span class="normal"> 434</span>
<span class="normal"> 435</span>
<span class="normal"> 436</span>
<span class="normal"> 437</span>
<span class="normal"> 438</span>
<span class="normal"> 439</span>
<span class="normal"> 440</span>
<span class="normal"> 441</span>
<span class="normal"> 442</span>
<span class="normal"> 443</span>
<span class="normal"> 444</span>
<span class="normal"> 445</span>
<span class="normal"> 446</span>
<span class="normal"> 447</span>
<span class="normal"> 448</span>
<span class="normal"> 449</span>
<span class="normal"> 450</span>
<span class="normal"> 451</span>
<span class="normal"> 452</span>
<span class="normal"> 453</span>
<span class="normal"> 454</span>
<span class="normal"> 455</span>
<span class="normal"> 456</span>
<span class="normal"> 457</span>
<span class="normal"> 458</span>
<span class="normal"> 459</span>
<span class="normal"> 460</span>
<span class="normal"> 461</span>
<span class="normal"> 462</span>
<span class="normal"> 463</span>
<span class="normal"> 464</span>
<span class="normal"> 465</span>
<span class="normal"> 466</span>
<span class="normal"> 467</span>
<span class="normal"> 468</span>
<span class="normal"> 469</span>
<span class="normal"> 470</span>
<span class="normal"> 471</span>
<span class="normal"> 472</span>
<span class="normal"> 473</span>
<span class="normal"> 474</span>
<span class="normal"> 475</span>
<span class="normal"> 476</span>
<span class="normal"> 477</span>
<span class="normal"> 478</span>
<span class="normal"> 479</span>
<span class="normal"> 480</span>
<span class="normal"> 481</span>
<span class="normal"> 482</span>
<span class="normal"> 483</span>
<span class="normal"> 484</span>
<span class="normal"> 485</span>
<span class="normal"> 486</span>
<span class="normal"> 487</span>
<span class="normal"> 488</span>
<span class="normal"> 489</span>
<span class="normal"> 490</span>
<span class="normal"> 491</span>
<span class="normal"> 492</span>
<span class="normal"> 493</span>
<span class="normal"> 494</span>
<span class="normal"> 495</span>
<span class="normal"> 496</span>
<span class="normal"> 497</span>
<span class="normal"> 498</span>
<span class="normal"> 499</span>
<span class="normal"> 500</span>
<span class="normal"> 501</span>
<span class="normal"> 502</span>
<span class="normal"> 503</span>
<span class="normal"> 504</span>
<span class="normal"> 505</span>
<span class="normal"> 506</span>
<span class="normal"> 507</span>
<span class="normal"> 508</span>
<span class="normal"> 509</span>
<span class="normal"> 510</span>
<span class="normal"> 511</span>
<span class="normal"> 512</span>
<span class="normal"> 513</span>
<span class="normal"> 514</span>
<span class="normal"> 515</span>
<span class="normal"> 516</span>
<span class="normal"> 517</span>
<span class="normal"> 518</span>
<span class="normal"> 519</span>
<span class="normal"> 520</span>
<span class="normal"> 521</span>
<span class="normal"> 522</span>
<span class="normal"> 523</span>
<span class="normal"> 524</span>
<span class="normal"> 525</span>
<span class="normal"> 526</span>
<span class="normal"> 527</span>
<span class="normal"> 528</span>
<span class="normal"> 529</span>
<span class="normal"> 530</span>
<span class="normal"> 531</span>
<span class="normal"> 532</span>
<span class="normal"> 533</span>
<span class="normal"> 534</span>
<span class="normal"> 535</span>
<span class="normal"> 536</span>
<span class="normal"> 537</span>
<span class="normal"> 538</span>
<span class="normal"> 539</span>
<span class="normal"> 540</span>
<span class="normal"> 541</span>
<span class="normal"> 542</span>
<span class="normal"> 543</span>
<span class="normal"> 544</span>
<span class="normal"> 545</span>
<span class="normal"> 546</span>
<span class="normal"> 547</span>
<span class="normal"> 548</span>
<span class="normal"> 549</span>
<span class="normal"> 550</span>
<span class="normal"> 551</span>
<span class="normal"> 552</span>
<span class="normal"> 553</span>
<span class="normal"> 554</span>
<span class="normal"> 555</span>
<span class="normal"> 556</span>
<span class="normal"> 557</span>
<span class="normal"> 558</span>
<span class="normal"> 559</span>
<span class="normal"> 560</span>
<span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LoraModel</span><span class="p">(</span><span class="n">BaseTuner</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates Low Rank Adapter (LoRA) model from a pretrained transformers model.</span>

<span class="sd">    The method is described in detail in https://arxiv.org/abs/2106.09685.</span>

<span class="sd">    Args:</span>
<span class="sd">        model ([`nn.Module`]): The model to be adapted.</span>
<span class="sd">        config ([`LoraConfig`]): The configuration of the Lora model.</span>
<span class="sd">        adapter_name (`str`): The name of the adapter, defaults to `&quot;default&quot;`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        LoraModel ([`mindspore.nn.Module`]): The Lora model.</span>

<span class="sd">    Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoModelForSeq2SeqLM</span>
<span class="sd">        &gt;&gt;&gt; from peft import LoraModel, LoraConfig</span>

<span class="sd">        &gt;&gt;&gt; config = LoraConfig(</span>
<span class="sd">        ...     task_type=&quot;SEQ_2_SEQ_LM&quot;,</span>
<span class="sd">        ...     r=8,</span>
<span class="sd">        ...     lora_alpha=32,</span>
<span class="sd">        ...     target_modules=[&quot;q&quot;, &quot;v&quot;],</span>
<span class="sd">        ...     lora_dropout=0.01,</span>
<span class="sd">        ... )</span>

<span class="sd">        &gt;&gt;&gt; model = AutoModelForSeq2SeqLM.from_pretrained(&quot;t5-base&quot;)</span>
<span class="sd">        &gt;&gt;&gt; lora_model = LoraModel(model, config, &quot;default&quot;)</span>
<span class="sd">        ```</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import transformers</span>
<span class="sd">        &gt;&gt;&gt; from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training</span>

<span class="sd">        &gt;&gt;&gt; rank = ...</span>
<span class="sd">        &gt;&gt;&gt; target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;out_proj&quot;, &quot;fc_in&quot;, &quot;fc_out&quot;, &quot;wte&quot;]</span>
<span class="sd">        &gt;&gt;&gt; config = LoraConfig(</span>
<span class="sd">        ...     r=4, lora_alpha=16, target_modules=target_modules, lora_dropout=0.1, bias=&quot;none&quot;, task_type=&quot;CAUSAL_LM&quot;</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)</span>

<span class="sd">        &gt;&gt;&gt; tokenizer = transformers.AutoTokenizer.from_pretrained(</span>
<span class="sd">        ...     &quot;kakaobrain/kogpt&quot;,</span>
<span class="sd">        ...     revision=&quot;KoGPT6B-ryan1.5b-float16&quot;,  # or float32 version: revision=KoGPT6B-ryan1.5b</span>
<span class="sd">        ...     bos_token=&quot;[BOS]&quot;,</span>
<span class="sd">        ...     eos_token=&quot;[EOS]&quot;,</span>
<span class="sd">        ...     unk_token=&quot;[UNK]&quot;,</span>
<span class="sd">        ...     pad_token=&quot;[PAD]&quot;,</span>
<span class="sd">        ...     mask_token=&quot;[MASK]&quot;,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; model = transformers.GPTJForCausalLM.from_pretrained(</span>
<span class="sd">        ...     &quot;kakaobrain/kogpt&quot;,</span>
<span class="sd">        ...     revision=&quot;KoGPT6B-ryan1.5b-float16&quot;,  # or float32 version: revision=KoGPT6B-ryan1.5b</span>
<span class="sd">        ...     pad_token_id=tokenizer.eos_token_id,</span>
<span class="sd">        ...     use_cache=False,</span>
<span class="sd">        ...     device_map={&quot;&quot;: rank},</span>
<span class="sd">        ...     ms_dtype=torch.float16,</span>
<span class="sd">        ...     quantization_config=quantization_config,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; model = prepare_model_for_kbit_training(model)</span>
<span class="sd">        &gt;&gt;&gt; lora_model = get_peft_model(model, config)</span>
<span class="sd">        ```</span>

<span class="sd">    &gt; **Attributes**:  </span>

<span class="sd">    &gt;   - **model** ([`transformers.PreTrainedModel`])— The model to be adapted. </span>

<span class="sd">    &gt;   - **peft_config** ([`LoraConfig`]): The configuration of the Lora model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lora_&quot;</span>

    <span class="k">def</span> <span class="nf">_check_new_adapter_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">LoraConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A helper method to check the config when a new adapter is being added.</span>

<span class="sd">        Raise a ValueError if there is something wrong with the config or if it conflicts with existing adapters.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: there should be a check if any of the existing adapters actually has bias != &quot;none&quot;, or else the check</span>
        <span class="c1"># does not fully correspond to the error message.</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">bias</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> supports only 1 adapter with bias. When using multiple adapters, &quot;</span>
                <span class="s2">&quot;set bias to &#39;none&#39; for all adapters.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_check_target_cell_exists</span><span class="p">(</span><span class="n">lora_config</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks if the target cell exists in the LoRa configuration.</span>

<span class="sd">        Args:</span>
<span class="sd">            lora_config (dict): A dictionary containing the LoRa configuration.</span>
<span class="sd">                This dictionary should have the following structure:</span>
<span class="sd">                {</span>
<span class="sd">                    &quot;target_modules&quot;: {</span>
<span class="sd">                        &quot;cell1&quot;: {</span>
<span class="sd">                            ...</span>
<span class="sd">                        },</span>
<span class="sd">                        &quot;cell2&quot;: {</span>
<span class="sd">                            ...</span>
<span class="sd">                        },</span>
<span class="sd">                        ...</span>
<span class="sd">                    },</span>
<span class="sd">                    ...</span>
<span class="sd">                }</span>
<span class="sd">                The &#39;target_modules&#39; key should contain the target cell information.</span>
<span class="sd">            key (str): The key to identify the target cell.</span>
<span class="sd">                The key should be a string that matches the key used in the &#39;target_modules&#39; dictionary.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None: This method does not raise any exceptions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">check_target_cell_exists</span><span class="p">(</span><span class="n">lora_config</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">:</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A private method to modify the model structure before adapter is applied.</span>

<span class="sd">        Args:</span>
<span class="sd">            peft_config (`PeftConfig`):</span>
<span class="sd">                The prepared adapter config.</span>
<span class="sd">            model (`nn.Module`):</span>
<span class="sd">                The model that is going to be adapted.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">peft_config</span><span class="o">.</span><span class="n">layer_replication</span><span class="p">:</span>
            <span class="n">replicate_layers</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="o">.</span><span class="n">layer_replication</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_create_and_replace</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">lora_config</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">target</span><span class="p">,</span>
        <span class="n">target_name</span><span class="p">,</span>
        <span class="n">parent</span><span class="p">,</span>
        <span class="n">current_key</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new cell and replaces an existing cell in the LoraModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (LoraModel): The instance of the LoraModel class.</span>
<span class="sd">            lora_config (LoraConfig): The LoraConfig object containing Lora configuration parameters.</span>
<span class="sd">            adapter_name (str): The name of the adapter.</span>
<span class="sd">            target (LoraLayer): The target LoraLayer or AdaLoraLayer object to update or replace.</span>
<span class="sd">            target_name (str): The name of the target layer.</span>
<span class="sd">            parent (nn.Module): The parent module to which the target layer belongs.</span>
<span class="sd">            current_key: The current key used for matching patterns.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. The method modifies the LoraModel by creating and replacing cells.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the current_key is None.</span>

<span class="sd">        Note:</span>
<span class="sd">            This method dynamically determines the appropriate rank (r) and alpha (lora_alpha) values</span>
<span class="sd">            based on the current_key and the pattern keys defined in the lora_config. It then creates</span>
<span class="sd">            a new cell with the specified lora configuration parameters and replaces the existing</span>
<span class="sd">            cell with the new cell in the LoraModel.</span>

<span class="sd">            If the target is an instance of LoraLayer (but not AdaLoraLayer), the method updates</span>
<span class="sd">            the layer with the specified adapter_name, rank (r), lora_alpha, lora_dropout,</span>
<span class="sd">            init_lora_weights, use_rslora, and use_dora parameters.</span>

<span class="sd">            If the target is not an instance of LoraLayer, the method creates a new cell using the</span>
<span class="sd">            _create_new_cell method with the specified lora configuration parameters. If the adapter_name</span>
<span class="sd">            is not in the active_adapters list, the requires_grad attribute of the new cell is set to False.</span>

<span class="sd">            The method then replaces the existing cell in the parent module with the new cell using</span>
<span class="sd">            the _replace_cell method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">current_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Current Key shouldn&#39;t be `None`&quot;</span><span class="p">)</span>

        <span class="c1"># Regexp matching - Find key which matches current target_name in patterns provided</span>
        <span class="n">pattern_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="n">lora_config</span><span class="o">.</span><span class="n">rank_pattern</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">alpha_pattern</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
        <span class="n">target_name_key</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">key</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="sa">rf</span><span class="s2">&quot;.*\.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="n">current_key</span><span class="p">),</span> <span class="n">pattern_keys</span><span class="p">),</span> <span class="n">current_key</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">rank_pattern</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">target_name_key</span><span class="p">,</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">r</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">alpha_pattern</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">target_name_key</span><span class="p">,</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">lora_alpha</span><span class="p">)</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;r&quot;</span><span class="p">:</span> <span class="n">r</span><span class="p">,</span>
            <span class="s2">&quot;lora_alpha&quot;</span><span class="p">:</span> <span class="n">alpha</span><span class="p">,</span>
            <span class="s2">&quot;lora_dropout&quot;</span><span class="p">:</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">,</span>
            <span class="s2">&quot;fan_in_fan_out&quot;</span><span class="p">:</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">fan_in_fan_out</span><span class="p">,</span>
            <span class="s2">&quot;init_lora_weights&quot;</span><span class="p">:</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">init_lora_weights</span><span class="p">,</span>
            <span class="s2">&quot;use_rslora&quot;</span><span class="p">:</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">use_rslora</span><span class="p">,</span>
            <span class="s2">&quot;use_dora&quot;</span><span class="p">:</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">use_dora</span><span class="p">,</span>
            <span class="s2">&quot;loaded_in_8bit&quot;</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;is_loaded_in_8bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
            <span class="s2">&quot;loaded_in_4bit&quot;</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;is_loaded_in_4bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
        <span class="p">}</span>

        <span class="c1"># quant_methods = [&quot;gptq&quot;, &quot;aqlm&quot;, &quot;awq&quot;]</span>
        <span class="c1"># for quant_method in quant_methods:</span>
        <span class="c1">#     quantization_config = get_quantization_config(self.model, method=quant_method)</span>
        <span class="c1">#     if quantization_config is not None:</span>
        <span class="c1">#         kwargs[f&quot;{quant_method}_quantization_config&quot;] = quantization_config</span>

        <span class="c1"># note: AdaLoraLayer is a subclass of LoraLayer, we need to exclude it</span>
        <span class="kn">from</span> <span class="nn">..adalora</span> <span class="kn">import</span> <span class="n">AdaLoraLayer</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">AdaLoraLayer</span><span class="p">):</span>
            <span class="n">target</span><span class="o">.</span><span class="n">update_layer</span><span class="p">(</span>
                <span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">r</span><span class="p">,</span>
                <span class="n">lora_alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                <span class="n">lora_dropout</span><span class="o">=</span><span class="n">lora_config</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">,</span>
                <span class="n">init_lora_weights</span><span class="o">=</span><span class="n">lora_config</span><span class="o">.</span><span class="n">init_lora_weights</span><span class="p">,</span>
                <span class="n">use_rslora</span><span class="o">=</span><span class="n">lora_config</span><span class="o">.</span><span class="n">use_rslora</span><span class="p">,</span>
                <span class="n">use_dora</span><span class="o">=</span><span class="n">lora_config</span><span class="o">.</span><span class="n">use_dora</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_cell</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_new_cell</span><span class="p">(</span><span class="n">lora_config</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapters</span><span class="p">:</span>
                <span class="c1"># adding an additional adapter: it is not automatically trainable</span>
                <span class="n">new_cell</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_replace_cell</span><span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">target_name</span><span class="p">,</span> <span class="n">new_cell</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_replace_cell</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parent</span><span class="p">,</span> <span class="n">child_name</span><span class="p">,</span> <span class="n">new_cell</span><span class="p">,</span> <span class="n">child</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method replaces a cell within the LoraModel by updating the specified child of the parent with a new cell.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (object): The instance of the LoraModel class.</span>
<span class="sd">            parent (object): The parent object where the cell replacement will occur.</span>
<span class="sd">            child_name (str): The name of the child attribute within the parent object.</span>
<span class="sd">            new_cell (object): The new cell object that will replace the existing child within the parent.</span>
<span class="sd">            child (object): The existing child object that will be replaced by the new_cell.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            No specific exceptions are raised within this method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">child_name</span><span class="p">,</span> <span class="n">new_cell</span><span class="p">)</span>
        <span class="c1"># It&#39;s not necessary to set requires_grad here, as that is handled by</span>
        <span class="c1"># _mark_only_adapters_as_trainable</span>

        <span class="c1"># child layer wraps the original cell, unpack it</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="s2">&quot;base_layer&quot;</span><span class="p">):</span>
            <span class="n">child</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">base_layer</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">new_cell</span><span class="p">,</span> <span class="s2">&quot;base_layer&quot;</span><span class="p">):</span>
            <span class="n">new_cell</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">weight</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">):</span>
                <span class="n">new_cell</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">bias</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="s2">&quot;state&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">new_cell</span><span class="p">,</span> <span class="s2">&quot;base_layer&quot;</span><span class="p">):</span>
                <span class="n">new_cell</span><span class="o">.</span><span class="n">base_layer</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">state</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_cell</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">state</span>

    <span class="k">def</span> <span class="nf">_mark_only_adapters_as_trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Marks only specific adapters in the model as trainable based on the specified bias configuration.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (LoraModel): The instance of the LoraModel class.</span>
<span class="sd">            model (nn.Module): The neural network model on which to apply the trainable markings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: If the requested bias configuration is not implemented.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters_and_names</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">n</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">for</span> <span class="n">active_adapter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapters</span><span class="p">:</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">active_adapter</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span>
            <span class="k">if</span> <span class="n">bias</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">if</span> <span class="n">bias</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters_and_names</span><span class="p">():</span>
                    <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">n</span><span class="p">:</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">elif</span> <span class="n">bias</span> <span class="o">==</span> <span class="s2">&quot;lora_only&quot;</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells</span><span class="p">():</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Requested bias: </span><span class="si">{</span><span class="n">bias</span><span class="si">}</span><span class="s2">, is not implemented.&quot;</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_create_new_cell</span><span class="p">(</span><span class="n">lora_config</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to create a new cell based on the provided parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            lora_config (dict): The configuration parameters for the Lora model.</span>
<span class="sd">            adapter_name (str): The name of the adapter to be used.</span>
<span class="sd">            target (torch.nn.Module): The target cell for which a new cell needs to be created.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. Returns the newly created cell based on the specified target.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the target cell is not supported. Currently supported cells include `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, and `transformers.pytorch_utils.Conv1D`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Collect dispatcher functions to decide what backend to use for the replaced LoRA layer. The order matters,</span>
        <span class="c1"># because the first match is always used. Therefore, the default layers should be checked last.</span>
        <span class="n">dispatchers</span> <span class="o">=</span> <span class="p">[</span><span class="n">dispatch_default</span><span class="p">]</span>

        <span class="n">new_cell</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">dispatcher</span> <span class="ow">in</span> <span class="n">dispatchers</span><span class="p">:</span>
            <span class="n">new_cell</span> <span class="o">=</span> <span class="n">dispatcher</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">lora_config</span><span class="o">=</span><span class="n">lora_config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">new_cell</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># first match wins</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="n">new_cell</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># no cell could be matched</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Target cell </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2"> is not supported. Currently, only the following cells are supported: &quot;</span>
                <span class="s2">&quot;`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">new_cell</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward missing attributes to the wrapped cell.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>  <span class="c1"># defer to nn.Module&#39;s logic</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_peft_config_as_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a dictionary representation of the PEFT config.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the LoraModel class.</span>
<span class="sd">            inference (bool): A flag indicating whether the method is called for inference. Default is False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: A dictionary containing the PEFT config. The keys represent the configuration options, and the values</span>
<span class="sd">                  represent their corresponding values. If &#39;inference&#39; is True, the dictionary will also include the</span>
<span class="sd">                  &#39;inference_mode&#39; key set to True.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        Note:</span>
<span class="sd">            - The method uses the &#39;peft_config&#39; attribute of the LoraModel instance to create the dictionary.</span>
<span class="sd">            - If a value in the &#39;peft_config&#39; attribute is an instance of Enum, its value will be extracted using the</span>
<span class="sd">              &#39;value&#39; attribute.</span>
<span class="sd">            - The &#39;config_dict&#39; dictionary will only contain one key-value pair. If the &#39;inference&#39; flag is True, the</span>
<span class="sd">              &#39;config_dict&#39; will be updated to include the &#39;inference_mode&#39; key.</span>

<span class="sd">        Example usage:</span>
<span class="sd">            model = LoraModel()</span>
<span class="sd">            config = model.get_peft_config_as_dict(inference=True)</span>
<span class="sd">            print(config)  # {&#39;inference_mode&#39;: True}</span>

<span class="sd">            config = model.get_peft_config_as_dict()</span>
<span class="sd">            print(config)  # {}</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">Enum</span><span class="p">)</span> <span class="k">else</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">asdict</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="k">if</span> <span class="n">inference</span><span class="p">:</span>
                <span class="n">config</span><span class="p">[</span><span class="s2">&quot;inference_mode&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">config_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span> <span class="c1"># pylint: disable=undefined-loop-variable</span>
        <span class="k">return</span> <span class="n">config</span>

    <span class="k">def</span> <span class="nf">_set_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the adapter layers for the LoraModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (LoraModel): The instance of the LoraModel class.</span>
<span class="sd">            enabled (bool, optional): A flag to enable or disable the adapter layers. Defaults to True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. This method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">cell</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cells</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="p">(</span><span class="n">BaseTunerLayer</span><span class="p">,</span> <span class="n">ModulesToSaveWrapper</span><span class="p">)):</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">enable_adapters</span><span class="p">(</span><span class="n">enabled</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">enable_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Enable all adapters.</span>

<span class="sd">        Call this if you have previously disabled all adapters and want to re-enable them.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_adapter_layers</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">disable_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Disable all adapters.</span>

<span class="sd">        When disabling all adapters, the model output corresponds to the output of the base model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">active_adapter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapters</span><span class="p">:</span>
            <span class="n">val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">active_adapter</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span>
            <span class="k">if</span> <span class="n">val</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Careful, disabling adapter layers with bias configured to be &#39;</span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2">&#39; does not produce the same &quot;</span>
                    <span class="s2">&quot;output as the the base model would without adaption.&quot;</span>
                <span class="p">)</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_adapter_layers</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the active adapter(s).</span>

<span class="sd">        Additionally, this function will set the specified adapters to trainable (i.e., requires_grad=True). If this is</span>
<span class="sd">        not desired, use the following code.</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; for name, param in model_peft.parameters_and_names():</span>
<span class="sd">        ...     if ...:  # some check on name (ex. if &#39;lora&#39; in name)</span>
<span class="sd">        ...         param.requires_grad = False</span>
<span class="sd">        ```</span>

<span class="sd">        Args:</span>
<span class="sd">            adapter_name (`str` or `list[str]`): Name of the adapter(s) to be activated.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">cell</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cells</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">cell</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Adapter cannot be set when the model is merged. Unmerging the model first.&quot;</span><span class="p">)</span>
                    <span class="n">cell</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">set_adapter</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">=</span> <span class="n">adapter_name</span>

    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">_enable_peft_forward_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enable PEFT forward hooks for the LoraModel class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (LoraModel): The instance of the LoraModel class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. This method is intended to be used as a context manager and does not explicitly return a value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the &#39;adapter_names&#39; parameter is provided while the model is in training mode.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If adapter_names is passed as an argument, we inject it into the forward arguments.</span>
        <span class="n">adapter_names</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;adapter_names&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">adapter_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># nothing to do</span>
            <span class="k">yield</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot pass `adapter_names` when the model is in training mode.&quot;</span><span class="p">)</span>

        <span class="n">hook_handles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">cell</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cells</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="n">pre_forward</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_adapter_names_pre_forward_hook</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">)</span>
                <span class="n">handle</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">pre_forward</span><span class="p">,</span> <span class="n">with_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">hook_handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

        <span class="k">yield</span>

        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">hook_handles</span><span class="p">:</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_check_merge_allowed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Verify that the configuration supports merging.</span>

<span class="sd">        Currently gptq quantization and replicated layers do not support merging.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;quantization_method&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;gptq&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot merge LORA layers when the model is gptq quantized&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layer_replication&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot merge LORA layers when base model layers are replicated&quot;</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_prepare_adapter_config</span><span class="p">(</span><span class="n">peft_config</span><span class="p">,</span> <span class="n">model_config</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare the adapter configuration for a LoraModel.</span>

<span class="sd">        This method takes two parameters, peft_config and model_config, and returns None.</span>

<span class="sd">        Args:</span>
<span class="sd">            peft_config (PeftConfig): The configuration for the adapter.</span>
<span class="sd">                - target_modules (set): The target cells for the adapter. If not specified, it will be determined based on the model type.</span>
<span class="sd">            model_config (dict): The configuration for the model.</span>
<span class="sd">                - model_type (str): The type of the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None. The method does not return any value.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the target_modules is not specified in peft_config and the model_type is not found in the TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">peft_config</span><span class="o">.</span><span class="n">target_modules</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">model_config</span><span class="p">[</span><span class="s2">&quot;model_type&quot;</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please specify `target_modules` in `peft_config`&quot;</span><span class="p">)</span>
            <span class="n">peft_config</span><span class="o">.</span><span class="n">target_modules</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
                <span class="n">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING</span><span class="p">[</span><span class="n">model_config</span><span class="p">[</span><span class="s2">&quot;model_type&quot;</span><span class="p">]]</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">peft_config</span>

    <span class="k">def</span> <span class="nf">_unload_and_optionally_merge</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">merge</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">progressbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">safe_merge</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to unload and optionally merge a LoraModel.</span>

<span class="sd">        Args:</span>
<span class="sd">        - self: The instance of the LoraModel class.</span>
<span class="sd">        - merge (bool): Flag indicating whether to perform a merge operation.</span>
<span class="sd">        - progressbar (bool): Flag indicating whether to display a progress bar during unloading.</span>
<span class="sd">        - safe_merge (bool): Flag indicating whether to perform a safe merge operation.</span>
<span class="sd">        - adapter_names (Optional[list[str]]): List of names of adapters to consider during unloading.</span>

<span class="sd">        Returns:</span>
<span class="sd">        None. The method modifies the model in place.</span>

<span class="sd">        Raises:</span>
<span class="sd">        - AttributeError: If an attribute error occurs during the unloading process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">merge</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_check_merge_allowed</span><span class="p">()</span>

        <span class="n">key_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">key</span><span class="p">]</span>
        <span class="n">desc</span> <span class="o">=</span> <span class="s2">&quot;Unloading &quot;</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;and merging &quot;</span> <span class="k">if</span> <span class="n">merge</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;model&quot;</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">key_list</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">progressbar</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="n">desc</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">parent</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_name</span> <span class="o">=</span> <span class="n">_get_subcells</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="c1"># with onload_layer(target):</span>
            <span class="c1">#     if hasattr(target, &quot;base_layer&quot;):</span>
            <span class="c1">#         if merge:</span>
            <span class="c1">#             target.merge(safe_merge=safe_merge, adapter_names=adapter_names)</span>
            <span class="c1">#         self._replace_cell(parent, target_name, target.get_base_layer(), target)</span>
            <span class="c1">#     elif isinstance(target, ModulesToSaveWrapper):</span>
            <span class="c1">#         # save any additional trainable cells part of `cells_to_save`</span>
            <span class="c1">#         new_cell = target.cells_to_save[target.active_adapter]</span>
            <span class="c1">#         if hasattr(new_cell, &quot;base_layer&quot;):</span>
            <span class="c1">#             # check if the cell is itself a tuner layer</span>
            <span class="c1">#             if merge:</span>
            <span class="c1">#                 new_cell.merge(safe_merge=safe_merge, adapter_names=adapter_names)</span>
            <span class="c1">#             new_cell = new_cell.get_base_layer()</span>
            <span class="c1">#         setattr(parent, target_name, new_cell)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>

    <span class="k">def</span> <span class="nf">_check_add_weighted_adapter</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">adapters</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">combination_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">svd_rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function to check if the arguments to add_weighted_adapter are valid and compatible with the underlying</span>
<span class="sd">        model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adapter </span><span class="si">{</span><span class="n">adapter</span><span class="si">}</span><span class="s2"> does not exist&quot;</span><span class="p">)</span>

        <span class="c1"># If more than one of the adapters targets the same cell with cells_to_save, raise an error, as these</span>
        <span class="c1"># cells cannot be merged. First, find the ModulesToSaveWrapper instances in the model, then check if they</span>
        <span class="c1"># have cells for the adapters to be merged.</span>
        <span class="n">cells_to_save_wrappers</span> <span class="o">=</span> <span class="p">[</span><span class="n">cell</span> <span class="k">for</span> <span class="n">cell</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cells</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">ModulesToSaveWrapper</span><span class="p">)]</span>
        <span class="n">problematic_wrappers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">wrapper</span>
            <span class="k">for</span> <span class="n">wrapper</span> <span class="ow">in</span> <span class="n">cells_to_save_wrappers</span>
            <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">adapter</span> <span class="ow">in</span> <span class="n">wrapper</span><span class="o">.</span><span class="n">cells_to_save</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="n">problematic_wrappers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot add weighted adapters if they target the same cell with cells_to_save, but found &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">problematic_wrappers</span><span class="p">)</span><span class="si">}</span><span class="s2"> such instance(s).&quot;</span>
            <span class="p">)</span>

        <span class="c1"># if there is only one adapter, we can only use linear merging</span>
        <span class="n">combination_type</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapters</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">combination_type</span>

        <span class="n">adapters_ranks</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">r</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">combination_type</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;ties&quot;</span><span class="p">,</span> <span class="s2">&quot;dare_ties&quot;</span><span class="p">,</span> <span class="s2">&quot;dare_linear&quot;</span><span class="p">,</span> <span class="s2">&quot;magnitude_prune&quot;</span><span class="p">):</span>
            <span class="c1"># all adapters ranks should be same, new rank is just this value</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">adapters_ranks</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;All adapters must have the same r value when using combination_type linear, ties, dare_ties or &quot;</span>
                    <span class="s2">&quot;dare_linear.&quot;</span>
                <span class="p">)</span>
            <span class="n">new_rank</span> <span class="o">=</span> <span class="n">adapters_ranks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;cat&quot;</span><span class="p">:</span>
            <span class="c1"># adapters ranks may be different, new rank is sum of all ranks</span>
            <span class="c1"># be careful, because output adapter rank may be really big if mixing a lot of adapters</span>
            <span class="n">new_rank</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">adapters_ranks</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">combination_type</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;svd&quot;</span><span class="p">):</span>
            <span class="c1"># new rank is the max of all ranks of the adapters if not provided</span>
            <span class="n">new_rank</span> <span class="o">=</span> <span class="n">svd_rank</span> <span class="ow">or</span> <span class="nb">max</span><span class="p">(</span><span class="n">adapters_ranks</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid combination_type: </span><span class="si">{</span><span class="n">combination_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">target_cell_types</span> <span class="o">=</span> <span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">target_modules</span><span class="p">)</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">target_cell_types</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found no adapter matching the names in </span><span class="si">{</span><span class="n">adapters</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">target_cell_types</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;all adapter configs should follow the same target cells type. &quot;</span>
                <span class="s2">&quot;Combining adapters with `target_modules` type being a mix of list/set and string is not supported.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">target_cell_types</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="nb">str</span><span class="p">:</span>
            <span class="n">new_target_modules</span> <span class="o">=</span> <span class="s2">&quot;|&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">target_modules</span><span class="si">}</span><span class="s2">)&quot;</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">target_cell_types</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="nb">set</span><span class="p">:</span>
            <span class="n">new_target_modules</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span>
                <span class="n">operator</span><span class="o">.</span><span class="n">or_</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">target_modules</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid type </span><span class="si">{</span><span class="n">target_cell_types</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> found in target_modules&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">combination_type</span><span class="p">,</span> <span class="n">new_rank</span><span class="p">,</span> <span class="n">new_target_modules</span>

    <span class="k">def</span> <span class="nf">add_weighted_adapter</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">adapters</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">weights</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">combination_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;svd&quot;</span><span class="p">,</span>
        <span class="n">svd_rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">svd_clamp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">svd_full_matrices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">density</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">majority_sign_method</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;total&quot;</span><span class="p">,</span> <span class="s2">&quot;frequency&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;total&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method adds a new adapter by merging the given adapters with the given weights.</span>

<span class="sd">        When using the `cat` combination_type you should be aware that rank of the resulting adapter will be equal to</span>
<span class="sd">        the sum of all adapters ranks. So it&#39;s possible that the mixed adapter may become too big and result in OOM</span>
<span class="sd">        errors.</span>

<span class="sd">        Args:</span>
<span class="sd">            adapters (`list`):</span>
<span class="sd">                List of adapter names to be merged.</span>
<span class="sd">            weights (`list`):</span>
<span class="sd">                List of weights for each adapter.</span>
<span class="sd">            adapter_name (`str`):</span>
<span class="sd">                Name of the new adapter.</span>
<span class="sd">            combination_type (`str`):</span>
<span class="sd">                The merging type can be one of [`svd`, `linear`, `cat`, `ties`, `ties_svd`, `dare_ties`, `dare_linear`,</span>
<span class="sd">                `dare_ties_svd`, `dare_linear_svd`, `magnitude_prune`, `magnitude_prune_svd`]. When using the `cat`</span>
<span class="sd">                combination_type, the rank of the resulting adapter is equal to the sum of all adapters ranks (the</span>
<span class="sd">                mixed adapter may be too big and result in OOM errors).</span>
<span class="sd">            svd_rank (`int`, *optional*):</span>
<span class="sd">                Rank of output adapter for svd. If None provided, will use max rank of merging adapters.</span>
<span class="sd">            svd_clamp (`float`, *optional*):</span>
<span class="sd">                A quantile threshold for clamping SVD decomposition output. If None is provided, do not perform</span>
<span class="sd">                clamping. Defaults to None.</span>
<span class="sd">            svd_full_matrices (`bool`, *optional*):</span>
<span class="sd">                Controls whether to compute the full or reduced SVD, and consequently, the shape of the returned</span>
<span class="sd">                tensors U and Vh. Defaults to True.</span>
<span class="sd">            density (`float`, *optional*):</span>
<span class="sd">                Value between 0 and 1. 0 means all values are pruned and 1 means no values are pruned. Should be used</span>
<span class="sd">                with [`ties`, `ties_svd`, `dare_ties`, `dare_linear`, `dare_ties_svd`, `dare_linear_svd`,</span>
<span class="sd">                `magnintude_prune`, `magnitude_prune_svd`]</span>
<span class="sd">            majority_sign_method (`str`):</span>
<span class="sd">                The method, should be one of [&quot;total&quot;, &quot;frequency&quot;], to use to get the magnitude of the sign values.</span>
<span class="sd">                Should be used with [`ties`, `ties_svd`, `dare_ties`, `dare_ties_svd`]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">return</span>
        <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adapter </span><span class="si">{</span><span class="n">adapter</span><span class="si">}</span><span class="s2"> does not exist&quot;</span><span class="p">)</span>

        <span class="n">combination_type</span><span class="p">,</span> <span class="n">new_rank</span><span class="p">,</span> <span class="n">new_target_modules</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_add_weighted_adapter</span><span class="p">(</span>
            <span class="n">adapters</span><span class="o">=</span><span class="n">adapters</span><span class="p">,</span>
            <span class="n">combination_type</span><span class="o">=</span><span class="n">combination_type</span><span class="p">,</span>
            <span class="n">svd_rank</span><span class="o">=</span><span class="n">svd_rank</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">replace</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapters</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
            <span class="n">r</span><span class="o">=</span><span class="n">new_rank</span><span class="p">,</span>
            <span class="n">lora_alpha</span><span class="o">=</span><span class="n">new_rank</span><span class="p">,</span>
            <span class="n">target_modules</span><span class="o">=</span><span class="n">new_target_modules</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inject_adapter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

        <span class="c1"># Do we really need that?</span>
        <span class="n">_freeze_adapter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

        <span class="n">key_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">key</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">key_list</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_get_subcells</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">:</span>
                    <span class="n">target_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                    <span class="n">target_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                <span class="k">elif</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">:</span>
                    <span class="n">target_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span>
                    <span class="n">target_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_B</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="mf">0.0</span>
                <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="mf">0.0</span>
                <span class="k">if</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;cat&quot;</span><span class="p">:</span>
                    <span class="n">loras_A</span><span class="p">,</span> <span class="n">loras_B</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">adapter</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">:</span>
                            <span class="n">current_adapter_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                            <span class="n">current_adapter_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                        <span class="k">elif</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">:</span>
                            <span class="n">current_adapter_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
                            <span class="n">current_adapter_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">continue</span>
                        <span class="n">loras_A</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_adapter_lora_A</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">target</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter</span><span class="p">])</span>
                        <span class="n">loras_B</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_adapter_lora_B</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loras_A</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No matching LoRAs found. Please raise an issue on GitHub.&quot;</span><span class="p">)</span>
                    <span class="n">loras_A</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">loras_A</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="n">loras_B</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">loras_B</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span> <span class="n">loras_A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">loras_A</span>
                    <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">loras_B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">loras_B</span>
                <span class="k">elif</span> <span class="n">combination_type</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="s2">&quot;svd&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;ties_svd&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;dare_linear_svd&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;dare_ties_svd&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;magnitude_prune_svd&quot;</span><span class="p">,</span>
                <span class="p">]:</span>
                    <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_svd_generalized_task_arithmetic_weighted_adapter</span><span class="p">(</span>
                        <span class="n">combination_type</span><span class="p">,</span>
                        <span class="n">adapters</span><span class="p">,</span>
                        <span class="n">weights</span><span class="p">,</span>
                        <span class="n">new_rank</span><span class="p">,</span>
                        <span class="n">target</span><span class="p">,</span>
                        <span class="n">target_lora_A</span><span class="p">,</span>
                        <span class="n">target_lora_B</span><span class="p">,</span>
                        <span class="n">density</span><span class="p">,</span>
                        <span class="n">majority_sign_method</span><span class="p">,</span>
                        <span class="n">svd_clamp</span><span class="p">,</span>
                        <span class="n">full_matrices</span><span class="o">=</span><span class="n">svd_full_matrices</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">combination_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;ties&quot;</span><span class="p">,</span> <span class="s2">&quot;dare_linear&quot;</span><span class="p">,</span> <span class="s2">&quot;dare_ties&quot;</span><span class="p">,</span> <span class="s2">&quot;magnitude_prune&quot;</span><span class="p">]:</span>
                    <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generalized_task_arithmetic_weighted_adapter</span><span class="p">(</span>
                        <span class="n">combination_type</span><span class="p">,</span> <span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">density</span><span class="p">,</span> <span class="n">majority_sign_method</span>
                    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_svd_generalized_task_arithmetic_weighted_adapter</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">combination_type</span><span class="p">,</span>
        <span class="n">adapters</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">,</span>
        <span class="n">new_rank</span><span class="p">,</span>
        <span class="n">target</span><span class="p">,</span>
        <span class="n">target_lora_A</span><span class="p">,</span>
        <span class="n">target_lora_B</span><span class="p">,</span>
        <span class="n">density</span><span class="p">,</span>
        <span class="n">majority_sign_method</span><span class="p">,</span>
        <span class="n">clamp</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Perform a Singular Value Decomposition (SVD) with various combination types on the given parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (LoraModel): The instance of the LoraModel class.</span>
<span class="sd">            combination_type (str): The type of combination to perform. Valid options are:</span>
<span class="sd">                - &#39;svd&#39;: Standard SVD combination.</span>
<span class="sd">                - &#39;ties_svd&#39;: Combination with ties.</span>
<span class="sd">                - &#39;dare_linear_svd&#39;: Combination with DARE (Density-Aware Ranking Evaluation) using linear interpolation.</span>
<span class="sd">                - &#39;dare_ties_svd&#39;: Combination with DARE (Density-Aware Ranking Evaluation) using ties.</span>
<span class="sd">                - &#39;magnitude_prune_svd&#39;: Combination with magnitude pruning.</span>
<span class="sd">            adapters (list): A list of adapters to consider for the combination.</span>
<span class="sd">            weights (list): A list of weights corresponding to the adapters.</span>
<span class="sd">            new_rank (int): The desired new rank after the combination.</span>
<span class="sd">            target: The target object.</span>
<span class="sd">            target_lora_A: The target LoRA A object.</span>
<span class="sd">            target_lora_B: The target LoRA B object.</span>
<span class="sd">            density (float): The density parameter used in combination types &#39;ties_svd&#39;, &#39;dare_linear_svd&#39;, &#39;dare_ties_svd&#39;, and &#39;magnitude_prune_svd&#39;.</span>
<span class="sd">            majority_sign_method (str): The majority sign method used in combination types &#39;ties_svd&#39; and &#39;dare_ties_svd&#39;. Valid options are:</span>
<span class="sd">                - &#39;positive&#39;: Majority sign is positive.</span>
<span class="sd">                - &#39;negative&#39;: Majority sign is negative.</span>
<span class="sd">                - &#39;absolute&#39;: Majority sign is absolute.</span>
<span class="sd">            clamp (float, optional): The clamping value. Defaults to None.</span>
<span class="sd">            full_matrices (bool, optional): Whether to compute full matrices in the SVD computation. Defaults to True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If no matching LoRAs are found.</span>
<span class="sd">            ValueError: If an invalid value is passed to the combination_type parameter.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">valid_adapters</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">valid_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">is_embedding</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">adapter</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">adapter</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span> <span class="ow">or</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">:</span>
                <span class="n">valid_adapters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">adapter</span><span class="p">)</span>
                <span class="n">valid_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="n">target</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter</span><span class="p">])</span>

        <span class="c1"># if no valid adapter, nothing to do</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_adapters</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No matching LoRAs found. Please raise an issue on Github.&quot;</span><span class="p">)</span>
        <span class="n">delta_weight</span> <span class="o">=</span> <span class="p">[</span><span class="n">target</span><span class="o">.</span><span class="n">get_delta_weight</span><span class="p">(</span><span class="n">adapter</span><span class="p">)</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">valid_adapters</span><span class="p">]</span>
        <span class="n">valid_weights</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">valid_weights</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;svd&quot;</span><span class="p">:</span>
            <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">task_arithmetic</span><span class="p">(</span><span class="n">delta_weight</span><span class="p">,</span> <span class="n">valid_weights</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;ties_svd&quot;</span><span class="p">:</span>
            <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">ties</span><span class="p">(</span><span class="n">delta_weight</span><span class="p">,</span> <span class="n">valid_weights</span><span class="p">,</span> <span class="n">density</span><span class="p">,</span> <span class="n">majority_sign_method</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;dare_linear_svd&quot;</span><span class="p">:</span>
            <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">dare_linear</span><span class="p">(</span><span class="n">delta_weight</span><span class="p">,</span> <span class="n">valid_weights</span><span class="p">,</span> <span class="n">density</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;dare_ties_svd&quot;</span><span class="p">:</span>
            <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">dare_ties</span><span class="p">(</span><span class="n">delta_weight</span><span class="p">,</span> <span class="n">valid_weights</span><span class="p">,</span> <span class="n">density</span><span class="p">,</span> <span class="n">majority_sign_method</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;magnitude_prune_svd&quot;</span><span class="p">:</span>
            <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">magnitude_prune</span><span class="p">(</span><span class="n">delta_weight</span><span class="p">,</span> <span class="n">valid_weights</span><span class="p">,</span> <span class="n">density</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid value passed to combination type: </span><span class="si">{</span><span class="n">combination_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">conv2d</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">Conv2d</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">conv2d</span><span class="p">:</span>
            <span class="n">conv2d_1x1</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">conv2d_1x1</span><span class="p">:</span>
                <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">delta_weight</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">delta_weight</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="s2">&quot;fan_in_fan_out&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">fan_in_fan_out</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_embedding</span><span class="p">:</span>
            <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">delta_weight</span><span class="o">.</span><span class="n">T</span>

        <span class="c1"># based on https://github.com/kohya-ss/sd-scripts/blob/main/networks/svd_merge_lora.py#L114-L131</span>
        <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">delta_weight</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="n">full_matrices</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">new_rank</span><span class="p">]</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">S</span><span class="p">[:</span><span class="n">new_rank</span><span class="p">]</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">ops</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
        <span class="n">Vh</span> <span class="o">=</span> <span class="n">Vh</span><span class="p">[:</span><span class="n">new_rank</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">if</span> <span class="n">clamp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">U</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">Vh</span><span class="o">.</span><span class="n">flatten</span><span class="p">()])</span>
            <span class="n">hi_val</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">clamp</span><span class="p">)</span>
            <span class="n">low_val</span> <span class="o">=</span> <span class="o">-</span><span class="n">hi_val</span>
            <span class="n">U</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">low_val</span><span class="p">,</span> <span class="n">hi_val</span><span class="p">)</span>
            <span class="n">Vh</span> <span class="o">=</span> <span class="n">Vh</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">low_val</span><span class="p">,</span> <span class="n">hi_val</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">conv2d</span><span class="p">:</span>
            <span class="n">U</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">Vh</span> <span class="o">=</span> <span class="n">Vh</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Vh</span><span class="p">,</span> <span class="n">U</span>

    <span class="k">def</span> <span class="nf">_generalized_task_arithmetic_weighted_adapter</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">combination_type</span><span class="p">,</span>
        <span class="n">adapters</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">,</span>
        <span class="n">target</span><span class="p">,</span>
        <span class="n">density</span><span class="p">,</span>
        <span class="n">majority_sign_method</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generalized Task Arithmetic Weighted Adapter.</span>

<span class="sd">        This method performs a weighted combination of task arithmetic operations on the given adapters and their corresponding weights.</span>
<span class="sd">        The combination type determines the specific arithmetic operation to be applied.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (LoraModel): The instance of the LoraModel class.</span>
<span class="sd">            combination_type (str): The type of combination to be performed. Valid values are:</span>
<span class="sd">                - &#39;linear&#39;: Perform a linear combination of the task tensors.</span>
<span class="sd">                - &#39;ties&#39;: Perform a combination of task tensors with tie handling.</span>
<span class="sd">                - &#39;dare_linear&#39;: Perform a linear combination of task tensors with density-aware regularization.</span>
<span class="sd">                - &#39;dare_ties&#39;: Perform a combination of task tensors with tie handling and density-aware regularization.</span>
<span class="sd">                - &#39;magnitude_prune&#39;: Perform a combination of task tensors with magnitude pruning.</span>
<span class="sd">            adapters (list): A list of adapter names.</span>
<span class="sd">            weights (list): A list of weights corresponding to the adapters.</span>
<span class="sd">            target (Target): The target object containing the lora_A, lora_B, lora_embedding_A, and lora_embedding_B attributes.</span>
<span class="sd">            density (float): The density parameter for density-aware regularization.</span>
<span class="sd">            majority_sign_method (str): The method to determine the sign of the majority in tie handling. Valid values are:</span>
<span class="sd">                - &#39;positive&#39;: The majority is considered positive.</span>
<span class="sd">                - &#39;negative&#39;: The majority is considered negative.</span>

<span class="sd">        Returns:</span>
<span class="sd">            list: A list containing the combined task tensors for lora_A and lora_B.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the combination_type parameter is not one of the valid combination types.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># account weights for LoRA A and B layers.</span>
        <span class="n">valid_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">lora_A_deltas</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">lora_B_deltas</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">adapter</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">:</span>
                <span class="n">current_adapter_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                <span class="n">current_adapter_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
            <span class="k">elif</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">:</span>
                <span class="n">current_adapter_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
                <span class="n">current_adapter_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">valid_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="n">target</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter</span><span class="p">]))</span>
            <span class="n">lora_A_deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_adapter_lora_A</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
            <span class="n">lora_B_deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_adapter_lora_B</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">valid_weights</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">valid_weights</span><span class="p">)</span>
        <span class="n">lora_deltas</span> <span class="o">=</span> <span class="p">[</span><span class="n">lora_A_deltas</span><span class="p">,</span> <span class="n">lora_B_deltas</span><span class="p">]</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">lora_A_deltas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">task_tensors</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lora_deltas</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
                <span class="n">lora_deltas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">task_arithmetic</span><span class="p">(</span><span class="n">task_tensors</span><span class="p">,</span> <span class="n">valid_weights</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;ties&quot;</span><span class="p">:</span>
                <span class="n">lora_deltas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ties</span><span class="p">(</span><span class="n">task_tensors</span><span class="p">,</span> <span class="n">valid_weights</span><span class="p">,</span> <span class="n">density</span><span class="p">,</span> <span class="n">majority_sign_method</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;dare_linear&quot;</span><span class="p">:</span>
                <span class="n">lora_deltas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">dare_linear</span><span class="p">(</span><span class="n">task_tensors</span><span class="p">,</span> <span class="n">valid_weights</span><span class="p">,</span> <span class="n">density</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;dare_ties&quot;</span><span class="p">:</span>
                <span class="n">lora_deltas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">dare_ties</span><span class="p">(</span><span class="n">task_tensors</span><span class="p">,</span> <span class="n">valid_weights</span><span class="p">,</span> <span class="n">density</span><span class="p">,</span> <span class="n">majority_sign_method</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;magnitude_prune&quot;</span><span class="p">:</span>
                <span class="n">lora_deltas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">magnitude_prune</span><span class="p">(</span><span class="n">task_tensors</span><span class="p">,</span> <span class="n">valid_weights</span><span class="p">,</span> <span class="n">density</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid combination type&quot;</span><span class="p">)</span>
        <span class="n">lora_deltas</span> <span class="o">=</span> <span class="p">[</span><span class="n">delta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span> <span class="k">for</span> <span class="n">delta</span> <span class="ow">in</span> <span class="n">lora_deltas</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">lora_deltas</span>

    <span class="k">def</span> <span class="nf">delete_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deletes an existing adapter.</span>

<span class="sd">        Args:</span>
<span class="sd">            adapter_name (str): Name of the adapter to be deleted.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adapter </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> does not exist&quot;</span><span class="p">)</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span>

        <span class="n">key_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">key</span><span class="p">]</span>
        <span class="n">new_adapter</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">key_list</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_get_subcells</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="n">target</span><span class="o">.</span><span class="n">delete_adapter</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">new_adapter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">new_adapter</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">active_adapters</span><span class="p">[:]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">=</span> <span class="n">new_adapter</span> <span class="ow">or</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">merge_and_unload</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">progressbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">safe_merge</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model</span>
<span class="sd">        as a standalone model.</span>

<span class="sd">        Args:</span>
<span class="sd">            progressbar (`bool`):</span>
<span class="sd">                whether to show a progressbar indicating the unload and merge process</span>
<span class="sd">            safe_merge (`bool`):</span>
<span class="sd">                whether to activate the safe merging check to check if there is any potential Nan in the adapter</span>
<span class="sd">                weights</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                The list of adapter names that should be merged. If None, all active adapters will be merged. Defaults</span>
<span class="sd">                to `None`.</span>
<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoModelForCausalLM</span>
<span class="sd">        &gt;&gt;&gt; from peft import PeftModel</span>

<span class="sd">        &gt;&gt;&gt; base_model = AutoModelForCausalLM.from_pretrained(&quot;tiiuae/falcon-40b&quot;)</span>
<span class="sd">        &gt;&gt;&gt; peft_model_id = &quot;smangrul/falcon-40B-int4-peft-lora-sfttrainer-sample&quot;</span>
<span class="sd">        &gt;&gt;&gt; model = PeftModel.from_pretrained(base_model, peft_model_id)</span>
<span class="sd">        &gt;&gt;&gt; merged_model = model.merge_and_unload()</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unload_and_optionally_merge</span><span class="p">(</span>
            <span class="n">progressbar</span><span class="o">=</span><span class="n">progressbar</span><span class="p">,</span> <span class="n">safe_merge</span><span class="o">=</span><span class="n">safe_merge</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">unload</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets back the base model by removing all the lora cells without merging. This gives back the original base</span>
<span class="sd">        model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unload_and_optionally_merge</span><span class="p">(</span><span class="n">merge</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.peft.tuners.lora.model.LoraModel.__getattr__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">peft</span><span class="o">.</span><span class="n">tuners</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">LoraModel</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span></code>

<a href="#mindnlp.peft.tuners.lora.model.LoraModel.__getattr__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Forward missing attributes to the wrapped cell.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\peft\tuners\lora\model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward missing attributes to the wrapped cell.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>  <span class="c1"># defer to nn.Module&#39;s logic</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.peft.tuners.lora.model.LoraModel.add_weighted_adapter" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">peft</span><span class="o">.</span><span class="n">tuners</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">LoraModel</span><span class="o">.</span><span class="n">add_weighted_adapter</span><span class="p">(</span><span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">combination_type</span><span class="o">=</span><span class="s1">&#39;svd&#39;</span><span class="p">,</span> <span class="n">svd_rank</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">svd_clamp</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">svd_full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">majority_sign_method</span><span class="o">=</span><span class="s1">&#39;total&#39;</span><span class="p">)</span></code>

<a href="#mindnlp.peft.tuners.lora.model.LoraModel.add_weighted_adapter" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method adds a new adapter by merging the given adapters with the given weights.</p>
<p>When using the <code>cat</code> combination_type you should be aware that rank of the resulting adapter will be equal to
the sum of all adapters ranks. So it's possible that the mixed adapter may become too big and result in OOM
errors.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>adapters</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of adapter names to be merged.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`list`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>weights</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of weights for each adapter.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`list`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Name of the new adapter.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>combination_type</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The merging type can be one of [<code>svd</code>, <code>linear</code>, <code>cat</code>, <code>ties</code>, <code>ties_svd</code>, <code>dare_ties</code>, <code>dare_linear</code>,
<code>dare_ties_svd</code>, <code>dare_linear_svd</code>, <code>magnitude_prune</code>, <code>magnitude_prune_svd</code>]. When using the <code>cat</code>
combination_type, the rank of the resulting adapter is equal to the sum of all adapters ranks (the
mixed adapter may be too big and result in OOM errors).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;svd&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>svd_rank</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Rank of output adapter for svd. If None provided, will use max rank of merging adapters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>svd_clamp</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A quantile threshold for clamping SVD decomposition output. If None is provided, do not perform
clamping. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>svd_full_matrices</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls whether to compute the full or reduced SVD, and consequently, the shape of the returned
tensors U and Vh. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>density</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Value between 0 and 1. 0 means all values are pruned and 1 means no values are pruned. Should be used
with [<code>ties</code>, <code>ties_svd</code>, <code>dare_ties</code>, <code>dare_linear</code>, <code>dare_ties_svd</code>, <code>dare_linear_svd</code>,
<code>magnintude_prune</code>, <code>magnitude_prune_svd</code>]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>majority_sign_method</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The method, should be one of ["total", "frequency"], to use to get the magnitude of the sign values.
Should be used with [<code>ties</code>, <code>ties_svd</code>, <code>dare_ties</code>, <code>dare_ties_svd</code>]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;total&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\peft\tuners\lora\model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">add_weighted_adapter</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">adapters</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">weights</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">combination_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;svd&quot;</span><span class="p">,</span>
    <span class="n">svd_rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">svd_clamp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">svd_full_matrices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">density</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">majority_sign_method</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;total&quot;</span><span class="p">,</span> <span class="s2">&quot;frequency&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;total&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method adds a new adapter by merging the given adapters with the given weights.</span>

<span class="sd">    When using the `cat` combination_type you should be aware that rank of the resulting adapter will be equal to</span>
<span class="sd">    the sum of all adapters ranks. So it&#39;s possible that the mixed adapter may become too big and result in OOM</span>
<span class="sd">    errors.</span>

<span class="sd">    Args:</span>
<span class="sd">        adapters (`list`):</span>
<span class="sd">            List of adapter names to be merged.</span>
<span class="sd">        weights (`list`):</span>
<span class="sd">            List of weights for each adapter.</span>
<span class="sd">        adapter_name (`str`):</span>
<span class="sd">            Name of the new adapter.</span>
<span class="sd">        combination_type (`str`):</span>
<span class="sd">            The merging type can be one of [`svd`, `linear`, `cat`, `ties`, `ties_svd`, `dare_ties`, `dare_linear`,</span>
<span class="sd">            `dare_ties_svd`, `dare_linear_svd`, `magnitude_prune`, `magnitude_prune_svd`]. When using the `cat`</span>
<span class="sd">            combination_type, the rank of the resulting adapter is equal to the sum of all adapters ranks (the</span>
<span class="sd">            mixed adapter may be too big and result in OOM errors).</span>
<span class="sd">        svd_rank (`int`, *optional*):</span>
<span class="sd">            Rank of output adapter for svd. If None provided, will use max rank of merging adapters.</span>
<span class="sd">        svd_clamp (`float`, *optional*):</span>
<span class="sd">            A quantile threshold for clamping SVD decomposition output. If None is provided, do not perform</span>
<span class="sd">            clamping. Defaults to None.</span>
<span class="sd">        svd_full_matrices (`bool`, *optional*):</span>
<span class="sd">            Controls whether to compute the full or reduced SVD, and consequently, the shape of the returned</span>
<span class="sd">            tensors U and Vh. Defaults to True.</span>
<span class="sd">        density (`float`, *optional*):</span>
<span class="sd">            Value between 0 and 1. 0 means all values are pruned and 1 means no values are pruned. Should be used</span>
<span class="sd">            with [`ties`, `ties_svd`, `dare_ties`, `dare_linear`, `dare_ties_svd`, `dare_linear_svd`,</span>
<span class="sd">            `magnintude_prune`, `magnitude_prune_svd`]</span>
<span class="sd">        majority_sign_method (`str`):</span>
<span class="sd">            The method, should be one of [&quot;total&quot;, &quot;frequency&quot;], to use to get the magnitude of the sign values.</span>
<span class="sd">            Should be used with [`ties`, `ties_svd`, `dare_ties`, `dare_ties_svd`]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="k">return</span>
    <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adapter </span><span class="si">{</span><span class="n">adapter</span><span class="si">}</span><span class="s2"> does not exist&quot;</span><span class="p">)</span>

    <span class="n">combination_type</span><span class="p">,</span> <span class="n">new_rank</span><span class="p">,</span> <span class="n">new_target_modules</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_add_weighted_adapter</span><span class="p">(</span>
        <span class="n">adapters</span><span class="o">=</span><span class="n">adapters</span><span class="p">,</span>
        <span class="n">combination_type</span><span class="o">=</span><span class="n">combination_type</span><span class="p">,</span>
        <span class="n">svd_rank</span><span class="o">=</span><span class="n">svd_rank</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">replace</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapters</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
        <span class="n">r</span><span class="o">=</span><span class="n">new_rank</span><span class="p">,</span>
        <span class="n">lora_alpha</span><span class="o">=</span><span class="n">new_rank</span><span class="p">,</span>
        <span class="n">target_modules</span><span class="o">=</span><span class="n">new_target_modules</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inject_adapter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

    <span class="c1"># Do we really need that?</span>
    <span class="n">_freeze_adapter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

    <span class="n">key_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">key</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">key_list</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_get_subcells</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">:</span>
                <span class="n">target_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                <span class="n">target_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
            <span class="k">elif</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">:</span>
                <span class="n">target_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span>
                <span class="n">target_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_B</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="mf">0.0</span>
            <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="mf">0.0</span>
            <span class="k">if</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;cat&quot;</span><span class="p">:</span>
                <span class="n">loras_A</span><span class="p">,</span> <span class="n">loras_B</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">adapter</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">:</span>
                        <span class="n">current_adapter_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                        <span class="n">current_adapter_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                    <span class="k">elif</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">:</span>
                        <span class="n">current_adapter_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
                        <span class="n">current_adapter_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">loras_A</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_adapter_lora_A</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">target</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter</span><span class="p">])</span>
                    <span class="n">loras_B</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_adapter_lora_B</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loras_A</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No matching LoRAs found. Please raise an issue on GitHub.&quot;</span><span class="p">)</span>
                <span class="n">loras_A</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">loras_A</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">loras_B</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">loras_B</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span> <span class="n">loras_A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">loras_A</span>
                <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">loras_B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">loras_B</span>
            <span class="k">elif</span> <span class="n">combination_type</span> <span class="ow">in</span> <span class="p">[</span>
                <span class="s2">&quot;svd&quot;</span><span class="p">,</span>
                <span class="s2">&quot;ties_svd&quot;</span><span class="p">,</span>
                <span class="s2">&quot;dare_linear_svd&quot;</span><span class="p">,</span>
                <span class="s2">&quot;dare_ties_svd&quot;</span><span class="p">,</span>
                <span class="s2">&quot;magnitude_prune_svd&quot;</span><span class="p">,</span>
            <span class="p">]:</span>
                <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_svd_generalized_task_arithmetic_weighted_adapter</span><span class="p">(</span>
                    <span class="n">combination_type</span><span class="p">,</span>
                    <span class="n">adapters</span><span class="p">,</span>
                    <span class="n">weights</span><span class="p">,</span>
                    <span class="n">new_rank</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">,</span>
                    <span class="n">target_lora_A</span><span class="p">,</span>
                    <span class="n">target_lora_B</span><span class="p">,</span>
                    <span class="n">density</span><span class="p">,</span>
                    <span class="n">majority_sign_method</span><span class="p">,</span>
                    <span class="n">svd_clamp</span><span class="p">,</span>
                    <span class="n">full_matrices</span><span class="o">=</span><span class="n">svd_full_matrices</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">combination_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;ties&quot;</span><span class="p">,</span> <span class="s2">&quot;dare_linear&quot;</span><span class="p">,</span> <span class="s2">&quot;dare_ties&quot;</span><span class="p">,</span> <span class="s2">&quot;magnitude_prune&quot;</span><span class="p">]:</span>
                <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generalized_task_arithmetic_weighted_adapter</span><span class="p">(</span>
                    <span class="n">combination_type</span><span class="p">,</span> <span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">density</span><span class="p">,</span> <span class="n">majority_sign_method</span>
                <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.peft.tuners.lora.model.LoraModel.delete_adapter" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">peft</span><span class="o">.</span><span class="n">tuners</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">LoraModel</span><span class="o">.</span><span class="n">delete_adapter</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span></code>

<a href="#mindnlp.peft.tuners.lora.model.LoraModel.delete_adapter" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Deletes an existing adapter.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Name of the adapter to be deleted.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\peft\tuners\lora\model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">delete_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deletes an existing adapter.</span>

<span class="sd">    Args:</span>
<span class="sd">        adapter_name (str): Name of the adapter to be deleted.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adapter </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> does not exist&quot;</span><span class="p">)</span>
    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span>

    <span class="n">key_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">key</span><span class="p">]</span>
    <span class="n">new_adapter</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">key_list</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_get_subcells</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
            <span class="n">target</span><span class="o">.</span><span class="n">delete_adapter</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">new_adapter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">new_adapter</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">active_adapters</span><span class="p">[:]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">=</span> <span class="n">new_adapter</span> <span class="ow">or</span> <span class="p">[]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.peft.tuners.lora.model.LoraModel.disable_adapter_layers" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">peft</span><span class="o">.</span><span class="n">tuners</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">LoraModel</span><span class="o">.</span><span class="n">disable_adapter_layers</span><span class="p">()</span></code>

<a href="#mindnlp.peft.tuners.lora.model.LoraModel.disable_adapter_layers" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Disable all adapters.</p>
<p>When disabling all adapters, the model output corresponds to the output of the base model.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\peft\tuners\lora\model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">disable_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Disable all adapters.</span>

<span class="sd">    When disabling all adapters, the model output corresponds to the output of the base model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">active_adapter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapters</span><span class="p">:</span>
        <span class="n">val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">active_adapter</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">if</span> <span class="n">val</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Careful, disabling adapter layers with bias configured to be &#39;</span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2">&#39; does not produce the same &quot;</span>
                <span class="s2">&quot;output as the the base model would without adaption.&quot;</span>
            <span class="p">)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_set_adapter_layers</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.peft.tuners.lora.model.LoraModel.enable_adapter_layers" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">peft</span><span class="o">.</span><span class="n">tuners</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">LoraModel</span><span class="o">.</span><span class="n">enable_adapter_layers</span><span class="p">()</span></code>

<a href="#mindnlp.peft.tuners.lora.model.LoraModel.enable_adapter_layers" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Enable all adapters.</p>
<p>Call this if you have previously disabled all adapters and want to re-enable them.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\peft\tuners\lora\model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">enable_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Enable all adapters.</span>

<span class="sd">    Call this if you have previously disabled all adapters and want to re-enable them.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_set_adapter_layers</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.peft.tuners.lora.model.LoraModel.get_peft_config_as_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">peft</span><span class="o">.</span><span class="n">tuners</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">LoraModel</span><span class="o">.</span><span class="n">get_peft_config_as_dict</span><span class="p">(</span><span class="n">inference</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.peft.tuners.lora.model.LoraModel.get_peft_config_as_dict" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns a dictionary representation of the PEFT config.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the LoraModel class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inference</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A flag indicating whether the method is called for inference. Default is False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>dict</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A dictionary containing the PEFT config. The keys represent the configuration options, and the values
  represent their corresponding values. If 'inference' is True, the dictionary will also include the
  'inference_mode' key set to True.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>The method uses the 'peft_config' attribute of the LoraModel instance to create the dictionary.</li>
<li>If a value in the 'peft_config' attribute is an instance of Enum, its value will be extracted using the
  'value' attribute.</li>
<li>The 'config_dict' dictionary will only contain one key-value pair. If the 'inference' flag is True, the
  'config_dict' will be updated to include the 'inference_mode' key.</li>
</ul>
</details>

<details class="example-usage" open>
  <summary>Example usage</summary>
  <p>model = LoraModel()
config = model.get_peft_config_as_dict(inference=True)
print(config)  # {'inference_mode': True}</p>
<p>config = model.get_peft_config_as_dict()
print(config)  # {}</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\peft\tuners\lora\model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_peft_config_as_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a dictionary representation of the PEFT config.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: An instance of the LoraModel class.</span>
<span class="sd">        inference (bool): A flag indicating whether the method is called for inference. Default is False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary containing the PEFT config. The keys represent the configuration options, and the values</span>
<span class="sd">              represent their corresponding values. If &#39;inference&#39; is True, the dictionary will also include the</span>
<span class="sd">              &#39;inference_mode&#39; key set to True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>

<span class="sd">    Note:</span>
<span class="sd">        - The method uses the &#39;peft_config&#39; attribute of the LoraModel instance to create the dictionary.</span>
<span class="sd">        - If a value in the &#39;peft_config&#39; attribute is an instance of Enum, its value will be extracted using the</span>
<span class="sd">          &#39;value&#39; attribute.</span>
<span class="sd">        - The &#39;config_dict&#39; dictionary will only contain one key-value pair. If the &#39;inference&#39; flag is True, the</span>
<span class="sd">          &#39;config_dict&#39; will be updated to include the &#39;inference_mode&#39; key.</span>

<span class="sd">    Example usage:</span>
<span class="sd">        model = LoraModel()</span>
<span class="sd">        config = model.get_peft_config_as_dict(inference=True)</span>
<span class="sd">        print(config)  # {&#39;inference_mode&#39;: True}</span>

<span class="sd">        config = model.get_peft_config_as_dict()</span>
<span class="sd">        print(config)  # {}</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">Enum</span><span class="p">)</span> <span class="k">else</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">asdict</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">if</span> <span class="n">inference</span><span class="p">:</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;inference_mode&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">config_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span> <span class="c1"># pylint: disable=undefined-loop-variable</span>
    <span class="k">return</span> <span class="n">config</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.peft.tuners.lora.model.LoraModel.merge_and_unload" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">peft</span><span class="o">.</span><span class="n">tuners</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">LoraModel</span><span class="o">.</span><span class="n">merge_and_unload</span><span class="p">(</span><span class="n">progressbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">safe_merge</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.peft.tuners.lora.model.LoraModel.merge_and_unload" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model
as a standalone model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>progressbar</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>whether to show a progressbar indicating the unload and merge process</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_merge</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>whether to activate the safe merging check to check if there is any potential Nan in the adapter
weights</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The list of adapter names that should be merged. If None, all active adapters will be merged. Defaults
to <code>None</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftModel</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;tiiuae/falcon-40b&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">peft_model_id</span> <span class="o">=</span> <span class="s2">&quot;smangrul/falcon-40B-int4-peft-lora-sfttrainer-sample&quot;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">peft_model_id</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">merged_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">merge_and_unload</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindnlp\peft\tuners\lora\model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">merge_and_unload</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">progressbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">safe_merge</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model</span>
<span class="sd">    as a standalone model.</span>

<span class="sd">    Args:</span>
<span class="sd">        progressbar (`bool`):</span>
<span class="sd">            whether to show a progressbar indicating the unload and merge process</span>
<span class="sd">        safe_merge (`bool`):</span>
<span class="sd">            whether to activate the safe merging check to check if there is any potential Nan in the adapter</span>
<span class="sd">            weights</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            The list of adapter names that should be merged. If None, all active adapters will be merged. Defaults</span>
<span class="sd">            to `None`.</span>
<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    &gt;&gt;&gt; from transformers import AutoModelForCausalLM</span>
<span class="sd">    &gt;&gt;&gt; from peft import PeftModel</span>

<span class="sd">    &gt;&gt;&gt; base_model = AutoModelForCausalLM.from_pretrained(&quot;tiiuae/falcon-40b&quot;)</span>
<span class="sd">    &gt;&gt;&gt; peft_model_id = &quot;smangrul/falcon-40B-int4-peft-lora-sfttrainer-sample&quot;</span>
<span class="sd">    &gt;&gt;&gt; model = PeftModel.from_pretrained(base_model, peft_model_id)</span>
<span class="sd">    &gt;&gt;&gt; merged_model = model.merge_and_unload()</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unload_and_optionally_merge</span><span class="p">(</span>
        <span class="n">progressbar</span><span class="o">=</span><span class="n">progressbar</span><span class="p">,</span> <span class="n">safe_merge</span><span class="o">=</span><span class="n">safe_merge</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.peft.tuners.lora.model.LoraModel.set_adapter" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">peft</span><span class="o">.</span><span class="n">tuners</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">LoraModel</span><span class="o">.</span><span class="n">set_adapter</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span></code>

<a href="#mindnlp.peft.tuners.lora.model.LoraModel.set_adapter" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set the active adapter(s).</p>
<p>Additionally, this function will set the specified adapters to trainable (i.e., requires_grad=True). If this is
not desired, use the following code.</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model_peft</span><span class="o">.</span><span class="n">parameters_and_names</span><span class="p">():</span>
<span class="o">...</span>     <span class="k">if</span> <span class="o">...</span><span class="p">:</span>  <span class="c1"># some check on name (ex. if &#39;lora&#39; in name)</span>
<span class="o">...</span>         <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Name of the adapter(s) to be activated.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `list[str]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\peft\tuners\lora\model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Set the active adapter(s).</span>

<span class="sd">    Additionally, this function will set the specified adapters to trainable (i.e., requires_grad=True). If this is</span>
<span class="sd">    not desired, use the following code.</span>

<span class="sd">    ```py</span>
<span class="sd">    &gt;&gt;&gt; for name, param in model_peft.parameters_and_names():</span>
<span class="sd">    ...     if ...:  # some check on name (ex. if &#39;lora&#39; in name)</span>
<span class="sd">    ...         param.requires_grad = False</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">        adapter_name (`str` or `list[str]`): Name of the adapter(s) to be activated.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">cell</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cells</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">cell</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Adapter cannot be set when the model is merged. Unmerging the model first.&quot;</span><span class="p">)</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">set_adapter</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">=</span> <span class="n">adapter_name</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.peft.tuners.lora.model.LoraModel.unload" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">peft</span><span class="o">.</span><span class="n">tuners</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">LoraModel</span><span class="o">.</span><span class="n">unload</span><span class="p">()</span></code>

<a href="#mindnlp.peft.tuners.lora.model.LoraModel.unload" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Gets back the base model by removing all the lora cells without merging. This gives back the original base
model.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\peft\tuners\lora\model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">unload</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets back the base model by removing all the lora cells without merging. This gives back the original base</span>
<span class="sd">    model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unload_and_optionally_merge</span><span class="p">(</span><span class="n">merge</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../lokr/" class="md-footer__link md-footer__link--prev" aria-label="上一页: LoKr">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                LoKr
              </div>
            </div>
          </a>
        
        
          
          <a href="../prompt_tuning/" class="md-footer__link md-footer__link--next" aria-label="下一页: Prompt tuning">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                Prompt tuning
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 MindSpore Lab and CQU NLP Team.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:lvyufeng@cqu.edu.cn" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindnlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/lu-yu-feng-46-1" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>